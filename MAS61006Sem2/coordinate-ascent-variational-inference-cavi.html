<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 9 Coordinate ascent variational inference (CAVI) | MAS61006 Bayesian Statistics and Computational Methods</title>
  <meta name="description" content="Chapter 9 Coordinate ascent variational inference (CAVI) | MAS61006 Bayesian Statistics and Computational Methods" />
  <meta name="generator" content="bookdown 0.27 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 9 Coordinate ascent variational inference (CAVI) | MAS61006 Bayesian Statistics and Computational Methods" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 9 Coordinate ascent variational inference (CAVI) | MAS61006 Bayesian Statistics and Computational Methods" />
  
  
  

<meta name="author" content="Jeremy Oakley" />


<meta name="date" content="2023-03-04" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="variational-inference.html"/>
<link rel="next" href="references.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script>
  $(document).ready(function () {
    process_solutions();
  });
function process_solutions() {
  $("div.section[id^='solution']").each(function(i) {
    var soln_wrapper_id = "cvxr_ex_" + i;
    var solution_id = $(this).attr('id');
    var button = $("<button onclick=\"toggle_solution('" + soln_wrapper_id + "')\">Show/Hide</button>");
    var new_div = $("<div id='" + soln_wrapper_id + "' class='solution' style='display: none;'></div>");
    var h = $(this).first();
    var others = $(this).children().slice(1);
    $(others).each(function() {
      $(this).appendTo($(new_div));
    });
    $(button).insertAfter($(h));
    $(new_div).insertAfter($(button));
  })
}
function toggle_solution(el_id) {
  $("#" + el_id).toggle();
}
</script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">MAS61006(Sem2)</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Acknowledgement</a></li>
<li class="chapter" data-level="" data-path="literature.html"><a href="literature.html"><i class="fa fa-check"></i>Literature</a></li>
<li class="part"><span><b>I Inference using advanced samplers</b></span></li>
<li class="chapter" data-level="1" data-path="mcmc-sampling-recap.html"><a href="mcmc-sampling-recap.html"><i class="fa fa-check"></i><b>1</b> MCMC Sampling Recap</a>
<ul>
<li class="chapter" data-level="1.1" data-path="mcmc-sampling-recap.html"><a href="mcmc-sampling-recap.html#bayesian-inference-continued"><i class="fa fa-check"></i><b>1.1</b> Bayesian inference continued</a></li>
<li class="chapter" data-level="1.2" data-path="mcmc-sampling-recap.html"><a href="mcmc-sampling-recap.html#monte-carlo"><i class="fa fa-check"></i><b>1.2</b> Monte Carlo estimation</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="mcmc-sampling-recap.html"><a href="mcmc-sampling-recap.html#properties-of-the-monte-carlo-estimate"><i class="fa fa-check"></i><b>1.2.1</b> Properties of the Monte Carlo estimate</a></li>
<li class="chapter" data-level="1.2.2" data-path="mcmc-sampling-recap.html"><a href="mcmc-sampling-recap.html#expressing-quantities-as-expectations"><i class="fa fa-check"></i><b>1.2.2</b> Expressing quantities as expectations</a></li>
<li class="chapter" data-level="1.2.3" data-path="mcmc-sampling-recap.html"><a href="mcmc-sampling-recap.html#estimation-of-general-integrals"><i class="fa fa-check"></i><b>1.2.3</b> Estimation of general integrals</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="mcmc-sampling-recap.html"><a href="mcmc-sampling-recap.html#markov-chain-monte-carlo-mcmc"><i class="fa fa-check"></i><b>1.3</b> Markov chain Monte Carlo (MCMC)</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="mcmc-sampling-recap.html"><a href="mcmc-sampling-recap.html#mhsection"><i class="fa fa-check"></i><b>1.3.1</b> The Metroplis-Hastings (MH) algorithm</a></li>
<li class="chapter" data-level="1.3.2" data-path="mcmc-sampling-recap.html"><a href="mcmc-sampling-recap.html#the-problem-with-random-walk-proposals"><i class="fa fa-check"></i><b>1.3.2</b> The problem with random walk proposals</a></li>
<li class="chapter" data-level="1.3.3" data-path="mcmc-sampling-recap.html"><a href="mcmc-sampling-recap.html#the-gibbs-algorithm"><i class="fa fa-check"></i><b>1.3.3</b> The Gibbs algorithm</a></li>
<li class="chapter" data-level="1.3.4" data-path="mcmc-sampling-recap.html"><a href="mcmc-sampling-recap.html#the-problem-with-gibbs"><i class="fa fa-check"></i><b>1.3.4</b> The problem with Gibbs</a></li>
<li class="chapter" data-level="1.3.5" data-path="mcmc-sampling-recap.html"><a href="mcmc-sampling-recap.html#a-solution"><i class="fa fa-check"></i><b>1.3.5</b> A solution?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="hamiltonian-monte-carlo-hmc.html"><a href="hamiltonian-monte-carlo-hmc.html"><i class="fa fa-check"></i><b>2</b> Hamiltonian Monte Carlo (HMC)</a>
<ul>
<li class="chapter" data-level="2.1" data-path="hamiltonian-monte-carlo-hmc.html"><a href="hamiltonian-monte-carlo-hmc.html#generating-proposals-intuition"><i class="fa fa-check"></i><b>2.1</b> Generating proposals: intuition</a></li>
<li class="chapter" data-level="2.2" data-path="hamiltonian-monte-carlo-hmc.html"><a href="hamiltonian-monte-carlo-hmc.html#hamiltonian-dynamics"><i class="fa fa-check"></i><b>2.2</b> Hamiltonian dynamics</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="hamiltonian-monte-carlo-hmc.html"><a href="hamiltonian-monte-carlo-hmc.html#conservation-of-energy"><i class="fa fa-check"></i><b>2.2.1</b> Conservation of energy</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="hamiltonian-monte-carlo-hmc.html"><a href="hamiltonian-monte-carlo-hmc.html#using-hamiltons-equations-to-generate-proposals"><i class="fa fa-check"></i><b>2.3</b> Using Hamilton’s equations to generate proposals</a></li>
<li class="chapter" data-level="2.4" data-path="hamiltonian-monte-carlo-hmc.html"><a href="hamiltonian-monte-carlo-hmc.html#the-joint-distribution-for-theta-m"><i class="fa fa-check"></i><b>2.4</b> The joint distribution for <span class="math inline">\((\theta, m)\)</span></a></li>
<li class="chapter" data-level="2.5" data-path="hamiltonian-monte-carlo-hmc.html"><a href="hamiltonian-monte-carlo-hmc.html#the-hmc-algorithm"><i class="fa fa-check"></i><b>2.5</b> The HMC algorithm</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="hamiltonian-monte-carlo-hmc.html"><a href="hamiltonian-monte-carlo-hmc.html#approximate-solution-of-hamiltons-equations"><i class="fa fa-check"></i><b>2.5.1</b> Approximate solution of Hamilton’s equations</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="hamiltonian-monte-carlo-hmc.html"><a href="hamiltonian-monte-carlo-hmc.html#multivariate-theta"><i class="fa fa-check"></i><b>2.6</b> Multivariate <span class="math inline">\(\theta\)</span></a></li>
<li class="chapter" data-level="2.7" data-path="hamiltonian-monte-carlo-hmc.html"><a href="hamiltonian-monte-carlo-hmc.html#tuning-parameters"><i class="fa fa-check"></i><b>2.7</b> Tuning parameters</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="hamiltonian-monte-carlo-hmc.html"><a href="hamiltonian-monte-carlo-hmc.html#the-tuning-parameter-sigma2"><i class="fa fa-check"></i><b>2.7.1</b> The tuning parameter <span class="math inline">\(\sigma^2\)</span></a></li>
<li class="chapter" data-level="2.7.2" data-path="hamiltonian-monte-carlo-hmc.html"><a href="hamiltonian-monte-carlo-hmc.html#the-hamiltonian-movement-tuning-parameters-t-and-epsilon"><i class="fa fa-check"></i><b>2.7.2</b> The Hamiltonian movement tuning parameters <span class="math inline">\(T\)</span> and <span class="math inline">\(\epsilon\)</span></a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="hamiltonian-monte-carlo-hmc.html"><a href="hamiltonian-monte-carlo-hmc.html#implementing-hmc-by-hand"><i class="fa fa-check"></i><b>2.8</b> Implementing HMC ‘by hand’</a>
<ul>
<li class="chapter" data-level="2.8.1" data-path="hamiltonian-monte-carlo-hmc.html"><a href="hamiltonian-monte-carlo-hmc.html#random-walk-mh"><i class="fa fa-check"></i><b>2.8.1</b> Random-walk MH</a></li>
<li class="chapter" data-level="2.8.2" data-path="hamiltonian-monte-carlo-hmc.html"><a href="hamiltonian-monte-carlo-hmc.html#gibbs"><i class="fa fa-check"></i><b>2.8.2</b> Gibbs</a></li>
<li class="chapter" data-level="2.8.3" data-path="hamiltonian-monte-carlo-hmc.html"><a href="hamiltonian-monte-carlo-hmc.html#hmc"><i class="fa fa-check"></i><b>2.8.3</b> HMC</a></li>
<li class="chapter" data-level="2.8.4" data-path="hamiltonian-monte-carlo-hmc.html"><a href="hamiltonian-monte-carlo-hmc.html#comparing-random-walk-and-hmc-samplers"><i class="fa fa-check"></i><b>2.8.4</b> Comparing random-walk and HMC samplers</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="hamiltonian-monte-carlo-hmc.html"><a href="hamiltonian-monte-carlo-hmc.html#summary"><i class="fa fa-check"></i><b>2.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="implementing-hmc-in-stan.html"><a href="implementing-hmc-in-stan.html"><i class="fa fa-check"></i><b>3</b> Implementing HMC in Stan</a>
<ul>
<li class="chapter" data-level="3.1" data-path="implementing-hmc-in-stan.html"><a href="implementing-hmc-in-stan.html#getting-set-up-with-stan"><i class="fa fa-check"></i><b>3.1</b> Getting set up with Stan</a></li>
<li class="chapter" data-level="3.2" data-path="implementing-hmc-in-stan.html"><a href="implementing-hmc-in-stan.html#rstan-options"><i class="fa fa-check"></i><b>3.2</b> <code>rstan</code> options</a></li>
<li class="chapter" data-level="3.3" data-path="implementing-hmc-in-stan.html"><a href="implementing-hmc-in-stan.html#an-example-model"><i class="fa fa-check"></i><b>3.3</b> An example model</a></li>
<li class="chapter" data-level="3.4" data-path="implementing-hmc-in-stan.html"><a href="implementing-hmc-in-stan.html#specifying-a-model-in-rstudio"><i class="fa fa-check"></i><b>3.4</b> Specifying a model in RStudio</a></li>
<li class="chapter" data-level="3.5" data-path="implementing-hmc-in-stan.html"><a href="implementing-hmc-in-stan.html#stan-code-blocks"><i class="fa fa-check"></i><b>3.5</b> Stan code blocks</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="implementing-hmc-in-stan.html"><a href="implementing-hmc-in-stan.html#data-block"><i class="fa fa-check"></i><b>3.5.1</b> <code>data</code> block</a></li>
<li class="chapter" data-level="3.5.2" data-path="implementing-hmc-in-stan.html"><a href="implementing-hmc-in-stan.html#parameters-block"><i class="fa fa-check"></i><b>3.5.2</b> <code>parameters</code> block</a></li>
<li class="chapter" data-level="3.5.3" data-path="implementing-hmc-in-stan.html"><a href="implementing-hmc-in-stan.html#model-block"><i class="fa fa-check"></i><b>3.5.3</b> <code>model</code> block</a></li>
<li class="chapter" data-level="3.5.4" data-path="implementing-hmc-in-stan.html"><a href="implementing-hmc-in-stan.html#generated-quantities-block"><i class="fa fa-check"></i><b>3.5.4</b> <code>generated quantities</code> block</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="implementing-hmc-in-stan.html"><a href="implementing-hmc-in-stan.html#running-the-hmc-algorithm"><i class="fa fa-check"></i><b>3.6</b> Running the HMC algorithm</a></li>
<li class="chapter" data-level="3.7" data-path="implementing-hmc-in-stan.html"><a href="implementing-hmc-in-stan.html#extracting-and-analysing-the-samples"><i class="fa fa-check"></i><b>3.7</b> Extracting and analysing the samples</a>
<ul>
<li class="chapter" data-level="3.7.1" data-path="implementing-hmc-in-stan.html"><a href="implementing-hmc-in-stan.html#r-packages-for-plotting-outputs"><i class="fa fa-check"></i><b>3.7.1</b> R packages for plotting outputs</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="implementing-hmc-in-stan.html"><a href="implementing-hmc-in-stan.html#no-u-turn-sampler-nuts"><i class="fa fa-check"></i><b>3.8</b> No U-Turn Sampler (NUTS)</a></li>
<li class="chapter" data-level="3.9" data-path="implementing-hmc-in-stan.html"><a href="implementing-hmc-in-stan.html#further-reading"><i class="fa fa-check"></i><b>3.9</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="bayesian-regression-in-stan.html"><a href="bayesian-regression-in-stan.html"><i class="fa fa-check"></i><b>4</b> Bayesian regression in Stan</a>
<ul>
<li class="chapter" data-level="4.1" data-path="bayesian-regression-in-stan.html"><a href="bayesian-regression-in-stan.html#simple-linear-regression"><i class="fa fa-check"></i><b>4.1</b> Simple linear regression</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="bayesian-regression-in-stan.html"><a href="bayesian-regression-in-stan.html#holiday-hangover-cures"><i class="fa fa-check"></i><b>4.1.1</b> Holiday hangover cures</a></li>
<li class="chapter" data-level="4.1.2" data-path="bayesian-regression-in-stan.html"><a href="bayesian-regression-in-stan.html#least-squares-fit"><i class="fa fa-check"></i><b>4.1.2</b> Least squares fit</a></li>
<li class="chapter" data-level="4.1.3" data-path="bayesian-regression-in-stan.html"><a href="bayesian-regression-in-stan.html#bayesian-approach-in-stan"><i class="fa fa-check"></i><b>4.1.3</b> Bayesian approach in Stan</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="bayesian-regression-in-stan.html"><a href="bayesian-regression-in-stan.html#recap-of-logistic-regression"><i class="fa fa-check"></i><b>4.2</b> Recap of logistic regression</a></li>
<li class="chapter" data-level="4.3" data-path="bayesian-regression-in-stan.html"><a href="bayesian-regression-in-stan.html#overview-of-mixed-effects"><i class="fa fa-check"></i><b>4.3</b> Overview of mixed effects</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="bayesian-regression-in-stan.html"><a href="bayesian-regression-in-stan.html#simple-example-of-a-mixed-effects-model"><i class="fa fa-check"></i><b>4.3.1</b> Simple example of a mixed effects model</a></li>
<li class="chapter" data-level="4.3.2" data-path="bayesian-regression-in-stan.html"><a href="bayesian-regression-in-stan.html#fixed-or-random-effects"><i class="fa fa-check"></i><b>4.3.2</b> Fixed or random effects?</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="bayesian-regression-in-stan.html"><a href="bayesian-regression-in-stan.html#mixed-effect-logistic-regression"><i class="fa fa-check"></i><b>4.4</b> Mixed effect logistic regression</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="bayesian-regression-in-stan.html"><a href="bayesian-regression-in-stan.html#balance-experiment"><i class="fa fa-check"></i><b>4.4.1</b> Balance experiment</a></li>
<li class="chapter" data-level="4.4.2" data-path="bayesian-regression-in-stan.html"><a href="bayesian-regression-in-stan.html#exploratory-data-analysis"><i class="fa fa-check"></i><b>4.4.2</b> Exploratory data analysis</a></li>
<li class="chapter" data-level="4.4.3" data-path="bayesian-regression-in-stan.html"><a href="bayesian-regression-in-stan.html#maximum-likelihood-approach"><i class="fa fa-check"></i><b>4.4.3</b> Maximum likelihood approach</a></li>
<li class="chapter" data-level="4.4.4" data-path="bayesian-regression-in-stan.html"><a href="bayesian-regression-in-stan.html#bayesian-approach-using-stan"><i class="fa fa-check"></i><b>4.4.4</b> Bayesian approach using Stan</a></li>
<li class="chapter" data-level="4.4.5" data-path="bayesian-regression-in-stan.html"><a href="bayesian-regression-in-stan.html#comparing-stan-and-lme4"><i class="fa fa-check"></i><b>4.4.5</b> Comparing Stan and lme4</a></li>
<li class="chapter" data-level="4.4.6" data-path="bayesian-regression-in-stan.html"><a href="bayesian-regression-in-stan.html#easy-bayesian-approach-with-brms"><i class="fa fa-check"></i><b>4.4.6</b> ‘Easy’ Bayesian approach with <code>brms</code></a></li>
<li class="chapter" data-level="4.4.7" data-path="bayesian-regression-in-stan.html"><a href="bayesian-regression-in-stan.html#what-have-we-found"><i class="fa fa-check"></i><b>4.4.7</b> What have we found?</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Inference with missing data</b></span></li>
<li class="chapter" data-level="5" data-path="multiple-imputation-for-missing-data.html"><a href="multiple-imputation-for-missing-data.html"><i class="fa fa-check"></i><b>5</b> Multiple imputation for missing data</a>
<ul>
<li class="chapter" data-level="5.1" data-path="multiple-imputation-for-missing-data.html"><a href="multiple-imputation-for-missing-data.html#introduction"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="multiple-imputation-for-missing-data.html"><a href="multiple-imputation-for-missing-data.html#example-nhanes-data"><i class="fa fa-check"></i><b>5.2</b> Example: <code>nhanes</code> data</a></li>
<li class="chapter" data-level="5.3" data-path="multiple-imputation-for-missing-data.html"><a href="multiple-imputation-for-missing-data.html#mechanisms-of-missingness"><i class="fa fa-check"></i><b>5.3</b> Mechanisms of missingness</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="multiple-imputation-for-missing-data.html"><a href="multiple-imputation-for-missing-data.html#missing-completely-at-random-mcar"><i class="fa fa-check"></i><b>5.3.1</b> Missing completely at random (MCAR)</a></li>
<li class="chapter" data-level="5.3.2" data-path="multiple-imputation-for-missing-data.html"><a href="multiple-imputation-for-missing-data.html#missing-at-random-mar"><i class="fa fa-check"></i><b>5.3.2</b> Missing at random (MAR)</a></li>
<li class="chapter" data-level="5.3.3" data-path="multiple-imputation-for-missing-data.html"><a href="multiple-imputation-for-missing-data.html#not-missing-at-random-nmar"><i class="fa fa-check"></i><b>5.3.3</b> Not missing at random (NMAR)</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="multiple-imputation-for-missing-data.html"><a href="multiple-imputation-for-missing-data.html#ignoring-information-about-missingness"><i class="fa fa-check"></i><b>5.4</b> Ignoring information about missingness</a></li>
<li class="chapter" data-level="5.5" data-path="multiple-imputation-for-missing-data.html"><a href="multiple-imputation-for-missing-data.html#inference-via-imputation"><i class="fa fa-check"></i><b>5.5</b> Inference via imputation</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="multiple-imputation-for-missing-data.html"><a href="multiple-imputation-for-missing-data.html#further-simplifications"><i class="fa fa-check"></i><b>5.5.1</b> Further simplifications</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="multiple-imputation-for-missing-data.html"><a href="multiple-imputation-for-missing-data.html#pooling"><i class="fa fa-check"></i><b>5.6</b> Pooling multiple imputations</a></li>
<li class="chapter" data-level="5.7" data-path="multiple-imputation-for-missing-data.html"><a href="multiple-imputation-for-missing-data.html#simple-example"><i class="fa fa-check"></i><b>5.7</b> Simple example</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="multiple-imputation-for-missing-data.html"><a href="multiple-imputation-for-missing-data.html#multiple-imputation-by-hand"><i class="fa fa-check"></i><b>5.7.1</b> Multiple imputation by hand</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="multiple-imputation-for-missing-data.html"><a href="multiple-imputation-for-missing-data.html#imputing-missing-data-chained-equation-multiple-imputation"><i class="fa fa-check"></i><b>5.8</b> Imputing missing data: chained equation multiple imputation</a>
<ul>
<li class="chapter" data-level="5.8.1" data-path="multiple-imputation-for-missing-data.html"><a href="multiple-imputation-for-missing-data.html#how-many-iterations"><i class="fa fa-check"></i><b>5.8.1</b> How many iterations?</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="multiple-imputation-for-missing-data.html"><a href="multiple-imputation-for-missing-data.html#mice-example-the-nhanes-dataset"><i class="fa fa-check"></i><b>5.9</b> MICE example: the <code>nhanes</code> dataset</a>
<ul>
<li class="chapter" data-level="5.9.1" data-path="multiple-imputation-for-missing-data.html"><a href="multiple-imputation-for-missing-data.html#complete-case-analysis"><i class="fa fa-check"></i><b>5.9.1</b> Complete case analysis</a></li>
<li class="chapter" data-level="5.9.2" data-path="multiple-imputation-for-missing-data.html"><a href="multiple-imputation-for-missing-data.html#imputing-the-missing-data"><i class="fa fa-check"></i><b>5.9.2</b> Imputing the missing data</a></li>
<li class="chapter" data-level="5.9.3" data-path="multiple-imputation-for-missing-data.html"><a href="multiple-imputation-for-missing-data.html#imputation-analysis"><i class="fa fa-check"></i><b>5.9.3</b> Imputation analysis</a></li>
<li class="chapter" data-level="5.9.4" data-path="multiple-imputation-for-missing-data.html"><a href="multiple-imputation-for-missing-data.html#analysis-pooling"><i class="fa fa-check"></i><b>5.9.4</b> Analysis pooling</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Resampling methods</b></span></li>
<li class="chapter" data-level="6" data-path="bootstrapping.html"><a href="bootstrapping.html"><i class="fa fa-check"></i><b>6</b> Bootstrapping</a>
<ul>
<li class="chapter" data-level="6.1" data-path="bootstrapping.html"><a href="bootstrapping.html#bootstrap-estimates-of-standard-errors"><i class="fa fa-check"></i><b>6.1</b> Bootstrap estimates of standard errors</a></li>
<li class="chapter" data-level="6.2" data-path="bootstrapping.html"><a href="bootstrapping.html#estimating-a-distribution-using-the-empirical-cumulative-distribution-function"><i class="fa fa-check"></i><b>6.2</b> Estimating a distribution using the empirical cumulative distribution function</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="bootstrapping.html"><a href="bootstrapping.html#sampling-from-an-ecdf"><i class="fa fa-check"></i><b>6.2.1</b> Sampling from an ECDF</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="bootstrapping.html"><a href="bootstrapping.html#notation-summary"><i class="fa fa-check"></i><b>6.3</b> Notation summary</a></li>
<li class="chapter" data-level="6.4" data-path="bootstrapping.html"><a href="bootstrapping.html#example-bootstrap-standard-errors-of-a-sample-mean-and-sample-variance"><i class="fa fa-check"></i><b>6.4</b> Example: Bootstrap standard errors of a sample mean and sample variance</a></li>
<li class="chapter" data-level="6.5" data-path="bootstrapping.html"><a href="bootstrapping.html#confidence-intervals"><i class="fa fa-check"></i><b>6.5</b> Confidence intervals</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="bootstrapping.html"><a href="bootstrapping.html#confidence-intervals-using-the-estimated-standard-error"><i class="fa fa-check"></i><b>6.5.1</b> Confidence intervals using the estimated standard error</a></li>
<li class="chapter" data-level="6.5.2" data-path="bootstrapping.html"><a href="bootstrapping.html#confidence-intervals-using-percentiles"><i class="fa fa-check"></i><b>6.5.2</b> Confidence intervals using percentiles</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="bootstrapping.html"><a href="bootstrapping.html#properties-of-samples-from-the-empirical-cdf"><i class="fa fa-check"></i><b>6.6</b> Properties of samples from the empirical CDF</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="bootstrapping.html"><a href="bootstrapping.html#expectation-and-variance"><i class="fa fa-check"></i><b>6.6.1</b> Expectation and variance</a></li>
<li class="chapter" data-level="6.6.2" data-path="bootstrapping.html"><a href="bootstrapping.html#sample-percentiles"><i class="fa fa-check"></i><b>6.6.2</b> Sample percentiles</a></li>
<li class="chapter" data-level="6.6.3" data-path="bootstrapping.html"><a href="bootstrapping.html#sources-of-error-and-sample-sizes-in-bootstrapping"><i class="fa fa-check"></i><b>6.6.3</b> Sources of error and sample sizes in bootstrapping</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="bootstrapping.html"><a href="bootstrapping.html#example-measuring-observer-agreement"><i class="fa fa-check"></i><b>6.7</b> Example: Measuring observer agreement</a>
<ul>
<li class="chapter" data-level="6.7.1" data-path="bootstrapping.html"><a href="bootstrapping.html#the-data"><i class="fa fa-check"></i><b>6.7.1</b> The data</a></li>
<li class="chapter" data-level="6.7.2" data-path="bootstrapping.html"><a href="bootstrapping.html#the-kappa-statistic"><i class="fa fa-check"></i><b>6.7.2</b> The kappa statistic</a></li>
<li class="chapter" data-level="6.7.3" data-path="bootstrapping.html"><a href="bootstrapping.html#bootstrapping-bivariate-data"><i class="fa fa-check"></i><b>6.7.3</b> Bootstrapping bivariate data</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="bootstrapping.html"><a href="bootstrapping.html#parametric-bootstrapping-and-hypothesis-testing"><i class="fa fa-check"></i><b>6.8</b> Parametric bootstrapping and hypothesis testing</a>
<ul>
<li class="chapter" data-level="6.8.1" data-path="bootstrapping.html"><a href="bootstrapping.html#the-milk-data-set"><i class="fa fa-check"></i><b>6.8.1</b> The <code>Milk</code> data set</a></li>
<li class="chapter" data-level="6.8.2" data-path="bootstrapping.html"><a href="bootstrapping.html#the-model-and-hypothesis"><i class="fa fa-check"></i><b>6.8.2</b> The model and hypothesis</a></li>
<li class="chapter" data-level="6.8.3" data-path="bootstrapping.html"><a href="bootstrapping.html#the-generalized-likelihood-ratio-test"><i class="fa fa-check"></i><b>6.8.3</b> The generalized likelihood ratio test</a></li>
<li class="chapter" data-level="6.8.4" data-path="bootstrapping.html"><a href="bootstrapping.html#the-parametric-bootstrap-test"><i class="fa fa-check"></i><b>6.8.4</b> The parametric bootstrap test</a></li>
<li class="chapter" data-level="6.8.5" data-path="bootstrapping.html"><a href="bootstrapping.html#implementation-with-r"><i class="fa fa-check"></i><b>6.8.5</b> Implementation with R</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="cross-validation.html"><a href="cross-validation.html"><i class="fa fa-check"></i><b>7</b> Cross-validation</a>
<ul>
<li class="chapter" data-level="7.1" data-path="cross-validation.html"><a href="cross-validation.html#cross-validation-in-classification"><i class="fa fa-check"></i><b>7.1</b> Cross-validation in classification</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="cross-validation.html"><a href="cross-validation.html#the-palmer-penguins-data"><i class="fa fa-check"></i><b>7.1.1</b> The Palmer penguins data</a></li>
<li class="chapter" data-level="7.1.2" data-path="cross-validation.html"><a href="cross-validation.html#the-k-nearest-neighbours-algorithm-knn"><i class="fa fa-check"></i><b>7.1.2</b> The <span class="math inline">\(K\)</span>-nearest neighbours algorithm (KNN)</a></li>
<li class="chapter" data-level="7.1.3" data-path="cross-validation.html"><a href="cross-validation.html#implementing-cross-validation-in-knn"><i class="fa fa-check"></i><b>7.1.3</b> Implementing cross-validation in KNN</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="cross-validation.html"><a href="cross-validation.html#cross-validation-in-regression"><i class="fa fa-check"></i><b>7.2</b> Cross-validation in regression</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="cross-validation.html"><a href="cross-validation.html#the-flint-tools-data"><i class="fa fa-check"></i><b>7.2.1</b> The flint tools data</a></li>
<li class="chapter" data-level="7.2.2" data-path="cross-validation.html"><a href="cross-validation.html#implementing-cross-validation-in-regression"><i class="fa fa-check"></i><b>7.2.2</b> Implementing cross-validation in regression</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="cross-validation.html"><a href="cross-validation.html#parameter-estimation-with-cross-validation"><i class="fa fa-check"></i><b>7.3</b> Parameter estimation with cross-validation</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="cross-validation.html"><a href="cross-validation.html#example-choosing-the-value-k-in-knn"><i class="fa fa-check"></i><b>7.3.1</b> Example: choosing the value <span class="math inline">\(K\)</span> in KNN</a></li>
<li class="chapter" data-level="7.3.2" data-path="cross-validation.html"><a href="cross-validation.html#cross-validation-as-an-alternative-to-maximum-likelihood"><i class="fa fa-check"></i><b>7.3.2</b> Cross-validation as an alternative to maximum likelihood</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="cross-validation.html"><a href="cross-validation.html#computational-short-cuts"><i class="fa fa-check"></i><b>7.4</b> Computational short-cuts</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="cross-validation.html"><a href="cross-validation.html#example-returning-to-the-flint-data"><i class="fa fa-check"></i><b>7.4.1</b> Example: Returning to the flint data</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="cross-validation.html"><a href="cross-validation.html#relationship-with-aic"><i class="fa fa-check"></i><b>7.5</b> Relationship with AIC</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="cross-validation.html"><a href="cross-validation.html#example-the-cars-data"><i class="fa fa-check"></i><b>7.5.1</b> Example: The cars data</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="cross-validation.html"><a href="cross-validation.html#non-examinable-proof-of-the-computational-short-cut"><i class="fa fa-check"></i><b>7.6</b> (Non-examinable) Proof of the computational short cut</a>
<ul>
<li class="chapter" data-level="7.6.1" data-path="cross-validation.html"><a href="cross-validation.html#helpful-results-we-will-use"><i class="fa fa-check"></i><b>7.6.1</b> Helpful results we will use</a></li>
<li class="chapter" data-level="7.6.2" data-path="cross-validation.html"><a href="cross-validation.html#relate-the-estimated-coefficients-of-full-and-reduced-models"><i class="fa fa-check"></i><b>7.6.2</b> Relate the estimated coefficients of full and reduced models</a></li>
<li class="chapter" data-level="7.6.3" data-path="cross-validation.html"><a href="cross-validation.html#relating-the-residuals"><i class="fa fa-check"></i><b>7.6.3</b> Relating the residuals</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IV Approximate inference without sampling</b></span></li>
<li class="chapter" data-level="8" data-path="variational-inference.html"><a href="variational-inference.html"><i class="fa fa-check"></i><b>8</b> Variational inference</a>
<ul>
<li class="chapter" data-level="8.1" data-path="variational-inference.html"><a href="variational-inference.html#background-theory"><i class="fa fa-check"></i><b>8.1</b> Background theory</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="variational-inference.html"><a href="variational-inference.html#jensens-inequality"><i class="fa fa-check"></i><b>8.1.1</b> Jensen’s inequality</a></li>
<li class="chapter" data-level="8.1.2" data-path="variational-inference.html"><a href="variational-inference.html#kullback-leibler-divergence"><i class="fa fa-check"></i><b>8.1.2</b> Kullback-Leibler Divergence</a></li>
<li class="chapter" data-level="8.1.3" data-path="variational-inference.html"><a href="variational-inference.html#optimisation-with-coordinate-ascent"><i class="fa fa-check"></i><b>8.1.3</b> Optimisation with coordinate ascent</a></li>
<li class="chapter" data-level="8.1.4" data-path="variational-inference.html"><a href="variational-inference.html#stochastic-optimisation"><i class="fa fa-check"></i><b>8.1.4</b> Stochastic optimisation</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="variational-inference.html"><a href="variational-inference.html#motivation-for-approximate-inference-approaches"><i class="fa fa-check"></i><b>8.2</b> Motivation for approximate inference approaches</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="variational-inference.html"><a href="variational-inference.html#intractable-integrals"><i class="fa fa-check"></i><b>8.2.1</b> Intractable integrals</a></li>
<li class="chapter" data-level="8.2.2" data-path="variational-inference.html"><a href="variational-inference.html#variational-approach-to-intractable-integrals"><i class="fa fa-check"></i><b>8.2.2</b> Variational approach to intractable integrals</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="variational-inference.html"><a href="variational-inference.html#approximate-inference-as-an-optimisation-problem"><i class="fa fa-check"></i><b>8.3</b> Approximate inference as an optimisation problem</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="variational-inference.html"><a href="variational-inference.html#exploring-the-elbo"><i class="fa fa-check"></i><b>8.3.1</b> Exploring the ELBO</a></li>
<li class="chapter" data-level="8.3.2" data-path="variational-inference.html"><a href="variational-inference.html#forward-and-reverse-variational-inference"><i class="fa fa-check"></i><b>8.3.2</b> Forward and reverse variational inference</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="variational-inference.html"><a href="variational-inference.html#the-variational-family-of-distributions"><i class="fa fa-check"></i><b>8.4</b> The variational family of distributions</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="variational-inference.html"><a href="variational-inference.html#mean-field-family"><i class="fa fa-check"></i><b>8.4.1</b> Mean-field family</a></li>
<li class="chapter" data-level="8.4.2" data-path="variational-inference.html"><a href="variational-inference.html#correlation-cannot-be-replicated"><i class="fa fa-check"></i><b>8.4.2</b> Correlation cannot be replicated</a></li>
<li class="chapter" data-level="8.4.3" data-path="variational-inference.html"><a href="variational-inference.html#why-mean-field-is-useful"><i class="fa fa-check"></i><b>8.4.3</b> Why mean-field is useful</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="coordinate-ascent-variational-inference-cavi.html"><a href="coordinate-ascent-variational-inference-cavi.html"><i class="fa fa-check"></i><b>9</b> Coordinate ascent variational inference (CAVI)</a>
<ul>
<li class="chapter" data-level="9.1" data-path="coordinate-ascent-variational-inference-cavi.html"><a href="coordinate-ascent-variational-inference-cavi.html#the-cavi-algorithm"><i class="fa fa-check"></i><b>9.1</b> The CAVI algorithm</a></li>
<li class="chapter" data-level="9.2" data-path="coordinate-ascent-variational-inference-cavi.html"><a href="coordinate-ascent-variational-inference-cavi.html#example-mixture-of-gaussians"><i class="fa fa-check"></i><b>9.2</b> Example: Mixture of Gaussians</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="coordinate-ascent-variational-inference-cavi.html"><a href="coordinate-ascent-variational-inference-cavi.html#the-observation-model"><i class="fa fa-check"></i><b>9.2.1</b> The observation model</a></li>
<li class="chapter" data-level="9.2.2" data-path="coordinate-ascent-variational-inference-cavi.html"><a href="coordinate-ascent-variational-inference-cavi.html#the-prior"><i class="fa fa-check"></i><b>9.2.2</b> The prior</a></li>
<li class="chapter" data-level="9.2.3" data-path="coordinate-ascent-variational-inference-cavi.html"><a href="coordinate-ascent-variational-inference-cavi.html#the-joint-likelihood"><i class="fa fa-check"></i><b>9.2.3</b> The joint likelihood</a></li>
<li class="chapter" data-level="9.2.4" data-path="coordinate-ascent-variational-inference-cavi.html"><a href="coordinate-ascent-variational-inference-cavi.html#the-mean-field-family-approximation"><i class="fa fa-check"></i><b>9.2.4</b> The mean-field family approximation</a></li>
<li class="chapter" data-level="9.2.5" data-path="coordinate-ascent-variational-inference-cavi.html"><a href="coordinate-ascent-variational-inference-cavi.html#cavi"><i class="fa fa-check"></i><b>9.2.5</b> CAVI</a></li>
<li class="chapter" data-level="9.2.6" data-path="coordinate-ascent-variational-inference-cavi.html"><a href="coordinate-ascent-variational-inference-cavi.html#implementing-cavi-in-r"><i class="fa fa-check"></i><b>9.2.6</b> Implementing CAVI in R</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="coordinate-ascent-variational-inference-cavi.html"><a href="coordinate-ascent-variational-inference-cavi.html#comment"><i class="fa fa-check"></i><b>9.3</b> Comment</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">MAS61006 Bayesian Statistics and Computational Methods</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="coordinate-ascent-variational-inference-cavi" class="section level1 hasAnchor" number="9">
<h1><span class="header-section-number">Chapter 9</span> Coordinate ascent variational inference (CAVI)<a href="coordinate-ascent-variational-inference-cavi.html#coordinate-ascent-variational-inference-cavi" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<!--- For HTML Only --->
<p><span class="math inline">\(\def \mb{\mathbb}\)</span>
<span class="math inline">\(\def \E{\mb{E}}\)</span>
<span class="math inline">\(\def \P{\mb{P}}\)</span>
<span class="math inline">\(\DeclareMathOperator{\var}{Var}\)</span>
<span class="math inline">\(\DeclareMathOperator{\cov}{Cov}\)</span></p>
<table>
<colgroup>
<col width="100%" />
</colgroup>
<thead>
<tr class="header">
<th>Aims of this section</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1. Present one method for implementing variational inference: the CAVI algorithm.</td>
</tr>
<tr class="even">
<td>2. Demonstrate its application in a simple mixture modelling example.</td>
</tr>
</tbody>
</table>
<div id="the-cavi-algorithm" class="section level2 hasAnchor" number="9.1">
<h2><span class="header-section-number">9.1</span> The CAVI algorithm<a href="coordinate-ascent-variational-inference-cavi.html#the-cavi-algorithm" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Given a mean-field family of approximate distributions, we can use the CAVI algorithm to maximise the ELBO. For an <span class="math inline">\(m\)</span>-dimensional parameter <span class="math inline">\(\boldsymbol\theta\)</span>, the algorithm is as follows.</p>
<ol style="list-style-type: decimal">
<li>Choose some initial mean-field approximation
<span class="math display">\[q(\boldsymbol\theta)=\prod_{i=1}^m q_i(\theta_i).\]</span></li>
<li>Update each <span class="math inline">\(q_i\)</span> in turn for <span class="math inline">\(i=1,\ldots,m\)</span>. Holding <span class="math inline">\(q_j(\theta_j)\)</span> fixed for all <span class="math inline">\(j\neq i\)</span>, replace the current <span class="math inline">\(q_i\)</span> in the mean-field approximation with
<span class="math display" id="eq:cavi">\[\begin{equation}
q_i(\theta_i)^* \propto \exp \left\lbrace \E_{q_{-i}} \left[\log p\left( \theta_i , \boldsymbol\theta_{-i}, x \right)\right]\right\rbrace.
\tag{9.1}
\end{equation}\]</span></li>
<li>Repeat step 2 until the ELBO has converged.</li>
</ol>
<p>The update for <span class="math inline">\(q_i\)</span> in equation <a href="coordinate-ascent-variational-inference-cavi.html#eq:cavi">(9.1)</a> is optimal, in that it gives the greatest increase in the ELBO. To see this, we consider the ELBO as a function of <span class="math inline">\(q_i\)</span> only and write</p>
<p><span class="math display">\[
ELBO(q_i) = \E_{q_i}[\E_{q_{-i}} [\log p(\theta_i, \boldsymbol\theta_{-i},x)]] -  \E_{q_i}[\log q_i(\theta_i)] + K,
\]</span>
for some constant <span class="math inline">\(K\)</span>. This hold because of the independence in the mean-field approximation. Then, we note that</p>
<p><span class="math display">\[\begin{align}
-KL(q_i ||q^*_i) &amp;= -\E_{q_i}[\log q_i(\theta_i) - \log q^*_i(\theta_i)] \\
&amp;=-  \E_{q_i}[\log q_i(\theta_i)] +   \E_{q_i}[\E_{q_{-i}} [\log p(\theta_i, \boldsymbol\theta_{-i},x)]] + C,
\end{align}\]</span>
for some constant <span class="math inline">\(C\)</span>, so that <span class="math inline">\(ELBO(q_i)\)</span> is equal to <span class="math inline">\(-KL(q_i ||q^*_i)\)</span> plus some constant term. As <span class="math inline">\(KL(q_i ||q^*_i)\)</span> is minimised by setting <span class="math inline">\(q_i = q^*_i\)</span>, this must maximise the ELBO as a function of <span class="math inline">\(q_i\)</span>.</p>
<p>Note there is some similarity with Gibbs sampling here. In Gibbs sampling, we iterate through the conditional distributions, drawing samples at each time. In CAVI, we again iterate through the dimensions, but we are now <em>updating the distribution</em> at each iteration, rather than sampling.</p>
</div>
<div id="example-mixture-of-gaussians" class="section level2 hasAnchor" number="9.2">
<h2><span class="header-section-number">9.2</span> Example: Mixture of Gaussians<a href="coordinate-ascent-variational-inference-cavi.html#example-mixture-of-gaussians" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="the-observation-model" class="section level3 hasAnchor" number="9.2.1">
<h3><span class="header-section-number">9.2.1</span> The observation model<a href="coordinate-ascent-variational-inference-cavi.html#the-observation-model" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Consider <span class="math inline">\(K\)</span> mixtures of Gaussians, where each has a different mean <span class="math inline">\(\mu_i\)</span> but a common variance of 1. We have <span class="math inline">\(n\)</span> observations <span class="math inline">\(x_1,\ldots,x_n\)</span>, where each observation is distributed according to one of the <span class="math inline">\(K\)</span> mixture components, so that
<span class="math display">\[x_i \sim N(\mu_{c_i},1),\]</span>
where <span class="math inline">\(c_i\)</span> is the mixture assignment of observation <span class="math inline">\(i\)</span>. Note that we do not observe the <span class="math inline">\(c_i\)</span>, and <span class="math inline">\(c_i\in \lbrace 1,\ldots,K\rbrace\)</span>. This model therefore has the set of unknown parameters <span class="math inline">\(\boldsymbol{\theta}=\lbrace \boldsymbol\mu,\mathbf{c}\rbrace\)</span>: the <span class="math inline">\(K\)</span> class means and the <span class="math inline">\(n\)</span> class assignments. Note that we can also refer to the class assignments as <span class="math inline">\(\mathbf{C}_i\)</span>, associated with observation <span class="math inline">\(x_i\)</span>, which is a vector of length <span class="math inline">\(K\)</span>, where all entries are 0 except the <span class="math inline">\(c_i^\text{th}\)</span>, which is 1. This alternative treatment seems overly complex, but it will turn out to be useful for some algebra later.</p>
<p>Note that one might think of the class probabilities/proportions as being included in the uncertain quantities of interest: the uncertain population proportion belonging to class <span class="math inline">\(i\)</span>, for <span class="math inline">\(i=1,\ldots,K\)</span>. But this is <em>not</em> the focus here: our interest is in the uncertain class assignments <span class="math inline">\(c_i\)</span> for each
<em>observation</em>.</p>
</div>
<div id="the-prior" class="section level3 hasAnchor" number="9.2.2">
<h3><span class="header-section-number">9.2.2</span> The prior<a href="coordinate-ascent-variational-inference-cavi.html#the-prior" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We assume that there is a global distribution that these component means independently arise from, so that the prior for the class means is
<span class="math display">\[\mu_i \sim N(0, \sigma^2),\]</span>
for <span class="math inline">\(i=1,\ldots,K\)</span>. We will assume that we know the hyperparameter <span class="math inline">\(\sigma^2\)</span>. In the following we will keep this hyperparameter general, and remember that it is a known constant.</p>
<p>The class assignment <span class="math inline">\(c_i\)</span> of observation <span class="math inline">\(i\)</span> is a categorical variable, taking one of the values <span class="math inline">\(1,\ldots,K\)</span>. We assume a prior for this which is a categorical distribution with equal weightings, i.e. <span class="math inline">\((1/K,\ldots,1/K)\)</span>.</p>
</div>
<div id="the-joint-likelihood" class="section level3 hasAnchor" number="9.2.3">
<h3><span class="header-section-number">9.2.3</span> The joint likelihood<a href="coordinate-ascent-variational-inference-cavi.html#the-joint-likelihood" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>This is a classic example for variational inference because the expression for <span class="math inline">\(p(\mathbf{x})\)</span> is computationally burdensome, being exponential in the number of mixture components, <span class="math inline">\(K\)</span>. We can express the joint density as
<span class="math display">\[\log p(\boldsymbol{\theta},\mathbf{x})=\sum_{k=1}^K \log p(\mu_k)+\sum_{i=1}^n \left[\log p(c_i)+\log p(x_i|c_i,\mu_i)\right],\]</span>
where:</p>
<ul>
<li><span class="math inline">\(p(\mu_k)\)</span> is the Gaussian prior, where for each <span class="math inline">\(k\)</span> they are identically distributed, having mean 0 and known variance <span class="math inline">\(\sigma^2\)</span>. We can write <span class="math inline">\(\log p(\mu_k)\propto -\frac{\mu_k^2}{2\sigma^2}\)</span>.</li>
<li><span class="math inline">\(p(c_i)\)</span> is a categorical distribution describing the weightings/probability of each mixture component arising (all equal at <span class="math inline">\(1/K\)</span>). It is easiest here to work with the vector form of this variable, <span class="math inline">\(\mathbf{C}_i\)</span>. Using our prior of equal weightings, this means <span class="math inline">\(p(\mathbf{C}_i)=(\frac{1}{K},\ldots,\frac{1}{K})\)</span> for all <span class="math inline">\(i\)</span> and <span class="math inline">\(\log p(C_{ik})=-\log K\)</span>. This is a constant with respect to our parameters of interest and so, as we are only interested in likelihoods up to proportionality when considering gradients, we can drop this contribution to the joint likelihood.</li>
<li><span class="math inline">\(p(x_i|c_i,\mu_i)\)</span> is the Gaussian with mean <span class="math inline">\(\mu_{c_i}\)</span> and variance <span class="math inline">\(1\)</span>. We are again going to make use of our vector <span class="math inline">\(\mathbf{C}_i\)</span> here, as although it is intuitive to think of the mean as <span class="math inline">\(\mu_{c_i}\)</span>, it’s tricky to work with this. We have
<span class="math display">\[\log p(x_i|c_i,\mu_i)=\log \prod_{k=1}^K p(x_i|\mu_k)^{C_{ik}}=\sum_{k=1}^K C_{ik} \log p(x_i|\mu_k)\propto -\frac{1}{2}\sum_{k=1}^K C_{ik} (x_i-\mu_k)^2.\]</span></li>
</ul>
<p>Combining all of the above, we can write
<span class="math display" id="eq:jointlik">\[\begin{equation}
\log p(\boldsymbol{\theta},\mathbf{x}) \propto -\frac{1}{2\sigma^2}\sum_{k=1}^K \mu_k^2 - \frac{1}{2}\sum_{i=1}^n \sum_{k=1}^K C_{ik} (x_i-\mu_k)^2.
\tag{9.2}
\end{equation}\]</span></p>
</div>
<div id="the-mean-field-family-approximation" class="section level3 hasAnchor" number="9.2.4">
<h3><span class="header-section-number">9.2.4</span> The mean-field family approximation<a href="coordinate-ascent-variational-inference-cavi.html#the-mean-field-family-approximation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>If we are to assume a mean-field approximation to this system, we need a family of distributions
<span class="math display" id="eq:genmeanfield">\[\begin{equation}
q(\boldsymbol{\theta})=\prod_{k=1}^K q(\mu_k | m_k,s_k^2) \prod_{i=1}^n q(\mathbf{C}_i | \boldsymbol\phi_i),
\tag{9.3}
\end{equation}\]</span>
as we assume that our parameters are independent from one another.</p>
<p>We will assume that the class means <span class="math inline">\(\mu_k\)</span> have Gaussian distributions, and so there are variational parameters <span class="math inline">\(m_k\)</span> and <span class="math inline">\(s_k^2\)</span> that describe the range of Gaussians within the variational family. It is common practice to assume that the approximate distribution of a continuous variable is Gaussian.</p>
<p>We will then assume a categorical distribution for the <span class="math inline">\(i^\text{th}\)</span> observation’s class, which has a vector of <span class="math inline">\(K\)</span> component weights/probabilities given by <span class="math inline">\(\phi_i\)</span>.</p>
<p>Again, note that we have one set of variational parameters <span class="math inline">\((m_k, s^2_k)\)</span> per mixture component, and one set of variational parameters <span class="math inline">\(\boldsymbol\phi_i\)</span> <em>per observation</em>, because we’re interested in the class membership of each observation here.</p>
<p>We can express our variational family as
<span class="math display" id="eq:meanfield">\[\begin{equation}
\log q(\boldsymbol{\theta}) \propto \sum_{i=1}^n \sum_{k=1}^K \log \phi_{ik} - \frac{1}{2}\sum_{k=1}^K \left( \log(2\pi s_k^2)+ \frac{(\mu_k-m_k)^2}{s_k^2} \right).
\tag{9.4}
\end{equation}\]</span></p>
<p>You may notice that we have assumed the same functional form for our approximate distribution for <span class="math inline">\(\boldsymbol\theta|\mathbf{x}\)</span> as we did for our prior <span class="math inline">\(\boldsymbol\theta\)</span>. This isn’t mandatory for variational inference, but is common. The variational parameter <span class="math inline">\(\boldsymbol\phi_i\)</span> features in the prior as <span class="math inline">\((\frac{1}{K},\ldots,\frac{1}{K})\)</span>, and the parameters <span class="math inline">\(m_k\)</span> and <span class="math inline">\(s_k^2\)</span> feature as <span class="math inline">\(0\)</span> and <span class="math inline">\(\sigma^2\)</span>, respectively, for all <span class="math inline">\(k\in1,\ldots,K\)</span>. It’s useful to compare the prior to posterior as part of our analysis.</p>
</div>
<div id="cavi" class="section level3 hasAnchor" number="9.2.5">
<h3><span class="header-section-number">9.2.5</span> CAVI<a href="coordinate-ascent-variational-inference-cavi.html#cavi" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In this example, our <span class="math inline">\(q(\cdot)\in \mathcal{Q}\)</span> is defined by the values of <span class="math inline">\(\mathbf{m},\mathbf{s}^2,\boldsymbol{\phi}_i\)</span>, so we are optimising the ELBO with respect to this set of <em>variational parameters</em>. An iteration of the optimisation process will involve updating each of <span class="math inline">\(m_1,\ldots,m_K,s_1^2,\ldots,s_K^2,\boldsymbol{\phi}_1,\ldots,\boldsymbol\phi_n\)</span> in turn. This sounds like a lot, but we will see the update process of that in Equation <a href="coordinate-ascent-variational-inference-cavi.html#eq:cavi">(9.1)</a> for each of <span class="math inline">\(m_k,s_k^2,\boldsymbol{\phi}_i\)</span> simplifies down to a set of neat, short equations.</p>
<p>We have
<span class="math display">\[\begin{align}
ELBO(q) =&amp; \ \E_q \left[ \log p(\boldsymbol\theta,\mathbf{x})\right] - \E_q \left[ \log q(\boldsymbol\theta) \right]
\\
\propto&amp; -\frac{1}{2\sigma^2}\sum_{k=1}^K \E_q\left[\mu_k^2\right] - \frac{1}{2}\sum_{i=1}^n \E_q\left[\sum_{k=1}^K C_{ik} (x_i-\mu_k)^2\right] \\
&amp;-\sum_{i=1}^n \sum_{k=1}^K \E_q\left[\log \phi_{ik}\right] + \frac{1}{2}\sum_{k=1}^K \E_q\left[ \log(2\pi s_k^2)+ \frac{(\mu_k-m_k)^2}{s_k^2} \right],
\end{align}\]</span></p>
<p>We are going to derive in detail the update for <span class="math inline">\(\boldsymbol\phi_i\)</span>. We will then only give results for the update for <span class="math inline">\(m_k,s_k^2\)</span>; you can derive these results for yourself if you wish.</p>
<div id="the-cavi-update-for-the-mixture-class-assignment" class="section level4 hasAnchor" number="9.2.5.1">
<h4><span class="header-section-number">9.2.5.1</span> The CAVI update for the mixture class assignment<a href="coordinate-ascent-variational-inference-cavi.html#the-cavi-update-for-the-mixture-class-assignment" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The variational family for each class assignment <span class="math inline">\(\mathbf{C}_i\)</span> is a categorical distribution of the form
<span class="math display">\[
q(\mathbf{C}_i|\boldsymbol\phi_i) = \prod_{j=1}^K \phi_{ij}^{C_{ij}},
\]</span>
so that
<span class="math display" id="eq:logqcat">\[\begin{equation}
\log q(\mathbf{C}_i|\boldsymbol\phi_i) = \sum_{i=1}^K C_{ij}\log \phi_{ij}
\tag{9.5}
\end{equation}\]</span></p>
<p>The CAVI update for <span class="math inline">\(\boldsymbol\phi_i\)</span> means that we are updating just part of our variational family. Recall that we implement CAVI parameter by parameter because of our independence assumption. So our full variational distribution is that in Equation <a href="coordinate-ascent-variational-inference-cavi.html#eq:genmeanfield">(9.3)</a>, and our update for <span class="math inline">\(\boldsymbol\phi_i\)</span> means we are updating the part of the variational distribution <span class="math inline">\(q(\mathbf{C}_i|\boldsymbol\phi_i)\)</span>. From Equation <a href="coordinate-ascent-variational-inference-cavi.html#eq:cavi">(9.1)</a> we need to set
<span class="math display">\[q(\mathbf{C}_i|\boldsymbol\phi_i)^* \propto \exp \left\lbrace \E_{-\boldsymbol\phi_i} \left[\log p\left( \boldsymbol{\phi}_i , \boldsymbol\theta_{-\boldsymbol{\phi}_i}, \mathbf{x} \right)\right]\right\rbrace.\]</span>
As this is a function of <span class="math inline">\(\boldsymbol\phi_i\)</span>, we only need to take all the elements in our expression for <span class="math inline">\(p\left( \boldsymbol{\phi}_i , \boldsymbol\theta_{-\boldsymbol{\phi}_i}, \mathbf{x} \right)\)</span> in Equation <a href="coordinate-ascent-variational-inference-cavi.html#eq:jointlik">(9.2)</a> that involve <span class="math inline">\(\boldsymbol\phi_i\)</span> (i.e. we only consider up to proportionality). This is therefore
<span class="math display">\[\begin{align}
q(\mathbf{C}_i|\boldsymbol\phi_i)^*
&amp;\propto \exp \left\lbrace \E_{-\boldsymbol\phi_i} \left[ -\frac{1}{2}\sum_{k=1}^K C_{ik} (x_i-\mu_k)^2\right]\right\rbrace  \\
&amp;= \exp \left\lbrace -\frac{1}{2}\sum_{k=1}^K C_{ik}\E_{-\boldsymbol\phi_i}\left[ x_i^2-2x_i\mu_k+\mu_k^2\right]\right\rbrace \\
&amp;\propto \exp \left\lbrace \sum_{k=1}^K C_{ik}\left(x_i\E_{-\boldsymbol\phi_i}\left[\mu_k\right]-\frac{1}{2}\E_{-\boldsymbol\phi_i}\left[\mu_k^2\right]\right)\right\rbrace \\
&amp;=\exp \left\lbrace \sum_{k=1}^K C_{ik}\left(x_im_k-\frac{m_k^2+s_k^2}{2}\right)\right\rbrace.
\end{align}\]</span></p>
<p>We have simplified this expression as much as possible, but where does that leave us? Now we compare the expression above, with Equation <a href="coordinate-ascent-variational-inference-cavi.html#eq:logqcat">(9.5)</a>
, and we set
<span class="math display">\[\phi_{ij} \propto \exp \left\lbrace x_im_k-\frac{m_k^2+s_k^2}{2}\right\rbrace.\]</span></p>
</div>
<div id="the-complete-cavi-update" class="section level4 hasAnchor" number="9.2.5.2">
<h4><span class="header-section-number">9.2.5.2</span> The complete CAVI update<a href="coordinate-ascent-variational-inference-cavi.html#the-complete-cavi-update" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>In summary, in an iterated update of the CAVI approach, we replace the current set of <span class="math inline">\(\boldsymbol\phi_1,\ldots,\boldsymbol\phi_n\)</span> with the above expression, where <span class="math inline">\(m,s^2\)</span> are the current values of these parameters. A complete iteration of CAVI will then involve updating <span class="math inline">\(m_1,\ldots,m_K\)</span> given the current values of the remaining parameters, and then updating <span class="math inline">\(s_1^2,\ldots,s_K^2\)</span> given the current values of the remaining parameters. We will not show the derivation, but these updates are:
<span class="math display">\[\begin{align}
m_k &amp;= \frac{\sum_{i=1}^n \phi_{ik}x_i}{\frac{1}{\sigma^2} + \sum_{i=1}^n \phi_{ik}}, \\
s_k^2 &amp;= \frac{1}{\frac{1}{\sigma^2} + \sum_{i=1}^n \phi_{ik}}.
\end{align}\]</span></p>
</div>
</div>
<div id="implementing-cavi-in-r" class="section level3 hasAnchor" number="9.2.6">
<h3><span class="header-section-number">9.2.6</span> Implementing CAVI in R<a href="coordinate-ascent-variational-inference-cavi.html#implementing-cavi-in-r" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We show CAVI in practice for an example of a mixture of 3 Gaussians. We generate some observations:</p>
<div class="sourceCode" id="cb145"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb145-1"><a href="coordinate-ascent-variational-inference-cavi.html#cb145-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create observations</span></span>
<span id="cb145-2"><a href="coordinate-ascent-variational-inference-cavi.html#cb145-2" aria-hidden="true" tabindex="-1"></a>true_mu <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">2</span>,<span class="dv">0</span>,<span class="dv">3</span>)</span>
<span id="cb145-3"><a href="coordinate-ascent-variational-inference-cavi.html#cb145-3" aria-hidden="true" tabindex="-1"></a>sigma <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb145-4"><a href="coordinate-ascent-variational-inference-cavi.html#cb145-4" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb145-5"><a href="coordinate-ascent-variational-inference-cavi.html#cb145-5" aria-hidden="true" tabindex="-1"></a>K <span class="ot">&lt;-</span> <span class="dv">3</span></span>
<span id="cb145-6"><a href="coordinate-ascent-variational-inference-cavi.html#cb145-6" aria-hidden="true" tabindex="-1"></a>true_c <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>K, n, <span class="at">replace =</span> T)</span>
<span id="cb145-7"><a href="coordinate-ascent-variational-inference-cavi.html#cb145-7" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, true_mu[true_c], <span class="dv">1</span>)</span></code></pre></div>
<p>Then run the CAVI algorithm:</p>
<div class="sourceCode" id="cb146"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb146-1"><a href="coordinate-ascent-variational-inference-cavi.html#cb146-1" aria-hidden="true" tabindex="-1"></a>n_iter <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb146-2"><a href="coordinate-ascent-variational-inference-cavi.html#cb146-2" aria-hidden="true" tabindex="-1"></a>stored_m <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow =</span> n_iter, <span class="at">ncol =</span> K)</span>
<span id="cb146-3"><a href="coordinate-ascent-variational-inference-cavi.html#cb146-3" aria-hidden="true" tabindex="-1"></a><span class="co"># initial values</span></span>
<span id="cb146-4"><a href="coordinate-ascent-variational-inference-cavi.html#cb146-4" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(K)</span>
<span id="cb146-5"><a href="coordinate-ascent-variational-inference-cavi.html#cb146-5" aria-hidden="true" tabindex="-1"></a>s <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">1</span>, K)</span>
<span id="cb146-6"><a href="coordinate-ascent-variational-inference-cavi.html#cb146-6" aria-hidden="true" tabindex="-1"></a>phi <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow =</span> n_iter, <span class="at">ncol =</span> K)</span>
<span id="cb146-7"><a href="coordinate-ascent-variational-inference-cavi.html#cb146-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb146-8"><a href="coordinate-ascent-variational-inference-cavi.html#cb146-8" aria-hidden="true" tabindex="-1"></a><span class="co"># CAVI</span></span>
<span id="cb146-9"><a href="coordinate-ascent-variational-inference-cavi.html#cb146-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_iter) {</span>
<span id="cb146-10"><a href="coordinate-ascent-variational-inference-cavi.html#cb146-10" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n) {</span>
<span id="cb146-11"><a href="coordinate-ascent-variational-inference-cavi.html#cb146-11" aria-hidden="true" tabindex="-1"></a>    phi[i, ] <span class="ot">&lt;-</span> <span class="fu">exp</span>(m<span class="sc">*</span>x[i] <span class="sc">-</span> (m<span class="sc">^</span><span class="dv">2</span><span class="sc">+</span>s)<span class="sc">/</span><span class="dv">2</span>)</span>
<span id="cb146-12"><a href="coordinate-ascent-variational-inference-cavi.html#cb146-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># normalise phi as they are supposed to sum to 1</span></span>
<span id="cb146-13"><a href="coordinate-ascent-variational-inference-cavi.html#cb146-13" aria-hidden="true" tabindex="-1"></a>    phi[i, ] <span class="ot">&lt;-</span> phi[i, ] <span class="sc">/</span> <span class="fu">sqrt</span>(<span class="fu">sum</span>(phi[i, ])<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb146-14"><a href="coordinate-ascent-variational-inference-cavi.html#cb146-14" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb146-15"><a href="coordinate-ascent-variational-inference-cavi.html#cb146-15" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>K) {</span>
<span id="cb146-16"><a href="coordinate-ascent-variational-inference-cavi.html#cb146-16" aria-hidden="true" tabindex="-1"></a>    m[k] <span class="ot">&lt;-</span> <span class="fu">sum</span>(phi[ , k] <span class="sc">*</span> x) <span class="sc">/</span> (<span class="dv">1</span><span class="sc">/</span>sigma<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> <span class="fu">sum</span>(phi[ , k]))</span>
<span id="cb146-17"><a href="coordinate-ascent-variational-inference-cavi.html#cb146-17" aria-hidden="true" tabindex="-1"></a>    s[k] <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">/</span> (<span class="dv">1</span><span class="sc">/</span>sigma<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> <span class="fu">sum</span>(phi[ , k]))</span>
<span id="cb146-18"><a href="coordinate-ascent-variational-inference-cavi.html#cb146-18" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb146-19"><a href="coordinate-ascent-variational-inference-cavi.html#cb146-19" aria-hidden="true" tabindex="-1"></a>  <span class="co"># keep track of the m&#39;s each iteration to see convergence</span></span>
<span id="cb146-20"><a href="coordinate-ascent-variational-inference-cavi.html#cb146-20" aria-hidden="true" tabindex="-1"></a>  stored_m[j, ] <span class="ot">&lt;-</span> m</span>
<span id="cb146-21"><a href="coordinate-ascent-variational-inference-cavi.html#cb146-21" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>Results of this analysis are plotted in Figures <a href="coordinate-ascent-variational-inference-cavi.html#fig:classification">9.1</a> and <a href="coordinate-ascent-variational-inference-cavi.html#fig:CAVI-mu">9.2</a>. Remember that the outcome of CAVI is a set of optimal values for <span class="math inline">\(m_k,s_k^2,\boldsymbol\phi_i\)</span>, and these optimal values define the joint distribution of <span class="math inline">\(\boldsymbol\theta\)</span> that is used as an approximation to the posterior.</p>
<p>We have observations <span class="math inline">\(x\)</span> that arise from one of 3 Gaussians, and the true data is shown in the left panel of Figure <a href="coordinate-ascent-variational-inference-cavi.html#fig:classification">9.1</a>, showing the general range of <span class="math inline">\(x\)</span> values produced from each group classification. In the right panel of Figure <a href="coordinate-ascent-variational-inference-cavi.html#fig:classification">9.1</a> we show the most probable group classification, defined for each observation <span class="math inline">\(x_i\)</span> as the most probable class from <span class="math inline">\(\boldsymbol\phi_i=(\phi_{i1},\phi_{i2},\phi_{i3})\)</span>. This is a fairly easy classification problem, as you can see that the observations from the three groups are fairly well separated, with only a small overlap. The VI classification has clear cut boundaries between the three groups when we look at the most probable classifications, but note that if we were to delve into the <span class="math inline">\(\boldsymbol\phi_i\)</span> for an observation that is on the boundary between groupings (i.e. an outlier in the box plots in the left panel), there will likely be uncertainty in the predicted classification. For example, observation <span class="math inline">\(13\)</span> had value <span class="math inline">\(x_{13}=-0.848\)</span> and was simulated from group <span class="math inline">\(1\)</span>. This value is in the tail of that Gaussian distribution, and the estimate of the class probabilities was <span class="math inline">\((0.457, 0.542, 6.1\times 10^{-4})\)</span>, so although it was deemed most likely to have arisen from group 2 (incorrectly), we can see it was almost equally deemed to be from groups 1 or 2.</p>
<p>We can also produce a confusion matrix with the <code>caret</code> package. We have the true classes in <code>true_c</code>, and we extract the predictions as follows:</p>
<div class="sourceCode" id="cb147"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb147-1"><a href="coordinate-ascent-variational-inference-cavi.html#cb147-1" aria-hidden="true" tabindex="-1"></a>most_lik <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, n)</span>
<span id="cb147-2"><a href="coordinate-ascent-variational-inference-cavi.html#cb147-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n) {</span>
<span id="cb147-3"><a href="coordinate-ascent-variational-inference-cavi.html#cb147-3" aria-hidden="true" tabindex="-1"></a>  most_lik[i] <span class="ot">&lt;-</span> <span class="fu">which</span>(phi[i, ] <span class="sc">==</span> <span class="fu">max</span>(phi[i, ]))</span>
<span id="cb147-4"><a href="coordinate-ascent-variational-inference-cavi.html#cb147-4" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>Then we do</p>
<div class="sourceCode" id="cb148"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb148-1"><a href="coordinate-ascent-variational-inference-cavi.html#cb148-1" aria-hidden="true" tabindex="-1"></a>caret<span class="sc">::</span><span class="fu">confusionMatrix</span>(<span class="fu">as.factor</span>(true_c),</span>
<span id="cb148-2"><a href="coordinate-ascent-variational-inference-cavi.html#cb148-2" aria-hidden="true" tabindex="-1"></a>                       <span class="fu">as.factor</span>(most_lik))<span class="sc">$</span>table</span></code></pre></div>
<pre><code>##           Reference
## Prediction   1   2   3
##          1 254   1  56
##          2  52  26 264
##          3   0 328  19</code></pre>
<p>In Figure <a href="coordinate-ascent-variational-inference-cavi.html#fig:CAVI-mu">9.2</a> we show the results of the group means, the <span class="math inline">\(\mu_1,\mu_2,\mu_3\)</span>. The distributions are the results from CAVI. Recall that CAVI gives the optimal <span class="math inline">\(m_k\)</span> and <span class="math inline">\(s_k^2\)</span> that define the approximate distribution of independent Gaussians for the <span class="math inline">\(\mu_k\)</span>. The true mixture means are shown with dashed lines, and we can see these have been replicated well by this analysis. Something to note it that this approach cannot include correlations between the parameters. If the distribution for <span class="math inline">\(\mu_1\)</span> was to be lowered, then this would obviously affect the group classification probabilities <span class="math inline">\(\boldsymbol\phi_i\)</span>, i.e. there is correlation in our model system that we expect to exist that we cannot estimate. This is important to always bear in mind with VI.</p>
<div class="figure"><span style="display:block;" id="fig:classification"></span>
<img src="MAS61006-S2-Notes_files/figure-html/classification-1.png" alt="The distribution of observations, split by the true underlying mixture group classification (left panel) and the most probable classification from applying CAVI (right panel)." width="45%" /><img src="MAS61006-S2-Notes_files/figure-html/classification-2.png" alt="The distribution of observations, split by the true underlying mixture group classification (left panel) and the most probable classification from applying CAVI (right panel)." width="45%" />
<p class="caption">
Figure 9.1: The distribution of observations, split by the true underlying mixture group classification (left panel) and the most probable classification from applying CAVI (right panel).
</p>
</div>
<div class="figure"><span style="display:block;" id="fig:CAVI-mu"></span>
<img src="MAS61006-S2-Notes_files/figure-html/CAVI-mu-1.png" alt="The approximate posterior distributions of the mixture means from CAVI (solid), along with the true means used for the simulation (dashed)." width="50%" />
<p class="caption">
Figure 9.2: The approximate posterior distributions of the mixture means from CAVI (solid), along with the true means used for the simulation (dashed).
</p>
</div>
</div>
</div>
<div id="comment" class="section level2 hasAnchor" number="9.3">
<h2><span class="header-section-number">9.3</span> Comment<a href="coordinate-ascent-variational-inference-cavi.html#comment" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>CAVI was useful for the example, because it was possible to derive closed-form expressions for the variational distribution update equations. Note the similarity with Gibbs sampling: this is useful when the full conditional distributions are relatively easy to sample from.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="variational-inference.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="references.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": null,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section",
"scroll-highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
