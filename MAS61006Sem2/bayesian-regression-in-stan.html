<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Bayesian regression in Stan | MAS61006 Bayesian Statistics and Computational Methods</title>
  <meta name="description" content="Chapter 4 Bayesian regression in Stan | MAS61006 Bayesian Statistics and Computational Methods" />
  <meta name="generator" content="bookdown 0.27 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Bayesian regression in Stan | MAS61006 Bayesian Statistics and Computational Methods" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Bayesian regression in Stan | MAS61006 Bayesian Statistics and Computational Methods" />
  
  
  

<meta name="author" content="Jeremy Oakley" />


<meta name="date" content="2023-03-04" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="implementing-hmc-in-stan.html"/>
<link rel="next" href="multiple-imputation-for-missing-data.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script>
  $(document).ready(function () {
    process_solutions();
  });
function process_solutions() {
  $("div.section[id^='solution']").each(function(i) {
    var soln_wrapper_id = "cvxr_ex_" + i;
    var solution_id = $(this).attr('id');
    var button = $("<button onclick=\"toggle_solution('" + soln_wrapper_id + "')\">Show/Hide</button>");
    var new_div = $("<div id='" + soln_wrapper_id + "' class='solution' style='display: none;'></div>");
    var h = $(this).first();
    var others = $(this).children().slice(1);
    $(others).each(function() {
      $(this).appendTo($(new_div));
    });
    $(button).insertAfter($(h));
    $(new_div).insertAfter($(button));
  })
}
function toggle_solution(el_id) {
  $("#" + el_id).toggle();
}
</script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">MAS61006(Sem2)</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Acknowledgement</a></li>
<li class="chapter" data-level="" data-path="literature.html"><a href="literature.html"><i class="fa fa-check"></i>Literature</a></li>
<li class="part"><span><b>I Inference using advanced samplers</b></span></li>
<li class="chapter" data-level="1" data-path="mcmc-sampling-recap.html"><a href="mcmc-sampling-recap.html"><i class="fa fa-check"></i><b>1</b> MCMC Sampling Recap</a>
<ul>
<li class="chapter" data-level="1.1" data-path="mcmc-sampling-recap.html"><a href="mcmc-sampling-recap.html#bayesian-inference-continued"><i class="fa fa-check"></i><b>1.1</b> Bayesian inference continued</a></li>
<li class="chapter" data-level="1.2" data-path="mcmc-sampling-recap.html"><a href="mcmc-sampling-recap.html#monte-carlo"><i class="fa fa-check"></i><b>1.2</b> Monte Carlo estimation</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="mcmc-sampling-recap.html"><a href="mcmc-sampling-recap.html#properties-of-the-monte-carlo-estimate"><i class="fa fa-check"></i><b>1.2.1</b> Properties of the Monte Carlo estimate</a></li>
<li class="chapter" data-level="1.2.2" data-path="mcmc-sampling-recap.html"><a href="mcmc-sampling-recap.html#expressing-quantities-as-expectations"><i class="fa fa-check"></i><b>1.2.2</b> Expressing quantities as expectations</a></li>
<li class="chapter" data-level="1.2.3" data-path="mcmc-sampling-recap.html"><a href="mcmc-sampling-recap.html#estimation-of-general-integrals"><i class="fa fa-check"></i><b>1.2.3</b> Estimation of general integrals</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="mcmc-sampling-recap.html"><a href="mcmc-sampling-recap.html#markov-chain-monte-carlo-mcmc"><i class="fa fa-check"></i><b>1.3</b> Markov chain Monte Carlo (MCMC)</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="mcmc-sampling-recap.html"><a href="mcmc-sampling-recap.html#mhsection"><i class="fa fa-check"></i><b>1.3.1</b> The Metroplis-Hastings (MH) algorithm</a></li>
<li class="chapter" data-level="1.3.2" data-path="mcmc-sampling-recap.html"><a href="mcmc-sampling-recap.html#the-problem-with-random-walk-proposals"><i class="fa fa-check"></i><b>1.3.2</b> The problem with random walk proposals</a></li>
<li class="chapter" data-level="1.3.3" data-path="mcmc-sampling-recap.html"><a href="mcmc-sampling-recap.html#the-gibbs-algorithm"><i class="fa fa-check"></i><b>1.3.3</b> The Gibbs algorithm</a></li>
<li class="chapter" data-level="1.3.4" data-path="mcmc-sampling-recap.html"><a href="mcmc-sampling-recap.html#the-problem-with-gibbs"><i class="fa fa-check"></i><b>1.3.4</b> The problem with Gibbs</a></li>
<li class="chapter" data-level="1.3.5" data-path="mcmc-sampling-recap.html"><a href="mcmc-sampling-recap.html#a-solution"><i class="fa fa-check"></i><b>1.3.5</b> A solution?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="hamiltonian-monte-carlo-hmc.html"><a href="hamiltonian-monte-carlo-hmc.html"><i class="fa fa-check"></i><b>2</b> Hamiltonian Monte Carlo (HMC)</a>
<ul>
<li class="chapter" data-level="2.1" data-path="hamiltonian-monte-carlo-hmc.html"><a href="hamiltonian-monte-carlo-hmc.html#generating-proposals-intuition"><i class="fa fa-check"></i><b>2.1</b> Generating proposals: intuition</a></li>
<li class="chapter" data-level="2.2" data-path="hamiltonian-monte-carlo-hmc.html"><a href="hamiltonian-monte-carlo-hmc.html#hamiltonian-dynamics"><i class="fa fa-check"></i><b>2.2</b> Hamiltonian dynamics</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="hamiltonian-monte-carlo-hmc.html"><a href="hamiltonian-monte-carlo-hmc.html#conservation-of-energy"><i class="fa fa-check"></i><b>2.2.1</b> Conservation of energy</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="hamiltonian-monte-carlo-hmc.html"><a href="hamiltonian-monte-carlo-hmc.html#using-hamiltons-equations-to-generate-proposals"><i class="fa fa-check"></i><b>2.3</b> Using Hamilton’s equations to generate proposals</a></li>
<li class="chapter" data-level="2.4" data-path="hamiltonian-monte-carlo-hmc.html"><a href="hamiltonian-monte-carlo-hmc.html#the-joint-distribution-for-theta-m"><i class="fa fa-check"></i><b>2.4</b> The joint distribution for <span class="math inline">\((\theta, m)\)</span></a></li>
<li class="chapter" data-level="2.5" data-path="hamiltonian-monte-carlo-hmc.html"><a href="hamiltonian-monte-carlo-hmc.html#the-hmc-algorithm"><i class="fa fa-check"></i><b>2.5</b> The HMC algorithm</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="hamiltonian-monte-carlo-hmc.html"><a href="hamiltonian-monte-carlo-hmc.html#approximate-solution-of-hamiltons-equations"><i class="fa fa-check"></i><b>2.5.1</b> Approximate solution of Hamilton’s equations</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="hamiltonian-monte-carlo-hmc.html"><a href="hamiltonian-monte-carlo-hmc.html#multivariate-theta"><i class="fa fa-check"></i><b>2.6</b> Multivariate <span class="math inline">\(\theta\)</span></a></li>
<li class="chapter" data-level="2.7" data-path="hamiltonian-monte-carlo-hmc.html"><a href="hamiltonian-monte-carlo-hmc.html#tuning-parameters"><i class="fa fa-check"></i><b>2.7</b> Tuning parameters</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="hamiltonian-monte-carlo-hmc.html"><a href="hamiltonian-monte-carlo-hmc.html#the-tuning-parameter-sigma2"><i class="fa fa-check"></i><b>2.7.1</b> The tuning parameter <span class="math inline">\(\sigma^2\)</span></a></li>
<li class="chapter" data-level="2.7.2" data-path="hamiltonian-monte-carlo-hmc.html"><a href="hamiltonian-monte-carlo-hmc.html#the-hamiltonian-movement-tuning-parameters-t-and-epsilon"><i class="fa fa-check"></i><b>2.7.2</b> The Hamiltonian movement tuning parameters <span class="math inline">\(T\)</span> and <span class="math inline">\(\epsilon\)</span></a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="hamiltonian-monte-carlo-hmc.html"><a href="hamiltonian-monte-carlo-hmc.html#implementing-hmc-by-hand"><i class="fa fa-check"></i><b>2.8</b> Implementing HMC ‘by hand’</a>
<ul>
<li class="chapter" data-level="2.8.1" data-path="hamiltonian-monte-carlo-hmc.html"><a href="hamiltonian-monte-carlo-hmc.html#random-walk-mh"><i class="fa fa-check"></i><b>2.8.1</b> Random-walk MH</a></li>
<li class="chapter" data-level="2.8.2" data-path="hamiltonian-monte-carlo-hmc.html"><a href="hamiltonian-monte-carlo-hmc.html#gibbs"><i class="fa fa-check"></i><b>2.8.2</b> Gibbs</a></li>
<li class="chapter" data-level="2.8.3" data-path="hamiltonian-monte-carlo-hmc.html"><a href="hamiltonian-monte-carlo-hmc.html#hmc"><i class="fa fa-check"></i><b>2.8.3</b> HMC</a></li>
<li class="chapter" data-level="2.8.4" data-path="hamiltonian-monte-carlo-hmc.html"><a href="hamiltonian-monte-carlo-hmc.html#comparing-random-walk-and-hmc-samplers"><i class="fa fa-check"></i><b>2.8.4</b> Comparing random-walk and HMC samplers</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="hamiltonian-monte-carlo-hmc.html"><a href="hamiltonian-monte-carlo-hmc.html#summary"><i class="fa fa-check"></i><b>2.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="implementing-hmc-in-stan.html"><a href="implementing-hmc-in-stan.html"><i class="fa fa-check"></i><b>3</b> Implementing HMC in Stan</a>
<ul>
<li class="chapter" data-level="3.1" data-path="implementing-hmc-in-stan.html"><a href="implementing-hmc-in-stan.html#getting-set-up-with-stan"><i class="fa fa-check"></i><b>3.1</b> Getting set up with Stan</a></li>
<li class="chapter" data-level="3.2" data-path="implementing-hmc-in-stan.html"><a href="implementing-hmc-in-stan.html#rstan-options"><i class="fa fa-check"></i><b>3.2</b> <code>rstan</code> options</a></li>
<li class="chapter" data-level="3.3" data-path="implementing-hmc-in-stan.html"><a href="implementing-hmc-in-stan.html#an-example-model"><i class="fa fa-check"></i><b>3.3</b> An example model</a></li>
<li class="chapter" data-level="3.4" data-path="implementing-hmc-in-stan.html"><a href="implementing-hmc-in-stan.html#specifying-a-model-in-rstudio"><i class="fa fa-check"></i><b>3.4</b> Specifying a model in RStudio</a></li>
<li class="chapter" data-level="3.5" data-path="implementing-hmc-in-stan.html"><a href="implementing-hmc-in-stan.html#stan-code-blocks"><i class="fa fa-check"></i><b>3.5</b> Stan code blocks</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="implementing-hmc-in-stan.html"><a href="implementing-hmc-in-stan.html#data-block"><i class="fa fa-check"></i><b>3.5.1</b> <code>data</code> block</a></li>
<li class="chapter" data-level="3.5.2" data-path="implementing-hmc-in-stan.html"><a href="implementing-hmc-in-stan.html#parameters-block"><i class="fa fa-check"></i><b>3.5.2</b> <code>parameters</code> block</a></li>
<li class="chapter" data-level="3.5.3" data-path="implementing-hmc-in-stan.html"><a href="implementing-hmc-in-stan.html#model-block"><i class="fa fa-check"></i><b>3.5.3</b> <code>model</code> block</a></li>
<li class="chapter" data-level="3.5.4" data-path="implementing-hmc-in-stan.html"><a href="implementing-hmc-in-stan.html#generated-quantities-block"><i class="fa fa-check"></i><b>3.5.4</b> <code>generated quantities</code> block</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="implementing-hmc-in-stan.html"><a href="implementing-hmc-in-stan.html#running-the-hmc-algorithm"><i class="fa fa-check"></i><b>3.6</b> Running the HMC algorithm</a></li>
<li class="chapter" data-level="3.7" data-path="implementing-hmc-in-stan.html"><a href="implementing-hmc-in-stan.html#extracting-and-analysing-the-samples"><i class="fa fa-check"></i><b>3.7</b> Extracting and analysing the samples</a>
<ul>
<li class="chapter" data-level="3.7.1" data-path="implementing-hmc-in-stan.html"><a href="implementing-hmc-in-stan.html#r-packages-for-plotting-outputs"><i class="fa fa-check"></i><b>3.7.1</b> R packages for plotting outputs</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="implementing-hmc-in-stan.html"><a href="implementing-hmc-in-stan.html#no-u-turn-sampler-nuts"><i class="fa fa-check"></i><b>3.8</b> No U-Turn Sampler (NUTS)</a></li>
<li class="chapter" data-level="3.9" data-path="implementing-hmc-in-stan.html"><a href="implementing-hmc-in-stan.html#further-reading"><i class="fa fa-check"></i><b>3.9</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="bayesian-regression-in-stan.html"><a href="bayesian-regression-in-stan.html"><i class="fa fa-check"></i><b>4</b> Bayesian regression in Stan</a>
<ul>
<li class="chapter" data-level="4.1" data-path="bayesian-regression-in-stan.html"><a href="bayesian-regression-in-stan.html#simple-linear-regression"><i class="fa fa-check"></i><b>4.1</b> Simple linear regression</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="bayesian-regression-in-stan.html"><a href="bayesian-regression-in-stan.html#holiday-hangover-cures"><i class="fa fa-check"></i><b>4.1.1</b> Holiday hangover cures</a></li>
<li class="chapter" data-level="4.1.2" data-path="bayesian-regression-in-stan.html"><a href="bayesian-regression-in-stan.html#least-squares-fit"><i class="fa fa-check"></i><b>4.1.2</b> Least squares fit</a></li>
<li class="chapter" data-level="4.1.3" data-path="bayesian-regression-in-stan.html"><a href="bayesian-regression-in-stan.html#bayesian-approach-in-stan"><i class="fa fa-check"></i><b>4.1.3</b> Bayesian approach in Stan</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="bayesian-regression-in-stan.html"><a href="bayesian-regression-in-stan.html#recap-of-logistic-regression"><i class="fa fa-check"></i><b>4.2</b> Recap of logistic regression</a></li>
<li class="chapter" data-level="4.3" data-path="bayesian-regression-in-stan.html"><a href="bayesian-regression-in-stan.html#overview-of-mixed-effects"><i class="fa fa-check"></i><b>4.3</b> Overview of mixed effects</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="bayesian-regression-in-stan.html"><a href="bayesian-regression-in-stan.html#simple-example-of-a-mixed-effects-model"><i class="fa fa-check"></i><b>4.3.1</b> Simple example of a mixed effects model</a></li>
<li class="chapter" data-level="4.3.2" data-path="bayesian-regression-in-stan.html"><a href="bayesian-regression-in-stan.html#fixed-or-random-effects"><i class="fa fa-check"></i><b>4.3.2</b> Fixed or random effects?</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="bayesian-regression-in-stan.html"><a href="bayesian-regression-in-stan.html#mixed-effect-logistic-regression"><i class="fa fa-check"></i><b>4.4</b> Mixed effect logistic regression</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="bayesian-regression-in-stan.html"><a href="bayesian-regression-in-stan.html#balance-experiment"><i class="fa fa-check"></i><b>4.4.1</b> Balance experiment</a></li>
<li class="chapter" data-level="4.4.2" data-path="bayesian-regression-in-stan.html"><a href="bayesian-regression-in-stan.html#exploratory-data-analysis"><i class="fa fa-check"></i><b>4.4.2</b> Exploratory data analysis</a></li>
<li class="chapter" data-level="4.4.3" data-path="bayesian-regression-in-stan.html"><a href="bayesian-regression-in-stan.html#maximum-likelihood-approach"><i class="fa fa-check"></i><b>4.4.3</b> Maximum likelihood approach</a></li>
<li class="chapter" data-level="4.4.4" data-path="bayesian-regression-in-stan.html"><a href="bayesian-regression-in-stan.html#bayesian-approach-using-stan"><i class="fa fa-check"></i><b>4.4.4</b> Bayesian approach using Stan</a></li>
<li class="chapter" data-level="4.4.5" data-path="bayesian-regression-in-stan.html"><a href="bayesian-regression-in-stan.html#comparing-stan-and-lme4"><i class="fa fa-check"></i><b>4.4.5</b> Comparing Stan and lme4</a></li>
<li class="chapter" data-level="4.4.6" data-path="bayesian-regression-in-stan.html"><a href="bayesian-regression-in-stan.html#easy-bayesian-approach-with-brms"><i class="fa fa-check"></i><b>4.4.6</b> ‘Easy’ Bayesian approach with <code>brms</code></a></li>
<li class="chapter" data-level="4.4.7" data-path="bayesian-regression-in-stan.html"><a href="bayesian-regression-in-stan.html#what-have-we-found"><i class="fa fa-check"></i><b>4.4.7</b> What have we found?</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Inference with missing data</b></span></li>
<li class="chapter" data-level="5" data-path="multiple-imputation-for-missing-data.html"><a href="multiple-imputation-for-missing-data.html"><i class="fa fa-check"></i><b>5</b> Multiple imputation for missing data</a>
<ul>
<li class="chapter" data-level="5.1" data-path="multiple-imputation-for-missing-data.html"><a href="multiple-imputation-for-missing-data.html#introduction"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="multiple-imputation-for-missing-data.html"><a href="multiple-imputation-for-missing-data.html#example-nhanes-data"><i class="fa fa-check"></i><b>5.2</b> Example: <code>nhanes</code> data</a></li>
<li class="chapter" data-level="5.3" data-path="multiple-imputation-for-missing-data.html"><a href="multiple-imputation-for-missing-data.html#mechanisms-of-missingness"><i class="fa fa-check"></i><b>5.3</b> Mechanisms of missingness</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="multiple-imputation-for-missing-data.html"><a href="multiple-imputation-for-missing-data.html#missing-completely-at-random-mcar"><i class="fa fa-check"></i><b>5.3.1</b> Missing completely at random (MCAR)</a></li>
<li class="chapter" data-level="5.3.2" data-path="multiple-imputation-for-missing-data.html"><a href="multiple-imputation-for-missing-data.html#missing-at-random-mar"><i class="fa fa-check"></i><b>5.3.2</b> Missing at random (MAR)</a></li>
<li class="chapter" data-level="5.3.3" data-path="multiple-imputation-for-missing-data.html"><a href="multiple-imputation-for-missing-data.html#not-missing-at-random-nmar"><i class="fa fa-check"></i><b>5.3.3</b> Not missing at random (NMAR)</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="multiple-imputation-for-missing-data.html"><a href="multiple-imputation-for-missing-data.html#ignoring-information-about-missingness"><i class="fa fa-check"></i><b>5.4</b> Ignoring information about missingness</a></li>
<li class="chapter" data-level="5.5" data-path="multiple-imputation-for-missing-data.html"><a href="multiple-imputation-for-missing-data.html#inference-via-imputation"><i class="fa fa-check"></i><b>5.5</b> Inference via imputation</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="multiple-imputation-for-missing-data.html"><a href="multiple-imputation-for-missing-data.html#further-simplifications"><i class="fa fa-check"></i><b>5.5.1</b> Further simplifications</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="multiple-imputation-for-missing-data.html"><a href="multiple-imputation-for-missing-data.html#pooling"><i class="fa fa-check"></i><b>5.6</b> Pooling multiple imputations</a></li>
<li class="chapter" data-level="5.7" data-path="multiple-imputation-for-missing-data.html"><a href="multiple-imputation-for-missing-data.html#simple-example"><i class="fa fa-check"></i><b>5.7</b> Simple example</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="multiple-imputation-for-missing-data.html"><a href="multiple-imputation-for-missing-data.html#multiple-imputation-by-hand"><i class="fa fa-check"></i><b>5.7.1</b> Multiple imputation by hand</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="multiple-imputation-for-missing-data.html"><a href="multiple-imputation-for-missing-data.html#imputing-missing-data-chained-equation-multiple-imputation"><i class="fa fa-check"></i><b>5.8</b> Imputing missing data: chained equation multiple imputation</a>
<ul>
<li class="chapter" data-level="5.8.1" data-path="multiple-imputation-for-missing-data.html"><a href="multiple-imputation-for-missing-data.html#how-many-iterations"><i class="fa fa-check"></i><b>5.8.1</b> How many iterations?</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="multiple-imputation-for-missing-data.html"><a href="multiple-imputation-for-missing-data.html#mice-example-the-nhanes-dataset"><i class="fa fa-check"></i><b>5.9</b> MICE example: the <code>nhanes</code> dataset</a>
<ul>
<li class="chapter" data-level="5.9.1" data-path="multiple-imputation-for-missing-data.html"><a href="multiple-imputation-for-missing-data.html#complete-case-analysis"><i class="fa fa-check"></i><b>5.9.1</b> Complete case analysis</a></li>
<li class="chapter" data-level="5.9.2" data-path="multiple-imputation-for-missing-data.html"><a href="multiple-imputation-for-missing-data.html#imputing-the-missing-data"><i class="fa fa-check"></i><b>5.9.2</b> Imputing the missing data</a></li>
<li class="chapter" data-level="5.9.3" data-path="multiple-imputation-for-missing-data.html"><a href="multiple-imputation-for-missing-data.html#imputation-analysis"><i class="fa fa-check"></i><b>5.9.3</b> Imputation analysis</a></li>
<li class="chapter" data-level="5.9.4" data-path="multiple-imputation-for-missing-data.html"><a href="multiple-imputation-for-missing-data.html#analysis-pooling"><i class="fa fa-check"></i><b>5.9.4</b> Analysis pooling</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Resampling methods</b></span></li>
<li class="chapter" data-level="6" data-path="bootstrapping.html"><a href="bootstrapping.html"><i class="fa fa-check"></i><b>6</b> Bootstrapping</a>
<ul>
<li class="chapter" data-level="6.1" data-path="bootstrapping.html"><a href="bootstrapping.html#bootstrap-estimates-of-standard-errors"><i class="fa fa-check"></i><b>6.1</b> Bootstrap estimates of standard errors</a></li>
<li class="chapter" data-level="6.2" data-path="bootstrapping.html"><a href="bootstrapping.html#estimating-a-distribution-using-the-empirical-cumulative-distribution-function"><i class="fa fa-check"></i><b>6.2</b> Estimating a distribution using the empirical cumulative distribution function</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="bootstrapping.html"><a href="bootstrapping.html#sampling-from-an-ecdf"><i class="fa fa-check"></i><b>6.2.1</b> Sampling from an ECDF</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="bootstrapping.html"><a href="bootstrapping.html#notation-summary"><i class="fa fa-check"></i><b>6.3</b> Notation summary</a></li>
<li class="chapter" data-level="6.4" data-path="bootstrapping.html"><a href="bootstrapping.html#example-bootstrap-standard-errors-of-a-sample-mean-and-sample-variance"><i class="fa fa-check"></i><b>6.4</b> Example: Bootstrap standard errors of a sample mean and sample variance</a></li>
<li class="chapter" data-level="6.5" data-path="bootstrapping.html"><a href="bootstrapping.html#confidence-intervals"><i class="fa fa-check"></i><b>6.5</b> Confidence intervals</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="bootstrapping.html"><a href="bootstrapping.html#confidence-intervals-using-the-estimated-standard-error"><i class="fa fa-check"></i><b>6.5.1</b> Confidence intervals using the estimated standard error</a></li>
<li class="chapter" data-level="6.5.2" data-path="bootstrapping.html"><a href="bootstrapping.html#confidence-intervals-using-percentiles"><i class="fa fa-check"></i><b>6.5.2</b> Confidence intervals using percentiles</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="bootstrapping.html"><a href="bootstrapping.html#properties-of-samples-from-the-empirical-cdf"><i class="fa fa-check"></i><b>6.6</b> Properties of samples from the empirical CDF</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="bootstrapping.html"><a href="bootstrapping.html#expectation-and-variance"><i class="fa fa-check"></i><b>6.6.1</b> Expectation and variance</a></li>
<li class="chapter" data-level="6.6.2" data-path="bootstrapping.html"><a href="bootstrapping.html#sample-percentiles"><i class="fa fa-check"></i><b>6.6.2</b> Sample percentiles</a></li>
<li class="chapter" data-level="6.6.3" data-path="bootstrapping.html"><a href="bootstrapping.html#sources-of-error-and-sample-sizes-in-bootstrapping"><i class="fa fa-check"></i><b>6.6.3</b> Sources of error and sample sizes in bootstrapping</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="bootstrapping.html"><a href="bootstrapping.html#example-measuring-observer-agreement"><i class="fa fa-check"></i><b>6.7</b> Example: Measuring observer agreement</a>
<ul>
<li class="chapter" data-level="6.7.1" data-path="bootstrapping.html"><a href="bootstrapping.html#the-data"><i class="fa fa-check"></i><b>6.7.1</b> The data</a></li>
<li class="chapter" data-level="6.7.2" data-path="bootstrapping.html"><a href="bootstrapping.html#the-kappa-statistic"><i class="fa fa-check"></i><b>6.7.2</b> The kappa statistic</a></li>
<li class="chapter" data-level="6.7.3" data-path="bootstrapping.html"><a href="bootstrapping.html#bootstrapping-bivariate-data"><i class="fa fa-check"></i><b>6.7.3</b> Bootstrapping bivariate data</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="bootstrapping.html"><a href="bootstrapping.html#parametric-bootstrapping-and-hypothesis-testing"><i class="fa fa-check"></i><b>6.8</b> Parametric bootstrapping and hypothesis testing</a>
<ul>
<li class="chapter" data-level="6.8.1" data-path="bootstrapping.html"><a href="bootstrapping.html#the-milk-data-set"><i class="fa fa-check"></i><b>6.8.1</b> The <code>Milk</code> data set</a></li>
<li class="chapter" data-level="6.8.2" data-path="bootstrapping.html"><a href="bootstrapping.html#the-model-and-hypothesis"><i class="fa fa-check"></i><b>6.8.2</b> The model and hypothesis</a></li>
<li class="chapter" data-level="6.8.3" data-path="bootstrapping.html"><a href="bootstrapping.html#the-generalized-likelihood-ratio-test"><i class="fa fa-check"></i><b>6.8.3</b> The generalized likelihood ratio test</a></li>
<li class="chapter" data-level="6.8.4" data-path="bootstrapping.html"><a href="bootstrapping.html#the-parametric-bootstrap-test"><i class="fa fa-check"></i><b>6.8.4</b> The parametric bootstrap test</a></li>
<li class="chapter" data-level="6.8.5" data-path="bootstrapping.html"><a href="bootstrapping.html#implementation-with-r"><i class="fa fa-check"></i><b>6.8.5</b> Implementation with R</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="cross-validation.html"><a href="cross-validation.html"><i class="fa fa-check"></i><b>7</b> Cross-validation</a>
<ul>
<li class="chapter" data-level="7.1" data-path="cross-validation.html"><a href="cross-validation.html#cross-validation-in-classification"><i class="fa fa-check"></i><b>7.1</b> Cross-validation in classification</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="cross-validation.html"><a href="cross-validation.html#the-palmer-penguins-data"><i class="fa fa-check"></i><b>7.1.1</b> The Palmer penguins data</a></li>
<li class="chapter" data-level="7.1.2" data-path="cross-validation.html"><a href="cross-validation.html#the-k-nearest-neighbours-algorithm-knn"><i class="fa fa-check"></i><b>7.1.2</b> The <span class="math inline">\(K\)</span>-nearest neighbours algorithm (KNN)</a></li>
<li class="chapter" data-level="7.1.3" data-path="cross-validation.html"><a href="cross-validation.html#implementing-cross-validation-in-knn"><i class="fa fa-check"></i><b>7.1.3</b> Implementing cross-validation in KNN</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="cross-validation.html"><a href="cross-validation.html#cross-validation-in-regression"><i class="fa fa-check"></i><b>7.2</b> Cross-validation in regression</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="cross-validation.html"><a href="cross-validation.html#the-flint-tools-data"><i class="fa fa-check"></i><b>7.2.1</b> The flint tools data</a></li>
<li class="chapter" data-level="7.2.2" data-path="cross-validation.html"><a href="cross-validation.html#implementing-cross-validation-in-regression"><i class="fa fa-check"></i><b>7.2.2</b> Implementing cross-validation in regression</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="cross-validation.html"><a href="cross-validation.html#parameter-estimation-with-cross-validation"><i class="fa fa-check"></i><b>7.3</b> Parameter estimation with cross-validation</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="cross-validation.html"><a href="cross-validation.html#example-choosing-the-value-k-in-knn"><i class="fa fa-check"></i><b>7.3.1</b> Example: choosing the value <span class="math inline">\(K\)</span> in KNN</a></li>
<li class="chapter" data-level="7.3.2" data-path="cross-validation.html"><a href="cross-validation.html#cross-validation-as-an-alternative-to-maximum-likelihood"><i class="fa fa-check"></i><b>7.3.2</b> Cross-validation as an alternative to maximum likelihood</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="cross-validation.html"><a href="cross-validation.html#computational-short-cuts"><i class="fa fa-check"></i><b>7.4</b> Computational short-cuts</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="cross-validation.html"><a href="cross-validation.html#example-returning-to-the-flint-data"><i class="fa fa-check"></i><b>7.4.1</b> Example: Returning to the flint data</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="cross-validation.html"><a href="cross-validation.html#relationship-with-aic"><i class="fa fa-check"></i><b>7.5</b> Relationship with AIC</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="cross-validation.html"><a href="cross-validation.html#example-the-cars-data"><i class="fa fa-check"></i><b>7.5.1</b> Example: The cars data</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="cross-validation.html"><a href="cross-validation.html#non-examinable-proof-of-the-computational-short-cut"><i class="fa fa-check"></i><b>7.6</b> (Non-examinable) Proof of the computational short cut</a>
<ul>
<li class="chapter" data-level="7.6.1" data-path="cross-validation.html"><a href="cross-validation.html#helpful-results-we-will-use"><i class="fa fa-check"></i><b>7.6.1</b> Helpful results we will use</a></li>
<li class="chapter" data-level="7.6.2" data-path="cross-validation.html"><a href="cross-validation.html#relate-the-estimated-coefficients-of-full-and-reduced-models"><i class="fa fa-check"></i><b>7.6.2</b> Relate the estimated coefficients of full and reduced models</a></li>
<li class="chapter" data-level="7.6.3" data-path="cross-validation.html"><a href="cross-validation.html#relating-the-residuals"><i class="fa fa-check"></i><b>7.6.3</b> Relating the residuals</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IV Approximate inference without sampling</b></span></li>
<li class="chapter" data-level="8" data-path="variational-inference.html"><a href="variational-inference.html"><i class="fa fa-check"></i><b>8</b> Variational inference</a>
<ul>
<li class="chapter" data-level="8.1" data-path="variational-inference.html"><a href="variational-inference.html#background-theory"><i class="fa fa-check"></i><b>8.1</b> Background theory</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="variational-inference.html"><a href="variational-inference.html#jensens-inequality"><i class="fa fa-check"></i><b>8.1.1</b> Jensen’s inequality</a></li>
<li class="chapter" data-level="8.1.2" data-path="variational-inference.html"><a href="variational-inference.html#kullback-leibler-divergence"><i class="fa fa-check"></i><b>8.1.2</b> Kullback-Leibler Divergence</a></li>
<li class="chapter" data-level="8.1.3" data-path="variational-inference.html"><a href="variational-inference.html#optimisation-with-coordinate-ascent"><i class="fa fa-check"></i><b>8.1.3</b> Optimisation with coordinate ascent</a></li>
<li class="chapter" data-level="8.1.4" data-path="variational-inference.html"><a href="variational-inference.html#stochastic-optimisation"><i class="fa fa-check"></i><b>8.1.4</b> Stochastic optimisation</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="variational-inference.html"><a href="variational-inference.html#motivation-for-approximate-inference-approaches"><i class="fa fa-check"></i><b>8.2</b> Motivation for approximate inference approaches</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="variational-inference.html"><a href="variational-inference.html#intractable-integrals"><i class="fa fa-check"></i><b>8.2.1</b> Intractable integrals</a></li>
<li class="chapter" data-level="8.2.2" data-path="variational-inference.html"><a href="variational-inference.html#variational-approach-to-intractable-integrals"><i class="fa fa-check"></i><b>8.2.2</b> Variational approach to intractable integrals</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="variational-inference.html"><a href="variational-inference.html#approximate-inference-as-an-optimisation-problem"><i class="fa fa-check"></i><b>8.3</b> Approximate inference as an optimisation problem</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="variational-inference.html"><a href="variational-inference.html#exploring-the-elbo"><i class="fa fa-check"></i><b>8.3.1</b> Exploring the ELBO</a></li>
<li class="chapter" data-level="8.3.2" data-path="variational-inference.html"><a href="variational-inference.html#forward-and-reverse-variational-inference"><i class="fa fa-check"></i><b>8.3.2</b> Forward and reverse variational inference</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="variational-inference.html"><a href="variational-inference.html#the-variational-family-of-distributions"><i class="fa fa-check"></i><b>8.4</b> The variational family of distributions</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="variational-inference.html"><a href="variational-inference.html#mean-field-family"><i class="fa fa-check"></i><b>8.4.1</b> Mean-field family</a></li>
<li class="chapter" data-level="8.4.2" data-path="variational-inference.html"><a href="variational-inference.html#correlation-cannot-be-replicated"><i class="fa fa-check"></i><b>8.4.2</b> Correlation cannot be replicated</a></li>
<li class="chapter" data-level="8.4.3" data-path="variational-inference.html"><a href="variational-inference.html#why-mean-field-is-useful"><i class="fa fa-check"></i><b>8.4.3</b> Why mean-field is useful</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="coordinate-ascent-variational-inference-cavi.html"><a href="coordinate-ascent-variational-inference-cavi.html"><i class="fa fa-check"></i><b>9</b> Coordinate ascent variational inference (CAVI)</a>
<ul>
<li class="chapter" data-level="9.1" data-path="coordinate-ascent-variational-inference-cavi.html"><a href="coordinate-ascent-variational-inference-cavi.html#the-cavi-algorithm"><i class="fa fa-check"></i><b>9.1</b> The CAVI algorithm</a></li>
<li class="chapter" data-level="9.2" data-path="coordinate-ascent-variational-inference-cavi.html"><a href="coordinate-ascent-variational-inference-cavi.html#example-mixture-of-gaussians"><i class="fa fa-check"></i><b>9.2</b> Example: Mixture of Gaussians</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="coordinate-ascent-variational-inference-cavi.html"><a href="coordinate-ascent-variational-inference-cavi.html#the-observation-model"><i class="fa fa-check"></i><b>9.2.1</b> The observation model</a></li>
<li class="chapter" data-level="9.2.2" data-path="coordinate-ascent-variational-inference-cavi.html"><a href="coordinate-ascent-variational-inference-cavi.html#the-prior"><i class="fa fa-check"></i><b>9.2.2</b> The prior</a></li>
<li class="chapter" data-level="9.2.3" data-path="coordinate-ascent-variational-inference-cavi.html"><a href="coordinate-ascent-variational-inference-cavi.html#the-joint-likelihood"><i class="fa fa-check"></i><b>9.2.3</b> The joint likelihood</a></li>
<li class="chapter" data-level="9.2.4" data-path="coordinate-ascent-variational-inference-cavi.html"><a href="coordinate-ascent-variational-inference-cavi.html#the-mean-field-family-approximation"><i class="fa fa-check"></i><b>9.2.4</b> The mean-field family approximation</a></li>
<li class="chapter" data-level="9.2.5" data-path="coordinate-ascent-variational-inference-cavi.html"><a href="coordinate-ascent-variational-inference-cavi.html#cavi"><i class="fa fa-check"></i><b>9.2.5</b> CAVI</a></li>
<li class="chapter" data-level="9.2.6" data-path="coordinate-ascent-variational-inference-cavi.html"><a href="coordinate-ascent-variational-inference-cavi.html#implementing-cavi-in-r"><i class="fa fa-check"></i><b>9.2.6</b> Implementing CAVI in R</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="coordinate-ascent-variational-inference-cavi.html"><a href="coordinate-ascent-variational-inference-cavi.html#comment"><i class="fa fa-check"></i><b>9.3</b> Comment</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">MAS61006 Bayesian Statistics and Computational Methods</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="bayesian-regression-in-stan" class="section level1 hasAnchor" number="4">
<h1><span class="header-section-number">Chapter 4</span> Bayesian regression in Stan<a href="bayesian-regression-in-stan.html#bayesian-regression-in-stan" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<!--- For HTML Only --->
<p><span class="math inline">\(\def \mb{\mathbb}\)</span>
<span class="math inline">\(\def \E{\mb{E}}\)</span>
<span class="math inline">\(\def \P{\mb{P}}\)</span>
<span class="math inline">\(\DeclareMathOperator{\var}{Var}\)</span>
<span class="math inline">\(\DeclareMathOperator{\cov}{Cov}\)</span></p>
<table>
<colgroup>
<col width="100%" />
</colgroup>
<thead>
<tr class="header">
<th>Aims of this chapter</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1. Implement simple linear regression as a Bayesian using Stan.</td>
</tr>
<tr class="even">
<td>2. Explore mixed effects and generalised linear regression using Stan.</td>
</tr>
<tr class="odd">
<td>3. Identify appropriate modelling assumptions for applied examples based on regression.</td>
</tr>
<tr class="even">
<td>4. Interpret model output and compare with maximum likelihood approaches.</td>
</tr>
</tbody>
</table>
<p>In this chapter we’re going to explore how the Bayesian approach to regression inference can be conveniently implemented using Stan. We will start with a simple example, before moving on to demonstrate the potential with a more complex model. The focus here is on gaining familiarity of using Stan and gaining confidence identifying appropriate modelling assumptions for applied scenarios. The balance experiment example that we cover here in these lecture notes is <em>not</em> a ‘blueprint’ for implementing regression in any applied scenario, but rather aims to highlight the types issues we would investigate in a dataset and how we might justify the particular modelling assumptions we implement.</p>
<div id="simple-linear-regression" class="section level2 hasAnchor" number="4.1">
<h2><span class="header-section-number">4.1</span> Simple linear regression<a href="bayesian-regression-in-stan.html#simple-linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This example is a quick application of applying the simple linear regression model to see how we would implement this in Stan, and the Bayesian approach to regression fitting, in a real data setting.</p>
<div id="holiday-hangover-cures" class="section level3 hasAnchor" number="4.1.1">
<h3><span class="header-section-number">4.1.1</span> Holiday hangover cures<a href="bayesian-regression-in-stan.html#holiday-hangover-cures" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <code>hangover</code> dataset (available on the course page, and also see Ben Lambert’s book <em>A Student’s Guide to Bayesian Statistics</em>) contains information from Google Trends with estimates of the search traffic volume for the term <em>hangover cure</em> in the UK each week between February 2012 and January 2016. An extract is shown in Table <a href="bayesian-regression-in-stan.html#tab:hang-data">4.1</a> and displayed in Figure <a href="bayesian-regression-in-stan.html#fig:hang-vol-plot">4.1</a>.</p>
<p>Figure <a href="bayesian-regression-in-stan.html#fig:hang-vol-plot">4.1</a> shows that search traffic varies over the course of a year, fluctuating reasonably evenly around some average. There is, however, a clear increase annually over the Christmas holiday period. Further, this increase occurs gradually as the holiday period is entered and gradually decreases as it is completed.</p>
<p>In relation to this data, we might be interested in estimating what the average annual uplift in searches is during the holiday period (which we will class as 10 December to 7 January). Regression could be a sensible approach to this problem because we have recognised:</p>
<ul>
<li>that there is a ‘base-line’ average level of search traffic (<em>an intercept</em>),</li>
<li>an average increase over the holiday period (<em>an explanatory gradient</em>), and</li>
<li>that there is random variation around this expected level (<em>error/noise</em>).</li>
</ul>
<table>
<caption><span id="tab:hang-data">Table 4.1: </span>Extract of the hangover dataset.</caption>
<thead>
<tr class="header">
<th align="left">date</th>
<th align="right">volume</th>
<th align="right">holiday</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">2012-02-05</td>
<td align="right">18</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="left">2012-02-12</td>
<td align="right">27</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td align="left">2012-02-19</td>
<td align="right">22</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="left">2012-02-26</td>
<td align="right">27</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td align="left">2012-03-04</td>
<td align="right">35</td>
<td align="right">0</td>
</tr>
</tbody>
</table>
<div class="figure"><span style="display:block;" id="fig:hang-vol-plot"></span>
<img src="MAS61006-S2-Notes_files/figure-html/hang-vol-plot-1.png" alt="The hangover dataset. Google search traffic volume for the term hangover cure in the UK between Feb 2012 and Jan 2016. Red dashed lines highlight Christmas day each year and grey bars represent the region between 10 Dec and 7 Jan, classed as the holiday season each year." width="70%" />
<p class="caption">
Figure 4.1: The hangover dataset. Google search traffic volume for the term hangover cure in the UK between Feb 2012 and Jan 2016. Red dashed lines highlight Christmas day each year and grey bars represent the region between 10 Dec and 7 Jan, classed as the holiday season each year.
</p>
</div>
<p>Introducing the holiday period as a variable could be implemented in a number of ways. Here, we encode information about the holiday season as the variable <code>holiday</code>. If the given week is not in the <em>holiday season</em>, then this variable is 0, if part of the week is included then this is given by the appropriate proportion, and if the whole week if included in the holiday then the variable takes the value 1.</p>
<p>A simple linear regression to explore our aim is given by
<span class="math display">\[\begin{equation}
V_i = \beta_0 + \beta_1 h_i + \epsilon_i,
\end{equation}\]</span>
where <span class="math inline">\(V_i\)</span> is the search traffic volume of week <span class="math inline">\(i\)</span>, <span class="math inline">\(h_i\)</span> is the holiday season variable describing the proportion of week <span class="math inline">\(i\)</span> that falls within the holiday period, and <span class="math inline">\(\epsilon_i\sim N(0,\sigma^2)\)</span>. The parameters of this model are interpreted as <span class="math inline">\(\beta_0\)</span> being the average hangover search volume for weeks that are not during the holiday season and <span class="math inline">\(\beta_1\)</span> is the average additional search traffic for a wholly holiday week. The proportional increase, or <em>uplift</em> as we will refer to it, is given by the ratio <span class="math inline">\(\frac{\beta_1}{\beta_0}\)</span>. The parameter <span class="math inline">\(\sigma^2\)</span> describes the weekly variation in traffic volume, after the holiday season expected uplift has been accounted for.</p>
</div>
<div id="least-squares-fit" class="section level3 hasAnchor" number="4.1.2">
<h3><span class="header-section-number">4.1.2</span> Least squares fit<a href="bayesian-regression-in-stan.html#least-squares-fit" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We’ll first fit the model using least squares. Even if the intention is to use a Bayesian approach, it can help to have some results obtained via another method for comparison.</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb28-1"><a href="bayesian-regression-in-stan.html#cb28-1" aria-hidden="true" tabindex="-1"></a>ls_hang <span class="ot">&lt;-</span> <span class="fu">lm</span>(volume <span class="sc">~</span> holiday, <span class="at">data =</span> hangover)</span>
<span id="cb28-2"><a href="bayesian-regression-in-stan.html#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(ls_hang)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = volume ~ holiday, data = hangover)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -38.326  -3.988   0.806   5.012  31.070 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   37.988      0.624   60.88   &lt;2e-16 ***
## holiday       30.942      2.344   13.20   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 8.616 on 207 degrees of freedom
## Multiple R-squared:  0.4571, Adjusted R-squared:  0.4544 
## F-statistic: 174.3 on 1 and 207 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>The least squares estimate would therefore lead to the estimate of the uplift being an 81.5% increase in search traffic during a week in the holiday season.</p>
</div>
<div id="bayesian-approach-in-stan" class="section level3 hasAnchor" number="4.1.3">
<h3><span class="header-section-number">4.1.3</span> Bayesian approach in Stan<a href="bayesian-regression-in-stan.html#bayesian-approach-in-stan" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The most naive implementation of the hangover model in a Bayesian setting is to assume improper, flat priors for the three parameters, <span class="math inline">\(\lbrace \beta_0,\beta_1,\sigma^2\rbrace\)</span>. Under such an assumption, the <code>.stan</code> program code is given by:</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode stan"><code class="sourceCode stan"><span id="cb30-1"><a href="bayesian-regression-in-stan.html#cb30-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-2"><a href="bayesian-regression-in-stan.html#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="kw">data</span> {</span>
<span id="cb30-3"><a href="bayesian-regression-in-stan.html#cb30-3" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span> T ;</span>
<span id="cb30-4"><a href="bayesian-regression-in-stan.html#cb30-4" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[T] V ; <span class="co">// volume</span></span>
<span id="cb30-5"><a href="bayesian-regression-in-stan.html#cb30-5" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[T] h ; <span class="co">// holiday</span></span>
<span id="cb30-6"><a href="bayesian-regression-in-stan.html#cb30-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb30-7"><a href="bayesian-regression-in-stan.html#cb30-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-8"><a href="bayesian-regression-in-stan.html#cb30-8" aria-hidden="true" tabindex="-1"></a><span class="kw">parameters</span> {</span>
<span id="cb30-9"><a href="bayesian-regression-in-stan.html#cb30-9" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span> beta0 ;</span>
<span id="cb30-10"><a href="bayesian-regression-in-stan.html#cb30-10" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span> beta1 ;</span>
<span id="cb30-11"><a href="bayesian-regression-in-stan.html#cb30-11" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span> = <span class="dv">0</span>&gt; sigma ;</span>
<span id="cb30-12"><a href="bayesian-regression-in-stan.html#cb30-12" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb30-13"><a href="bayesian-regression-in-stan.html#cb30-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-14"><a href="bayesian-regression-in-stan.html#cb30-14" aria-hidden="true" tabindex="-1"></a><span class="kw">model</span> {</span>
<span id="cb30-15"><a href="bayesian-regression-in-stan.html#cb30-15" aria-hidden="true" tabindex="-1"></a>  V ~ normal(beta0 + beta1*h, sigma) ; </span>
<span id="cb30-16"><a href="bayesian-regression-in-stan.html#cb30-16" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb30-17"><a href="bayesian-regression-in-stan.html#cb30-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-18"><a href="bayesian-regression-in-stan.html#cb30-18" aria-hidden="true" tabindex="-1"></a><span class="kw">generated quantities</span> {</span>
<span id="cb30-19"><a href="bayesian-regression-in-stan.html#cb30-19" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span> uplift ;</span>
<span id="cb30-20"><a href="bayesian-regression-in-stan.html#cb30-20" aria-hidden="true" tabindex="-1"></a>  uplift = beta1 / beta0 ;</span>
<span id="cb30-21"><a href="bayesian-regression-in-stan.html#cb30-21" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>Note that we’ve used a <code>generated quantities</code> code block to monitor a quantify of interest that is a function of some parameters in the model:
<span class="math display">\[uplift=\frac{\beta_1}{\beta_0}.\]</span>
For each iteration of the sampler, the current samples of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are used to calculate a sample of the uplift. Generated quantities will appear in the parameter summary table. The ability to specify new variables as combinations of our parameters is extremely useful. In the likelihood approach above we can obtain a point estimate of the uplift, based on our maximum likelihood estimates, but quantifying the uncertainty of this is not so straightforward. But by specifying the uplift as a generated quantity, because it is calculated for each sampled pair of the two parameters, we can easily quantify uncertainty.</p>
<p>As in our previous examples, we use the following <code>sample</code> command to implement our HMC sampler. Here we have run two chains to demonstrate the output this produces (an overall summary and separate chain summaries). Each of the two chains ran for 1,000 iterations, with the first half removed as ‘burn-in’.</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="bayesian-regression-in-stan.html#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="co"># set data in a way Stan understands</span></span>
<span id="cb31-2"><a href="bayesian-regression-in-stan.html#cb31-2" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">T =</span> <span class="fu">nrow</span>(hangover), <span class="at">V =</span> hangover<span class="sc">$</span>volume, <span class="at">h =</span> hangover<span class="sc">$</span>holiday) </span>
<span id="cb31-3"><a href="bayesian-regression-in-stan.html#cb31-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-4"><a href="bayesian-regression-in-stan.html#cb31-4" aria-hidden="true" tabindex="-1"></a>sampling_iterations <span class="ot">&lt;-</span> <span class="fl">1e4</span> </span>
<span id="cb31-5"><a href="bayesian-regression-in-stan.html#cb31-5" aria-hidden="true" tabindex="-1"></a>fit_improper <span class="ot">&lt;-</span> <span class="fu">sampling</span>(hangimproper, </span>
<span id="cb31-6"><a href="bayesian-regression-in-stan.html#cb31-6" aria-hidden="true" tabindex="-1"></a>                <span class="at">data =</span> data,</span>
<span id="cb31-7"><a href="bayesian-regression-in-stan.html#cb31-7" aria-hidden="true" tabindex="-1"></a>                <span class="at">chains =</span> <span class="dv">2</span>, </span>
<span id="cb31-8"><a href="bayesian-regression-in-stan.html#cb31-8" aria-hidden="true" tabindex="-1"></a>                <span class="at">iter =</span> sampling_iterations, </span>
<span id="cb31-9"><a href="bayesian-regression-in-stan.html#cb31-9" aria-hidden="true" tabindex="-1"></a>                <span class="at">warmup =</span> sampling_iterations<span class="sc">/</span><span class="dv">2</span>)</span></code></pre></div>
<p>A summary of the results is given:</p>
<pre><code>## $summary
##                mean      se_mean         sd         2.5%          50%
## beta0    37.9906300 0.0071924263 0.63568553   36.7381845   37.9871900
## beta1    30.9596242 0.0261558570 2.36568298   26.3229674   30.9711890
## sigma     8.6744835 0.0049039809 0.43725740    7.8678092    8.6550348
## uplift    0.8154774 0.0007847387 0.06775083    0.6836838    0.8156254
## lp__   -552.9759663 0.0176707715 1.25326271 -556.2108417 -552.6493043
##              97.5%    n_eff      Rhat
## beta0    39.223919 7811.489 0.9998941
## beta1    35.592301 8180.412 0.9998168
## sigma     9.586811 7950.177 0.9999100
## uplift    0.949653 7453.825 0.9998113
## lp__   -551.556051 5030.061 1.0001219
## 
## $c_summary
## , , chains = chain:1
## 
##          stats
## parameter         mean         sd         2.5%          50%        97.5%
##    beta0    37.9910379 0.63401834   36.7517638   37.9813528   39.2262303
##    beta1    30.9513961 2.32965854   26.3433769   30.9348518   35.5377681
##    sigma     8.6772193 0.43215957    7.8834217    8.6585727    9.5637623
##    uplift    0.8152383 0.06671415    0.6862336    0.8147219    0.9491596
##    lp__   -552.9514129 1.21760081 -556.0581414 -552.6439069 -551.5577478
## 
## , , chains = chain:2
## 
##          stats
## parameter         mean         sd         2.5%          50%       97.5%
##    beta0    37.9902221 0.63741151   36.7256590   37.9919937   39.221869
##    beta1    30.9678523 2.40137192   26.2721119   31.0061655   35.647604
##    sigma     8.6717477 0.44232280    7.8596864    8.6514699    9.611406
##    uplift    0.8157165 0.06877774    0.6811207    0.8169106    0.949670
##    lp__   -553.0005197 1.28759129 -556.3285699 -552.6606701 -551.554346</code></pre>
<p>Note that the posterior means of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are similar to the least squares estimates.</p>
<p>The results of this implementation give a posterior mean for the uplift of 81.5% and a 95% credible interval of (68.4%, 95%).</p>
</div>
</div>
<div id="recap-of-logistic-regression" class="section level2 hasAnchor" number="4.2">
<h2><span class="header-section-number">4.2</span> Recap of logistic regression<a href="bayesian-regression-in-stan.html#recap-of-logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>You may have previously encountered generalised linear models (GLMs) in other courses and, in particular, that of logistic regression. Here we will give an overview of logistic regression as we are going to apply this to a more substantial modelling example in this chapter.</p>
<p>The simple linear regression model that you have encountered assumes that
<span class="math display">\[Y_i \sim N(\mu_i, \sigma^2),\]</span>
where
<span class="math display">\[\mu_i = \sum_{j=1}^m \beta_j X_{ij}.\]</span></p>
<p>Logistic regression is used when the dependent variable is not continuous, but is instead a binary variable (such as success and failure encoded as 1 and 0, respectively). The motivation behind this modelling approach is shown in Figure <a href="bayesian-regression-in-stan.html#fig:logistic-motivation">4.2</a>.</p>
<div class="figure"><span style="display:block;" id="fig:logistic-motivation"></span>
<img src="MAS61006-S2-Notes_files/figure-html/logistic-motivation-1.png" alt="Motivation for using logistic regression. If our response variable is binary, simple linear regression (left) is problematic because predictions can exceed the (0,1) range. Alternatively, logistic regression (right) only produces predictions within the valid range." width="70%" />
<p class="caption">
Figure 4.2: Motivation for using logistic regression. If our response variable is binary, simple linear regression (left) is problematic because predictions can exceed the (0,1) range. Alternatively, logistic regression (right) only produces predictions within the valid range.
</p>
</div>
<div class="result">
<p>To implement logistic regression we model the binary outcome as a Bernoulli response,
<span class="math display">\[Y_i \sim Bernoulli(\mu_i),\]</span>
where the probability of success is defined in terms of the explanatory variables as
<span class="math display">\[\mu_i=\P(Y_i=1)=\frac{e^{\sum_{j=1}^m \beta_j X_{ij}}}{1+e^{\sum_{j=1}^m \beta_j X_{ij}}}.\]</span></p>
</div>
<p>You can see that although we assume a different distribution for the response variable <span class="math inline">\(Y\)</span> in logistic regression, we still retain a similar process of expressing the parameter of the distribution as a linear combination of the variables <span class="math inline">\(\lbrace X_j \rbrace\)</span>. The reason that the expression above differs to the expression for <span class="math inline">\(\mu\)</span> in simple linear regression is because this now represents a probability and so is constrained between 0 and 1. Explicitly, we have applied a <strong>log-odds</strong>/<strong>logit</strong> transformation to <span class="math inline">\(\mu\)</span>, given by
<span class="math display">\[\text{logit}(\mu) = \log \left( \frac{\mu}{1-\mu}\right),\]</span>
and defined the regression model as
<span class="math display">\[\text{logit}(\mu_i)=\sum_{j=1}^m \beta_j X_{ij}.\]</span>
The most common method of estimating the <span class="math inline">\(\beta\)</span> coefficients in a logistic regression model is based on maximum likelihood, using iteratively reweighted least squares. We will not cover the details of this during this course, but you may encounter this in your other courses. To implement model fitting in <code>R</code>, we use the <code>glm</code> package, where the <code>glm</code> function uses the syntax you are familiar with from implementing simple linear regression with <code>lm</code>.</p>
</div>
<div id="overview-of-mixed-effects" class="section level2 hasAnchor" number="4.3">
<h2><span class="header-section-number">4.3</span> Overview of mixed effects<a href="bayesian-regression-in-stan.html#overview-of-mixed-effects" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A random effects model (or multilevel, hierarchical, mixed effects) is an extension to the regression theory you have previously met. In the simple linear regression model, a key assumption is that the observations <span class="math inline">\(Y_i\)</span>, for <span class="math inline">\(i=1,\ldots,n\)</span> are independent from one another, with the joint distribution being
<span class="math display">\[\mathbf{Y}\sim N_n(\mathbf{X}\boldsymbol{\beta}, \sigma^2I_n),\]</span>
where <span class="math inline">\(I_n\)</span> is the identity matrix. The vector of coefficients, <span class="math inline">\(\boldsymbol{\beta}\)</span>, is referred to as the coefficients of the <em>fixed effects</em>. The variation that we observe between the observations <span class="math inline">\(Y_i\)</span> arises from the fixed structure defined by <span class="math inline">\(\mathbf{X}\boldsymbol{\beta}\)</span> and the error associated with the shared variance <span class="math inline">\(\sigma^2\)</span>.</p>
<p>A common situation, however, is that observations arise in groups that are correlated, such as blocked group experiments or repeated measurements. The main idea of random effects models is recognising that the variables defining the grouping structure should be treated differently to how we would usually treat explanatory variables. We do this by introducing an additional source of variation that is group-specific. We can think of this as missing information on the groupings that, if it were available, would be included in the statistical model.</p>
<div class="result">
<p>We can express a general mixed effects model as
<span class="math display">\[\mathbf{Y}=\mathbf{X}\boldsymbol{\beta}+\mathbf{Z}\boldsymbol{\gamma}+\boldsymbol{\epsilon},\]</span>
where:</p>
<ul>
<li><span class="math inline">\(\mathbf{X}\boldsymbol{\beta}\)</span> is defined as previously, with a design matrix <span class="math inline">\(\mathbf{X}\)</span> based on independent variables, and a vector of coefficients that are to be estimated <span class="math inline">\(\boldsymbol{\beta}\)</span>,</li>
<li><span class="math inline">\(\mathbf{Z}\boldsymbol{\gamma}\)</span> consists of another design matrix <span class="math inline">\(\mathbf{Z}\)</span> that describes the grouping structure, and a vector of random effects <span class="math inline">\(\boldsymbol{\gamma}\sim N(\mathbf{0},G)\)</span>, and</li>
<li><span class="math inline">\(\boldsymbol{\epsilon}\sim N(\mathbf{0},\sigma^2I)\)</span> is the error, as defined previously.</li>
</ul>
</div>
<div id="simple-example-of-a-mixed-effects-model" class="section level3 hasAnchor" number="4.3.1">
<h3><span class="header-section-number">4.3.1</span> Simple example of a mixed effects model<a href="bayesian-regression-in-stan.html#simple-example-of-a-mixed-effects-model" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let’s consider a simple example. A set of observations correspond to repeated measures on the same group of subjects, so that we have observations <span class="math inline">\(Y_{ij}\)</span> representing the <span class="math inline">\(j^\text{th}\)</span> response from the <span class="math inline">\(i^\text{th}\)</span> subject, for <span class="math inline">\(j=1,\ldots,k\)</span> and <span class="math inline">\(i=1,\ldots,n\)</span>. Additionally we have observations of a single explanatory variable <span class="math inline">\(X_{ij}\)</span> corresponding to the response. We could propose the model
<span class="math display">\[Y_{ij}=\beta_0+\beta_1X_{ij}+\gamma_i+\epsilon_{ij},\]</span>
so that there is a subject-specific random effect <span class="math inline">\(\gamma_i\sim N(0,\sigma_\gamma^2)\)</span> that represents the assumption that observations from the same individual share some commonality, but that individuals differ from one another, having arisen from some common population that has a variation of <span class="math inline">\(\sigma_\gamma^2\)</span>. The within-group variation is then given by the residuals <span class="math inline">\(\epsilon_{ij}\sim N(0,\sigma^2)\)</span>. We note that, as we do not observe the random effects <span class="math inline">\(\gamma_i\)</span>, this random effects model has the following features:</p>
<ul>
<li>a general observation has the distribution <span class="math inline">\(Y_{ij}\sim N(\beta_0+\beta_1X_{ij},\sigma_\gamma^2+\sigma^2)\)</span>,</li>
<li>observations from the same individual, say <span class="math inline">\(Y_{i1}\)</span> and <span class="math inline">\(Y_{i2}\)</span>, have covariance <span class="math inline">\(\sigma_\gamma^2\)</span>, and</li>
<li>observations from different individuals, say <span class="math inline">\(Y_{11}\)</span> and <span class="math inline">\(Y_{32}\)</span>, are independent.</li>
</ul>
<p>The above model differs from including a fixed effect that is a factor variable for the individual, with coefficient <span class="math inline">\(\gamma_i\)</span>. In that scenario the observations from the same individual would be uncorrelated. The results of model fitting would produce estimates of the effect of each subject in the experiment, but we would not have any information about the general population to predict anything about a <em>new</em> individual we have not seen.</p>
</div>
<div id="fixed-or-random-effects" class="section level3 hasAnchor" number="4.3.2">
<h3><span class="header-section-number">4.3.2</span> Fixed or random effects?<a href="bayesian-regression-in-stan.html#fixed-or-random-effects" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>This will depend on the aims of your analysis. If you’re interested in learning about the specific subjects/groups that you observed in your data, you want the fixed effects estimates of the groups. If you’re interested in learning about the general population that your subjects were sampled from, you want the random effect variation estimate. Statistical efficiency may also play a part in your decision making: if you have a large number of groups, then a fixed effects treatment involves many more parameters than the random effects counterpart.</p>
</div>
</div>
<div id="mixed-effect-logistic-regression" class="section level2 hasAnchor" number="4.4">
<h2><span class="header-section-number">4.4</span> Mixed effect logistic regression<a href="bayesian-regression-in-stan.html#mixed-effect-logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this section we will be using some experimental data to explore implementing mixed effect logistic regression. We are primarily interested in implementing this approach in a Bayesian framework, using Stan, but as a comparison we will also be implementing more traditional maximum likelihood approaches. As with any regression modeling application, we will be considering how the various independent variables that we have information on affect the dependent variable, which includes asking whether these variables have <em>any</em> significant effect upon the observed balance variable.</p>
<div id="balance-experiment" class="section level3 hasAnchor" number="4.4.1">
<h3><span class="header-section-number">4.4.1</span> Balance experiment<a href="bayesian-regression-in-stan.html#balance-experiment" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <code>balance</code> dataset, presented in Faraway (2016) contains the results of an experiments on the effects of surface and vision on balance. More details about the sources and an exploratory data analysis are available <a href="https://jeremy-oakley.staff.shef.ac.uk/mas61004/EDAtutorial/eda-for-logistic-regression.html#ref-Faraway2016">in these MAS61004 notes</a>, but we will also perform an EDA here.</p>
<p>Balance was assessed qualitatively based on observation by the experimenter. This dependent variable, <code>CTSIB</code>, was on a four point ordinal scale, with 1 meaning ‘stable’ and 4 meaning ‘unstable’. Faraway converted this to a binary variable called <code>Stable</code>, where:</p>
<ul>
<li>1 represents the original level 1 (stable), and</li>
<li>0 represents the original levels 2-4 (varying degrees of instability).</li>
</ul>
<p>The balance of subjects were observed for:</p>
<ul>
<li><code>Surface</code>: two different surfaces (foam or normal), and</li>
<li><code>Vision</code>: restricted and unrestricted vision (eyes closed, eyes open or with a dome placed over the head).</li>
</ul>
<p>Forty subjects were studied (<code>Subject</code>), twenty males and twenty females (<code>Sex</code>), ranging in age from 18 to 38 (<code>Age</code>), with heights given in <em>cm</em> (<code>Height</code>) and weights in <em>kg</em> (<code>Weight</code>). Each subject was tested twice in each of the surface (two) and eye combinations (three), which results in a total of 12 measures per subject.</p>
<p>We will centre the <code>Age</code>, <code>Height</code> and <code>Weight</code> variables by subtracting the mean and dividing by the range. This can help with numerical computation, by weakening dependence between model parameters.</p>
<p>This study design is known as a <strong>full factorial design</strong> between subject, surface and vision with two replicates.</p>
<p>In addition to the observed dependent variables listed above, we also add a final variable (<code>Dummy</code>) which is an independent variable with no effect on the response. These are independent draws from a standard normal. We therefore have a variable that we <em>know</em> is not linked to the balance of individuals. This will be useful for considering whether other dependent variables display useful predictive information.</p>
<p>The data are constructed with the following code.</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="bayesian-regression-in-stan.html#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb33-2"><a href="bayesian-regression-in-stan.html#cb33-2" aria-hidden="true" tabindex="-1"></a>balance <span class="ot">&lt;-</span> faraway<span class="sc">::</span>ctsib <span class="sc">%&gt;%</span></span>
<span id="cb33-3"><a href="bayesian-regression-in-stan.html#cb33-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">Stable =</span> <span class="fu">ifelse</span>(CTSIB <span class="sc">==</span> <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>),</span>
<span id="cb33-4"><a href="bayesian-regression-in-stan.html#cb33-4" aria-hidden="true" tabindex="-1"></a>         <span class="at">Subject =</span> <span class="fu">factor</span>(Subject),</span>
<span id="cb33-5"><a href="bayesian-regression-in-stan.html#cb33-5" aria-hidden="true" tabindex="-1"></a>         <span class="at">Age =</span> (Age <span class="sc">-</span> <span class="fu">mean</span>(Age)) <span class="sc">/</span> <span class="fu">diff</span>(<span class="fu">range</span>(Age)),</span>
<span id="cb33-6"><a href="bayesian-regression-in-stan.html#cb33-6" aria-hidden="true" tabindex="-1"></a>         <span class="at">Weight =</span> (Weight <span class="sc">-</span> <span class="fu">mean</span>(Weight)) <span class="sc">/</span> <span class="fu">diff</span>(<span class="fu">range</span>(Weight)),</span>
<span id="cb33-7"><a href="bayesian-regression-in-stan.html#cb33-7" aria-hidden="true" tabindex="-1"></a>         <span class="at">Height =</span> (Height <span class="sc">-</span> <span class="fu">mean</span>(Height)) <span class="sc">/</span> <span class="fu">diff</span>(<span class="fu">range</span>(Height)), </span>
<span id="cb33-8"><a href="bayesian-regression-in-stan.html#cb33-8" aria-hidden="true" tabindex="-1"></a>         <span class="at">Dummy =</span> <span class="fu">round</span>(<span class="fu">rnorm</span>(<span class="dv">480</span>), <span class="dv">1</span>))</span></code></pre></div>
<p>The full factorial experimental design/set-up is displayed in Table <a href="bayesian-regression-in-stan.html#tab:sub1">4.2</a>, which includes all observations for <code>subject 1</code>. The full data frame also contains the information on sex, age, height and weight for subject 1, along with that of the 39 other subjects. An extract of the data frame is shown in Table <a href="bayesian-regression-in-stan.html#tab:bal-data">4.3</a>.</p>
<table>
<caption><span id="tab:sub1">Table 4.2: </span>Extract of the balance dataset showing the experimental design of subject, suface and vision. This includes all observations for subject 1.</caption>
<thead>
<tr class="header">
<th align="left">Surface</th>
<th align="left">Vision</th>
<th align="right">CTSIB</th>
<th align="right">Stable</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">norm</td>
<td align="left">open</td>
<td align="right">1</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td align="left">norm</td>
<td align="left">open</td>
<td align="right">1</td>
<td align="right">1</td>
</tr>
<tr class="odd">
<td align="left">norm</td>
<td align="left">closed</td>
<td align="right">2</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="left">norm</td>
<td align="left">closed</td>
<td align="right">2</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td align="left">norm</td>
<td align="left">dome</td>
<td align="right">1</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td align="left">norm</td>
<td align="left">dome</td>
<td align="right">2</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td align="left">foam</td>
<td align="left">open</td>
<td align="right">2</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="left">foam</td>
<td align="left">open</td>
<td align="right">2</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td align="left">foam</td>
<td align="left">closed</td>
<td align="right">2</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="left">foam</td>
<td align="left">closed</td>
<td align="right">2</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td align="left">foam</td>
<td align="left">dome</td>
<td align="right">2</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="left">foam</td>
<td align="left">dome</td>
<td align="right">2</td>
<td align="right">0</td>
</tr>
</tbody>
</table>
<table>
<caption><span id="tab:bal-data">Table 4.3: </span>Extract of the balance dataset showing all variables that will be considered in our regression analysis.</caption>
<colgroup>
<col width="11%" />
<col width="11%" />
<col width="10%" />
<col width="7%" />
<col width="8%" />
<col width="14%" />
<col width="16%" />
<col width="8%" />
<col width="10%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Subject</th>
<th align="left">Surface</th>
<th align="left">Vision</th>
<th align="left">Sex</th>
<th align="right">Age</th>
<th align="right">Height</th>
<th align="right">Weight</th>
<th align="right">Dummy</th>
<th align="right">Stable</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">1</td>
<td align="left">norm</td>
<td align="left">open</td>
<td align="left">male</td>
<td align="right">-0.24</td>
<td align="right">0.0822917</td>
<td align="right">-0.0506014</td>
<td align="right">-0.6</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td align="left">1</td>
<td align="left">norm</td>
<td align="left">open</td>
<td align="left">male</td>
<td align="right">-0.24</td>
<td align="right">0.0822917</td>
<td align="right">-0.0506014</td>
<td align="right">-0.2</td>
<td align="right">1</td>
</tr>
<tr class="odd">
<td align="left">1</td>
<td align="left">norm</td>
<td align="left">closed</td>
<td align="left">male</td>
<td align="right">-0.24</td>
<td align="right">0.0822917</td>
<td align="right">-0.0506014</td>
<td align="right">1.6</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="left">1</td>
<td align="left">norm</td>
<td align="left">closed</td>
<td align="left">male</td>
<td align="right">-0.24</td>
<td align="right">0.0822917</td>
<td align="right">-0.0506014</td>
<td align="right">0.1</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td align="left">1</td>
<td align="left">norm</td>
<td align="left">dome</td>
<td align="left">male</td>
<td align="right">-0.24</td>
<td align="right">0.0822917</td>
<td align="right">-0.0506014</td>
<td align="right">0.1</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td align="left">1</td>
<td align="left">norm</td>
<td align="left">dome</td>
<td align="left">male</td>
<td align="right">-0.24</td>
<td align="right">0.0822917</td>
<td align="right">-0.0506014</td>
<td align="right">1.7</td>
<td align="right">0</td>
</tr>
</tbody>
</table>
<p>The response variable for this experiment is binary, as we have <em>stable</em> or <em>unstable</em> observations. It is therefore sensible to model this using logistic regression. This is our first modelling decision.</p>
<p>There are multiple explanatory variables that could be incorporated in such a model, and these could feature as fixed or random effects. Determining the appropriateness of variables as fixed or random effects is an important skill. In the following we carry out some exploratory data analysis to explore the appropriateness of some modelling decisions we could implement. For clarity, we will refer to the variables as:</p>
<ul>
<li><code>Surface</code> and <code>Vision</code> as <em>design variables</em>. Note that these are both factor variables.</li>
<li><code>Sex</code>, <code>Age</code>, <code>Height</code>, <code>Weight</code> as <em>covariates</em>. Note that <code>Sex</code> is a factor variable.</li>
<li><code>Subject</code> as the <em>grouping variable</em>. Note that this is a factor variable.</li>
</ul>
</div>
<div id="exploratory-data-analysis" class="section level3 hasAnchor" number="4.4.2">
<h3><span class="header-section-number">4.4.2</span> Exploratory data analysis<a href="bayesian-regression-in-stan.html#exploratory-data-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="considering-the-grouping-variable-as-a-fixed-or-random-effect" class="section level4 hasAnchor" number="4.4.2.1">
<h4><span class="header-section-number">4.4.2.1</span> Considering the grouping variable as a fixed or random effect<a href="bayesian-regression-in-stan.html#considering-the-grouping-variable-as-a-fixed-or-random-effect" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Let’s consider the variable <code>Subject</code>, which is a factor variable of which we have observed 40 categories of. By applying logistic regression we are modeling the probability of stability, given some combination of observed variables. If a given subject was only ever observed to give the same outcome (say all unstable) then this would cause problems if <code>Subject</code> was included as a fixed effect term, because the probability of stability would be 0 for that subject. For any data application you are working on, this is an important aspect of your data that you should check in the exploratory analysis stage.</p>
<p>Considering the <code>balance</code> data, we can compute the proportion of stable outcomes for the 12 observations of each subject, and tabulate how many subjects have each possible proportion. This is given in Table <a href="bayesian-regression-in-stan.html#tab:grouping">4.4</a>. From this we see that five subjects were unstable for all observations. We therefore cannot include a subject-specific fixed effect in our model for this experiment. If we would like to incorporate the concept of correlation between repeated measurements on the same subject, we can make a decision to include subject as a random effect.</p>
<table>
<caption><span id="tab:grouping">Table 4.4: </span>The number of subjects with the range of possible mean stability scores. The mean stability for a subject is calculated as the proportion of their 12 binary stability observations that were classed as stable.</caption>
<thead>
<tr class="header">
<th align="right">propStable</th>
<th align="right">n</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0.0000000</td>
<td align="right">5</td>
</tr>
<tr class="even">
<td align="right">0.0833333</td>
<td align="right">4</td>
</tr>
<tr class="odd">
<td align="right">0.1666667</td>
<td align="right">15</td>
</tr>
<tr class="even">
<td align="right">0.2500000</td>
<td align="right">3</td>
</tr>
<tr class="odd">
<td align="right">0.3333333</td>
<td align="right">4</td>
</tr>
<tr class="even">
<td align="right">0.4166667</td>
<td align="right">3</td>
</tr>
<tr class="odd">
<td align="right">0.5000000</td>
<td align="right">4</td>
</tr>
<tr class="even">
<td align="right">0.6666667</td>
<td align="right">2</td>
</tr>
</tbody>
</table>
</div>
<div id="considering-the-design-variables-effect" class="section level4 hasAnchor" number="4.4.2.2">
<h4><span class="header-section-number">4.4.2.2</span> Considering the design variables effect<a href="bayesian-regression-in-stan.html#considering-the-design-variables-effect" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Another set of factor variables we would be interested in including in our balance model are the <code>Surface</code> and <code>Vision</code> variables, with two and three categories, respectively. These were the experimental design variables and so are of particular importance in determining their effect on balance.</p>
<p>We produce contingency tables for number of observations that were stable/unstable at the different levels of the design variables:</p>
<pre><code>##       Surface
## Stable foam norm
##      0  230  136
##      1   10  104</code></pre>
<pre><code>##       Vision
## Stable closed dome open
##      0    143  138   85
##      1     17   22   75</code></pre>
<p>This shows that there is a mixture of stability observations for the two categories of surface, and similarly for the three categories of vision. These contingency tables also highlight that there does appear to be an effect on balance between the categories. We could consider including both <code>Surface</code> and <code>Vision</code> are fixed effects in our regression.</p>
<p>Note, however, if we look at the combinations of these variables together we see in Table <a href="bayesian-regression-in-stan.html#tab:design-var">4.5</a> the proportions of stability. Here we see that two combinations of surface and vision were only ever observed to be unstable. We therefore would find complications if we attempt to include a fixed effects <strong>interaction term</strong> between <code>Surface</code> and <code>Vision</code>. Our initial model will therefore feature both the design variables as fixed effects, but will not feature the possible interaction term.</p>
<table>
<caption><span id="tab:design-var">Table 4.5: </span>The proportion of stable observations observed, given the combinations of surface and vision variables.</caption>
<thead>
<tr class="header">
<th align="left">Surface</th>
<th align="left">Vision</th>
<th align="right">propStable</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">foam</td>
<td align="left">closed</td>
<td align="right">0.0000</td>
</tr>
<tr class="even">
<td align="left">foam</td>
<td align="left">dome</td>
<td align="right">0.0000</td>
</tr>
<tr class="odd">
<td align="left">foam</td>
<td align="left">open</td>
<td align="right">0.1250</td>
</tr>
<tr class="even">
<td align="left">norm</td>
<td align="left">closed</td>
<td align="right">0.2125</td>
</tr>
<tr class="odd">
<td align="left">norm</td>
<td align="left">dome</td>
<td align="right">0.2750</td>
</tr>
<tr class="even">
<td align="left">norm</td>
<td align="left">open</td>
<td align="right">0.8125</td>
</tr>
</tbody>
</table>
</div>
<div id="considering-the-covariates" class="section level4 hasAnchor" number="4.4.2.3">
<h4><span class="header-section-number">4.4.2.3</span> Considering the covariates<a href="bayesian-regression-in-stan.html#considering-the-covariates" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The remaining variables in our data are the covariates <code>Sex</code>, <code>Age</code>, <code>Height</code>, <code>Weight</code>, and our dummy variable <code>Dummy</code>. Before deciding on our proposed regression model, we can produce exploratory plots of whether the probability of stability is affected by each variable. This exploratory analysis is important, because it could highlight important issues with our observations that were otherwise would not be aware of. It may also be the case that we have a very large number of covariates in an application, and including all of them in a model may not be appropriate.</p>
<p>In Figure <a href="bayesian-regression-in-stan.html#fig:sex-stability">4.3</a>, we have calculated the proportion of stable observations for each subject, and plotted this against <code>Sex</code>. This suggests that this variable has some affect on stability and would be an appropriate fixed effect choice for inclusion in our model. Remember that our fitted model may still indicate that this effect is not significant.</p>
<div class="figure"><span style="display:block;" id="fig:sex-stability"></span>
<img src="MAS61006-S2-Notes_files/figure-html/sex-stability-1.png" alt="The proportion of stable observations out of the 12 carried out for each subject, split by the sex variable." width="40%" />
<p class="caption">
Figure 4.3: The proportion of stable observations out of the 12 carried out for each subject, split by the sex variable.
</p>
</div>
<p>In Figure <a href="bayesian-regression-in-stan.html#fig:explan-boxp">4.4</a>, we have plotted the age, weight, height and dummy variables, split by sex and stability. These plots are useful for considering the effects of these four covariates on stability, but also their interaction with the sex covariate. We can see that there is a fairly clear separation between the two sexes in terms of the height and weight of individuals (males are taller and heavier). Let’s explore this a little more with pairs plots in Figure <a href="bayesian-regression-in-stan.html#fig:height-weight">4.5</a>. The separation of the two sexes is obvious, as is the high correlation between height and weight. This structure in the data is useful to be aware of, and suggests that it may not be necessary (or even useful) to include both height and weight as variables in our model.</p>
<p>When considering the possible effects of the covariates on stability in Figure <a href="bayesian-regression-in-stan.html#fig:explan-boxp">4.4</a> it is useful to have our dummy variable. We know that this variable is independent of stability, and so when comparing the stable/unstable box plot distributions we see patterns that can only have arisen by chance. This provides some perspective for considering whether the remaining three covariates in this plot also display differences in stability through random chance or an actual effect. These plots suggest that age, weight and height may not be at all important in determining stability.</p>
<div class="figure"><span style="display:block;" id="fig:explan-boxp"></span>
<img src="MAS61006-S2-Notes_files/figure-html/explan-boxp-1.png" alt="The height/weight/age/dummy variable of individuals split by sex and stability." width="80%" />
<p class="caption">
Figure 4.4: The height/weight/age/dummy variable of individuals split by sex and stability.
</p>
</div>
<div class="figure"><span style="display:block;" id="fig:height-weight"></span>
<img src="MAS61006-S2-Notes_files/figure-html/height-weight-1.png" alt="The combinations of sex, height and weight variables for the balance experiment." width="80%" />
<p class="caption">
Figure 4.5: The combinations of sex, height and weight variables for the balance experiment.
</p>
</div>
</div>
</div>
<div id="maximum-likelihood-approach" class="section level3 hasAnchor" number="4.4.3">
<h3><span class="header-section-number">4.4.3</span> Maximum likelihood approach<a href="bayesian-regression-in-stan.html#maximum-likelihood-approach" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="ordinary-logistic-regression-no-grouping-variable" class="section level4 hasAnchor" number="4.4.3.1">
<h4><span class="header-section-number">4.4.3.1</span> Ordinary logistic regression (no grouping variable)<a href="bayesian-regression-in-stan.html#ordinary-logistic-regression-no-grouping-variable" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>In the above we found that we cannot include <code>Subject</code> as a fixed effect for this experiment. We would like to include <code>Subject</code> as a random effect to take into account the variability between individuals. However, first for simplicity, we will implement the model without this feature. We are therefore modeling stability as a logistic regression, with fixed effects for the design variables and the covariates. Let’s set this up with formal notation.</p>
<div class="result">
<p>Define <span class="math inline">\(Y_{ijkl}\)</span> to be the stability response for subject <span class="math inline">\(i=1,\ldots,12\)</span>, surface <span class="math inline">\(j=1,2\)</span>, vision setting <span class="math inline">\(k=1,2,3\)</span>, and replicate <span class="math inline">\(l=1,2\)</span>. We have
<span class="math display">\[Y_{ijkl} \sim Bernoulli(\mu_{ijkl}),\]</span>
and we suppose
<span class="math display">\[\text{logit}(\mu_{ijkl}) = \beta_0 + \alpha_j + \gamma_k + \beta_1 x_{1,i} + \beta_2 x_{2,i} + \beta_3 x_{3,i} + \beta_4 x_{4,i} +  \beta_5 x_{5,ijkl},\]</span>
where</p>
<ul>
<li><span class="math inline">\(\alpha_j\)</span> is the additive contribution of the surface factor, with <span class="math inline">\(\alpha_1=0\)</span>,</li>
<li><span class="math inline">\(\gamma_k\)</span> is the additive contribution of the vision factor, with <span class="math inline">\(\gamma_1=0\)</span>,</li>
<li><span class="math inline">\(\beta_1\)</span> is the additive contribution of the sex factor where <span class="math inline">\(x_{1, i}\)</span> is the sex of subject <span class="math inline">\(i\)</span>, with <span class="math inline">\(x_{1, i}=1\)</span> for a male and 0 for a female,</li>
<li><span class="math inline">\(x_{2,i}, x_{3,i}, x_{4,i}\)</span> are the corresponding age, height, weight covariates of subject <span class="math inline">\(i\)</span>, and</li>
<li><span class="math inline">\(x_{5,ijkl}\)</span> is the dummy variable.</li>
</ul>
</div>
<p>Note that we have not included the subject as an effect here, and that the subscript <span class="math inline">\(i\)</span> is needed only to identify the subject’s covariate values.</p>
<div class="exbox">
<p><strong>Exercise:</strong><br />
Spend some time making sure you understand the interpretation of the coefficients in this model definition. For example, what is the significance of <span class="math inline">\(\beta_0\)</span>? Or what combination of coefficients produces the expected probability of stability for a 20 year old female with restricted vision?</p>
</div>
<p>Our <code>glm</code> model definition and summary is given here:</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="bayesian-regression-in-stan.html#cb36-1" aria-hidden="true" tabindex="-1"></a>glmNoSubject <span class="ot">&lt;-</span></span>
<span id="cb36-2"><a href="bayesian-regression-in-stan.html#cb36-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">glm</span>(Stable <span class="sc">~</span> Surface <span class="sc">+</span> Vision <span class="sc">+</span> </span>
<span id="cb36-3"><a href="bayesian-regression-in-stan.html#cb36-3" aria-hidden="true" tabindex="-1"></a>        Sex <span class="sc">+</span> </span>
<span id="cb36-4"><a href="bayesian-regression-in-stan.html#cb36-4" aria-hidden="true" tabindex="-1"></a>        Age <span class="sc">+</span> Height <span class="sc">+</span> Weight <span class="sc">+</span> Dummy, </span>
<span id="cb36-5"><a href="bayesian-regression-in-stan.html#cb36-5" aria-hidden="true" tabindex="-1"></a>      <span class="at">family =</span> binomial, </span>
<span id="cb36-6"><a href="bayesian-regression-in-stan.html#cb36-6" aria-hidden="true" tabindex="-1"></a>      <span class="at">data =</span> balance)</span>
<span id="cb36-7"><a href="bayesian-regression-in-stan.html#cb36-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-8"><a href="bayesian-regression-in-stan.html#cb36-8" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(glmNoSubject)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = Stable ~ Surface + Vision + Sex + Age + Height + 
##     Weight + Dummy, family = binomial, data = balance)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -2.41266  -0.51936  -0.13654  -0.04721   2.62161  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -6.29989    0.65959  -9.551  &lt; 2e-16 ***
## Surfacenorm  4.05639    0.45701   8.876  &lt; 2e-16 ***
## Visiondome   0.38653    0.38537   1.003 0.315851    
## Visionopen   3.26062    0.42425   7.686 1.52e-14 ***
## Sexmale      1.46294    0.52152   2.805 0.005029 ** 
## Age          0.07167    0.48681   0.147 0.882950    
## Height      -4.86326    1.31178  -3.707 0.000209 ***
## Weight       2.71654    1.05972   2.563 0.010364 *  
## Dummy        0.28974    0.16266   1.781 0.074881 .  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 526.25  on 479  degrees of freedom
## Residual deviance: 291.96  on 471  degrees of freedom
## AIC: 309.96
## 
## Number of Fisher Scoring iterations: 6</code></pre>
<div class="exbox">
<p><strong>Exercise:</strong><br />
Spend some time interpreting this model output. What do these results indicate about the three levels of the <code>Vision</code> variable? Do these results agree with your understanding of the data given our exploratory data analysis? What conclusions would you report in a summary to a client if you were analysing this data for them?</p>
</div>
</div>
<div id="random-effects-including-the-grouping-variable" class="section level4 hasAnchor" number="4.4.3.2">
<h4><span class="header-section-number">4.4.3.2</span> Random effects (including the grouping variable)<a href="bayesian-regression-in-stan.html#random-effects-including-the-grouping-variable" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Will will extend the previous model to include a random effect for each of the 12 different subjects.</p>
<div class="result">
<p>The full model is now
<span class="math display">\[\text{logit}(\mu_{ijkl}) = \beta_0 + \alpha_j + \gamma_k + \beta_1 x_{1,i} + \beta_2 x_{2,i} + \beta_3 x_{3,i} + \beta_4 x_{4,i} +  \beta_5 x_{5,ijkl} + b_i,\]</span>
with the extra random effects term <span class="math inline">\(b_i \sim N(0,\sigma^2_b)\)</span>.</p>
</div>
<p>Let’s fit this random effects GLM with the <code>lme4</code> package:</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb38-1"><a href="bayesian-regression-in-stan.html#cb38-1" aria-hidden="true" tabindex="-1"></a>glmSubjectRandom <span class="ot">&lt;-</span> </span>
<span id="cb38-2"><a href="bayesian-regression-in-stan.html#cb38-2" aria-hidden="true" tabindex="-1"></a>  lme4<span class="sc">::</span><span class="fu">glmer</span>(Stable <span class="sc">~</span> Surface <span class="sc">+</span> Vision <span class="sc">+</span> </span>
<span id="cb38-3"><a href="bayesian-regression-in-stan.html#cb38-3" aria-hidden="true" tabindex="-1"></a>                Sex <span class="sc">+</span> </span>
<span id="cb38-4"><a href="bayesian-regression-in-stan.html#cb38-4" aria-hidden="true" tabindex="-1"></a>                Age <span class="sc">+</span> Height <span class="sc">+</span> Weight <span class="sc">+</span> Dummy <span class="sc">+</span> </span>
<span id="cb38-5"><a href="bayesian-regression-in-stan.html#cb38-5" aria-hidden="true" tabindex="-1"></a>                (<span class="dv">1</span> <span class="sc">|</span> Subject),</span>
<span id="cb38-6"><a href="bayesian-regression-in-stan.html#cb38-6" aria-hidden="true" tabindex="-1"></a>              <span class="at">family =</span> binomial, </span>
<span id="cb38-7"><a href="bayesian-regression-in-stan.html#cb38-7" aria-hidden="true" tabindex="-1"></a>              <span class="at">data =</span> balance, </span>
<span id="cb38-8"><a href="bayesian-regression-in-stan.html#cb38-8" aria-hidden="true" tabindex="-1"></a>              <span class="at">nAGQ =</span> <span class="dv">25</span>)</span></code></pre></div>
<pre><code>## Warning in checkConv(attr(opt, &quot;derivs&quot;), opt$par, ctrl = control$checkConv, :
## Model failed to converge with max|grad| = 0.320305 (tol = 0.002, component 1)</code></pre>
<p>We get a message that the method for model fitting failed to converge! It’s important to remember that there is not a closed form expression for the maximum likelihood estimates in mixed effects generalised linear models, and numerical methods must be used. This means that a <em>solution</em> is not always viable.</p>
<p>Let’s return to our exploratory data analysis, and recall how we suspected little information was being gained by our collection of covariates, and in particular, that height and weight were highly correlated. So we’re going to proceed by removing the <code>Height</code> variable.</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="bayesian-regression-in-stan.html#cb40-1" aria-hidden="true" tabindex="-1"></a>glmSubjectRandom <span class="ot">&lt;-</span> </span>
<span id="cb40-2"><a href="bayesian-regression-in-stan.html#cb40-2" aria-hidden="true" tabindex="-1"></a>  lme4<span class="sc">::</span><span class="fu">glmer</span>(Stable <span class="sc">~</span> Surface <span class="sc">+</span> Vision <span class="sc">+</span> </span>
<span id="cb40-3"><a href="bayesian-regression-in-stan.html#cb40-3" aria-hidden="true" tabindex="-1"></a>                Sex <span class="sc">+</span> </span>
<span id="cb40-4"><a href="bayesian-regression-in-stan.html#cb40-4" aria-hidden="true" tabindex="-1"></a>                Age <span class="sc">+</span> Weight <span class="sc">+</span> Dummy <span class="sc">+</span> </span>
<span id="cb40-5"><a href="bayesian-regression-in-stan.html#cb40-5" aria-hidden="true" tabindex="-1"></a>                (<span class="dv">1</span> <span class="sc">|</span> Subject),</span>
<span id="cb40-6"><a href="bayesian-regression-in-stan.html#cb40-6" aria-hidden="true" tabindex="-1"></a>              <span class="at">family =</span> binomial, </span>
<span id="cb40-7"><a href="bayesian-regression-in-stan.html#cb40-7" aria-hidden="true" tabindex="-1"></a>              <span class="at">data =</span> balance, </span>
<span id="cb40-8"><a href="bayesian-regression-in-stan.html#cb40-8" aria-hidden="true" tabindex="-1"></a>              <span class="at">nAGQ =</span> <span class="dv">25</span>)</span>
<span id="cb40-9"><a href="bayesian-regression-in-stan.html#cb40-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-10"><a href="bayesian-regression-in-stan.html#cb40-10" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(glmSubjectRandom)</span></code></pre></div>
<pre><code>## Generalized linear mixed model fit by maximum likelihood (Adaptive
##   Gauss-Hermite Quadrature, nAGQ = 25) [glmerMod]
##  Family: binomial  ( logit )
## Formula: Stable ~ Surface + Vision + Sex + Age + Weight + Dummy + (1 |  
##     Subject)
##    Data: balance
## 
##      AIC      BIC   logLik deviance df.resid 
##    251.2    288.8   -116.6    233.2      471 
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -4.9493 -0.1411 -0.0196 -0.0007  5.6453 
## 
## Random effects:
##  Groups  Name        Variance Std.Dev.
##  Subject (Intercept) 8.491    2.914   
## Number of obs: 480, groups:  Subject, 40
## 
## Fixed effects:
##              Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -11.20462    1.87176  -5.986 2.15e-09 ***
## Surfacenorm   7.32839    1.07053   6.846 7.62e-12 ***
## Visiondome    0.67763    0.52966   1.279    0.201    
## Visionopen    6.14100    0.99263   6.187 6.15e-10 ***
## Sexmale       1.89216    1.64924   1.147    0.251    
## Age           0.05018    1.63334   0.031    0.975    
## Weight       -0.05908    3.03973  -0.019    0.984    
## Dummy         0.23971    0.23094   1.038    0.299    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Correlation of Fixed Effects:
##             (Intr) Srfcnr Visndm Visnpn Sexmal Age    Weight
## Surfacenorm -0.821                                          
## Visiondome  -0.246  0.109                                   
## Visionopen  -0.809  0.831  0.377                            
## Sexmale     -0.591  0.174  0.023  0.184                     
## Age         -0.058  0.002  0.003  0.006  0.131              
## Weight       0.366 -0.028 -0.003 -0.039 -0.765 -0.172       
## Dummy       -0.014 -0.012  0.006  0.007  0.027  0.014  0.007</code></pre>
<p>This has helped and convergence has been reached.</p>
<div class="exbox">
<p><strong>Exercise:</strong>
How do the fitted estimates compare to the fitted model that did not consider a subject random effect?</p>
</div>
<p>The summary of this fitted model suggests that the covariates are not significantly useful in explaining the variation in our data. So we can explore the possibility of removing these (the sex, age, height, weight, and of course the dummy variable we know for certain is uninformative). This reduced model is fit as in the below, and the nested model comparison shows that indeed, we could reasonably drop the subject covariate information.</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="bayesian-regression-in-stan.html#cb42-1" aria-hidden="true" tabindex="-1"></a>glmSubjectRandomReduced <span class="ot">&lt;-</span> </span>
<span id="cb42-2"><a href="bayesian-regression-in-stan.html#cb42-2" aria-hidden="true" tabindex="-1"></a>  lme4<span class="sc">::</span><span class="fu">glmer</span>(Stable <span class="sc">~</span> Surface <span class="sc">+</span> Vision <span class="sc">+</span> </span>
<span id="cb42-3"><a href="bayesian-regression-in-stan.html#cb42-3" aria-hidden="true" tabindex="-1"></a>                (<span class="dv">1</span> <span class="sc">|</span> Subject),</span>
<span id="cb42-4"><a href="bayesian-regression-in-stan.html#cb42-4" aria-hidden="true" tabindex="-1"></a>              <span class="at">family =</span> binomial, </span>
<span id="cb42-5"><a href="bayesian-regression-in-stan.html#cb42-5" aria-hidden="true" tabindex="-1"></a>              <span class="at">data =</span> balance)</span>
<span id="cb42-6"><a href="bayesian-regression-in-stan.html#cb42-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-7"><a href="bayesian-regression-in-stan.html#cb42-7" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(glmSubjectRandomReduced,</span>
<span id="cb42-8"><a href="bayesian-regression-in-stan.html#cb42-8" aria-hidden="true" tabindex="-1"></a>      glmSubjectRandom)</span></code></pre></div>
<pre><code>## Data: balance
## Models:
## glmSubjectRandomReduced: Stable ~ Surface + Vision + (1 | Subject)
## glmSubjectRandom: Stable ~ Surface + Vision + Sex + Age + Weight + Dummy + (1 | Subject)
##                         npar    AIC    BIC  logLik deviance  Chisq Df
## glmSubjectRandomReduced    5 246.40 267.26 -118.20   236.40          
## glmSubjectRandom           9 251.23 288.79 -116.61   233.23 3.1682  4
##                         Pr(&gt;Chisq)
## glmSubjectRandomReduced           
## glmSubjectRandom            0.5301</code></pre>
<p>Let’s summarise this reduced model:</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="bayesian-regression-in-stan.html#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(glmSubjectRandomReduced)</span></code></pre></div>
<pre><code>## Generalized linear mixed model fit by maximum likelihood (Laplace
##   Approximation) [glmerMod]
##  Family: binomial  ( logit )
## Formula: Stable ~ Surface + Vision + (1 | Subject)
##    Data: balance
## 
##      AIC      BIC   logLik deviance df.resid 
##    246.4    267.3   -118.2    236.4      475 
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -4.2407 -0.1570 -0.0160 -0.0004  6.3710 
## 
## Random effects:
##  Groups  Name        Variance Std.Dev.
##  Subject (Intercept) 11.2     3.346   
## Number of obs: 480, groups:  Subject, 40
## 
## Fixed effects:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -10.9807     1.9061  -5.761 8.37e-09 ***
## Surfacenorm   7.8425     1.3011   6.027 1.67e-09 ***
## Visiondome    0.6904     0.5344   1.292    0.196    
## Visionopen    6.5930     1.2099   5.449 5.05e-08 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Correlation of Fixed Effects:
##             (Intr) Srfcnr Visndm
## Surfacenorm -0.930              
## Visiondome  -0.259  0.116       
## Visionopen  -0.918  0.882  0.336</code></pre>
</div>
</div>
<div id="bayesian-approach-using-stan" class="section level3 hasAnchor" number="4.4.4">
<h3><span class="header-section-number">4.4.4</span> Bayesian approach using Stan<a href="bayesian-regression-in-stan.html#bayesian-approach-using-stan" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We’re going to define our mixed effects GLM in Stan using the building blocks that we have previously used. The following <code>.stan</code> program defines our model, and we will explain each of the blocks contained here in the following.</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode stan"><code class="sourceCode stan"><span id="cb46-1"><a href="bayesian-regression-in-stan.html#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="kw">data</span> {</span>
<span id="cb46-2"><a href="bayesian-regression-in-stan.html#cb46-2" aria-hidden="true" tabindex="-1"></a>  <span class="co">// number of observations</span></span>
<span id="cb46-3"><a href="bayesian-regression-in-stan.html#cb46-3" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span> = <span class="dv">0</span>&gt; Nobs ; </span>
<span id="cb46-4"><a href="bayesian-regression-in-stan.html#cb46-4" aria-hidden="true" tabindex="-1"></a>  <span class="co">// number of random effects groups (40 subjects)</span></span>
<span id="cb46-5"><a href="bayesian-regression-in-stan.html#cb46-5" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span> = <span class="dv">0</span>&gt; Nsubs ; </span>
<span id="cb46-6"><a href="bayesian-regression-in-stan.html#cb46-6" aria-hidden="true" tabindex="-1"></a>  <span class="co">// number of independent variables</span></span>
<span id="cb46-7"><a href="bayesian-regression-in-stan.html#cb46-7" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span> = <span class="dv">0</span>&gt; Npreds ; </span>
<span id="cb46-8"><a href="bayesian-regression-in-stan.html#cb46-8" aria-hidden="true" tabindex="-1"></a>  <span class="co">// the observations, note that they are a vector of binary outcomes</span></span>
<span id="cb46-9"><a href="bayesian-regression-in-stan.html#cb46-9" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span> = <span class="dv">0</span>, <span class="kw">upper</span> = <span class="dv">1</span>&gt; y[Nobs] ; </span>
<span id="cb46-10"><a href="bayesian-regression-in-stan.html#cb46-10" aria-hidden="true" tabindex="-1"></a>  <span class="co">// vector associating each observation with its corresponding subject</span></span>
<span id="cb46-11"><a href="bayesian-regression-in-stan.html#cb46-11" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span> = <span class="dv">1</span>, <span class="kw">upper</span> = Nsubs&gt; subject[Nobs] ; </span>
<span id="cb46-12"><a href="bayesian-regression-in-stan.html#cb46-12" aria-hidden="true" tabindex="-1"></a>  <span class="co">// the independent variables</span></span>
<span id="cb46-13"><a href="bayesian-regression-in-stan.html#cb46-13" aria-hidden="true" tabindex="-1"></a>  <span class="dt">matrix</span>[Nobs, Npreds] X ; </span>
<span id="cb46-14"><a href="bayesian-regression-in-stan.html#cb46-14" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb46-15"><a href="bayesian-regression-in-stan.html#cb46-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-16"><a href="bayesian-regression-in-stan.html#cb46-16" aria-hidden="true" tabindex="-1"></a><span class="kw">parameters</span> {</span>
<span id="cb46-17"><a href="bayesian-regression-in-stan.html#cb46-17" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[Nsubs] b ;</span>
<span id="cb46-18"><a href="bayesian-regression-in-stan.html#cb46-18" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span> = <span class="dv">0</span>&gt; sigmab ;</span>
<span id="cb46-19"><a href="bayesian-regression-in-stan.html#cb46-19" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[Npreds] beta ;</span>
<span id="cb46-20"><a href="bayesian-regression-in-stan.html#cb46-20" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb46-21"><a href="bayesian-regression-in-stan.html#cb46-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-22"><a href="bayesian-regression-in-stan.html#cb46-22" aria-hidden="true" tabindex="-1"></a><span class="kw">model</span> {</span>
<span id="cb46-23"><a href="bayesian-regression-in-stan.html#cb46-23" aria-hidden="true" tabindex="-1"></a>  b ~ normal(<span class="dv">0</span>, sigmab) ;</span>
<span id="cb46-24"><a href="bayesian-regression-in-stan.html#cb46-24" aria-hidden="true" tabindex="-1"></a>  sigmab ~ cauchy(<span class="dv">0</span>, <span class="dv">1</span>) ;</span>
<span id="cb46-25"><a href="bayesian-regression-in-stan.html#cb46-25" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(n <span class="cf">in</span> <span class="dv">1</span>:Nobs) {</span>
<span id="cb46-26"><a href="bayesian-regression-in-stan.html#cb46-26" aria-hidden="true" tabindex="-1"></a>    y[n] ~ bernoulli_logit(X[n] * beta + b[subject[n]]) ;</span>
<span id="cb46-27"><a href="bayesian-regression-in-stan.html#cb46-27" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb46-28"><a href="bayesian-regression-in-stan.html#cb46-28" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb46-29"><a href="bayesian-regression-in-stan.html#cb46-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-30"><a href="bayesian-regression-in-stan.html#cb46-30" aria-hidden="true" tabindex="-1"></a><span class="kw">generated quantities</span> {</span>
<span id="cb46-31"><a href="bayesian-regression-in-stan.html#cb46-31" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span> bTilde = normal_rng(<span class="dv">0</span>, sigmab) ;</span>
<span id="cb46-32"><a href="bayesian-regression-in-stan.html#cb46-32" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span> = <span class="dv">0</span>, <span class="kw">upper</span> = <span class="dv">1</span>&gt; yTilde = bernoulli_logit_rng(X[<span class="dv">1</span>] * beta + bTilde) ;</span>
<span id="cb46-33"><a href="bayesian-regression-in-stan.html#cb46-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-34"><a href="bayesian-regression-in-stan.html#cb46-34" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<div id="the-data-block" class="section level4 hasAnchor" number="4.4.4.1">
<h4><span class="header-section-number">4.4.4.1</span> The data block<a href="bayesian-regression-in-stan.html#the-data-block" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Remember that the <code>data</code> block specifies the information that we observe. Previously in our <code>hangover</code> example, this involved the dependent variable and the single independent variable. Here we have the same process, but because we have multiple independent variables, we must combine these into a <em>design matrix</em>. Let’s discuss the structure of this matrix in more detail.</p>
<p>If the observations are collected in a vector <span class="math inline">\(\mathbf{Y}\)</span>, with mean vector <span class="math inline">\(\boldsymbol{\mu}\)</span>, we have
<span class="math display">\[\boldsymbol{\mu} = X\boldsymbol{\beta} + Z\mathbf{b},\]</span>
with <span class="math inline">\(\mathbf{b}\)</span> the vector of random effects. When we use functions such as <code>glmer</code>, the function will implicitly have been creating our design matrix for us from the model definition we provide. This is actually stored as part of the results of the fitted model, so we can use the command <code>model.matrix</code> to extract the design matrix <span class="math inline">\(X\)</span> from the models we have already implemented. Here we’ll implement the model equivalent to <code>glmSubjectRandom</code> from earlier, and this shows an extract of the design matrix:</p>
<pre><code>##   (Intercept) Surfacenorm Visiondome Visionopen Sexmale   Age      Weight Dummy
## 1           1           1          0          1       1 -0.24 -0.05060137  -0.6
## 2           1           1          0          1       1 -0.24 -0.05060137  -0.2
## 3           1           1          0          0       1 -0.24 -0.05060137   1.6
## 4           1           1          0          0       1 -0.24 -0.05060137   0.1
## 5           1           1          1          0       1 -0.24 -0.05060137   0.1
## 6           1           1          1          0       1 -0.24 -0.05060137   1.7</code></pre>
</div>
<div id="the-parameter-block" class="section level4 hasAnchor" number="4.4.4.2">
<h4><span class="header-section-number">4.4.4.2</span> The parameter block<a href="bayesian-regression-in-stan.html#the-parameter-block" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>In our simple <code>hangover</code> regression, we had three parameters: the intercept, slope and error variance. We specified these as three separate continuous variables. In this more complex case we do not need to create a long list of parameters, but can instead use vectors. We declare as parameters:</p>
<ul>
<li><code>sigmab</code>: the standard deviation <span class="math inline">\(\sigma_b\)</span> in the random effects distribution <span class="math inline">\(b_i \sim N(0,\sigma^2_b)\)</span>,</li>
<li><code>beta</code>: the coefficients <span class="math inline">\(\boldsymbol{\beta}\)</span> for all the fixed effects. Note in our notation above this is the collection <span class="math inline">\(\lbrace \alpha_2,\gamma_2, \gamma_3, \beta_0, \beta_1, \beta_2, \beta_4, \beta_5 \rbrace\)</span> (remember, we have removed <span class="math inline">\(\beta_3\)</span> as this is the height coefficient), and</li>
<li><code>b</code>: the random effects themselves.</li>
</ul>
</div>
<div id="the-model-block" class="section level4 hasAnchor" number="4.4.4.3">
<h4><span class="header-section-number">4.4.4.3</span> The model block<a href="bayesian-regression-in-stan.html#the-model-block" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Our random effects GLM definition consists of:</p>
<ul>
<li>The random effects that are normally distributed with zero mean, i.e. <span class="math inline">\(b_i \sim N(0,\sigma_b^2)\)</span> for <span class="math inline">\(i=1,\ldots,40\)</span>.</li>
<li>A prior distribution for the random effect variation, <span class="math inline">\(\sigma_b \sim Cauchy(0,1)\)</span>.</li>
<li>The distribution of the response variable, which is Bernoulli, but with the logit link equation <span class="math inline">\(\log (\frac{\mu}{1-\mu})\)</span>, i.e. <span class="math inline">\(Y_j \sim Bernoulli (\mu_j)\)</span> where <span class="math inline">\(\text{logit}(\mu_j)=X_j\boldsymbol{\beta}+b_i\)</span> for <span class="math inline">\(j=1,\ldots,480\)</span> and <span class="math inline">\(i\)</span> is the subject corresponding to observation <span class="math inline">\(j\)</span>.</li>
<li>A prior distribution for the vector coefficients, <span class="math inline">\(\boldsymbol{\beta}\)</span>. By leaving out this specification, Stan will assume an improper uniform prior, i.e. <span class="math inline">\(f(\beta_l)\propto 1\)</span> for the <span class="math inline">\(l=1,\ldots,8\)</span> as we have a set of 8 independent variables in our chosen model.</li>
</ul>
<p>Some detail on the choice of the prior for <span class="math inline">\(\sigma_b\)</span>. A suggestion (in Faraway) is to use a ‘half-Cauchy’, truncated at 0. This half-Cauchy distribution is given by
<span class="math display">\[f(\sigma_b) = 2\frac{1}{\pi (1 + \sigma_b^2)},\]</span>
for <span class="math inline">\(\sigma_b &gt;0\)</span>, and <span class="math inline">\(f(\sigma_b) = 0\)</span> otherwise. To set this up in Stan, we specify <code>sigmab</code> as a real-valued parameter with lower bound 0, and then separately, give it a Cauchy distribution in the model block.</p>
<p>It’s important to question our prior choices we make and whether the impact they have on our posterior is appropriate. The definition of the prior we have used involves <span class="math inline">\(v=1\)</span>, and has prior median for <span class="math inline">\(\sigma_b\)</span> equal to 1. The 99th percentile of the prior is 64, which is implausibly large. We will return to this when we analyse our posterior results and consider whether this was appropriate.</p>
</div>
<div id="the-generated-quantities-block" class="section level4 hasAnchor" number="4.4.4.4">
<h4><span class="header-section-number">4.4.4.4</span> The generated quantities block<a href="bayesian-regression-in-stan.html#the-generated-quantities-block" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>As we are building up our experience with using Stan, we will now introduce the <code>generated quantities</code> block. We can get Stan to sample directly from the posterior predictive distribution, and this is the most common use of the <code>generated_quantities</code> block. We’ll consider a new subject, with design and covariate values <span class="math inline">\(\tilde{x}\)</span> which are equal to the first observation (i.e. the first row of the design matrix):</p>
<pre><code>## (Intercept) Surfacenorm  Visiondome  Visionopen     Sexmale         Age 
##        1.00        1.00        0.00        1.00        1.00       -0.24 
##      Weight       Dummy 
##       -0.05       -0.60</code></pre>
<p>Let <span class="math inline">\(\tilde{Y}\)</span> be the observed response for this new subject. We want to sample from the distribution <span class="math inline">\(\P(\tilde{Y}|Y, X, \tilde{x})\)</span>: we do so via
<span class="math display">\[\P(\tilde{Y}|Y, X, \tilde{x}) =\int   \P(\tilde{Y}|\tilde{b}, \tilde{x},\beta)f(\tilde{b}|\sigma_b)f(\sigma_b, \beta|Y, X)d\tilde{b}d\sigma_b d\beta,\]</span>
where <span class="math inline">\(\tilde{b}\)</span> is the random effect for the new subject. Hence we need to get Stan to sample <span class="math inline">\(\tilde{b}\)</span>, and then sample <span class="math inline">\(\tilde{Y}\)</span> given the current sampled value of the parameters <span class="math inline">\(\beta, \sigma_b\)</span>. Notice that we therefore do not predict the response of a new individual only as the contribution from the fixed effects (i.e. the expected value <span class="math inline">\(\mathbf{X}\boldsymbol{\beta}\)</span>), but we sample a subject random effect to add to this fixed effects value <em>and</em> then sample from the binomial distribution with such an expected value.</p>
</div>
<div id="sampling-and-output" class="section level4 hasAnchor" number="4.4.4.5">
<h4><span class="header-section-number">4.4.4.5</span> Sampling and output<a href="bayesian-regression-in-stan.html#sampling-and-output" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>As with previous examples, we group our data into the list form that Stan requires, and use the <code>sampling</code> function to implement the HMC algorithm:</p>
<div class="sourceCode" id="cb49"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb49-1"><a href="bayesian-regression-in-stan.html#cb49-1" aria-hidden="true" tabindex="-1"></a>glmData <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">Nobs =</span> <span class="fu">nrow</span>(balance),</span>
<span id="cb49-2"><a href="bayesian-regression-in-stan.html#cb49-2" aria-hidden="true" tabindex="-1"></a>                <span class="at">Nsubs =</span> <span class="fu">length</span>(<span class="fu">unique</span>(balance<span class="sc">$</span>Subject)),</span>
<span id="cb49-3"><a href="bayesian-regression-in-stan.html#cb49-3" aria-hidden="true" tabindex="-1"></a>                <span class="at">Npreds =</span> <span class="fu">ncol</span>(designMatrix),</span>
<span id="cb49-4"><a href="bayesian-regression-in-stan.html#cb49-4" aria-hidden="true" tabindex="-1"></a>                <span class="at">y =</span> balance<span class="sc">$</span>Stable,</span>
<span id="cb49-5"><a href="bayesian-regression-in-stan.html#cb49-5" aria-hidden="true" tabindex="-1"></a>                <span class="at">subject =</span> <span class="fu">as.numeric</span>(balance<span class="sc">$</span>Subject),</span>
<span id="cb49-6"><a href="bayesian-regression-in-stan.html#cb49-6" aria-hidden="true" tabindex="-1"></a>                <span class="at">X =</span> designMatrix)</span>
<span id="cb49-7"><a href="bayesian-regression-in-stan.html#cb49-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-8"><a href="bayesian-regression-in-stan.html#cb49-8" aria-hidden="true" tabindex="-1"></a>myfit <span class="ot">&lt;-</span> <span class="fu">sampling</span>(glmRandomEffects,</span>
<span id="cb49-9"><a href="bayesian-regression-in-stan.html#cb49-9" aria-hidden="true" tabindex="-1"></a>                  <span class="at">data =</span> glmData,</span>
<span id="cb49-10"><a href="bayesian-regression-in-stan.html#cb49-10" aria-hidden="true" tabindex="-1"></a>                  <span class="at">cores =</span> <span class="dv">7</span>,</span>
<span id="cb49-11"><a href="bayesian-regression-in-stan.html#cb49-11" aria-hidden="true" tabindex="-1"></a>                  <span class="at">refresh =</span> <span class="dv">0</span>)</span></code></pre></div>
<p>Let’s explore the output from our fitted model in Stan. We can print out a summary of our results, as previously, and explore the trace-plots for convergence. Note that the summary print-out includes:</p>
<ul>
<li>the subject random effects variance, <span class="math inline">\(\sigma_b^2\)</span>,</li>
<li>the fixed effects coefficients <span class="math inline">\(\boldsymbol{\beta}\)</span>,</li>
<li>all 40 of the subject-specific random effects themselves, <span class="math inline">\(b_1,\ldots,b_{40}\)</span>,</li>
<li>the samples from the posterior predictive distribution, <span class="math inline">\(\tilde{b},\tilde{Y}\)</span>, and</li>
<li>the Hamiltonian energy itself.</li>
</ul>
<p>In reality, we are probably not interested in the actual random effects, so we will omit these from the results plots in the following.</p>
<pre><code>## Inference for Stan model: c808de51faf75c5c13f75aad1c4debdc.
## 4 chains, each with iter=2000; warmup=1000; thin=1; 
## post-warmup draws per chain=1000, total post-warmup draws=4000.
## 
##            mean se_mean   sd    2.5%     50%   97.5% n_eff Rhat
## b[1]       0.02    0.04 1.69   -3.32    0.02    3.32  1550    1
## b[2]      -1.76    0.05 1.91   -5.70   -1.72    1.78  1793    1
## b[3]      -6.40    0.07 2.44  -11.90   -6.10   -2.37  1079    1
## b[4]      -4.12    0.05 2.01   -8.56   -3.99   -0.47  1349    1
## b[5]      -2.16    0.04 1.91   -6.07   -2.13    1.44  2044    1
## b[6]       2.36    0.04 1.56   -0.59    2.34    5.51  1330    1
## b[7]      -1.60    0.04 1.78   -5.18   -1.50    1.72  2338    1
## b[8]      -0.03    0.04 1.91   -3.72   -0.01    3.62  2300    1
## b[9]      -0.01    0.03 1.82   -3.70    0.02    3.55  3642    1
## b[10]      1.41    0.03 1.40   -1.32    1.42    4.11  1714    1
## b[11]     -1.56    0.04 1.79   -5.19   -1.51    1.77  2237    1
## b[12]     -1.59    0.04 1.84   -5.51   -1.53    1.86  2280    1
## b[13]     -1.53    0.04 1.88   -5.33   -1.49    1.99  1882    1
## b[14]     -5.11    0.06 2.56  -11.14   -4.78   -1.06  1587    1
## b[15]      0.05    0.03 1.77   -3.38    0.06    3.49  3619    1
## b[16]      4.24    0.05 1.72    1.01    4.18    7.83  1344    1
## b[17]      3.22    0.04 1.59    0.23    3.15    6.48  1390    1
## b[18]      0.21    0.04 1.88   -3.45    0.20    3.89  2188    1
## b[19]      0.33    0.04 1.69   -3.15    0.37    3.59  1715    1
## b[20]      1.20    0.05 1.75   -2.19    1.19    4.69  1282    1
## b[21]      3.50    0.06 1.86    0.13    3.38    7.42   957    1
## b[22]     -2.24    0.04 1.92   -6.21   -2.20    1.50  2021    1
## b[23]     -1.61    0.05 2.17   -5.95   -1.55    2.61  2037    1
## b[24]     -1.63    0.04 1.80   -5.38   -1.56    1.71  2299    1
## b[25]      5.54    0.05 1.82    2.35    5.42    9.44  1406    1
## b[26]      3.45    0.05 1.75    0.24    3.39    7.20  1267    1
## b[27]      7.50    0.07 2.05    4.07    7.31   12.02   980    1
## b[28]     -0.01    0.04 1.93   -3.76   -0.01    3.81  2355    1
## b[29]      4.83    0.05 1.91    1.39    4.73    8.96  1292    1
## b[30]     -4.84    0.06 2.41  -10.30   -4.57   -0.89  1595    1
## b[31]      2.29    0.05 1.81   -1.15    2.22    6.09  1305    1
## b[32]      2.29    0.04 1.62   -0.88    2.28    5.42  1550    1
## b[33]      3.37    0.05 1.70    0.25    3.29    6.81  1329    1
## b[34]     -1.63    0.04 1.84   -5.47   -1.56    1.74  2301    1
## b[35]      3.31    0.04 1.44    0.63    3.29    6.31  1441    1
## b[36]      0.10    0.04 1.95   -3.81    0.11    3.91  2600    1
## b[37]     -4.93    0.07 2.53  -10.93   -4.65   -0.77  1479    1
## b[38]     -4.73    0.07 2.57  -10.68   -4.54   -0.20  1541    1
## b[39]      0.22    0.04 1.91   -3.42    0.21    3.99  2251    1
## b[40]     -2.22    0.04 1.87   -6.08   -2.17    1.39  2230    1
## sigmab     3.63    0.03 0.79    2.39    3.54    5.44   660    1
## beta[1]  -12.98    0.09 2.34  -18.40  -12.72   -9.09   621    1
## beta[2]    8.38    0.05 1.30    6.18    8.26   11.36   678    1
## beta[3]    0.75    0.01 0.55   -0.28    0.75    1.84  3526    1
## beta[4]    7.08    0.05 1.24    5.03    6.96   10.00   651    1
## beta[5]    2.43    0.07 2.08   -1.30    2.30    6.81   886    1
## beta[6]    0.12    0.06 2.12   -4.14    0.13    4.16  1234    1
## beta[7]   -0.62    0.12 3.76   -8.15   -0.60    6.39  1047    1
## beta[8]    0.24    0.00 0.25   -0.25    0.24    0.72  4292    1
## bTilde    -0.04    0.06 3.79   -7.44   -0.05    7.65  3693    1
## yTilde     0.86    0.01 0.35    0.00    1.00    1.00  3667    1
## lp__    -154.44    0.28 7.12 -169.55 -154.05 -141.87   654    1
## 
## Samples were drawn using NUTS(diag_e) at Sun Mar 20 17:05:22 2022.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).</code></pre>
<p>The trace plots of the four chains (only showing the samples after the burn-in period) are given in Figure <a href="bayesian-regression-in-stan.html#fig:stan-trace-balance">4.6</a>. This shows the <span class="math inline">\(\sigma_b^2\)</span> parameter and the fixed effects coefficients <span class="math inline">\(\boldsymbol{\beta}\)</span>, but re-labelled to add some descriptive context to the interpretation of each.</p>
<div class="figure"><span style="display:block;" id="fig:stan-trace-balance"></span>
<img src="MAS61006-S2-Notes_files/figure-html/stan-trace-balance-1.png" alt="Trace plots of the sampled parameters for the balance experiment, fitting a random effects GLM in Stan. Note that the beta parameters have been relabelled here descriptively to add context." width="90%" />
<p class="caption">
Figure 4.6: Trace plots of the sampled parameters for the balance experiment, fitting a random effects GLM in Stan. Note that the beta parameters have been relabelled here descriptively to add context.
</p>
</div>
<p>Posterior credible intervals are shown for these parameters in Figure <a href="bayesian-regression-in-stan.html#fig:stan-interval">4.7</a>. Again, the parameters have been relabelled to add some descriptive context to each.</p>
<div class="figure"><span style="display:block;" id="fig:stan-interval"></span>
<img src="MAS61006-S2-Notes_files/figure-html/stan-interval-1.png" alt="Posterior credible interval summaries for the balance experiment, fitting a random effects GLM in Stan. Note that the beta parameters have been relabelled here descriptively to add context. Point gives the posterior median, bar gives the interval with quantiles at 0.25 and 0.75, and the line gives the quantiles at 0.025 and 0.975." width="80%" />
<p class="caption">
Figure 4.7: Posterior credible interval summaries for the balance experiment, fitting a random effects GLM in Stan. Note that the beta parameters have been relabelled here descriptively to add context. Point gives the posterior median, bar gives the interval with quantiles at 0.25 and 0.75, and the line gives the quantiles at 0.025 and 0.975.
</p>
</div>
<div class="exbox">
<p><strong>Exercise</strong></p>
<p>What information can we determine from these plots?<br />
Are you satisfied about the convergence of the HMC sampler? What other diagnostics could you explore?<br />
What do the posterior credible intervals suggest about our full proposed model?</p>
</div>
</div>
<div id="examining-the-effect-of-our-choice-of-prior" class="section level4 hasAnchor" number="4.4.4.6">
<h4><span class="header-section-number">4.4.4.6</span> Examining the effect of our choice of prior<a href="bayesian-regression-in-stan.html#examining-the-effect-of-our-choice-of-prior" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We will look more carefully at <span class="math inline">\(\sigma_b\)</span>, the subject random effect. Recall that this was the only parameter we set a ‘proper’ prior for, using a half-Cauchy, and it featured a heavy right tail. It’s important to check that our posterior is not just a reflection of our prior, and that there truly is information from that data here.</p>
<p>We compare the prior and posterior tails for <span class="math inline">\(\sigma_b\)</span> in Table <a href="bayesian-regression-in-stan.html#tab:prior-table">4.6</a>. We see that the data does seem to have pulled in the posterior; both away from the heavy tail, and away from the prior median. To explore this visually, Figure <a href="bayesian-regression-in-stan.html#fig:prior-posterior-plot">4.8</a> shows the prior density for <span class="math inline">\(\sigma_b\)</span> (black line), along with a histogram of the posterior samples (blue) and the estimate of <span class="math inline">\(\sigma_b\)</span> from the likelihood-based approach using <code>lme4</code> that we implemented earlier. Note that <code>lme4</code> does not give error estimates of random effect parameters, so we would have nothing more than this point estimate were we using the maximum likelihood approach.</p>
<table>
<caption><span id="tab:prior-table">Table 4.6: </span>Prior and Posterior percentiles of the subject random effects parameter.</caption>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">Prior</th>
<th align="right">Posterior</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">1%</td>
<td align="right">0.016</td>
<td align="right">2.2</td>
</tr>
<tr class="even">
<td align="left">5%</td>
<td align="right">0.079</td>
<td align="right">2.5</td>
</tr>
<tr class="odd">
<td align="left">50%</td>
<td align="right">1.000</td>
<td align="right">3.5</td>
</tr>
<tr class="even">
<td align="left">95%</td>
<td align="right">13.000</td>
<td align="right">5.1</td>
</tr>
<tr class="odd">
<td align="left">99%</td>
<td align="right">64.000</td>
<td align="right">6.0</td>
</tr>
</tbody>
</table>
<div class="figure"><span style="display:block;" id="fig:prior-posterior-plot"></span>
<img src="MAS61006-S2-Notes_files/figure-html/prior-posterior-plot-1.png" alt="Prior and posterior comparison of the subject random effects parameter. The prior density is given (black) along with posterior histogram (blue), and the likelihood-based estimate from lme4 (red)." width="60%" />
<p class="caption">
Figure 4.8: Prior and posterior comparison of the subject random effects parameter. The prior density is given (black) along with posterior histogram (blue), and the likelihood-based estimate from lme4 (red).
</p>
</div>
</div>
<div id="examining-the-new-subject-prediction" class="section level4 hasAnchor" number="4.4.4.7">
<h4><span class="header-section-number">4.4.4.7</span> Examining the new subject prediction<a href="bayesian-regression-in-stan.html#examining-the-new-subject-prediction" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Recall that we included a command in our Stan program to sample from the posterior predictive distribution of a new subject who has the same covariate values as subject 1. Note that we are therefore assuming that this individual has the same age, height, weight, etc. as ‘subject 1’, but is not a new observation on that particular subject, but merely a different person with those covariate measurements. We can extract the results of these two variables to obtain a predictive credible interval. Remember that <span class="math inline">\(\tilde{b}\)</span> is the posterior predicted random effect of the new subject, and <span class="math inline">\(\tilde{Y}\)</span> is the posterior predicted balance outcome of that subject.</p>
<pre><code>## Inference for Stan model: c808de51faf75c5c13f75aad1c4debdc.
## 4 chains, each with iter=2000; warmup=1000; thin=1; 
## post-warmup draws per chain=1000, total post-warmup draws=4000.
## 
##         mean se_mean   sd  2.5%   25%   50% 75% 97.5% n_eff Rhat
## bTilde -0.04    0.06 3.79 -7.44 -2.48 -0.05 2.3  7.65  3693    1
## yTilde  0.86    0.01 0.35  0.00  1.00  1.00 1.0  1.00  3667    1
## 
## Samples were drawn using NUTS(diag_e) at Sun Mar 20 17:05:22 2022.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).</code></pre>
</div>
</div>
<div id="comparing-stan-and-lme4" class="section level3 hasAnchor" number="4.4.5">
<h3><span class="header-section-number">4.4.5</span> Comparing Stan and lme4<a href="bayesian-regression-in-stan.html#comparing-stan-and-lme4" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let’s compare the results that are obtained from fitting this random effect GLM using a likelihood-based approach (<code>lme4</code>) and a Bayesian approach (<code>rstan</code>). The estimated mean and standard deviation for the fixed effects parameters are given in Table <a href="bayesian-regression-in-stan.html#tab:compare-table">4.7</a>. We also show these results in Figure <a href="bayesian-regression-in-stan.html#fig:comparison-intervals">4.9</a>. In this figure, the Bayesian approach from <code>rstan</code> is displayed as posterior credible intervals using the samples from the posterior, whereas the likelihood approach from <code>lme4</code> is displayed as confidence intervals obtained by applying a normal approximation (i.e. <span class="math inline">\(2\times s.e.\)</span>).</p>
<table>
<caption><span id="tab:compare-table">Table 4.7: </span>Comparison of the fixed effects parameter estimates between Stan (HMC) and lme4 (likelihood-based).</caption>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">Stan mean</th>
<th align="right">Stan sd</th>
<th align="right">lme4 mean</th>
<th align="right">lme4 sd</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">(Intercept)</td>
<td align="right">-13.000</td>
<td align="right">2.340</td>
<td align="right">-11.2000</td>
<td align="right">1.870</td>
</tr>
<tr class="even">
<td align="left">Surfacenorm</td>
<td align="right">8.380</td>
<td align="right">1.300</td>
<td align="right">7.3300</td>
<td align="right">1.070</td>
</tr>
<tr class="odd">
<td align="left">Visiondome</td>
<td align="right">0.752</td>
<td align="right">0.550</td>
<td align="right">0.6780</td>
<td align="right">0.530</td>
</tr>
<tr class="even">
<td align="left">Visionopen</td>
<td align="right">7.080</td>
<td align="right">1.240</td>
<td align="right">6.1400</td>
<td align="right">0.993</td>
</tr>
<tr class="odd">
<td align="left">Sexmale</td>
<td align="right">2.430</td>
<td align="right">2.080</td>
<td align="right">1.8900</td>
<td align="right">1.650</td>
</tr>
<tr class="even">
<td align="left">Age</td>
<td align="right">0.118</td>
<td align="right">2.120</td>
<td align="right">0.0502</td>
<td align="right">1.630</td>
</tr>
<tr class="odd">
<td align="left">Weight</td>
<td align="right">-0.619</td>
<td align="right">3.760</td>
<td align="right">-0.0591</td>
<td align="right">3.040</td>
</tr>
<tr class="even">
<td align="left">Dummy</td>
<td align="right">0.242</td>
<td align="right">0.247</td>
<td align="right">0.2400</td>
<td align="right">0.231</td>
</tr>
</tbody>
</table>
<div class="figure"><span style="display:block;" id="fig:comparison-intervals"></span>
<img src="MAS61006-S2-Notes_files/figure-html/comparison-intervals-1.png" alt="Estimates of the parameters in the balance model. The blue circles/bars give the posterior meadian/(0.025,0.975) credible interval from the Bayesian approach, and the red circles/bars give the estimate/(0.025,0.975) approximate confidence interval. Note that lme4 does not report a standard error for random effects parameters." width="80%" />
<p class="caption">
Figure 4.9: Estimates of the parameters in the balance model. The blue circles/bars give the posterior meadian/(0.025,0.975) credible interval from the Bayesian approach, and the red circles/bars give the estimate/(0.025,0.975) approximate confidence interval. Note that lme4 does not report a standard error for random effects parameters.
</p>
</div>
<p>Additionally, we can explore the subject-specific random effects, including the predictive distribution. Figure <a href="bayesian-regression-in-stan.html#fig:comparison-random-effects">4.10</a> shows the estimates of the random effects for each of the 40 subjects in the experiment. It also shows the posterior predictive interval for the subject-specific effect of a new, unknown individual. The Bayesian results are shown in blue. For this new individual, we compare with <span class="math inline">\(\pm 2 \hat{\sigma}_b\)</span>, where <span class="math inline">\(\hat{\sigma}_b\)</span> is the estimate obtained via <code>lme4</code> (red).</p>
<div class="figure"><span style="display:block;" id="fig:comparison-random-effects"></span>
<img src="MAS61006-S2-Notes_files/figure-html/comparison-random-effects-1.png" alt="Comparison of the subject-specific random effects. The (0.025,0.975) posterior credible intervals of each subject's effect obtained via a Bayesian approach is shown in blue, and the approximate confidence interval obtained via a likelihood approach is shown in red. In additon to the estimates of the 40 subjects from the experiment, we also include the prediction for a new unknown individual, using bTilde. In blue we show the posterior predictive median (point) and (0.025,0.975) credible predictive interval obtained via the Bayesian approach. In red we show the interval given by plus and minus 2 times the estimate of the random effect deviation." width="672" />
<p class="caption">
Figure 4.10: Comparison of the subject-specific random effects. The (0.025,0.975) posterior credible intervals of each subject’s effect obtained via a Bayesian approach is shown in blue, and the approximate confidence interval obtained via a likelihood approach is shown in red. In additon to the estimates of the 40 subjects from the experiment, we also include the prediction for a new unknown individual, using bTilde. In blue we show the posterior predictive median (point) and (0.025,0.975) credible predictive interval obtained via the Bayesian approach. In red we show the interval given by plus and minus 2 times the estimate of the random effect deviation.
</p>
</div>
</div>
<div id="easy-bayesian-approach-with-brms" class="section level3 hasAnchor" number="4.4.6">
<h3><span class="header-section-number">4.4.6</span> ‘Easy’ Bayesian approach with <code>brms</code><a href="bayesian-regression-in-stan.html#easy-bayesian-approach-with-brms" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We have implemented a Bayesian approach to regression analysis using Stan, and this took a fair amount of set-up, creating the <code>.stan</code> file with the appropriate design matrix construction. The flexibility of creating the <code>.stan</code> file ourselves is that we can define our model however we like, along with the priors and any generated quantities, such as the predictive distribution we sampled from in our example. However, if we do not need to include these flexible choices, there is a rather quick way we can implement this model, using the package <code>brms</code>, that retains the same syntax that we are familiar with from <code>lm</code> and <code>lme4</code>.</p>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="bayesian-regression-in-stan.html#cb52-1" aria-hidden="true" tabindex="-1"></a>glmBrm <span class="ot">&lt;-</span> <span class="fu">brm</span>(Stable <span class="sc">~</span> Surface <span class="sc">+</span> Vision <span class="sc">+</span></span>
<span id="cb52-2"><a href="bayesian-regression-in-stan.html#cb52-2" aria-hidden="true" tabindex="-1"></a>                Sex <span class="sc">+</span></span>
<span id="cb52-3"><a href="bayesian-regression-in-stan.html#cb52-3" aria-hidden="true" tabindex="-1"></a>                Age <span class="sc">+</span> Weight <span class="sc">+</span> Dummy <span class="sc">+</span></span>
<span id="cb52-4"><a href="bayesian-regression-in-stan.html#cb52-4" aria-hidden="true" tabindex="-1"></a>                (<span class="dv">1</span> <span class="sc">|</span> Subject),</span>
<span id="cb52-5"><a href="bayesian-regression-in-stan.html#cb52-5" aria-hidden="true" tabindex="-1"></a>              <span class="at">family =</span> bernoulli, </span>
<span id="cb52-6"><a href="bayesian-regression-in-stan.html#cb52-6" aria-hidden="true" tabindex="-1"></a>              <span class="at">data =</span> balance)</span></code></pre></div>
<p>We can summarise and compare the output here with what we obtained ourselves in Stan:</p>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb53-1"><a href="bayesian-regression-in-stan.html#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(glmBrm)</span></code></pre></div>
<pre><code>##  Family: bernoulli 
##   Links: mu = logit 
## Formula: Stable ~ Surface + Vision + Sex + Age + Weight + Dummy + (1 | Subject) 
##    Data: balance (Number of observations: 480) 
##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup draws = 4000
## 
## Group-Level Effects: 
## ~Subject (Number of levels: 40) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     3.49      0.71     2.30     5.06 1.00     1130     1636
## 
## Population-Level Effects: 
##             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     -12.11      2.08   -16.62    -8.48 1.00     1484     1744
## Surfacenorm     8.01      1.18     5.96    10.55 1.00     1755     2021
## Visiondome      0.72      0.56    -0.36     1.84 1.00     4799     2777
## Visionopen      6.76      1.12     4.87     9.26 1.00     1705     1963
## Sexmale         2.03      2.00    -1.77     6.18 1.00     1371     1515
## Age            -0.01      1.98    -4.11     3.82 1.00     1566     2195
## Weight         -0.03      3.64    -7.06     7.35 1.00     1374     2031
## Dummy           0.24      0.24    -0.22     0.71 1.00     6063     3250
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
<p>As you can see, this approach retains interpretable variable labelling without us needing to, which is a bonus! Comparable results to our analysis in Stan can be seen by the trace plot in Figure <a href="bayesian-regression-in-stan.html#fig:brms-trace-balance">4.11</a> and the caterpillar plot in Figure <a href="bayesian-regression-in-stan.html#fig:brms-intervals">4.12</a>. As we can see, these results are very similar to the Stan implementation. Note that both are stochastic processes so we cannot expect identical results. However, there is an implementation difference here because we used a specified prior in our custom Stan implementation.</p>
<pre><code>## Warning: &#39;parnames&#39; is deprecated. Please use &#39;variables&#39; instead.</code></pre>
<div class="figure"><span style="display:block;" id="fig:brms-trace-balance"></span>
<img src="MAS61006-S2-Notes_files/figure-html/brms-trace-balance-1.png" alt="Trace plots of the sampled parameters for the balance experiment, fitting a random effects GLM in Stan using the package brms." width="90%" />
<p class="caption">
Figure 4.11: Trace plots of the sampled parameters for the balance experiment, fitting a random effects GLM in Stan using the package brms.
</p>
</div>
<div class="figure"><span style="display:block;" id="fig:brms-intervals"></span>
<img src="MAS61006-S2-Notes_files/figure-html/brms-intervals-1.png" alt="Estimates of the parameters in the balance model. The dark grey is the credible interval from the Bayesian approach in Stan, the light grey is the approximate confidence interval from lme4. The new edition here is the blue credible intervals from the Bayesian approach with brms." width="80%" />
<p class="caption">
Figure 4.12: Estimates of the parameters in the balance model. The dark grey is the credible interval from the Bayesian approach in Stan, the light grey is the approximate confidence interval from lme4. The new edition here is the blue credible intervals from the Bayesian approach with brms.
</p>
</div>
</div>
<div id="what-have-we-found" class="section level3 hasAnchor" number="4.4.7">
<h3><span class="header-section-number">4.4.7</span> What have we found?<a href="bayesian-regression-in-stan.html#what-have-we-found" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The main aim of this example was to explore a sensible process for implementing a regression model using Stan for a non-simplified, real data example. However, it’s useful to summarise our analysis findings nonetheless.</p>
<p>We’ve found that there is significant evidence to suggest that there is a difference in balance for the two surfaces explored, and also between restricted and unrestricted vision. There’s evidence to suggest that there is a small difference between the two ways of restricting vision. There is also evidence to suggest a small difference in balance between the genders, but that other covariates are unlikely to provide useful predictive information. Somewhat comfortingly, we’ve shown (with little uncertainty!) that the dummy variable is uninformative for explaining balance. Finally, we’ve seen evidence of a variation between individuals.</p>
<p>Note that we have not investigated model fit or validity of assumptions here, though this is an important aspect of model fitting analysis.</p>

</div>
</div>
</div>



            </section>

          </div>
        </div>
      </div>
<a href="implementing-hmc-in-stan.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="multiple-imputation-for-missing-data.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": null,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section",
"scroll-highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
