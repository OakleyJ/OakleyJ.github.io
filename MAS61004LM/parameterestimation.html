<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>2&nbsp; Parameter estimation for linear models – A Second Course on Linear Models</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./qualitative.html" rel="next">
<link href="./basicconcepts.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./parameterestimation.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Parameter estimation for linear models</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">A Second Course on Linear Models</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./basicconcepts.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Basic concepts</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./parameterestimation.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Parameter estimation for linear models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./qualitative.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Qualitative independent variables</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./fitting-in-R.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Fitting a linear model and making predictions in R</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./hypothesisTesting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Hypothesis testing</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./cautionaryTales.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Cautionary tales</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./assumptions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Checking model assumptions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./transformations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Transformations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./modelSelection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Model Selection</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./matrixAlgebra.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Matrix algebra essentials</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./randomVectors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Random vectors</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./multivariateNormal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">The multivariate normal distribution</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-leastsquares" id="toc-sec-leastsquares" class="nav-link active" data-scroll-target="#sec-leastsquares"><span class="header-section-number">2.1</span> Estimating the coefficients</a></li>
  <li><a href="#mean-and-variance-of-the-least-squares-estimator" id="toc-mean-and-variance-of-the-least-squares-estimator" class="nav-link" data-scroll-target="#mean-and-variance-of-the-least-squares-estimator"><span class="header-section-number">2.2</span> Mean and variance of the least squares estimator</a></li>
  <li><a href="#sec-res_se" id="toc-sec-res_se" class="nav-link" data-scroll-target="#sec-res_se"><span class="header-section-number">2.3</span> Estimating the error variance <span class="math inline">\(\sigma^2\)</span></a>
  <ul class="collapse">
  <li><a href="#sec-mlevar" id="toc-sec-mlevar" class="nav-link" data-scroll-target="#sec-mlevar"><span class="header-section-number">2.3.1</span> Maximum likelihood estimation</a></li>
  <li><a href="#an-unbiased-estimator-of-sigma2" id="toc-an-unbiased-estimator-of-sigma2" class="nav-link" data-scroll-target="#an-unbiased-estimator-of-sigma2"><span class="header-section-number">2.3.2</span> An unbiased estimator of <span class="math inline">\(\sigma^2\)</span></a></li>
  </ul></li>
  <li><a href="#distributions-of-the-estimators" id="toc-distributions-of-the-estimators" class="nav-link" data-scroll-target="#distributions-of-the-estimators"><span class="header-section-number">2.4</span> Distributions of the estimators</a></li>
  <li><a href="#sec-beta_comps" id="toc-sec-beta_comps" class="nav-link" data-scroll-target="#sec-beta_comps"><span class="header-section-number">2.5</span> Confidence intervals for components of <span class="math inline">\(\boldsymbol{\beta}\)</span></a></li>
  <li><a href="#sec-R2" id="toc-sec-R2" class="nav-link" data-scroll-target="#sec-R2"><span class="header-section-number">2.6</span> Model fit: coefficient of determination <span class="math inline">\(R^2\)</span></a></li>
  <li><a href="#chapter-appendix" id="toc-chapter-appendix" class="nav-link" data-scroll-target="#chapter-appendix"><span class="header-section-number">2.7</span> Chapter appendix</a>
  <ul class="collapse">
  <li><a href="#deriving-the-least-squares-estimator" id="toc-deriving-the-least-squares-estimator" class="nav-link" data-scroll-target="#deriving-the-least-squares-estimator"><span class="header-section-number">2.7.1</span> Deriving the least squares estimator</a></li>
  <li><a href="#sec-bias" id="toc-sec-bias" class="nav-link" data-scroll-target="#sec-bias"><span class="header-section-number">2.7.2</span> Bias of the maximum likelihood estimator</a></li>
  <li><a href="#sec-independence" id="toc-sec-independence" class="nav-link" data-scroll-target="#sec-independence"><span class="header-section-number">2.7.3</span> Independence between <span class="math inline">\(\hat{\sigma}^2\)</span> and <span class="math inline">\(\boldsymbol{\hat{\beta}}\)</span></a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Parameter estimation for linear models</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>In this chapter, we will:</p>
<ol type="1">
<li>obtain estimates of all the parameters in a linear model;</li>
<li>discuss maximum likelihood and least squares estimation for linear models;</li>
<li>derive properties of our parameter estimates, and use these to obtain confidence intervals;</li>
<li>discuss the <span class="math inline">\(R^2\)</span> goodness-of-fit measure for a linear model.</li>
</ol>
<p>(Note that least squares is the primary method for estimating coefficients, and we only briefly consider maximum likelihood. If you want to revise likelihood and maximum likelihood <a href="https://oakleyj.github.io/MAS5052Part2/intro.html">notes are available here</a>.)</p>
<section id="sec-leastsquares" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="sec-leastsquares"><span class="header-section-number">2.1</span> Estimating the coefficients</h2>
<p>Given a linear model as in <span class="math display">\[
\boldsymbol{Y}=X\boldsymbol{\beta} +\boldsymbol{\varepsilon},
\]</span> we will want to use our observed data <span class="math inline">\(\boldsymbol{y}\)</span> to estimate the parameters <span class="math inline">\(\boldsymbol{\beta}\)</span>; for example in simple linear regression we are estimating an intercept <span class="math inline">\(\beta_0\)</span> and gradient term <span class="math inline">\(\beta_1\)</span>.</p>
<p>Suppose we have estimates of the parameters <span class="math inline">\(\hat{\beta_0},\hat{\beta_1},\ldots,\hat{\beta}_{p-1}\)</span>, or in vector form <span class="math inline">\(\boldsymbol{\hat{\beta}}\)</span>. Then the <em>fitted value</em> <span class="math inline">\(\hat{y}_i\)</span> for observation <span class="math inline">\(i\)</span> is defined as <span class="math display">\[\hat{y}_i:=\hat{\beta_0}+\hat{\beta_1}x_{i1}+\ldots+\hat{\beta}_{p-1}x_{i,p-1}=\boldsymbol{x}_i^T\boldsymbol{\hat{\beta}},\]</span> where <span class="math inline">\(\boldsymbol{x}_i^T\)</span> is the (row) vector consisting of row <span class="math inline">\(i\)</span> of <span class="math inline">\(X\)</span>. The matrix product <span class="math inline">\(X\boldsymbol{\hat{\beta}}\)</span> then gives the vector of fitted values for all observations.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The ‘hat’ notation
</div>
</div>
<div class="callout-body-container callout-body">
<p>Note the difference between <span class="math inline">\(\boldsymbol{\beta}\)</span> and <span class="math inline">\(\boldsymbol{\hat{\beta}}\)</span>: <span class="math inline">\(\boldsymbol{\beta}\)</span> is the vector of true parameter values, <span class="math inline">\(\boldsymbol{\hat{\beta}}\)</span> is the <em>estimate</em> of the vector of true parameter values.</p>
</div>
</div>
<p>For observation <span class="math inline">\(i\)</span>, the difference between the observed value <span class="math inline">\(y_i\)</span> and the fitted value <span class="math inline">\(\hat{y}_i\)</span> is called the <em>residual</em> for that observation, denoted by <span class="math inline">\(e_i\)</span>.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Errors and residuals
</div>
</div>
<div class="callout-body-container callout-body">
<p>Note the difference between the error (<span class="math inline">\(\varepsilon_i\)</span>) and the residual (<span class="math inline">\(e_i\)</span>) for observation <span class="math inline">\(i\)</span>. The error is part of the statistical model that we fit to our data. The residual is the observed value of the difference between the response and the fitted value. As we will see later, the observed residuals, the <span class="math inline">\(e_i\)</span>, allow us to estimate the variance <span class="math inline">\(\sigma^2\)</span> of the error terms, the <span class="math inline">\(\varepsilon_i\)</span></p>
</div>
</div>
<p>The vector of residuals will be <span class="math display">\[
\boldsymbol{e}:=\left(\begin{array}{c} e_1\\ \vdots
\\ e_n\end{array}\right)=\boldsymbol{y}-X\boldsymbol{\hat{\beta}}.
\]</span> Residuals play a key role in linear models. In later chapters we will see how they can be used to assess whether the statistical model we use to describe the relationship between our variables meets the assumptions underpinning linear model theory.</p>
<p>The sum of squares of the residuals, written as <span class="math inline">\(s_r\)</span> or <span class="math inline">\(s(\boldsymbol{\hat{\beta}})\)</span> is nicely expressed as <span class="math display">\[\begin{equation}
s_r=s(\boldsymbol{\hat{\beta}}):=\sum_{i=1}^ne_i^2=\boldsymbol{e}^T\boldsymbol{e}=(\boldsymbol{y}-X\boldsymbol{\hat{\beta}})^T(\boldsymbol{y}-X\boldsymbol{\hat{\beta}}).
\end{equation}\]</span> This sum is known as <em>residual sum of squares</em> and plays an important role in the the analysis of linear models. To see this, consider the likelihood of <span class="math inline">\(\boldsymbol{\beta}\)</span> and <span class="math inline">\(\sigma\)</span>; this comes directly from the definition of the multivariate normal density: <span class="math display">\[\begin{eqnarray*}
L(\boldsymbol{\beta},\sigma^2;\boldsymbol{y}) &amp;=&amp; f(\boldsymbol{y}|\boldsymbol{\beta},\sigma^2) \\
&amp;=&amp; \frac{1}{(2\pi \sigma^2)^{n/2}}
\exp\left(-\frac{1}{2\sigma^2}(\boldsymbol{y} -X\boldsymbol{\beta})^T (\boldsymbol{y}
-X\boldsymbol{\beta}) \right) \\ &amp; \propto &amp; \sigma^{-n}
\exp\left(-\frac{1}{2\sigma^2}(\boldsymbol{y} -X\boldsymbol{\beta})^T (\boldsymbol{y}
-X\boldsymbol{\beta}) \right)
\end{eqnarray*}\]</span> In this derivation we have used the fact that <span class="math inline">\(|\sigma^2I_n|=(\sigma^2)^n\)</span>.</p>
<p>The log likelihood is thus <span class="math display">\[\ell(\boldsymbol{\beta},\sigma^2;\boldsymbol{y})=-n\log \sigma-\frac{1}{2\sigma^2}(\boldsymbol{y} -X\boldsymbol{\beta})^T (\boldsymbol{y}
-X\boldsymbol{\beta})+c,\]</span> where <span class="math inline">\(c\)</span> is a constant term which does not depend on <span class="math inline">\(\boldsymbol{\beta}\)</span> or <span class="math inline">\(\sigma\)</span> and can be ignored when maximizing.</p>
<p>To maximize this log likelihood with respect to <span class="math inline">\(\boldsymbol{\beta}\)</span>, we obviously must minimize <span class="math inline">\((\boldsymbol{y}-X\boldsymbol{\beta})^T(\boldsymbol{y}-X\boldsymbol{\beta})\)</span>, which is exactly the residual sum of squares. So the maximum likelihood estimator <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> for the parameters <span class="math inline">\(\boldsymbol{\beta}\)</span> is found by minimizing the residual sum of squares.</p>
<p>Minimizing the residual sum of squares is called the method of <em>least squares</em>, and can also be used when the data are not assumed to be normally distributed. Intuitively, a value of <span class="math inline">\(\boldsymbol{\hat{\beta}}\)</span> that makes the residuals small ‘fits’ the data well, justifying the idea of the least squares estimator.</p>
<div id="thm-1" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2.1</strong></span> Assume that <span class="math inline">\(X\)</span> has rank <span class="math inline">\(p\)</span>. Then the least squares estimate of <span class="math inline">\(\boldsymbol{\beta}\)</span> is</p>
<p><span class="math display">\[
\boldsymbol{\hat{\beta}}=(X^TX)^{-1}X^T\boldsymbol{y}.
\]</span></p>
</div>
<p>We give a proof in the Chapter appendix.</p>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise
</div>
</div>
<div class="callout-body-container callout-body">
<div id="exr-singular" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2.1</strong></span> Note the requirement that <span class="math inline">\(X\)</span> is of rank <span class="math inline">\(p\)</span> (full column rank), i.e.&nbsp;that <span class="math inline">\(X^TX\)</span> is invertible. Now consider trying to fit a simple linear regression model to a data set in which the independent variable is held constant at a single value <span class="math inline">\(x\)</span>. Intuitively, why can we not fit the model, and how does this relate to the requirement on <span class="math inline">\(X\)</span>?</p>
</div>
</div>
</div>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Solution
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Intuitively, if the independent variable is held constant, our data set might look something like this:</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="parameterestimation_files/figure-html/unnamed-chunk-1-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>and there’s no way we could identify the slope parameter <span class="math inline">\(\beta_1\)</span> in the model <span class="math display">\[Y_i = \beta_0 + \beta_1x_i+\varepsilon_i,\]</span> from these data. The design matrix <span class="math inline">\(X\)</span> would be <span class="math display">\[
X = \left(\begin{array}{cc}1 &amp; x\\ 1 &amp; x \\ \vdots &amp; \vdots \\ 1 &amp; x\end{array}\right)
\]</span> and we can see that column 2 is <span class="math inline">\(x\)</span> multiplied by column 1: the matrix has one linearly independent column only, and so is of rank 1.</p>
</div>
</div>
</div>
</section>
<section id="mean-and-variance-of-the-least-squares-estimator" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="mean-and-variance-of-the-least-squares-estimator"><span class="header-section-number">2.2</span> Mean and variance of the least squares estimator</h2>
<p>With apologies for some confusing notation (though this is the convention), we are going to write both</p>
<p><span class="math display">\[
\boldsymbol{\hat{\beta}} = (X^TX)^{-1}X^T\boldsymbol{y},
\]</span> and <span class="math display">\[
\boldsymbol{\hat{\beta}} = (X^TX)^{-1}X^T\boldsymbol{Y},
\]</span> where the former is a constant, computed using the observed data <span class="math inline">\(\boldsymbol{y}\)</span>, and the latter is a random variable: a function of the random vector <span class="math inline">\(\boldsymbol{Y}\)</span>.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Estimators and estimates
</div>
</div>
<div class="callout-body-container callout-body">
<p>We refer to <span class="math display">\[
(X^TX)^{-1}X^T\boldsymbol{Y}
\]</span> as an estimat<strong>or</strong> (a function of a random variable) and <span class="math display">\[
(X^TX)^{-1}X^T\boldsymbol{y}
\]</span> as the corresponding estimat<strong>e</strong>: the observed value of the estimator.</p>
</div>
</div>
<p>You will get used to knowing which is meant from the context, but in this section, we are considering the random variable</p>
<p><span class="math display">\[
\boldsymbol{\hat{\beta}} = (X^TX)^{-1}X^T\boldsymbol{Y},
\]</span> so that we can consider the properties of the estimator: whether it is biased, how far our estimates might be from the true values and so on.</p>
<p>From the assumptions about the distribution of <span class="math inline">\(\boldsymbol{\varepsilon}\)</span> in <a href="basicconcepts.html#sec-parameters" class="quarto-xref"><span>Section 1.4</span></a>, it follows that <span class="math display">\[
E(\boldsymbol{Y})=X\boldsymbol{\beta} \text{ and }
Var(\boldsymbol{Y})=\sigma^2I_n,
\]</span> where <span class="math inline">\(I_n\)</span> is the <span class="math inline">\(n\times n\)</span> identity matrix, and that the distribution of <span class="math inline">\(\boldsymbol{Y}\)</span> is multivariate normal.</p>
<p>From the theory of transformations of multivariate distributions, the expected value of <span class="math inline">\(\boldsymbol{\hat{\beta}}\)</span> is <span class="math display">\[
E(\boldsymbol{\hat{\beta}}) = E((X^TX)^{-1}X^T\boldsymbol{Y})=(X^TX)^{-1}X^T
X\boldsymbol{\beta}=\boldsymbol{\beta}
\]</span> and so <span class="math inline">\(\boldsymbol{\hat{\beta}}\)</span> is an unbiased estimator for <span class="math inline">\(\boldsymbol{\beta}\)</span>.</p>
<p>The variance properties of <span class="math inline">\(\boldsymbol{\hat{\beta}}\)</span> are contained in its covariance matrix. Again from the theory of the multivariate normal, <span class="math display">\[\begin{eqnarray*}
Var(\boldsymbol{\hat{\beta}}) &amp;=&amp; Var( (X^TX)^{-1}X^T\boldsymbol{Y} )
\\ &amp;=&amp;
(X^TX)^{-1}X^TVar(\boldsymbol{y})((X^TX)^{-1}X^T)^T
\\ &amp;=&amp; (X^TX)^{-1}X^T \sigma^2I_nX
(X^TX)^{-1}\\ &amp;=&amp; \sigma^2 (X^TX)^{-1}.
\end{eqnarray*}\]</span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>For <em>designed</em> experiments (where we can choose the values of the independent variables), and more generally when considering different model choices, it can be useful to consider the values in the matrix <span class="math inline">\((X^TX)^{-1}\)</span> and the off-diagonal elements in particular. We usually want to avoid highly correlated parameter estimators; zero or relatively small off-diagonal elements are helpful. We’ll discuss this further when we study hypothesis testing.</p>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise
</div>
</div>
<div class="callout-body-container callout-body">
<div id="exr-slr-meancentred-inference" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2.2</strong></span> Recall the simple linear regression model with alternative parameterisation given in <a href="basicconcepts.html#exr-slr-meancentred-matrix" class="quarto-xref">Exercise&nbsp;<span>1.1</span></a>: <span class="math display">\[
Y_i = \beta_0 + \beta_1(x_i-\bar{x}) + \varepsilon_i.
\]</span> Using the matrix notation:</p>
<ol type="1">
<li>Find the least squares estimator of <span class="math inline">\(\boldsymbol{\beta}\)</span>.</li>
<li>What is the covariance between <span class="math inline">\(\hat{\beta_0}\)</span> and <span class="math inline">\(\hat{\beta_1}\)</span>?</li>
</ol>
</div>
</div>
</div>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Solution
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ol type="1">
<li>From the solution to <a href="basicconcepts.html#exr-slr-meancentred-matrix" class="quarto-xref">Exercise&nbsp;<span>1.1</span></a>, we have <span class="math display">\[
X=\left(\begin{array}{cc} 1 &amp; (x_1-\bar{x})\\
1 &amp; (x_2- \bar{x})\\
\vdots &amp; \vdots \\
1 &amp; (x_n- \bar{x})
\end{array}\right)
\]</span> and so <span class="math display">\[
X^TX = \left(\begin{array}{cc}n &amp; 0 \\
0 &amp; \sum_{i=1}^n(x_i- \bar{x})^2\end{array}\right), \quad (X^TX)^{-1} = \left(\begin{array}{cc}\frac1n &amp; 0 \\
0 &amp; \frac{1}{\sum_{i=1}^n(x_i- \bar{x})^2}\end{array}\right)
\]</span> as <span class="math display">\[
\sum_{i=1}^n(x_i- \bar{x}) = \left(\sum_{i=1}^nx_i\right)-n\bar{x} = n\bar{x} - n\bar{x} = 0.
\]</span> Hence, <span class="math display">\[
\hat{\boldsymbol{\beta}} = (X^TX)^{-1}X^T\boldsymbol{Y} = \left(\begin{array}{c}\bar{Y}\\
\frac{\sum_{i=1}^n (x_i-\bar{x})Y_i}{\sum_{i=1}^n (x_i-\bar{x})^2}\end{array}\right)
\]</span></li>
<li>The matrix <span class="math inline">\((X^TX)^{-1}\)</span> is diagonal (as <span class="math inline">\((X^TX)\)</span> is), and so <span class="math inline">\(Cov(\hat{\beta}_0, \hat{\beta}_1)=0\)</span>. Independence between parameter estimators can be desirable for computational reasons, and so it can be helpful to mean-centre the independent variable in this way.</li>
</ol>
</div>
</div>
</div>
</section>
<section id="sec-res_se" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="sec-res_se"><span class="header-section-number">2.3</span> Estimating the error variance <span class="math inline">\(\sigma^2\)</span></h2>
<p>To construct an estimator for <span class="math inline">\(\sigma^2\)</span>, we will first attempt maximum likelihood estimation. This turns out to produce a <em>biased</em> estimator, but it will be straightforward to apply a correction that gives an unbiased estimator.</p>
<section id="sec-mlevar" class="level3" data-number="2.3.1">
<h3 data-number="2.3.1" class="anchored" data-anchor-id="sec-mlevar"><span class="header-section-number">2.3.1</span> Maximum likelihood estimation</h3>
<p>We previously noted that the log-likelihood for <span class="math inline">\(\boldsymbol{\beta},\sigma^2\)</span> is <span class="math display">\[
\ell(\boldsymbol{\beta},\sigma^2;\boldsymbol{y})=-n\log \sigma-\frac{1}{2\sigma^2}(\boldsymbol{y} -X\boldsymbol{\beta})^T (\boldsymbol{y}
-X\boldsymbol{\beta})+c,
\]</span> where <span class="math inline">\(c\)</span> is a constant term.</p>
<p>After we have maximized this with respect to <span class="math inline">\(\boldsymbol{\beta}\)</span>, we have <span class="math display">\[
\ell(\boldsymbol{\hat{\beta}},\sigma^2;\boldsymbol{y})= -n\log \sigma
-\frac{1}{2\sigma^2}(\boldsymbol{y} -X\boldsymbol{\hat{\beta}})^T (\boldsymbol{y}
-X\boldsymbol{\hat{\beta}})
\]</span> and we now need to maximize this with respect to <span class="math inline">\(\sigma^2\)</span>. This is easily done (by differentiating with respect to <span class="math inline">\(\sigma^2\)</span> and setting the result equal to zero) and produces the MLE, <span class="math display">\[
\hat{\sigma}^2_{MLE}=n^{-1}(\boldsymbol{y} -X\boldsymbol{\hat{\beta}})^T (\boldsymbol{y} -X\boldsymbol{\hat{\beta}})=s_r/n,
\]</span> where <span class="math inline">\(s_r\)</span> was introduced as the residual sum of squares in <a href="#sec-leastsquares" class="quarto-xref"><span>Section 2.1</span></a>.</p>
<p>However, this is a biased estimator. If we define as <span class="math inline">\(S_r\)</span> as the random variable from replacing all instances of <span class="math inline">\(\boldsymbol{y}\)</span> with <span class="math inline">\(\boldsymbol{Y}\)</span> in <span class="math inline">\(s_r\)</span>, we show in the Chapter appendix that <span class="math inline">\(E(S_r)=\sigma^2(n-p)\)</span>.</p>
<p>Hence, if we consider the maximum likelihood estimator <span class="math inline">\(S_r/n\)</span>, we have <span class="math display">\[
E(S_r/n) = \frac{n-p}{n}\sigma^2.
\]</span></p>
</section>
<section id="an-unbiased-estimator-of-sigma2" class="level3" data-number="2.3.2">
<h3 data-number="2.3.2" class="anchored" data-anchor-id="an-unbiased-estimator-of-sigma2"><span class="header-section-number">2.3.2</span> An unbiased estimator of <span class="math inline">\(\sigma^2\)</span></h3>
<p>To obtain an unbiased estimator of <span class="math inline">\(\sigma^2\)</span> we simply divide <span class="math inline">\(S_r\)</span> by <span class="math inline">\(n-p\)</span> instead of <span class="math inline">\(n\)</span>, and so the estimate we compute from our data is</p>
<p><span class="math display">\[
\hat{\sigma}^2 = \frac{(\boldsymbol{y} -X\boldsymbol{\hat{\beta}})^T (\boldsymbol{y} -X\boldsymbol{\hat{\beta}})}{n-p}.
\]</span> Note that <span class="math inline">\(\sqrt{\hat{\sigma}^2}\)</span> is referred to as the <em>residual standard error</em> (and will be reported in R output).</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>This division by <span class="math inline">\(n-p\)</span> rather than <span class="math inline">\(n\)</span> to obtain an unbiased estimator is a generalization of the division by <span class="math inline">\(n-1\)</span> rather than <span class="math inline">\(n\)</span> to obtain an unbiased estimator of the variance in a simple normal sample, which corresponds to the case <span class="math inline">\(p=1\)</span>.</p>
</div>
</div>
</section>
</section>
<section id="distributions-of-the-estimators" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="distributions-of-the-estimators"><span class="header-section-number">2.4</span> Distributions of the estimators</h2>
<p>Considering the estimator <span class="math display">\[
\hat{\boldsymbol{\beta}} = (X^TX)^{-1}X^T\boldsymbol{Y},
\]</span> we have shown that <span class="math display">\[
E(\boldsymbol{\hat{\beta}}) = \boldsymbol{\beta}
\]</span> and <span class="math display">\[
Var(\boldsymbol{\hat{\beta}}) = \sigma^2 (X^TX)^{-1}.
\]</span> and as <span class="math inline">\(\boldsymbol{\hat{\beta}}\)</span> is a linear transformation of the multivariate normal random vector <span class="math inline">\(\boldsymbol{Y}\)</span>, we have the result that</p>
<p><span class="math display">\[
\boldsymbol{\hat{\beta}}\sim N_p(\boldsymbol{\beta},\sigma^2(X^TX)^{-1}).
\]</span> Considering the estimator <span class="math display">\[
\hat{\sigma}^2 = \frac{1}{n-p}(\boldsymbol{Y}-X\boldsymbol{\hat{\beta}})^T(\boldsymbol{Y}-X\boldsymbol{\hat{\beta}}),
\]</span> it can be shown that <span class="math display">\[
(n-p)\hat{\sigma}^2\sim \sigma^2\chi_{n-p}^2,
\]</span> but we will not prove this result in this module. We do show in the Chapter appendix, however, that <span class="math inline">\(\hat{\sigma}^2\)</span> is independent of <span class="math inline">\(\boldsymbol{\hat{\beta}}\)</span>.</p>
</section>
<section id="sec-beta_comps" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="sec-beta_comps"><span class="header-section-number">2.5</span> Confidence intervals for components of <span class="math inline">\(\boldsymbol{\beta}\)</span></h2>
<p>We have just shown that <span class="math display">\[
\boldsymbol{\hat{\beta}}\sim N_p(\boldsymbol{\beta},\sigma^2(X^TX)^{-1}).
\]</span></p>
<p>From the marginal distributions property of the multivariate normal distribution, if we let <span class="math inline">\(\hat{\beta}_i\)</span> be the <span class="math inline">\(i\)</span>th element of <span class="math inline">\(\boldsymbol{\hat{\beta}}\)</span>, then <span class="math display">\[
\hat{\beta}_i\sim N(\beta_i,\sigma^2g_{ii})
\]</span> where <span class="math inline">\(\beta_i\)</span> is the <span class="math inline">\(i\)</span>th element of <span class="math inline">\(\boldsymbol{\beta}\)</span> and <span class="math inline">\(g_{ii}\)</span> is the <span class="math inline">\(i\)</span>th diagonal element of <span class="math inline">\(G=(X^TX)^{-1}\)</span>.</p>
<p>Hence <span class="math display">\[
\frac{\hat{\beta}_i-\beta_i}{\sigma\sqrt{g_{ii}}}\sim N(0,1),\]</span></p>
<p>but we cannot construct confidence intervals using this, as we do not know <span class="math inline">\(\sigma\)</span>; instead we need to estimate it by <span class="math inline">\(\hat{\sigma}\)</span> and use some distribution theory.</p>
<p>It can be shown that <span class="math inline">\(\boldsymbol{\hat{\beta}}\)</span> is independent of <span class="math inline">\(S_r=(n-p)\hat{\sigma}^2\sim \sigma^2\chi_{n-p}^2\)</span>. Standard distributional theory tells us that if <span class="math inline">\(X \sim N(0,1)\)</span>, <span class="math inline">\(Y\sim \chi^2_{\nu}\)</span> and <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent then <span class="math inline">\(\frac{X}{\sqrt{Y/\nu}} \sim t_{\nu}\)</span>. It follows that if we use <span class="math inline">\(\hat{\sigma}\)</span> instead of <span class="math inline">\(\sigma\)</span> to standardise <span class="math inline">\(\hat{\beta}_i\)</span> we get <span class="math display">\[
\frac{\hat{\beta}_i-\beta_i}{\hat{\sigma}\sqrt{g_{ii}}}\sim t_{n-p}.
\]</span> From this we immediately derive a <span class="math inline">\(100(1-\alpha)\%\)</span> confidence interval. Let <span class="math inline">\(t_{m,\alpha}\)</span> denote the upper <span class="math inline">\(100\alpha \%\)</span> point of the <span class="math inline">\(t_m\)</span> distribution. Then <span class="math display">\[\begin{eqnarray*}
1-\alpha &amp;=&amp; P\left(-t_{n-p,1-\alpha/2}\leq
\frac{\hat{\beta}_i-\beta_i}{\hat{\sigma}\sqrt{g_{ii}}} \leq
t_{n-p,1-\alpha/2} \right) \\ &amp;=&amp;
P(-t_{n-p,1-\alpha/2}\hat{\sigma}\sqrt{g_{ii}} \leq
\hat{\beta}_i-\beta_i \leq t_{n-p,1-\alpha/2}
\hat{\sigma}\sqrt{g_{ii}} ) \\ &amp;=&amp;
P(\hat{\beta}_i-t_{n-p,1-\alpha/2}\hat{\sigma}\sqrt{g_{ii}} \leq
\beta_i \leq \hat{\beta}_i+t_{n-p,1-\alpha/2}
\hat{\sigma}\sqrt{g_{ii}}  )
\end{eqnarray*}\]</span> Therefore we have the interval <span class="math display">\[
\hat{\beta}_i\pm t_{n-p,1-\alpha/2} \hat{\sigma}\sqrt{g_{ii}}
\]</span> for <span class="math inline">\(\beta_i\)</span>. This is the two-sided interval. One-sided intervals are also easily constructed. We could also similarly devise a confidence interval for any linear function of the <span class="math inline">\(\beta_i\)</span>’s.</p>
<p>It should be noted that these intervals are for individual parameters. The simultaneous confidence region for two parameters <span class="math inline">\(\beta_j\)</span> and <span class="math inline">\(\beta_k\)</span> is not a rectangle formed from individual confidence intervals: it is an ellipse where the orientation of the axes is related to the correlation of the the estimates of <span class="math inline">\(\beta_j\)</span> and <span class="math inline">\(\beta_k\)</span>. Hence, although the corresponding estimates <span class="math inline">\(b_{j0}\)</span> may not look unlikely for <span class="math inline">\(\beta_j\)</span> and <span class="math inline">\(b_{k0}\)</span> not unlikely for <span class="math inline">\(\beta_k\)</span>, the point <span class="math inline">\((b_{j0},b_{k0})\)</span> may be quite unlikely for <span class="math inline">\((\beta_j, \beta_k)\)</span>.</p>
</section>
<section id="sec-R2" class="level2" data-number="2.6">
<h2 data-number="2.6" class="anchored" data-anchor-id="sec-R2"><span class="header-section-number">2.6</span> Model fit: coefficient of determination <span class="math inline">\(R^2\)</span></h2>
<p>When we are fitting a linear model, the estimate <span class="math inline">\(\boldsymbol{\hat{\beta}}\)</span> minimizes the sum of squares, and the residual sum of squares <span class="math inline">\(S_r\)</span> (<a href="#sec-leastsquares" class="quarto-xref"><span>Section 2.1</span></a>) can be thought of as a measure of fit. A model that achieves a lower residual sum of squares could be considered as giving a better fit to the data, so we could consider using <span class="math inline">\(S_r\)</span> directly as a measure of model fit.</p>
<p>There are two drawbacks to doing this. The first is that <span class="math inline">\(S_r\)</span> depends on the scale of the observations: a model fitted to data in which the response variable is measured in hundreds of some unit could have a larger <span class="math inline">\(S_r\)</span> than a model fitted to data in which the response variable is measured in single units, even though it has a much better fit. The second is that we might prefer a measure that increases as fit improves (whereas <span class="math inline">\(S_r\)</span> decreases). Based on the first drawback, it would make sense to relate <span class="math inline">\(S_r\)</span> to the total variation in the response so that the scale is taken into account, and considering the second as well a better measure of model fit is <span class="math display">\[
R^2=\frac{S_{yy}-S_r}{S_{yy}}.
\]</span> Here <span class="math inline">\(S_{yy}\)</span> (also written <span class="math inline">\(SS_{Total}\)</span>) is the total sum of squares, defined as <span class="math inline">\((\boldsymbol{y}-\boldsymbol{\bar y})^T(\boldsymbol{y}-\boldsymbol{\bar y})=\boldsymbol{y}^T\boldsymbol{y}-n\bar{y}^2\)</span>, which can be thought of as the sum of the squares of residuals when fitting a model which only has a constant term. This value <span class="math inline">\(R^2\)</span> is sometimes called the <em>coefficient of determination</em> and can be thought of the proportion of the total sum of squares that the regression model explains. The residual sum of squares <span class="math inline">\(S_r\)</span> is the <em>unexplained</em> part of the total sum of squares of <span class="math inline">\(y\)</span>.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Models without intercepts/constant term
</div>
</div>
<div class="callout-body-container callout-body">
<p>If the model does not have an intercept or constant term, e.g.&nbsp; <span class="math display">\[
Y_i = \beta x_i + \varepsilon_i,
\]</span> then the comparing <span class="math inline">\(S_r\)</span> and <span class="math inline">\(S_{yy}\)</span> isn’t particularly helpful, as if we were to remove the regressor <span class="math inline">\(x_i\)</span> from the the model, we would not be fitting a model with a constant term only. If a model is fitted in R with no constant term (see chapter 4), then <span class="math inline">\(R^2\)</span> is computed as <span class="math display">\[
\frac{\sum {y_i}^2 - S_r}{\sum {y_i}^2},
\]</span> noting that <span class="math inline">\(\sum {y_i}^2\)</span> is the residual sum of squares for a model with <span class="math inline">\(E(Y_i) = 0\)</span>.</p>
</div>
</div>
<p>Note that in the case of the simple linear regression model, <span class="math inline">\(R^2\)</span> equals <span class="math inline">\(r^2\)</span>, the squared sample correlation coefficient between <span class="math inline">\(\boldsymbol{y}\)</span> and <span class="math inline">\(\boldsymbol{x}\)</span>, so we can see <span class="math inline">\(R^2\)</span> as the generalization of <span class="math inline">\(r^2\)</span> to the more general linear model. <span class="math inline">\(R^2\)</span> is also called the squared multiple correlation coefficient and it always lies between 0 and 1.</p>
<p>There is also an ‘Adjusted R-squared’ value. This value takes into account the number of regressor variables used in the model. It is defined as <span class="math display">\[R^2(adj)=1-\frac{S_r/(n-p)}{SS_{total}/(n-1)}\]</span></p>
<p>You will see a lot more about model selection later in the course, but it does not make sense to simply pick the model with the largest <span class="math inline">\(R^2\)</span> as this necessarily increases as the number of regressor variables increases. The adjusted R-squared value does not necessarily increase as the number of regressor variables increases, suggesting that using the adjusted R-squared value in model selection is more sensible.</p>
</section>
<section id="chapter-appendix" class="level2" data-number="2.7">
<h2 data-number="2.7" class="anchored" data-anchor-id="chapter-appendix"><span class="header-section-number">2.7</span> Chapter appendix</h2>
<p>You will not be examined on your ability to derive results such as the following. However, you are encouraged to study these sections (and ask questions if you need to!) because understanding this sort of content can you help you understand other methods if you need to learn them independently.</p>
<section id="deriving-the-least-squares-estimator" class="level3" data-number="2.7.1">
<h3 data-number="2.7.1" class="anchored" data-anchor-id="deriving-the-least-squares-estimator"><span class="header-section-number">2.7.1</span> Deriving the least squares estimator</h3>
<p>To find the least squares estimates we must solve <span class="math display">\[
\frac{d (\boldsymbol{y}-X\boldsymbol\beta)^T(\boldsymbol{y}-X\boldsymbol\beta)}{d \boldsymbol\beta} =
\boldsymbol {0}.
\]</span> Now <span class="math display">\[
(\boldsymbol{y}-X\boldsymbol\beta)^T(\boldsymbol{y}-X\boldsymbol\beta) = \left(\boldsymbol{y}^T \boldsymbol{y}
-\boldsymbol\beta^TX^T\boldsymbol{y} -\boldsymbol{y}^TX\boldsymbol\beta + \boldsymbol\beta^T(X^T X)\boldsymbol\beta\right)
\]</span> and so <span class="math display">\[
\frac{d (\boldsymbol{y}-X\boldsymbol\beta)^T(\boldsymbol{y}-X\boldsymbol\beta)}{d \boldsymbol\beta}=-2X^T\boldsymbol{y} + 2(X^TX)\boldsymbol\beta.
\]</span></p>
<p>We equate the above to <span class="math inline">\(\boldsymbol{0}\)</span> for <span class="math inline">\(\boldsymbol\beta=\hat{\boldsymbol\beta}\)</span>, the least squares estimator, giving <span class="math display">\[
\boldsymbol{0}=-2X^T\boldsymbol{y} +
2(X^TX)\hat{\boldsymbol\beta},
\]</span> (sometimes referred to as the <em>normal equation</em>) which gives us the result <span class="math display">\[
\hat{\boldsymbol\beta}=(X^TX)^{-1}X^T\boldsymbol{y}
\]</span></p>
</section>
<section id="sec-bias" class="level3" data-number="2.7.2">
<h3 data-number="2.7.2" class="anchored" data-anchor-id="sec-bias"><span class="header-section-number">2.7.2</span> Bias of the maximum likelihood estimator</h3>
<p>We define <span class="math display">\[
\boldsymbol{e}:=\boldsymbol{Y}-X\boldsymbol{\hat{\beta}}=\boldsymbol{Y}-X(X^TX)^{-1}X^T\boldsymbol{Y},
\]</span></p>
<p>which we think of as a vector of random residuals. If we define <span class="math display">\[M:=I_n-X(X^TX)^{-1}X^T,\]</span> we can re-write this as <span class="math inline">\(\boldsymbol{e}=M\boldsymbol{Y}\)</span>.</p>
<p>Note that <span class="math inline">\(MX = \boldsymbol{0}\)</span> and <span class="math inline">\(M^2=M\)</span>, i.e.&nbsp;<span class="math inline">\(M\)</span> is an <em>idempotent matrix</em>.</p>
<p>We have <span class="math display">\[
E(\boldsymbol{e})=ME(\boldsymbol{Y})=MX\boldsymbol{\beta}=\boldsymbol{0},
\]</span> since <span class="math inline">\(MX=\boldsymbol{0}\)</span>.</p>
<p>We also have <span class="math display">\[
Var(\boldsymbol{e})=MVar(\boldsymbol{Y})M^T=M\sigma^2I_nM=\sigma^2M^2=\sigma^2M.
\]</span></p>
<p>Thus <span class="math inline">\(M\)</span> is also related to the variance-covariance matrix of the residuals. The variance of an individual residual is <span class="math inline">\(\sigma^2\)</span> times the corresponding diagonal element of <span class="math inline">\(M\)</span>.</p>
<p>Every idempotent matrix except the identity is singular (non-invertible), and its rank is equal to its trace. We have (using the result that <span class="math inline">\(\mathrm{tr}(AB)=\mathrm{tr}(BA)\)</span>) <span class="math display">\[
\mathrm{tr}(M)=\mathrm{tr}(I_n-X(X^TX)^{-1}X^T)=n-\mathrm{tr}((X^TX)^{-1}X^TX) =n-\mathrm{tr}(I_p)=n-p.
\]</span> and so the rank of <span class="math inline">\(M\)</span> is also <span class="math inline">\(n-p\)</span>.</p>
<p>Since <span class="math inline">\(E(e_i)=0\)</span> <span class="math inline">\((i=1,\ldots,n)\)</span>, we can observe that the diagonal elements of the covariance matrix <span class="math inline">\(Var(\boldsymbol{e})\)</span> are <span class="math inline">\(E(e_1^2),\ldots,E(e_n^2)\)</span> and so we have that <span class="math inline">\(E(e_i^2)=\sigma^2m_{ii}\)</span>, where <span class="math inline">\(m_{11},\ldots,m_{nn}\)</span> are diagonal elements of <span class="math inline">\(M\)</span>, writing <span class="math inline">\(M=(m_{ij})\)</span>. Then we can see <span class="math display">\[
E\left(\sum_{i=1}^ne_i^2\right)=\sum_{i=1}^n E(e_i^2) = \sigma^2
\sum_{i=1}^n m_{ii} = \sigma^2 tr(M) = \sigma^2(n-p)
\]</span> This shows us that the maximum likelihood estimator is biased: <span class="math display">\[
E(\hat{\sigma}^2_{MLE}) = E\left(\frac{\sum_{i=1}^ne_i^2}{n}\right)=\sigma^2\frac{(n-p)}{n}.
\]</span></p>
</section>
<section id="sec-independence" class="level3" data-number="2.7.3">
<h3 data-number="2.7.3" class="anchored" data-anchor-id="sec-independence"><span class="header-section-number">2.7.3</span> Independence between <span class="math inline">\(\hat{\sigma}^2\)</span> and <span class="math inline">\(\boldsymbol{\hat{\beta}}\)</span></h3>
<p>Using the definitions of <span class="math inline">\(\boldsymbol{e}\)</span> and <span class="math inline">\(M\)</span> from above</p>
<p><span class="math display">\[
\boldsymbol{e}:=\boldsymbol{Y}-X\boldsymbol{\hat{\beta}} = M\boldsymbol{Y}
\]</span> we have <span class="math display">\[
\hat{\sigma}^2 = \frac{1}{n-p}\boldsymbol{e}^T\boldsymbol{e}
\]</span> As <span class="math inline">\(\hat{\sigma}^2\)</span> is a transformation of the random vector <span class="math inline">\(\boldsymbol{e}\)</span>, we just need to show that is <span class="math inline">\(\boldsymbol{\hat{\beta}}\)</span> is independent of <span class="math inline">\(\boldsymbol{e}\)</span>.</p>
<p>Both <span class="math inline">\(\boldsymbol{e}\)</span> and <span class="math inline">\(\boldsymbol{\hat{\beta}}\)</span> are linear transformations of the multivariate normal random vector <span class="math inline">\(\boldsymbol{Y}\)</span>, and their joint distribution is multivariate normal.</p>
<p>We have <span class="math display">\[\begin{align}
Cov(\boldsymbol{e},\boldsymbol{\hat{\beta}}) &amp;= Cov(M\boldsymbol{Y}, (X^TX)^{-1}X^T\boldsymbol{Y}) \\
&amp;= MVar(\boldsymbol{Y})X(X^TX)^{-1}\\
&amp;= M\sigma^2 I_n X(X^TX)^{-1} \\
&amp;= \sigma^2MX(X^TX)^{-1}\\
&amp;= \boldsymbol{0},
\end{align}\]</span> as <span class="math inline">\(MX = \boldsymbol{0}\)</span>. For multivariate normal random vectors, zero covariance implies independence.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./basicconcepts.html" class="pagination-link" aria-label="Basic concepts">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Basic concepts</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./qualitative.html" class="pagination-link" aria-label="Qualitative independent variables">
        <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Qualitative independent variables</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>