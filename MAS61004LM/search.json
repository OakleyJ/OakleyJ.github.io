[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A Second Course on Linear Models",
    "section": "",
    "text": "Introduction\nThese notes are written for students on MAS61004. I have called this a second course because to get onto the MSc you will almost certainly have learned something about linear models already! These notes will still cover topics from the beginning, without assuming you have studied the content before, but we will work through some of the earlier topics fairly quickly.\nStudents who have taken the Graduate Certificate in Statistics to gain entry onto this MSc have studied linear models in the MAS5052 module. The Graduate Certificate lecture notes on linear models are available here. and you may find them helpful for revision.\nWe will be using R for implementing linear modelling methods, and I will not assume you have used R before. You will be learning R in the EDA with R part of this module, which is taught in parallel with this part."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "A Second Course on Linear Models",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nSome parts of the notes are new this year, others were written and modified by various colleagues who taught this content over the years: Eleanor Stillman, Jonathan Jordan and Kostas Triantafyllopoulos, Kevin Walters."
  },
  {
    "objectID": "basicconcepts.html#motivating-example",
    "href": "basicconcepts.html#motivating-example",
    "title": "1  Basic concepts",
    "section": "1.1 Motivating example",
    "text": "1.1 Motivating example\n\nExample 1.1 Respiration data\n\nThe following is an extract from a data set with data on 305 individuals:\n\nlibrary(tidyverse)\nrespiration &lt;- read_table(\"https://oakleyj.github.io/exampledata/resp_data.txt\")\nrespiration\n\n# A tibble: 305 × 8\n     vol exercise pulse pollution asbestos  home smoke asthma\n   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1  117.     15.8 17.5     11.9          1     1     1      0\n 2  148.     23.5 28.4      1.88         1     3     1      1\n 3  214.     13.8 15.0      2.38         1     1     0      0\n 4  162.     15.6 15.7      6.34         1     3     0      1\n 5  352.     26.6 19.7      1.63         0     4     0      0\n 6  304.     23.2 14.2      2.72         0     4     0      0\n 7  157.     29.0 33.7      4.03         0     1     1      0\n 8  110.     16.6 14.7      1.24         1     2     1      0\n 9  218.     15.1  9.16     1.14         0     2     0      0\n10  246.     16.9 21.9      0.731        0     4     0      0\n# ℹ 295 more rows\n\n\nThe meanings of the column headings are:\n\nvol - volume of air expelled whilst breathing out\nexer - number of hours exercise per month\npulse- scaled resting pulse rate\npoll - lifetime exposure to air pollution\nasbestos - has the individual been exposed to asbestos (1=exposed, 0=unexposed)\nhome - place of residence (1=England, 2=Scotland, 3=Wales, 4=Northern Ireland)\nsmoke - have they ever smoked (1=yes, 0=no)\nasthma - do they have asthma (1=yes, 0=no)\n\nThe key question we are interested in is how the volume of air expelled relates to the other variables.\nThere are a number of types of variables in this data set: vol, exer, pulse and poll are continuous variables, while asbestos, home, smoke and asthma are categorical (factor) variables. Of the latter, home has multiple levels while the others are binary.\nThe variable we are trying to predict is the response variable, also referred to as the dependent variable; in this example, the response is vol. The variables that we use to predict the response are called either predictor, explanatory or independent variables.\nQuestions we might like to answer include:\n\nWhich variables are ‘best’ at explaining the variation in the volume of air and do we need all the variables? (This amounts to choosing a statistical model.)\nHow much of the variation in the volume of air can we account for with our model?\nDoes the model that we come up with satisfy the implicit assumptions of a linear model?\nHow can we perform hypothesis tests relating to parameters in our statistical model?\nAre there observations that don’t fit our model?\nAre there observations that might be exerting a lot of influence over our model parameters?\nHow can we make predictions about volumes based on new observations and how can we calculate confidence intervals for these predictions?"
  },
  {
    "objectID": "basicconcepts.html#the-idea-of-a-linear-model",
    "href": "basicconcepts.html#the-idea-of-a-linear-model",
    "title": "1  Basic concepts",
    "section": "1.2 The idea of a linear model",
    "text": "1.2 The idea of a linear model\nIn simple linear regression, we have \\(n\\) observations of two variables, \\((x_i,y_i)\\) for \\(i=1,\\ldots,n\\). We treat \\(x_i\\) as an explanatory variable and \\(y_i\\) as the response variable (or dependent variable), and we attempt to fit a straight line \\(y=\\beta_0+\\beta_1 x\\) to our data points. We further suppose that each \\(y_i\\) is an observed value of a random variable \\(Y_i\\).\n\n\n\n\n\n\nNotation: lower and upper case letters\n\n\n\nA convention is to use an upper case letter to denote a random variable, and a lower case letter to denote the observed value of that random variable, e.g., in statistical modelling we treat some data \\(y_1,\\ldots,y_n\\) as observations of random variables \\(Y_1,\\ldots,Y_n\\), and our statistical model is a probability distribution for \\(Y_1,\\ldots,Y_n\\).\nWe will not always keep to this convention! as it can become notationally difficult. You will need to be alert to the context as to whether we are considering a random variable or an observed value, though I will try to make it as clear as possible in these notes.\n\n\nA statistical model here takes the form \\(Y_i=\\beta_0+\\beta_1 x_i+\\varepsilon_i\\), where \\(\\varepsilon_i\\) is a (random) “error” term giving the difference between the straight line value and the actual value \\(y_i\\). In this section we discuss the generalization of this known as multiple regression, which allows for more than one explanatory variable.\nSuppose that for each observation \\(i\\) we have a response \\(y_i\\) and \\(r\\) explanatory variables \\(x_{i1},\\ldots,x_{ir}\\), giving data \\[\n(x_{i1},\\ldots,x_{ir},y_i), \\quad i=1,\\ldots,n.\n\\] We could propose the statistical model \\[\nY_i=\\beta_0+\\beta_1 x_{i1}+\\cdots+\\beta_rx_{ir}+\\varepsilon_i.\n\\tag{1.1}\\] This has two parts, a linear predictor \\(\\beta_0+\\beta_1 x_{i1}+\\cdots+\\beta_r x_{ir}\\) and a random error \\(\\varepsilon_i\\). The linear predictor formalizes the idea that the response is a linear combination of terms involving the explanatory variables. The error term allows for variation in the response for identical values of the explanatory variables (not necessarily measurement errors); we later impose some conditions on \\(\\varepsilon_i\\).\nWhen \\(r=2\\), giving \\(Y_i=\\beta_0+\\beta_1 x_{i1}+\\beta_2 x_{i2}+\\varepsilon_i\\), we are fitting a plane to the data; as the number of parameters increases we fit increasingly higher-dimensional hyper-planes.\nAlternatively, reverting to a single explanatory variable, we might wish to express the relationship between \\(y\\) and \\(x\\) in a quadratic way: \\[\nY_i=\\beta_0+\\beta_1 x_i+\\beta_2 x_i^2+\\varepsilon_i.\n\\tag{1.2}\\] In this model \\(\\beta_0+\\beta_1 x_i+\\beta_2 x_i^2\\) is the linear predictor. Although the relationship is quadratic, we would still call this a linear model because it is linear in the parameters \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\beta_2\\). It is a common misconception that linear models require the relationship between the response and explanatory variables to be linear, but in this terminology a linear model is one which is linear in the parameters \\(\\beta_i\\), not necessarily in the explanatory variables."
  },
  {
    "objectID": "basicconcepts.html#matrix-formulation",
    "href": "basicconcepts.html#matrix-formulation",
    "title": "1  Basic concepts",
    "section": "1.3 Matrix formulation",
    "text": "1.3 Matrix formulation\n\n\n\n\n\n\nNote\n\n\n\nIn these notes vectors are written in bold and scalars in normal text, so \\(\\boldsymbol{y}\\) is a vector and \\(y\\) is a single value.\n\n\nIt is convenient to express linear models using vectors and matrices. We can always collect the observed values of the variable \\(y\\) into a vector \\(\\boldsymbol{y} =(y_1,\\ldots,y_n)^T\\) and we can similarly define \\(\\boldsymbol{x} =(x_1,\\ldots,x_n)^T\\) and \\(\\boldsymbol{\\varepsilon}=(\\varepsilon_1,\\ldots,\\varepsilon_n)^T\\). If we also define \\(\\boldsymbol{1}_n=(1,,\\ldots,1)^T\\) to be a vector of \\(n\\) ones, we can write the simple regression model as \\[\\begin{equation}\\label{eq3_3}\n\\boldsymbol{Y}=\\beta_0 \\boldsymbol{1}_n+\\beta_1 \\boldsymbol{x}+\\boldsymbol{\\varepsilon}.\n\\end{equation}\\] Even more neatly, if we define the parameter vector \\(\\boldsymbol{\\beta}=(\\beta_0,\\beta_1)^T\\) and the \\(n\\times 2\\) matrix \\(X\\) is defined to have \\(\\boldsymbol{1}_n\\) as its first column and \\(\\boldsymbol{x}\\) as its second, then\n\\[\n\\boldsymbol{Y}=X\\boldsymbol{\\beta} +\\boldsymbol{\\varepsilon}.\n\\tag{1.3}\\] The form (Equation 1.3) naturally generalizes to more complicated models; for a suitable definition of the parameter vector \\(\\boldsymbol{\\beta}\\) and the matrix \\(X\\) (also called the design matrix) we can express both Equation 1.1 and Equation 1.2 in this form.\n\n\n\n\n\n\nThe importance of matrix notation\n\n\n\nMatrix notation is really useful. As well as clarifying the definition of a linear model (any model that can be expressed in the form Equation 1.3), it means that any result obtained using the matrix notation can be applied to all linear models."
  },
  {
    "objectID": "basicconcepts.html#sec-parameters",
    "href": "basicconcepts.html#sec-parameters",
    "title": "1  Basic concepts",
    "section": "1.4 Parameters and assumptions",
    "text": "1.4 Parameters and assumptions\nThe linear model is expressed by (Equation 1.3). In its general form,\n\n\\(\\boldsymbol{y}\\) is a \\(n\\times 1\\) vector of observed random variables.\n\\(X\\) is an \\(n\\times p\\) matrix of known coefficients (perhaps observed or controlled values of other explanatory variables, but also perhaps functions of such explanatory variables, or just constants).\n\\(\\boldsymbol{\\beta}\\) is a \\(p\\times 1\\) vector of unknown parameters.\n\\(\\boldsymbol{\\varepsilon}\\) is an \\(n \\times 1\\) vector of unobserved random variables.\n\nThe number of components, \\(p\\), of \\(\\boldsymbol{\\beta}\\) is allowed to be whatever we wish, as long as it is not too large to be sensible for the number of observations. In the multiple regression model (Equation 1.1) it would be \\(r+1\\), while in the quadratic regression model (Equation 1.2) it would be 3.\nTo fully define our model we need to make some assumptions on \\(\\boldsymbol{\\varepsilon}\\). In the general linear model, we assume that \\(\\boldsymbol{\\varepsilon}\\) has a multivariate normal distribution, whose components have zero mean, are independent, and have common variance \\(\\sigma^2\\) (that they have the same variance is referred to as homoscedasticity), which is another unknown parameter. Testing these assumptions and what to do if they are not satisfied will be a major part of this module.\nFrom these assumptions, \\[\n\\boldsymbol{\\varepsilon}\\sim N_n(\\boldsymbol{0},\\sigma^2I_n)\n\\] where \\(N_n\\) represents an \\(n\\)-dimensional multivariate normal distribution.\nSince \\(\\boldsymbol{y}=X\\boldsymbol{\\beta}+\\boldsymbol{\\varepsilon}\\), it follows (from the linear transformation property for the multivariate normal) that \\[\n\\boldsymbol{y}\\sim N_n(X\\boldsymbol{\\beta},\\sigma^2I_n).\\]"
  },
  {
    "objectID": "basicconcepts.html#terminology",
    "href": "basicconcepts.html#terminology",
    "title": "1  Basic concepts",
    "section": "1.5 Terminology",
    "text": "1.5 Terminology\nWe can think of each column of the \\(X\\) matrix as comprising \\(n\\) values of a regressor variable, also called an independent variable. So we have in general one response variable and \\(p\\) (\\(p=r+1\\) in multiple regression, \\(p=3\\) in quadratic regression) regressor variables.\nSometimes this terminology may seem strange, because, for example, in the simple linear regression model the \\(X\\) matrix has two columns, the first of which is a column of ones. We still would say that this column defines a regressor variable, but this ‘variable’ is a constant.\nRegressor variables are also sometimes called explanatory variables, but in this course we will give this term a slightly different meaning. Consider the quadratic regression (Equation 1.2). Here there are three regressor variables (the constant, the \\(x_i\\) column and the \\(x_i^2\\) column), but there is really only one \\(x\\) variable. The three regressor variables are all functions of the \\(x\\) variable. In this course we will refer to the \\(x\\) variable as the single explanatory variable in the quadratic regression model. In general, the regressor variables will always be functions of the explanatory variables.\n\nExample 1.2 An example of the design matrix \\(X\\) - polynomial regression\n\nThe general polynomial regression with one explanatory variable, a generalization of the quadratic regression Equation 1.2, is \\[\ny_i=\\beta_0+\\beta_1x_i+\\cdots+\\beta_rx_i^r+\\varepsilon_i\n\\] which is turned into the general linear model form by \\[\n\\boldsymbol{y}=\\left(\\begin{array}{c} y_1 \\\\ y_2 \\\\ \\vdots \\\\\ny_n\\end{array}\\right), \\quad X=\\left(\\begin{array}{cccc} 1 & x_1 &\n\\cdots & x_1^r \\\\ 1 & x_2 & \\cdots & x_2^r \\\\ \\vdots & \\vdots &\n\\ddots & \\vdots \\\\ 1 & x_n & \\cdots & x_n^r\\end{array}\\right),\n\\quad \\boldsymbol{\\beta}=\\left(\\begin{array}{c} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\\n\\beta_r\\end{array}\\right), \\quad\n\\boldsymbol{\\varepsilon}=\\left(\\begin{array}{c} \\varepsilon_1 \\\\ \\varepsilon_2 \\\\\n\\vdots \\\\ \\varepsilon_n\\end{array}\\right)\n\\] and \\(p=r+1\\)."
  },
  {
    "objectID": "parameterestimation.html#sec-leastsquares",
    "href": "parameterestimation.html#sec-leastsquares",
    "title": "2  Parameter estimation for linear models",
    "section": "2.1 Estimating the coefficients",
    "text": "2.1 Estimating the coefficients\nGiven a linear model as in \\[\n\\boldsymbol{Y}=X\\boldsymbol{\\beta} +\\boldsymbol{\\varepsilon},\n\\] we will want to use our observed data \\(\\boldsymbol{y}\\) to estimate the parameters \\(\\boldsymbol{\\beta}\\); for example in simple linear regression we are estimating an intercept \\(\\beta_0\\) and gradient term \\(\\beta_1\\).\nSuppose we have estimates of the parameters \\(\\hat{\\beta_0},\\hat{\\beta_1},\\ldots,\\hat{\\beta}_{p-1}\\), or in vector form \\(\\boldsymbol{\\hat{\\beta}}\\). Then the fitted value \\(\\hat{y}_i\\) for observation \\(i\\) is defined as \\[\\hat{y}_i:=\\hat{\\beta_0}+\\hat{\\beta_1}x_{i1}+\\ldots+\\hat{\\beta}_{p-1}x_{i,p-1}=\\boldsymbol{x}_i^T\\boldsymbol{\\hat{\\beta}},\\] where \\(\\boldsymbol{x}_i^T\\) is the (row) vector consisting of row \\(i\\) of \\(X\\). The matrix product \\(X\\boldsymbol{\\hat{\\beta}}\\) then gives the vector of fitted values for all observations.\n\n\n\n\n\n\nThe ‘hat’ notation\n\n\n\nNote the difference between \\(\\boldsymbol{\\beta}\\) and \\(\\boldsymbol{\\hat{\\beta}}\\): \\(\\boldsymbol{\\beta}\\) is the vector of true parameter values, \\(\\boldsymbol{\\hat{\\beta}}\\) is the estimate of the vector of true parameter values.\n\n\nFor observation \\(i\\), the difference between the observed value \\(y_i\\) and the fitted value \\(\\hat{y}_i\\) is called the residual for that observation, denoted by \\(e_i\\).\n\n\n\n\n\n\nErrors and residuals\n\n\n\nNote the difference between the error (\\(\\varepsilon_i\\)) and the residual (\\(e_i\\)) for observation \\(i\\). The error is part of the statistical model that we fit to our data. The residual is the observed value of the difference between the response and the fitted value. As we will see later, the observed residuals, the \\(e_i\\), allow us to estimate the variance \\(\\sigma^2\\) of the error terms, the \\(\\varepsilon_i\\)\n\n\nThe vector of residuals will be \\[\n\\boldsymbol{e}:=\\left(\\begin{array}{c} e_1\\\\ \\vdots\n\\\\ e_n\\end{array}\\right)=\\boldsymbol{y}-X\\boldsymbol{\\hat{\\beta}}.\n\\] Residuals play a key role in linear models. In later chapters we will see how they can be used to assess whether the statistical model we use to describe the relationship between our variables meets the assumptions underpinning linear model theory.\nThe sum of squares of the residuals, written as \\(s_r\\) or \\(s(\\boldsymbol{\\hat{\\beta}})\\) is nicely expressed as \\[\\begin{equation}\ns_r=s(\\boldsymbol{\\hat{\\beta}}):=\\sum_{i=1}^ne_i^2=\\boldsymbol{e}^T\\boldsymbol{e}=(\\boldsymbol{y}-X\\boldsymbol{\\hat{\\beta}})^T(\\boldsymbol{y}-X\\boldsymbol{\\hat{\\beta}}).\n\\end{equation}\\] This sum is known as residual sum of squares and plays an important role in the the analysis of linear models. To see this, consider the likelihood of \\(\\boldsymbol{\\beta}\\) and \\(\\sigma\\); this comes directly from the definition of the multivariate normal density: \\[\\begin{eqnarray*}\nL(\\boldsymbol{\\beta},\\sigma^2;\\boldsymbol{y}) &=& f(\\boldsymbol{y}|\\boldsymbol{\\beta},\\sigma^2) \\\\\n&=& \\frac{1}{(2\\pi \\sigma^2)^{n/2}}\n\\exp\\left(-\\frac{1}{2\\sigma^2}(\\boldsymbol{y} -X\\boldsymbol{\\beta})^T (\\boldsymbol{y}\n-X\\boldsymbol{\\beta}) \\right) \\\\ & \\propto & \\sigma^{-n}\n\\exp\\left(-\\frac{1}{2\\sigma^2}(\\boldsymbol{y} -X\\boldsymbol{\\beta})^T (\\boldsymbol{y}\n-X\\boldsymbol{\\beta}) \\right)\n\\end{eqnarray*}\\] In this derivation we have used the fact that \\(|\\sigma^2I_n|=(\\sigma^2)^n\\).\nThe log likelihood is thus \\[\\ell(\\boldsymbol{\\beta},\\sigma^2;\\boldsymbol{y})=-n\\log \\sigma-\\frac{1}{2\\sigma^2}(\\boldsymbol{y} -X\\boldsymbol{\\beta})^T (\\boldsymbol{y}\n-X\\boldsymbol{\\beta})+c,\\] where \\(c\\) is a constant term which does not depend on \\(\\boldsymbol{\\beta}\\) or \\(\\sigma\\) and can be ignored when maximizing.\nTo maximize this log likelihood with respect to \\(\\boldsymbol{\\beta}\\), we obviously must minimize \\((\\boldsymbol{y}-X\\boldsymbol{\\beta})^T(\\boldsymbol{y}-X\\boldsymbol{\\beta})\\), which is exactly the residual sum of squares. So the maximum likelihood estimator \\(\\hat{\\boldsymbol{\\beta}}\\) for the parameters \\(\\boldsymbol{\\beta}\\) is found by minimizing the residual sum of squares.\nMinimizing the residual sum of squares is called the method of least squares, and can also be used when the data are not assumed to be normally distributed. Intuitively, a value of \\(\\boldsymbol{\\hat{\\beta}}\\) that makes the residuals small ‘fits’ the data well, justifying the idea of the least squares estimator.\n\nTheorem 2.1 Assume that \\(X\\) has rank \\(p\\). Then the least squares estimate of \\(\\boldsymbol{\\beta}\\) is \\[\\begin{equation}\\label{eq3_7}\n\\boldsymbol{\\hat{\\beta}}=(X^TX)^{-1}X^T\\boldsymbol{y}.\n\\end{equation}\\]\n\nWe give a proof in the Chapter appendix."
  },
  {
    "objectID": "parameterestimation.html#mean-and-variance-of-the-least-squares-estimator",
    "href": "parameterestimation.html#mean-and-variance-of-the-least-squares-estimator",
    "title": "2  Parameter estimation for linear models",
    "section": "2.2 Mean and variance of the least squares estimator",
    "text": "2.2 Mean and variance of the least squares estimator\nWith apologies for some confusing notation (though this is the convention), we are going to write both\n\\[\n\\boldsymbol{\\hat{\\beta}} = (X^TX)^{-1}X^T\\boldsymbol{y},\n\\] and \\[\n\\boldsymbol{\\hat{\\beta}} = (X^TX)^{-1}X^T\\boldsymbol{Y},\n\\] where the former is a constant, computed using the observed data \\(\\boldsymbol{y}\\), and the latter is a random variable: a function of the random vector \\(\\boldsymbol{Y}\\).\n\n\n\n\n\n\nEstimators and estimates\n\n\n\nWe refer to \\[\n(X^TX)^{-1}X^T\\boldsymbol{Y}\n\\] as an estimator (a function of a random variable) and \\[\n(X^TX)^{-1}X^T\\boldsymbol{y}\n\\] as the corresponding estimate: the observed value of the estimator.\n\n\nYou will get used to knowing which is meant from the context, but in this section, we are considering the random variable\n\\[\n\\boldsymbol{\\hat{\\beta}} = (X^TX)^{-1}X^T\\boldsymbol{Y},\n\\] so that we can consider the properties of the estimator: whether it is biased, how far our estimates might be from the true values and so on.\nFrom the assumptions about the distribution of \\(\\boldsymbol{\\varepsilon}\\) in Section 1.4, it follows that \\[\nE(\\boldsymbol{Y})=X\\boldsymbol{\\beta} \\text{ and }\nVar(\\boldsymbol{Y})=\\sigma^2I_n,\n\\] where \\(I_n\\) is the \\(n\\times n\\) identity matrix, and that the distribution of \\(\\boldsymbol{Y}\\) is multivariate normal.\nFrom the theory of transformations of multivariate distributions, the expected value of \\(\\boldsymbol{\\hat{\\beta}}\\) is \\[\nE(\\boldsymbol{\\hat{\\beta}}) = E((X^TX)^{-1}X^T\\boldsymbol{Y})=(X^TX)^{-1}X^T\nX\\boldsymbol{\\beta}=\\boldsymbol{\\beta}\n\\] and so \\(\\boldsymbol{\\hat{\\beta}}\\) is an unbiased estimator for \\(\\boldsymbol{\\beta}\\).\nThe variance properties of \\(\\boldsymbol{\\hat{\\beta}}\\) are contained in its covariance matrix. Again from the theory of the multivariate normal, \\[\\begin{eqnarray*}\nVar(\\boldsymbol{\\hat{\\beta}}) &=& Var( (X^TX)^{-1}X^T\\boldsymbol{Y} )\n\\\\ &=&\n(X^TX)^{-1}X^TVar(\\boldsymbol{y})((X^TX)^{-1}X^T)^T\n\\\\ &=& (X^TX)^{-1}X^T \\sigma^2I_nX\n(X^TX)^{-1}\\\\ &=& \\sigma^2 (X^TX)^{-1}.\n\\end{eqnarray*}\\]\n\n\n\n\n\n\nNote\n\n\n\nFor designed experiments (where we can choose the values of the independent variables), and more generally when considering different model choices, it can be useful to consider the values in the matrix \\((X^TX)^{-1}\\) and the off-diagonal elements in particular. We usually want to avoid highly correlated parameter estimators; zero or relatively small off-diagonal elements are helpful. We’ll discuss this further when we study hypothesis testing."
  },
  {
    "objectID": "parameterestimation.html#sec-res_se",
    "href": "parameterestimation.html#sec-res_se",
    "title": "2  Parameter estimation for linear models",
    "section": "2.3 Estimating the error variance \\(\\sigma^2\\)",
    "text": "2.3 Estimating the error variance \\(\\sigma^2\\)\nTo construct an estimator for \\(\\sigma^2\\), we will first attempt maximum likelihood estimation. This turns out to produce a biased estimator, but it will be straightforward to apply a correction that gives an unbiased estimator.\n\n2.3.1 Maximum likelihood estimation\nWe previously noted that the log-likelihood for \\(\\boldsymbol{\\beta},\\sigma^2\\) is \\[\n\\ell(\\boldsymbol{\\beta},\\sigma^2;\\boldsymbol{y})=-n\\log \\sigma-\\frac{1}{2\\sigma^2}(\\boldsymbol{y} -X\\boldsymbol{\\beta})^T (\\boldsymbol{y}\n-X\\boldsymbol{\\beta})+c,\n\\] where \\(c\\) is a constant term.\nAfter we have maximized this with respect to \\(\\boldsymbol{\\beta}\\), we have \\[\n\\ell(\\boldsymbol{\\hat{\\beta}},\\sigma^2;\\boldsymbol{y})= -n\\log \\sigma\n-\\frac{1}{2\\sigma^2}(\\boldsymbol{y} -X\\boldsymbol{\\hat{\\beta}})^T (\\boldsymbol{y}\n-X\\boldsymbol{\\hat{\\beta}})\n\\] and we now need to maximize this with respect to \\(\\sigma^2\\). This is easily done (by differentiating with respect to \\(\\sigma^2\\) and setting the result equal to zero) and produces the MLE, \\[\n\\hat{\\sigma}^2_{MLE}=n^{-1}(\\boldsymbol{y} -X\\boldsymbol{\\hat{\\beta}})^T (\\boldsymbol{y} -X\\boldsymbol{\\hat{\\beta}})=s_r/n,\n\\] where \\(s_r\\) was introduced as the residual sum of squares in Section 2.1.\nHowever, this is a biased estimator. If we define as \\(S_r\\) as the random variable from replacing all instances of \\(\\boldsymbol{y}\\) with \\(\\boldsymbol{Y}\\) in \\(s_r\\), we show in the Chapter appendix that \\(E(S_r)=\\sigma^2(n-p)\\).\nHence, if we consider the maximum likelihood estimator \\(S_r/n\\), we have \\[\nE(S_r/n) = \\frac{n-p}{n}\\sigma^2.\n\\]\n\n\n2.3.2 An unbiased estimator of \\(\\sigma^2\\)\nTo obtain an unbiased estimator of \\(\\sigma^2\\) we simply divide \\(S_r\\) by \\(n-p\\) instead of \\(n\\), and so the estimate we compute from our data is\n\\[\n\\hat{\\sigma}^2 = \\frac{(\\boldsymbol{y} -X\\boldsymbol{\\hat{\\beta}})^T (\\boldsymbol{y} -X\\boldsymbol{\\hat{\\beta}})}{n-p}.\n\\] Note that \\(\\sqrt{\\hat{\\sigma}^2}\\) is referred to as the residual standard error (and will be reported in R output).\n\n\n\n\n\n\nNote\n\n\n\nThis division by \\(n-p\\) rather than \\(n\\) to obtain an unbiased estimator is a generalization of the division by \\(n-1\\) rather than \\(n\\) to obtain an unbiased estimator of the variance in a simple normal sample, which corresponds to the case \\(p=1\\)."
  },
  {
    "objectID": "parameterestimation.html#distributions-of-the-estimators",
    "href": "parameterestimation.html#distributions-of-the-estimators",
    "title": "2  Parameter estimation for linear models",
    "section": "2.4 Distributions of the estimators",
    "text": "2.4 Distributions of the estimators\nConsidering the estimator \\[\n\\hat{\\boldsymbol{\\beta}} = (X^TX)^{-1}X^T\\boldsymbol{Y},\n\\] we have shown that \\[\nE(\\boldsymbol{\\hat{\\beta}}) = \\boldsymbol{\\beta}\n\\] and \\[\nVar(\\boldsymbol{\\hat{\\beta}}) = \\sigma^2 (X^TX)^{-1}.\n\\] and as \\(\\boldsymbol{\\hat{\\beta}}\\) is a linear transformation of the multivariate normal random vector \\(\\boldsymbol{Y}\\), we have the result that\n\\[\n\\boldsymbol{\\hat{\\beta}}\\sim N_p(\\boldsymbol{\\beta},\\sigma^2(X^TX)^{-1}).\n\\] Considering the estimator \\[\n\\hat{\\sigma}^2 = \\frac{1}{n-p}(\\boldsymbol{Y}-X\\boldsymbol{\\hat{\\beta}})^T(\\boldsymbol{Y}-X\\boldsymbol{\\hat{\\beta}}),\n\\] it can be shown that \\[\n(n-p)\\hat{\\sigma}^2\\sim \\sigma^2\\chi_{n-p}^2,\n\\] but we will not prove this result in this module. We do show in the Chapter appendix, however, that \\(\\hat{\\sigma}^2\\) is independent of \\(\\boldsymbol{\\hat{\\beta}}\\)."
  },
  {
    "objectID": "parameterestimation.html#sec-beta_comps",
    "href": "parameterestimation.html#sec-beta_comps",
    "title": "2  Parameter estimation for linear models",
    "section": "2.5 Confidence intervals for components of \\(\\boldsymbol{\\beta}\\)",
    "text": "2.5 Confidence intervals for components of \\(\\boldsymbol{\\beta}\\)\nWe have just shown that \\[\n\\boldsymbol{\\hat{\\beta}}\\sim N_p(\\boldsymbol{\\beta},\\sigma^2(X^TX)^{-1}).\n\\]\nFrom the marginal distributions property of the multivariate normal distribution, if we let \\(\\hat{\\beta}_i\\) be the \\(i\\)th element of \\(\\boldsymbol{\\hat{\\beta}}\\), then \\[\n\\hat{\\beta}_i\\sim N(\\beta_i,\\sigma^2g_{ii})\n\\] where \\(\\beta_i\\) is the \\(i\\)th element of \\(\\boldsymbol{\\beta}\\) and \\(g_{ii}\\) is the \\(i\\)th diagonal element of \\(G=(X^TX)^{-1}\\).\nHence \\[\n\\frac{\\hat{\\beta}_i-\\beta_i}{\\sigma\\sqrt{g_{ii}}}\\sim N(0,1),\\]\nbut we cannot construct confidence intervals using this, as we do not know \\(\\sigma\\); instead we need to estimate it by \\(\\hat{\\sigma}\\) and use some distribution theory.\nIt can be shown that \\(\\boldsymbol{\\hat{\\beta}}\\) is independent of \\(S_r=(n-p)\\hat{\\sigma}^2\\sim \\sigma^2\\chi_{n-p}^2\\). Standard distributional theory tells us that if \\(X \\sim N(0,1)\\), \\(Y\\sim \\chi^2_{\\nu}\\) and \\(X\\) and \\(Y\\) are independent then \\(\\frac{X}{\\sqrt{Y/\\nu}} \\sim t_{\\nu}\\). It follows that if we use \\(\\hat{\\sigma}\\) instead of \\(\\sigma\\) to standardise \\(\\hat{\\beta}_i\\) we get \\[\n\\frac{\\hat{\\beta}_i-\\beta_i}{\\hat{\\sigma}\\sqrt{g_{ii}}}\\sim t_{n-p}.\n\\] From this we immediately derive a \\(100(1-\\alpha)\\%\\) confidence interval. Let \\(t_{m,\\alpha}\\) denote the upper \\(100\\alpha \\%\\) point of the \\(t_m\\) distribution. Then \\[\\begin{eqnarray*}\n1-\\alpha &=& P\\left(-t_{n-p,1-\\alpha/2}\\leq\n\\frac{\\hat{\\beta}_i-\\beta_i}{\\hat{\\sigma}\\sqrt{g_{ii}}} \\leq\nt_{n-p,1-\\alpha/2} \\right) \\\\ &=&\nP(-t_{n-p,1-\\alpha/2}\\hat{\\sigma}\\sqrt{g_{ii}} \\leq\n\\hat{\\beta}_i-\\beta_i \\leq t_{n-p,1-\\alpha/2}\n\\hat{\\sigma}\\sqrt{g_{ii}} ) \\\\ &=&\nP(\\hat{\\beta}_i-t_{n-p,1-\\alpha/2}\\hat{\\sigma}\\sqrt{g_{ii}} \\leq\n\\beta_i \\leq \\hat{\\beta}_i+t_{n-p,1-\\alpha/2}\n\\hat{\\sigma}\\sqrt{g_{ii}}  )\n\\end{eqnarray*}\\] Therefore we have the interval \\[\n\\hat{\\beta}_i\\pm t_{n-p,1-\\alpha/2} \\hat{\\sigma}\\sqrt{g_{ii}}\n\\] for \\(\\beta_i\\). This is the two-sided interval. One-sided intervals are also easily constructed. We could also similarly devise a confidence interval for any linear function of the \\(\\beta_i\\)’s.\nIt should be noted that these intervals are for individual parameters. The simultaneous confidence region for two parameters \\(\\beta_j\\) and \\(\\beta_k\\) is not a rectangle formed from individual confidence intervals: it is an ellipse where the orientation of the axes is related to the correlation of the the estimates of \\(\\beta_j\\) and \\(\\beta_k\\). Hence, although the corresponding estimates \\(b_{j0}\\) may not look unlikely for \\(\\beta_j\\) and \\(b_{k0}\\) not unlikely for \\(\\beta_k\\), the point \\((b_{j0},b_{k0})\\) may be quite unlikely for \\((\\beta_j, \\beta_k)\\)."
  },
  {
    "objectID": "parameterestimation.html#sec-R2",
    "href": "parameterestimation.html#sec-R2",
    "title": "2  Parameter estimation for linear models",
    "section": "2.6 Model fit: coefficient of determination \\(R^2\\)",
    "text": "2.6 Model fit: coefficient of determination \\(R^2\\)\nWhen we are fitting a linear model, the estimate \\(\\boldsymbol{\\hat{\\beta}}\\) minimizes the sum of squares, and the residual sum of squares \\(S_r\\) (Section 2.1) can be thought of as a measure of fit. A model that achieves a lower residual sum of squares could be considered as giving a better fit to the data, so we could consider using \\(S_r\\) directly as a measure of model fit.\nThere are two drawbacks to doing this. The first is that \\(S_r\\) depends on the scale of the observations: a model fitted to data in which the response variable is measured in hundreds of some unit could have a larger \\(S_r\\) than a model fitted to data in which the response variable is measured in single units, even though it has a much better fit. The second is that we might prefer a measure that increases as fit improves (whereas \\(S_r\\) decreases). Based on the first drawback, it would make sense to relate \\(S_r\\) to the total variation in the response so that the scale is taken into account, and considering the second as well a better measure of model fit is \\[\nR^2=\\frac{S_{yy}-S_r}{S_{yy}}.\n\\] Here \\(S_{yy}\\) (also written \\(SS_{Total}\\)) is the total sum of squares, defined as \\((\\boldsymbol{y}-\\boldsymbol{\\bar y})^T(\\boldsymbol{y}-\\boldsymbol{\\bar y})=\\boldsymbol{y}^T\\boldsymbol{y}-n\\bar{y}^2\\), which can be thought of as the sum of the squares of residuals when fitting a model which only has a constant term. This value \\(R^2\\) is sometimes called the coefficient of determination and can be thought of the proportion of the total sum of squares that the regression model explains. The residual sum of squares \\(S_r\\) is the unexplained part of the total sum of squares of \\(y\\).\nNote that in the case of the simple linear regression model, \\(R^2\\) equals \\(r^2\\), the squared sample correlation coefficient between \\(\\boldsymbol{y}\\) and \\(\\boldsymbol{x}\\), so we can see \\(R^2\\) as the generalization of \\(r^2\\) to the more general linear model. \\(R^2\\) is also called the squared multiple correlation coefficient and it always lies between 0 and 1.\nThere is also an `Adjusted R-squared’ value. This value takes into account the number of regressor variables used in the model. It is defined as \\[R^2(adj)=1-\\frac{S_r/(n-p)}{SS_{total}/(n-1)}\\]\nYou will see a lot more about model selection later in the course, but it does not make sense to simply pick the model with the largest \\(R^2\\) as this necessarily increases as the number of regressor variables increases. The adjusted R-squared value does not necessarily increase as the number of regressor variables increases, suggesting that using the adjusted R-squared value in model selection is more sensible."
  },
  {
    "objectID": "parameterestimation.html#chapter-appendix",
    "href": "parameterestimation.html#chapter-appendix",
    "title": "2  Parameter estimation for linear models",
    "section": "2.7 Chapter appendix",
    "text": "2.7 Chapter appendix\nYou will not be examined on your ability to derive results such as the following. However, you are encouraged to study these sections (and ask questions if you need to!) because understanding this sort of content can you help you understand other methods if you need to learn them independently.\n\n2.7.1 Deriving the least squares estimator\nTo find the least squares estimates we must solve \\[\n\\frac{d (\\boldsymbol{y}-X\\boldsymbol\\beta)^T(\\boldsymbol{y}-X\\boldsymbol\\beta)}{d \\boldsymbol\\beta} =\n\\boldsymbol {0}.\n\\] Now \\[\n(\\boldsymbol{y}-X\\boldsymbol\\beta)^T(\\boldsymbol{y}-X\\boldsymbol\\beta) = \\left(\\boldsymbol{y}^T \\boldsymbol{y}\n-\\boldsymbol\\beta^TX^T\\boldsymbol{y} -\\boldsymbol{y}^TX\\boldsymbol\\beta + \\boldsymbol\\beta^T(X^T X)\\boldsymbol\\beta\\right)\n\\] and so \\[\n\\frac{d (\\boldsymbol{y}-X\\boldsymbol\\beta)^T(\\boldsymbol{y}-X\\boldsymbol\\beta)}{d \\boldsymbol\\beta}=-2X^T\\boldsymbol{y} + 2(X^TX)\\boldsymbol\\beta.\n\\]\nWe equate the above to \\(\\boldsymbol{0}\\) for \\(\\boldsymbol\\beta=\\hat{\\boldsymbol\\beta}\\), the least squares estimator, giving \\[\n\\boldsymbol{0}=-2X^T\\boldsymbol{y} +\n2(X^TX)\\hat{\\boldsymbol\\beta},\n\\] (sometimes referred to as the normal equation) which gives us the result \\[\n\\hat{\\boldsymbol\\beta}=(X^TX)^{-1}X^T\\boldsymbol{y}\n\\]\n\n\n2.7.2 Bias of the maximum likelihood estimator\nWe define \\[\n\\boldsymbol{e}:=\\boldsymbol{Y}-X\\boldsymbol{\\hat{\\beta}}=\\boldsymbol{Y}-X(X^TX)^{-1}X^T\\boldsymbol{Y},\n\\]\nwhich we think of as a vector of random residuals. If we define \\[M:=I_n-X(X^TX)^{-1}X^T,\\] we can re-write this as \\(\\boldsymbol{e}=M\\boldsymbol{Y}\\).\nNote that \\(MX = \\boldsymbol{0}\\) and \\(M^2=M\\), i.e. \\(M\\) is an idempotent matrix.\nWe have \\[\nE(\\boldsymbol{e})=ME(\\boldsymbol{Y})=MX\\boldsymbol{\\beta}=\\boldsymbol{0},\n\\] since \\(MX=\\boldsymbol{0}\\).\nWe also have \\[\nVar(\\boldsymbol{e})=MVar(\\boldsymbol{Y})M^T=M\\sigma^2I_nM=\\sigma^2M^2=\\sigma^2M.\n\\]\nThus \\(M\\) is also related to the variance-covariance matrix of the residuals. The variance of an individual residual is \\(\\sigma^2\\) times the corresponding diagonal element of \\(M\\).\nEvery idempotent matrix except the identity is singular (non-invertible), and its rank is equal to its trace. We have (using the result that \\(\\mathrm{tr}(AB)=\\mathrm{tr}(BA)\\)) \\[\n\\mathrm{tr}(M)=\\mathrm{tr}(I_n-X(X^TX)^{-1}X^T)=n-\\mathrm{tr}((X^TX)^{-1}X^TX) =n-\\mathrm{tr}(I_p)=n-p.\n\\] and so the rank of \\(M\\) is also \\(n-p\\).\nSince \\(E(e_i)=0\\) \\((i=1,\\ldots,n)\\), we can observe that the diagonal elements of the covariance matrix \\(Var(\\boldsymbol{e})\\) are \\(E(e_1^2),\\ldots,E(e_n^2)\\) and so we have that \\(E(e_i^2)=\\sigma^2m_{ii}\\), where \\(m_{11},\\ldots,m_{nn}\\) are diagonal elements of \\(M\\), writing \\(M=(m_{ij})\\). Then we can see \\[\nE\\left(\\sum_{i=1}^ne_i^2\\right)=\\sum_{i=1}^n E(e_i^2) = \\sigma^2\n\\sum_{i=1}^n m_{ii} = \\sigma^2 tr(M) = \\sigma^2(n-p)\n\\] This shows us that the maximum likelihood estimator is biased: \\[\nE(\\hat{\\sigma}^2_{MLE}) = E\\left(\\frac{\\sum_{i=1}^ne_i^2}{n}\\right)=\\sigma^2\\frac{(n-p)}{n}.\n\\]\n\n\n2.7.3 Independence between \\(\\hat{\\sigma}^2\\) and \\(\\boldsymbol{\\hat{\\beta}}\\)\nUsing the definitions of \\(\\boldsymbol{e}\\) and \\(M\\) from above\n\\[\n\\boldsymbol{e}:=\\boldsymbol{Y}-X\\boldsymbol{\\hat{\\beta}} = M\\boldsymbol{Y}\n\\] we have \\[\n\\hat{\\sigma}^2 = \\frac{1}{n-p}\\boldsymbol{e}^T\\boldsymbol{e}\n\\] As \\(\\hat{\\sigma}^2\\) is a transformation of the random vector \\(\\boldsymbol{e}\\), we just need to show that is \\(\\boldsymbol{\\hat{\\beta}}\\) is independent of \\(\\boldsymbol{e}\\).\nBoth \\(\\boldsymbol{e}\\) and \\(\\boldsymbol{\\hat{\\beta}}\\) are linear transformations of the multivariate normal random vector \\(\\boldsymbol{Y}\\), and their joint distribution is multivariate normal.\nWe have \\[\\begin{align}\nCov(\\boldsymbol{e},\\boldsymbol{\\hat{\\beta}}) &= Cov(M\\boldsymbol{Y}, (X^TX)^{-1}X^T\\boldsymbol{Y}) \\\\\n&= MVar(\\boldsymbol{Y})X(X^TX)^{-1}\\\\\n&= M\\sigma^2 I_n X(X^TX)^{-1} \\\\\n&= \\sigma^2MX(X^TX)^{-1}\\\\\n&= \\boldsymbol{0},\n\\end{align}\\] as \\(MX = \\boldsymbol{0}\\). For multivariate normal random vectors, zero covariance implies independence."
  },
  {
    "objectID": "qualitative.html#example-cancer-survival-data",
    "href": "qualitative.html#example-cancer-survival-data",
    "title": "3  Qualitative independent variables",
    "section": "3.1 Example: cancer survival data",
    "text": "3.1 Example: cancer survival data\nThe file cancer.csv has data from patients with advanced cancers of the stomach, bronchus, colon, ovary or breast were treated with ascorbate. We suppose that this aim is to determine if patient survival differs with respect to the organ affected by the cancer. The survival time in days was recorded for each patient.\n\nlibrary(tidyverse)\ncancer &lt;- read_csv(\"https://oakleyj.github.io/exampledata/cancer.csv\")\nhead(cancer)\n\n# A tibble: 6 × 2\n  survival organ  \n     &lt;dbl&gt; &lt;chr&gt;  \n1      124 Stomach\n2       42 Stomach\n3       25 Stomach\n4       45 Stomach\n5      412 Stomach\n6       51 Stomach\n\n\nWe can use a box plot to compare the survival times for each type (organ) of cancer patient:\n\nggplot(cancer, aes(x = organ, y = survival)) +\n  geom_boxplot()\n\n\n\n\nWe suppose that the survival time is the dependent variable of interest, which we can treat as continuous as before. But our independent variable, organ, is qualitative. Can we still use a linear model to analyse these data?"
  },
  {
    "objectID": "qualitative.html#a-linear-model-for-the-cancer-data",
    "href": "qualitative.html#a-linear-model-for-the-cancer-data",
    "title": "3  Qualitative independent variables",
    "section": "3.2 A linear model for the cancer data",
    "text": "3.2 A linear model for the cancer data\nClearly, it would not make sense to write a model such as \\[\nY_i=\\beta_0 + \\beta_1 x_i +\\varepsilon_i,\n\\] where \\(Y_i\\) is the survival time of the \\(i\\)-th patient and \\(x_i\\) is the cancer type of the \\(i\\)-th patient, because the independent variable (cancer type) is a categorical variable (i.e. it doesn’t make sense to say “survival time \\(= 50 +3\\times\\) stomach cancer”). We could write our model as \\[\nY_i= \\beta_1 x_{i,1} + \\beta_2 x_{i,2}+\\beta_3 x_{i,3}+\\beta_4 x_{i,4}+\\beta_5 x_{i,5} +\\varepsilon_i,\n\\] where \\(x_{i,j} = 1\\) if patient had cancer type \\(j\\) (\\(j=1\\) for Breast, \\(j=2\\) for Bronchus and so on) and \\(x_{i,j} = 0\\) otherwise. We say that \\(x_{i,j}\\) is a dummy variable. The five dummy variables here ensure that the correct \\(\\beta\\) term is selected for each observation. This notation can be a little cumbersome, so we typically write these sorts of models in a different way."
  },
  {
    "objectID": "qualitative.html#notation-for-qualitative-independent-variables",
    "href": "qualitative.html#notation-for-qualitative-independent-variables",
    "title": "3  Qualitative independent variables",
    "section": "3.3 Notation for qualitative independent variables",
    "text": "3.3 Notation for qualitative independent variables\nEach observation is associated with a particular group, where the group is specified by the value (level) of the qualitative independent variable. The organ variable can be one of five possibilities, so we think the data as being organised in five groups.\nWe write \\(Y_{ij}\\) as the \\(j\\)-th observation within group \\(i\\). Let \\(g\\) be the total number of groups (with \\(g=5\\) in the cancer data). We then have \\(i=1,\\ldots,g\\).\n\n\n\n\n\n\nNote\n\n\n\nWhereas a quantitative independent variable is typically represented by its own letter (e.g. \\(x_i\\)), we typically represent a qualitative independent variable using an additional subscript on the dependent variable.\n\n\nWithin group \\(i\\) we let \\(n_i\\) be the total number of observations, so that we have \\(j=1,\\ldots,n_i\\). In the cancer data, if we call the group of patients with breast cancer group 1, there are 11 patients in this group, so \\(n_1=11\\), and the 11 survival times for patients with stomach cancer are denoted \\(Y_{1,1},Y_{1,2},\\ldots,Y_{1,11}\\). As usual, we think of \\(Y_{ij}\\) as a random variable, and \\(y_{ij}\\) as the observed value of that random variable.\nWe let \\(n\\) denote the total number of observations, so that \\(n=\\sum_{i=1}^g n_i\\).\nNow let \\(\\mu_i\\) denote the population mean of the dependent variable in group \\(i\\). We can now write a model for the data as follows: \\[\nY_{ij}=\\mu_i + \\varepsilon_{ij},\n\\] for \\(i=1,\\ldots,g\\), \\(j=1,\\ldots,n_i\\) and \\(\\varepsilon_{ij}\\sim N(0,\\sigma^2)\\).\n\n\n\n\n\n\nNote\n\n\n\nAnalysis of data using this model is sometimes referred to as one-way analysis of variance (ANOVA), and we will refer to the above model as the one-way ANOVA model."
  },
  {
    "objectID": "qualitative.html#the-one-way-anova-model-in-matrix-form",
    "href": "qualitative.html#the-one-way-anova-model-in-matrix-form",
    "title": "3  Qualitative independent variables",
    "section": "3.4 The one-way ANOVA model in matrix form",
    "text": "3.4 The one-way ANOVA model in matrix form\nThe model \\[ Y_{ij}=\\mu_i + \\varepsilon_{ij},\\] for \\(i=1,\\ldots,g\\), \\(j=1,\\ldots,n_i\\) is written in matrix form as \\[\n\\mathbf{Y} = X\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon},\n\\] with \\[\n\\mathbf{Y}=\\left(\\begin{array}{c}Y_{1,1} \\\\ \\vdots \\\\ Y_{1,n_1} \\\\ Y_{2,1}\\\\\n\\vdots \\\\ Y_{2,n_2}  \\\\ \\vdots  \\\\ Y_{g,1} \\\\\n\\vdots\n\\\\ Y_{g,n_g}\\end{array}\\right),\\quad\nX=\\left(\\begin{array}{ccccc}1& 0 & 0  & \\ldots & 0 \\\\ \\vdots & \\vdots & \\vdots & & \\vdots  \\\\\n1& 0 & 0 & \\ldots & 0 \\\\ 0& 1 & 0  & \\ldots & 0 \\\\ \\vdots & \\vdots & \\vdots & & \\vdots  \\\\\n0& 1 & 0 & \\ldots & 0 \\\\ \\vdots & \\vdots & \\vdots & & \\vdots   \\\\ 0& 0 & 0  & \\ldots & 1 \\\\ \\vdots & \\vdots & \\vdots & & \\vdots  \\\\\n0& 0 & 0 & \\ldots & 1\n\\end{array}\\right),\\quad\n\\boldsymbol{\\beta}=\\left(\\begin{array}{c}\\mu_1 \\\\ \\vdots \\\\\n\\mu_g\\end{array}\\right),\\quad\n\\boldsymbol{\\varepsilon}=\\left(\\begin{array}{c}\\varepsilon_{1,1} \\\\ \\vdots \\\\ \\varepsilon_{1,n_1} \\\\ \\varepsilon_{2,1}\\\\\n\\vdots \\\\ \\varepsilon_{2,n_2}  \\\\ \\vdots  \\\\ \\varepsilon_{g,1} \\\\\n\\vdots\n\\\\ \\varepsilon_{g,n_g}\\end{array}\\right)\n\\]"
  },
  {
    "objectID": "qualitative.html#least-squares-estimates-for-the-one-way-anova-model",
    "href": "qualitative.html#least-squares-estimates-for-the-one-way-anova-model",
    "title": "3  Qualitative independent variables",
    "section": "3.5 Least squares estimates for the one-way ANOVA model",
    "text": "3.5 Least squares estimates for the one-way ANOVA model\nNow that we have written the model in matrix notation, we can immediately obtain least squares estimates for the unknown group means \\(\\mu_1,\\ldots,\\mu_g\\), using the formula \\(\\hat{\\boldsymbol{\\beta}}=(X^TX)^{-1}X^T\\mathbf{y}\\). Since \\[\n(X^TX)^{-1}=\\left(\\begin{array}{cccc}n_1 & 0 &  \\ldots & 0 \\\\ 0 &\nn_2  & \\ldots & 0 \\\\ \\vdots & & \\ddots &   \\vdots \\\\ 0 & 0 & \\ldots\n& n_g \\end{array} \\right)^{-1},\\quad\nX^T\\mathbf{y}=\\left(\\begin{array}{c} \\sum_{j=1}^{n_1} y_{1,j} \\\\ \\vdots \\\\\n\\sum_{j=1}^{n_g} y_{g,j} \\end{array}\\right)\n\\] we have \\[\n\\hat{\\boldsymbol{\\beta}}=\\left(\\begin{array}{c}\\hat{\\mu}_1 \\\\ \\vdots \\\\ \\hat{\\mu}_g \\end{array}\\right) = \\left(\\begin{array}{c} \\frac{1}{n_1} \\sum_{j=1}^{n_1} y_{1,j} \\\\ \\vdots \\\\\n\\frac{1}{n_g} \\sum_{j=1}^{n_g} y_{g,j} \\end{array}\\right)\n\\] This result is intuitive. For example, in group 1 we have \\(n_1\\) observations \\(y_{1,1},\\ldots,y_{1,n_1}\\), all with expected value \\(\\mu_1\\). The obvious estimate for \\(\\mu_1\\) is the sample mean \\(\\frac{1}{n_1} \\sum_{j=1}^{n_1} y_{1,j}\\)."
  },
  {
    "objectID": "qualitative.html#an-alternative-parameterisation",
    "href": "qualitative.html#an-alternative-parameterisation",
    "title": "3  Qualitative independent variables",
    "section": "3.6 An alternative parameterisation",
    "text": "3.6 An alternative parameterisation\nNone of the hypothesis tests in the summary() output above are likely to be of any interest; we are more likely to be interested in differences between groups, rather than whether some or all of the groups have a population mean response of 0.\nAn alternative way of writing the one-way ANOVA model is as follows: \\[\nY_{i,j}=\\mu + \\tau_i + \\varepsilon_{i,j},\n\\] for \\(i=1,\\ldots,g\\), \\(j=1,\\ldots,n_i\\) and with \\(\\varepsilon_{i,j}\\sim N(0,\\sigma^2)\\).\nThe intention of this parametrisation could be to think of \\(\\mu\\) as the grand mean, and \\(\\tau_i\\) as the difference between the mean of group \\(i\\) and the grand mean \\(\\mu\\). However, this model is over-parametrised.\nIn matrix notation, we would write this as \\[\n\\mathbf{Y} = X\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon},\n\\] with \\[\n\\mathbf{Y}=\\left(\\begin{array}{c}Y_{1,1} \\\\ \\vdots \\\\ Y_{1,n_1} \\\\ Y_{2,1}\\\\\n\\vdots \\\\ Y_{2,n_2} \\\\ \\vdots \\\\ Y_{g,1} \\\\\n\\vdots\n\\\\ Y_{g,n_g}\\end{array}\\right),\\quad\nX=\\left(\\begin{array}{cccccc}1&1& 0 & 0  & \\ldots & 0 \\\\ \\vdots & \\vdots & \\vdots & \\vdots & & \\vdots  \\\\\n1& 1& 0 & 0 & \\ldots & 0 \\\\ 1 & 0& 1 & 0  & \\ldots & 0 \\\\ \\vdots &  \\vdots & \\vdots & \\vdots & & \\vdots  \\\\\n1 & 0& 1 & 0 & \\ldots & 0 \\\\ \\vdots & \\vdots & \\vdots & \\vdots & & \\vdots  \\\\ 1 & 0& 0 & 0  & \\ldots & 1 \\\\ \\vdots & \\vdots & \\vdots & \\vdots & & \\vdots  \\\\\n1 & 0& 0 & 0 & \\ldots & 1\n\\end{array}\\right),\\quad\n\\boldsymbol{\\beta}=\\left(\\begin{array}{c}\\mu \\\\\\tau_1 \\\\ \\vdots \\\\\n\\tau_g\\end{array}\\right),\\quad\n\\boldsymbol{\\varepsilon}=\\left(\\begin{array}{c}\\varepsilon{1,1} \\\\ \\vdots \\\\ \\varepsilon{1,n_1} \\\\ \\varepsilon{2,1}\\\\\n\\vdots \\\\ \\varepsilon{2,n_2}  \\\\ \\vdots  \\\\ \\varepsilon{g,1} \\\\\n\\vdots\n\\\\ \\varepsilon{g,n_g}\\end{array}\\right).\n\\] For this particular design matrix \\(X\\), we find that \\[\nX^TX=\\left(\\begin{array}{ccccc}n & n_1 & n_2 & \\cdots & n_g\n\\\\ n_1 & n_1 & 0  & \\cdots & 0\\\\ n_2 & 0 & n_2 & \\cdots & 0 \\\\ \\vdots & \\vdots &\\vdots & \\ddots & \\vdots \\\\ n_g & 0 & 0 & \\cdots &\nn_g\n\\end{array}\\right).\n\\] The matrix \\(X^TX\\) cannot be inverted, as \\(\\det(X^TX)=0\\) (the first column is the sum of columns 2 to \\(g+1\\), noting that \\(\\sum_{i=1}^g n_i=n\\)). Hence it is not possible to obtain least squares parameter estimates for this model, as we cannot evaluate the expression \\(\\boldsymbol{\\beta}=(X^TX)^{-1}X^T\\mathbf{y}\\). Intuitively, this makes sense as we are trying to estimate \\(g+1\\) parameters representing group means (\\(\\mu\\) and \\(\\tau_1,\\ldots,\\tau_g\\)) with data from only \\(g\\) groups.\nThe solution is to apply constraints to the parameters. One possibility is to state that \\(\\tau_1=0\\), so that the model can be written as \\[\nY_{i,j}=\\left\\{\\begin{array}{ll}\\mu + \\varepsilon_{1,j} & \\mbox{$i=1$,\n$j=1,\\ldots n_1$}\\\\ \\mu + \\tau_i + \\varepsilon_{i,j} & \\mbox{$i=2,\\ldots,g$,\n$j=1,\\ldots n_i$} \\end{array}\\right.\n\\] For this parametrisation, \\(\\mu\\) is interpreted as the (population) mean for group 1, and \\(\\tau_i\\) gives the difference in means between group \\(i\\) and group 1, for \\(i\\neq 1\\). The null hypothesis of no difference between group means is written as \\[\nH_0:\\tau_2=\\ldots=\\tau_g=0,\n\\] and any individual \\(\\tau_i=0\\) implies no difference in means between groups \\(i\\) and 1. This model is written in matrix form as \\[\n\\mathbf{Y} = X\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon},\n\\] with \\[\n\\mathbf{Y}=\\left(\\begin{array}{c}Y_{1,1} \\\\ \\vdots \\\\ Y_{1,n_1} \\\\ Y_{2,1}\\\\\n\\vdots \\\\ Y_{2,n_2}  \\\\ \\vdots  \\\\ Y_{g,1} \\\\\n\\vdots\n\\\\ Y_{g,n_g}\\end{array}\\right),\\quad\nX=\\left(\\begin{array}{ccccc}1& 0 & 0  & \\ldots & 0 \\\\ \\vdots & \\vdots & \\vdots & & \\vdots  \\\\\n1& 0 & 0 & \\ldots & 0 \\\\ 1& 1 & 0  & \\ldots & 0 \\\\ \\vdots & \\vdots & \\vdots & & \\vdots  \\\\\n1& 1 & 0 & \\ldots & 0\n\\\\ \\vdots & \\vdots & \\vdots & & \\vdots   \\\\ 1& 0 & 0  & \\ldots & 1 \\\\ \\vdots & \\vdots & \\vdots & & \\vdots  \\\\\n1& 0 & 0 & \\ldots & 1\n\\end{array}\\right),\\quad\n\\boldsymbol{\\beta}=\\left(\\begin{array}{c}\\mu \\\\ \\tau_2 \\\\ \\vdots \\\\\n\\tau_g\\end{array}\\right),\\quad\n\\boldsymbol{\\varepsilon}=\\left(\\begin{array}{c}\\varepsilon{1,1} \\\\ \\vdots \\\\ \\varepsilon{1,n_1} \\\\ \\varepsilon{2,1}\\\\\n\\vdots \\\\ \\varepsilon{2,n_2} \\\\ \\vdots  \\\\ \\varepsilon{g,1} \\\\\n\\vdots\n\\\\ \\varepsilon{g,n_g}\\end{array}\\right).\n\\] For this design matrix we find that \\[\n\\hat{\\boldsymbol{\\beta}}=\\left(\\begin{array}{c}\\frac{1}{n_1}\\sum_{j=1}^{n_1} y_{1,j} \\\\\n\\frac{1}{n_2}\\sum_{j=1}^{n_2} y_{2,j} -\n\\frac{1}{n_1}\\sum_{j=1}^{n_1} y_{1,j} \\\\ \\vdots \\\\\n\\frac{1}{n_g}\\sum_{j=1}^{n_g} y_{g,j} -\n\\frac{1}{n_1}\\sum_{j=1}^{n_1} y_{1,j}\n\\end{array}\\right),\n\\] (details omitted), hence \\(\\hat{\\mu}\\) is the sample mean of the observations in group 1, and \\(\\hat{\\tau}_i\\) is the difference between the sample means of groups \\(i\\) and 1."
  },
  {
    "objectID": "fitting-in-R.html#fitting-a-simple-linear-regression-model",
    "href": "fitting-in-R.html#fitting-a-simple-linear-regression-model",
    "title": "4  Fitting a linear model and making predictions in R",
    "section": "4.1 Fitting a simple linear regression model",
    "text": "4.1 Fitting a simple linear regression model\nTo illustrate fitting a linear model in R, for convenience we’ll use one of R’s built in data sets: cars\n\nhead(cars)\n\n  speed dist\n1     4    2\n2     4   10\n3     7    4\n4     7   22\n5     8   16\n6     9   10\n\n\nOur dependent variable (dist) is the stopping distance in feet, and the independent variable is (speed) is the speed in miles per hour. The cars are very old (1920s!), but it’s a convenient data set for illustrating the linear modelling syntax.\nThe basic syntax is\nlm(formula, data )\nwhere the formula argument corresponds to the equation for \\(E(Y_i)\\) and uses column names from the data frame specified by the data argument. Some formula examples are as follows.\n\n\n\n\n\n\n\nModel\nformula\n\n\n\n\n\\(Y_i = \\beta_0 + \\varepsilon_i\\)\ndist ~ 1\n\n\n\\(Y_i = \\beta_0 + \\beta_1 x_i+ \\varepsilon_i\\)\ndist ~ speed\n\n\n\\(Y_i = \\beta_0 + \\beta_1 x_i+ \\beta_2 x_i^2+\\varepsilon_i\\)\ndist ~ speed + I(speed^2)\n\n\n\\(Y_i = \\beta_0 + \\beta_1 \\log x_i+\\varepsilon_i\\)\ndist ~ log(speed)\n\n\n\nFor example, to fit the simple linear regression model we do\n\nlm(dist ~ speed, cars)\n\n\nCall:\nlm(formula = dist ~ speed, data = cars)\n\nCoefficients:\n(Intercept)        speed  \n    -17.579        3.932  \n\n\nand from the Coefficients output we read off \\(\\hat{\\beta}_0= -17.579\\) and \\(\\hat{\\beta}_1 = 3.932\\)."
  },
  {
    "objectID": "fitting-in-R.html#viewing-a-fitted-regression-model",
    "href": "fitting-in-R.html#viewing-a-fitted-regression-model",
    "title": "4  Fitting a linear model and making predictions in R",
    "section": "4.2 Viewing a fitted regression model",
    "text": "4.2 Viewing a fitted regression model\nWe can plot the fitted model and the data with the commands\n\nlibrary(tidyverse)\nggplot(cars, aes(x = speed, y = dist))+\n  geom_point()+\n  geom_smooth(method = \"lm\", formula = \"y ~ x\", se = FALSE)\n\n\n\n\nNote that the formula argument corresponds to the fitted model, but with variables x and y rather than speed and dist. We will change the argument se to TRUE when we have learned more about confidence intervals in the next chapter."
  },
  {
    "objectID": "fitting-in-R.html#the-summary-command",
    "href": "fitting-in-R.html#the-summary-command",
    "title": "4  Fitting a linear model and making predictions in R",
    "section": "4.3 The summary() command",
    "text": "4.3 The summary() command\nThe summary command will give us more information about our model fit, if we first assign the fitted model to a variable.\nFor example:\n\nlmCars &lt;- lm(dist ~ speed, cars)\nsummary(lmCars)\n\n\nCall:\nlm(formula = dist ~ speed, data = cars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-29.069  -9.525  -2.272   9.215  43.201 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -17.5791     6.7584  -2.601   0.0123 *  \nspeed         3.9324     0.4155   9.464 1.49e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 15.38 on 48 degrees of freedom\nMultiple R-squared:  0.6511,    Adjusted R-squared:  0.6438 \nF-statistic: 89.57 on 1 and 48 DF,  p-value: 1.49e-12\n\n\nWe interpret the output as follows.\n\nResiduals refer to those residuals we defined previously: \\(e_i = y_i - \\hat{y}_i\\).\nThe Estimate column gives the least squares estimate.\nThe Std. Error column gives the estimated standard error for each least squares estimate.\nThe t value and Pr(&gt;|t|) refer to a hypothesis test that the corresponding model parameter is 0. We’ll discuss this in more detail in a later chapter.\nThe residual standard error is the estimated value \\(\\hat{\\sigma}\\) of \\(\\sigma\\) (standard deviation of the errors, not the variance). Recall that this is computed using the residual sum of squares: \\(\\hat{\\sigma}^2 = \\mathbf{e}^T\\mathbf{e}/(n-p)\\)\nThe Multiple R-squared and Adjusted R-squared both report how ‘useful’ the model is for predicting the dependent variable: they report the proportion of the variation in \\(y_1,\\ldots,y_n\\) that can be explained by variation in the dependent variables. The Adjusted R-squared corrects for the number of independent variables in the model.\nThe F-statistic and p-value also refer to a hypothesis test which we will discuss later."
  },
  {
    "objectID": "fitting-in-R.html#models-with-factor-variables",
    "href": "fitting-in-R.html#models-with-factor-variables",
    "title": "4  Fitting a linear model and making predictions in R",
    "section": "4.4 Models with factor variables",
    "text": "4.4 Models with factor variables\nRecall the cancer example from the previous chapter\n\nlibrary(tidyverse)\ncancer &lt;- read_csv(\"https://oakleyj.github.io/exampledata/cancer.csv\")\nhead(cancer)\n\n# A tibble: 6 × 2\n  survival organ  \n     &lt;dbl&gt; &lt;chr&gt;  \n1      124 Stomach\n2       42 Stomach\n3       25 Stomach\n4       45 Stomach\n5      412 Stomach\n6       51 Stomach\n\n\nWe defined \\(Y_{ij}\\) as the \\(j\\)-th observation (survival time) in group \\(i\\), with group defined by organ (one of stomach, bronchus, colon, ovary or breast)\nWe fit the model \\[ Y_{ij}=\\mu_i + \\varepsilon_{ij},\\] for \\(i=1,\\ldots,g\\), \\(j=1,\\ldots,n_i\\) to the cancer data as follows.\n\nlmCancer &lt;- lm(survival ~ organ - 1, cancer)\n\n(the reason for the - 1 in the formula will become clearer shortly.)\nWe can now use the summary() command to get the parameter estimates\n\nsummary(lmCancer)\n\n\nCall:\nlm(formula = survival ~ organ - 1, data = cancer)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1371.91  -241.75  -111.50    87.19  2412.09 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \norganBreast     1395.9      201.9   6.915 3.77e-09 ***\norganBronchus    211.6      162.4   1.303  0.19764    \norganColon       457.4      162.4   2.817  0.00659 ** \norganOvary       884.3      273.3   3.235  0.00199 ** \norganStomach     286.0      185.7   1.540  0.12887    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 669.5 on 59 degrees of freedom\nMultiple R-squared:  0.5437,    Adjusted R-squared:  0.505 \nF-statistic: 14.06 on 5 and 59 DF,  p-value: 4.766e-09\n\n\nWe have \\(\\hat{\\beta}_1 = 1395.9\\), \\(\\hat{\\beta}_2 = 211.6\\) and so on.\n\n\n\n\n\n\nWarning\n\n\n\nBe careful with datasets where a factor/qualitative variable has been recorded numerically! You will need to make sure R understands that the variable is really a factor and not a quantitative variable.\n\n\nAs an example, suppose the cancer data were coded as 1 = Stomach, 2 = Bronchus, 3 = Colon, 4 = Ovary, 5 = Breast. We’ll set up a variable organ2 to represent this:\n\ncancer2 &lt;- cancer %&gt;%\n  mutate(organ2 = rep(1:5, times = c(13, 17, 17, 6, 11)))\n\nNow we’ll try\n\nlm(survival ~ organ2, data = cancer2)\n\n\nCall:\nlm(formula = survival ~ organ2, data = cancer2)\n\nCoefficients:\n(Intercept)       organ2  \n     -240.3        288.9  \n\n\nWhat model has been fitted, what does the coefficient organ2 represent here, and why doesn’t it make sense?\nWe can get R to interpret the organ2 variable correctly, by specifying it as a factor variable:\n\nlm(survival ~ factor(organ2) - 1, data = cancer2)\n\n\nCall:\nlm(formula = survival ~ factor(organ2) - 1, data = cancer2)\n\nCoefficients:\nfactor(organ2)1  factor(organ2)2  factor(organ2)3  factor(organ2)4  \n          286.0            211.6            457.4            884.3  \nfactor(organ2)5  \n         1395.9  \n\n\n\n4.4.1 An alternative parameterisation\nRecall the alternative parametrisation: \\[\nY_{i,j}=\\mu + \\tau_i + \\varepsilon_{i,j},\n\\] for \\(i=1,\\ldots,g\\), \\(j=1,\\ldots,n_i\\) and with \\(\\varepsilon_{i,j}\\sim N(0,\\sigma^2)\\) and the constraint \\(\\tau_1=0\\).\nThis is actually the default parametrisation in R. If we leave out the -1 from the previous command, we just do\n\nlmCancer &lt;- lm(survival ~ organ, cancer)\n\nand then use the summary() command as before.\n\nsummary(lmCancer)\n\n\nCall:\nlm(formula = survival ~ organ, data = cancer)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1371.91  -241.75  -111.50    87.19  2412.09 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     1395.9      201.9   6.915 3.77e-09 ***\norganBronchus  -1184.3      259.1  -4.571 2.53e-05 ***\norganColon      -938.5      259.1  -3.622 0.000608 ***\norganOvary      -511.6      339.8  -1.506 0.137526    \norganStomach   -1109.9      274.3  -4.046 0.000153 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 669.5 on 59 degrees of freedom\nMultiple R-squared:  0.3037,    Adjusted R-squared:  0.2565 \nF-statistic: 6.433 on 4 and 59 DF,  p-value: 0.0002295\n\n\n(Intercept) refers to \\(\\mu\\). (So we can interpret the -1 term in the formula argument as saying that we do not want an intercept.)\nWe have \\(\\hat{\\mu} = 1395.9\\), \\(\\hat{\\tau}_2 = -1184.3,\\ldots,\\hat{\\tau}_5 = -1109.9\\)."
  },
  {
    "objectID": "fitting-in-R.html#ancova-and-models-with-interactions",
    "href": "fitting-in-R.html#ancova-and-models-with-interactions",
    "title": "4  Fitting a linear model and making predictions in R",
    "section": "4.5 ANCOVA and models with interactions",
    "text": "4.5 ANCOVA and models with interactions\nANCOVA (Analysis of Covariance) involves regression modelling where the regression line changes between different groups in the data (with groups defined by the values of a qualitative/factor variable.)\nFor a simple example, we’ll use the built in dataset mtcars (see ?mtcars for details)\n\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\nWe’ll be using the column am which describes transmission type. To make it more readable, we’ll do\n\nmtcars2 &lt;- mtcars %&gt;%\n  mutate(transmission = factor(am, labels = c(\"automatic\",  \"manual\")))\n\nAn ANCOVA model for this data would be\n\\[\nY_{ij} = \\mu + \\tau_i + \\beta_i x_{ij} + \\varepsilon_{ij}\n\\] where \\(Y_{ij}\\) is the fuel economy mpg of the \\(j\\)th car in group \\(i\\), and \\(x_{ij}\\) is the corresponding weight wt, for \\(i=1,2\\). Group \\(i=1\\) corresponds to automatic cars, and \\(i=2\\) corresponds to manual cars. We include the constraint \\(\\tau_1 = 0\\). So the regression line for automatic cars would be \\[\ny = \\mu + \\beta_1x,\n\\] and for manual cars would be \\[\ny = \\mu +\\tau_2+ \\beta_2x,\n\\] The effect of weight on fuel economy (the ‘beta’ parameter) changes depending on whether the car is a manual or automatic: there is an interaction between transmission and weight in affecting the fuel economy.\nInteractions are specified with a * in the formula argument:\n\nlmCars &lt;- lm(mpg ~ wt * transmission, mtcars2)\nsummary(lmCars)\n\n\nCall:\nlm(formula = mpg ~ wt * transmission, data = mtcars2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.6004 -1.5446 -0.5325  0.9012  6.0909 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)            31.4161     3.0201  10.402 4.00e-11 ***\nwt                     -3.7859     0.7856  -4.819 4.55e-05 ***\ntransmissionmanual     14.8784     4.2640   3.489  0.00162 ** \nwt:transmissionmanual  -5.2984     1.4447  -3.667  0.00102 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.591 on 28 degrees of freedom\nMultiple R-squared:  0.833, Adjusted R-squared:  0.8151 \nF-statistic: 46.57 on 3 and 28 DF,  p-value: 5.209e-11\n\n\nFrom this, we read off \\(\\hat{\\mu}=31.4161\\), \\(\\hat{\\beta}_1=-3.7859\\), \\(\\hat{\\tau}_2=14.8784\\), \\(\\hat{\\beta}_2=-3.7859 -5.2984\\).\nWe can plot the fitted model with the commands\n\nggplot(mtcars2, aes(x = wt, y = mpg, colour = transmission)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nUse the plot to check you have interpreted the summary output correctly. The absolute gradient is larger for the manual group, and the manual group has a higher intercept."
  },
  {
    "objectID": "fitting-in-R.html#predictions",
    "href": "fitting-in-R.html#predictions",
    "title": "4  Fitting a linear model and making predictions in R",
    "section": "4.6 Predictions",
    "text": "4.6 Predictions\nGiven a new observation \\(i\\) with regressor variables given by a row vector \\(\\boldsymbol{x}_i^T\\) (similar to a row of the design matrix \\(X\\)), there are two types of interval we may want to construct, a confidence interval for the expected value of the response, \\(\\boldsymbol{x}_i^T\\boldsymbol{\\beta}\\), and a prediction interval for the value of the response itself. We should expect the prediction interval to be wider as the response has the extra variability coming from the error term in the model.\n\n4.6.1 Confidence interval for the expected value of the response\nFor estimation of the mean we use the fact that \\[\\boldsymbol{\\hat{\\beta}} \\sim N_p(\\boldsymbol{\\beta},\\sigma^2(X^TX)^{-1})\\] to yield \\[\\boldsymbol{x}_i^T \\boldsymbol{\\hat{\\beta}} \\sim N(\\boldsymbol{x}_i^T\\boldsymbol{\\beta},\\sigma^2\\boldsymbol{x}_i^T(X^TX)^{-1}\\boldsymbol{x}_i)\\] so that \\[\\frac{\\boldsymbol{x}_i^T\\boldsymbol{\\hat{\\beta}}-\\boldsymbol{x}_i^T\\boldsymbol{\\beta}}{\\sqrt{\\sigma^2\\boldsymbol{x}_i^T(X^TX)^{-1}\\boldsymbol{x}_i}} \\sim N(0,1).\\]\nWe cannot calculate the confidence interval from this relationship because we do not know the value of \\(\\sigma^2\\), and we have to replace it with its estimate \\(\\hat \\sigma^2\\) and use the \\(t\\) distribution rather than \\(N(0,1)\\). Our standardized version of \\(\\boldsymbol{x}_i^T\\boldsymbol{\\hat{\\beta}}\\) becomes \\[T=\\frac{\\boldsymbol{x}_i^T \\boldsymbol{\\hat{\\beta}}-\\boldsymbol{x}_i^T \\boldsymbol{\\beta}}{\\sqrt{\\hat \\sigma^2\\boldsymbol{x}_i^T(X^TX)^{-1}\\boldsymbol{x}_i}}=\\frac{\\frac{\\boldsymbol{x}_i^T \\boldsymbol{\\hat{\\beta}}-\\boldsymbol{x}_i^T \\boldsymbol{\\beta}}{\\sqrt{\\sigma^2\\boldsymbol{x}_i^T(X^TX)^{-1}\\boldsymbol{x}_i}} }{\\sqrt{\\frac{\\hat\\sigma^2}{\\sigma^2}}},\\] and we have \\(T \\sim t_{n-p}\\). Hence a \\(100(1-\\alpha)\\%\\) confidence interval for the mean of the predicted value is given by \\[\\boldsymbol{x}_i^T \\boldsymbol{\\hat{\\beta}}\\pm t_{n-p,1-\\alpha/2}\\sqrt{\\hat \\sigma^2\\boldsymbol{x}_i^T(X^TX)^{-1}\\boldsymbol{x}_i}.\\]\n\n\n4.6.2 Prediction intervals for individual observations\nA similar argument leads to a \\(100(1-\\alpha)\\%\\) prediction interval for the future observation \\(y_i\\). It is given by \\[\\boldsymbol{x_i}^T \\boldsymbol{ \\hat{\\beta}}\\pm t_{n-p,1-\\alpha/2}\\sqrt{\\hat \\sigma^2(1+\\boldsymbol{x}_i^T(X^TX)^{-1}\\boldsymbol{x}_i)}.\\]\nThe prediction interval for an observation is wider than the confidence interval for the expected value of the observations because of the extra uncertainty coming from the error term (\\(\\hat \\sigma^2\\)).\n\n\n4.6.3 Obtaining and plotting the predictions and intervals in R\n\n\n\n\n\n\nNote\n\n\n\nTo get the predictions and either type of interval, we need to set up a new data frame with the column names for the independent variables the same as those in the original data frame used to fit the model.\n\n\nFor the cars simple linear regression model, we’ll obtain predictions and intervals for 30 values of speed evenly spaced between 1 and 30.\n\npredictDf &lt;- data.frame(speed = 1:30)\n\nWe fit the model, assign it to an object, and then use that object in a predict() command. For 95% confidence intervals for the mean response\n\nlmCars &lt;- lm(dist ~ speed, cars)\nciCars &lt;- predict(lmCars, predictDf,\n                           interval = \"confidence\",\n                           size = 0.95)\nhead(ciCars)\n\n         fit        lwr       upr\n1 -13.646686 -26.447265 -0.846107\n2  -9.714277 -21.733068  2.304513\n3  -5.781869 -17.026591  5.462853\n4  -1.849460 -12.329543  8.630624\n5   2.082949  -7.644150 11.810048\n6   6.015358  -2.973341 15.004056\n\n\nThe fit column gives the predictions, and lwr and upr give the confidence interval endpoints.\nTo get prediction intervals instead, we change the interval argument:\n\npiCars &lt;- predict(lmCars, predictDf,\n                           interval = \"prediction\",\n                           size = 0.95)\nhead(piCars)\n\n         fit       lwr      upr\n1 -13.646686 -47.11414 19.82076\n2  -9.714277 -42.89057 23.46202\n3  -5.781869 -38.68565 27.12192\n4  -1.849460 -34.49984 30.80092\n5   2.082949 -30.33359 34.49948\n6   6.015358 -26.18731 38.21803\n\n\n\n\n\n\n\n\nTip\n\n\n\nCheck the R output makes sense! The predicted values (the fit column) should be the same with either interval = \"confidence\" or interval = \"prediction\", but the intervals with the latter argument should be wider.\n\n\nIf we just want to plot confidence intervals for the mean response, we can get this directly with ggplot2: we use the argument se = TRUE in the geom_smooth command:\n\nggplot(cars, aes(x = speed, y = dist))+\n  geom_point()+\n  geom_smooth(method = \"lm\", formula = \"y ~ x\", se = TRUE)\n\n\n\n\nPlotting prediction intervals is more work! Recall that we created prediction intervals and stored them in piCars. We do\n\nggplot(cars, aes(x = speed, y = dist)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", formula = y ~ x, se = FALSE) +\n  geom_ribbon(data = data.frame(piCars, speed = 1:30),\n              aes(x = speed, ymin = lwr, ymax = upr, y = fit),\n              alpha = 0.1,\n              fill = \"red\")"
  },
  {
    "objectID": "matrixAlgebra.html#rank",
    "href": "matrixAlgebra.html#rank",
    "title": "Appendix A — Matrix algebra essentials",
    "section": "A.1 Rank of a matrix",
    "text": "A.1 Rank of a matrix\nConsider the \\(p\\times q\\) matrix \\(A\\) \\[A = \\left( \\begin{array}{cccc} a_{11} & a_{12} & \\cdots & a_{1q}\n\\\\ a_{21} & a_{22} & \\cdots & a_{2q} \\\\ \\vdots & \\vdots & \\ddots &\n\\vdots \\\\ a_{p1} & a_{p2} & \\cdots & a_{pq}\n\\end{array}\\right) = \\left(\\begin{array}{cccc} \\mathbf{a}_1, & \\mathbf{a}_2, & \\ldots, &\n\\mathbf{a}_q\\end{array}\\right) = \\left(\n\\begin{array}{c} \\mathbf{b}_1 \\\\ \\mathbf{b}_2 \\\\ \\vdots \\\\ \\mathbf{b}_p\n\\end{array}\\right)\\] where \\(\\mathbf{a}_i\\) is the \\(i\\)-th column and \\(\\mathbf{b}_j\\) is the \\(j\\)-th row \\((i=1,\\ldots,q;j=1,\\ldots,p)\\).\nThe rank of \\(A\\) is a non-negative integer, written as \\(rank(A)\\), which gives the number of linearly independent columns or rows of \\(A\\). According to this \\[rank(A)=k, \\quad 0\\leq k\\leq \\min(p,q)\\] so that there are \\(k\\) independent row vectors of \\(A\\), i.e. \\[\\lambda_1 \\mathbf{b_1}+\\cdots+\\lambda_k\\mathbf{b}_k =\\mathbf{0} \\Rightarrow\n\\lambda_1=\\cdots=\\lambda_k=0\\] and there are \\(k\\) independent column vectors of \\(A\\), i.e. \\[\\mu_1 \\mathbf{a}_1+\\cdots+\\mu_k\\mathbf{a}_k=\\mathbf{0} \\Rightarrow\n\\mu_1=\\cdots=\\mu_k=0\\] It follows that there are exactly \\(k\\) linearly independent row vectors and \\(k\\) linearly independent column vectors in \\(A\\) and any \\(k+1\\) or larger row vectors or column vectors will be linearly dependent. Below we give a simple example.\nConsider the following matrix \\[A=\\left(\\begin{array}{ccc} 2 & 0 & -1 \\\\ 1 & -4 &\n1\\end{array}\\right)\\] What is the rank of \\(A\\)?\nWe have \\(p=2\\) and \\(q=3\\). There are only 2 row vectors and so \\[\\lambda_1\\mathbf{b}_1+\\lambda_2\\mathbf{b}_2=\\lambda_1\\left(\\begin{array}{c}\n2\\\\ 0\\\\ -1\\end{array}\\right) +\\lambda_2\\left(\\begin{array}{c} 1\\\\\n-4\\\\ 1\\end{array}\\right)=\\left(\\begin{array}{c} 0\\\\ 0\\\\\n0\\end{array}\\right)\\] leads to the linear system \\[\\begin{gathered}\n2\\lambda_1+\\lambda_2=0 \\\\ -4\\lambda_2=0\\\\ -\\lambda_1+\\lambda_2=0\n\\end{gathered}\\] which of course gives \\(\\lambda_1=\\lambda_2=0\\) and so the 2 row vectors of \\(A\\) are linearly independent and the rank of \\(A\\) is 2.\nNow let us consider the column vectors of \\(A\\). There are 3 column vectors and we have \\[\\mu_1\\mathbf{a}_1+\\mu_2\\mathbf{a}_2+\\mu_3\\mathbf{a}_3=\\mu_1\\left(\\begin{array}{c}\n2\\\\ 1\\end{array}\\right) +\\mu_2\\left(\\begin{array}{c} 0\\\\\n-4\\end{array}\\right) +\\mu_3\\left(\\begin{array}{c} -1\\\\\n1\\end{array}\\right)=\\left(\\begin{array}{c} 0\\\\ 0\\end{array}\\right)\\] which leads to the linear system \\[\\begin{gathered}\n2\\mu_1-\\mu_3=0 \\\\ \\mu_1-4\\mu_2+\\mu_3=0\n\\end{gathered}\\] which gives the solution \\(\\mu_1=4\\mu_2/3\\), \\(\\mu_3=8\\mu_2/3\\) and \\(\\mu_2\\) runs free in the real line. Obviously the 3 column vectors are linearly dependent and so the rank of \\(A\\) can not be 3 (something we expected from before).\nNow let’s take the first 2 column vectors and write \\[\\nu_1\\left(\\begin{array}{c} 2\\\\ 1\\end{array}\\right) + \\nu_2\\left(\\begin{array}{c} 0\\\\\n-4\\end{array}\\right) =\\left(\\begin{array}{c} 0\\\\ 0\\end{array}\\right)\\] which leads to the linear system \\[\\begin{gathered}\n2\\nu_1=0 \\\\ \\nu_1-4\\nu_2=0\n\\end{gathered}\\] and gives the solution \\(\\nu_1=\\nu_2=0\\). So there are at most 2 linearly independent column vectors and so the rank of \\(A\\) is 2, same as using the row vectors!\nWe can now state some properties of the rank of a matrix.\n\n\\(0\\leq rank(A) \\leq \\min(p,q)\\). If \\(rank(A)=\\min(p,q)\\), then \\(A\\) is said to be of full rank.\nIf \\(rank(A)&lt;\\min(p,q)\\), we can find some linearly dependent columns or rows of \\(A\\).\nIf \\(A\\) is a \\(p\\times q\\) matrix with \\(p\\geq q\\) and \\(rank(A)=q\\), then the matrix \\(A^TA\\) is non-singular and the matrix \\(AA^T\\) is singular. A similar result can be stated for \\(p\\leq q\\)."
  },
  {
    "objectID": "matrixAlgebra.html#vector-differentiation",
    "href": "matrixAlgebra.html#vector-differentiation",
    "title": "Appendix A — Matrix algebra essentials",
    "section": "A.2 Vector differentiation",
    "text": "A.2 Vector differentiation\nSuppose that \\(\\mathbf{x}=(x_1,\\ldots,x_p)^T\\) is a \\(p\\times 1\\) vector of variables and \\(f(\\mathbf{x})\\) is a real-valued function of \\(p\\) variables \\(x_1,\\ldots,x_p\\), writing \\(f(\\mathbf{x})=f(x_1,\\ldots,x_p)\\), where the domain of \\(f(\\mathbf{x})\\) is a subset of \\(\\mathbb{R}^p\\). Let \\(\\partial f(\\mathbf{x})/\\partial x_i\\) denote the partial derivative of \\(f(\\mathbf{x})\\) with respect to \\(x_i\\), then by definition the partial derivative of \\(f(\\mathbf{x})\\) with respect to the vector \\(\\mathbf{x}\\) is \\[\\frac{\\partial f(\\mathbf{x}) }{ \\partial \\mathbf{x} } =\n\\left(\\begin{array}{c}\n\\partial f(\\mathbf{x}) / \\partial x_1 \\\\ \\vdots \\\\ \\partial f(\\mathbf{x}) / \\partial\nx_p \\end{array}\\right)\\] or in words: the partial derivative of \\(f(\\mathbf{x})\\) with respect to the vector \\(\\mathbf{x}\\) is the vector, in which elements are the respective partial derivatives of \\(f(\\mathbf{x})\\) with respect to the variables \\(x_1,\\ldots,x_p\\).\nSuppose now that \\(\\mathbf{c}=(c_1,\\ldots,c_p)^T\\) is a vector of constants and \\(f(\\mathbf{x})=\\mathbf{c}^T\\mathbf{x}\\). Then we have \\[\\frac{\\partial f(\\mathbf{x}) }{ \\partial \\mathbf{x} } = \\mathbf{c}\\] The proof of this is by noting that \\(\\mathbf{c}^T\\mathbf{x}=\\sum_{i=1}^p c_ix_i\\) with \\(\\partial f(\\mathbf{x})/\\partial x_i=c_i\\) and so \\[\\frac{\\partial f(\\mathbf{x}) }{ \\partial \\mathbf{x} } =\n\\left(\\begin{array}{c} c_1 \\\\ \\vdots \\\\ c_p\\end{array}\\right) =\n\\mathbf{c}\\] as required.\nNote that since \\(\\mathbf{c}^T\\mathbf{x}=\\mathbf{x}^T\\mathbf{c}\\), we immediately have that \\[\\label{ch1:app1}\n\\frac{ \\partial \\mathbf{c}^T\\mathbf{x} }{\\partial \\mathbf{x}} = \\frac{\n\\partial \\mathbf{x}^T \\mathbf{c}}{ \\partial \\mathbf{x}} = \\mathbf{c}\\]\nNow suppose that \\(A=(a_{ij})_{i,j=1,\\ldots,p}\\) is a \\(p\\times p\\) symmetric matrix of constants and write \\(A=(\\mathbf{a}_1,\\ldots,\\mathbf{a}_p)\\), where \\(\\mathbf{a}_i\\) is the \\(i\\)-th column of \\(A\\). Then with \\(f(\\mathbf{x})=\\mathbf{x}^TA\\mathbf{x}\\) we have \\[\\frac{\\partial f(\\mathbf{x}) }{ \\partial \\mathbf{x} } = 2A\\mathbf{x}\\] To prove this result, first we note that \\[\\mathbf{x}^TA\\mathbf{x}= \\sum_{i=1}^p\\sum_{j=1}^p a_{ij}x_ix_j= \\sum_{i=1}^p a_{ii}x_i^2 +2\\sum_{i&lt;j}^p\na_{ij}x_ix_j\\] Then \\[\\frac{ \\partial \\mathbf{x}^T A\\mathbf{x} }{\\partial x_i} = 2a_{ii}x_i\n+2\\sum_{j=2}^p a_{ij}x_j = 2 \\sum_{j=1}^p a_{ij}x_j =\n2\\mathbf{a}_i^T\\mathbf{x}\\] So we get \\[\\frac{ \\partial f(\\mathbf{x}) }{\\partial \\mathbf{x}} =\n2\\left(\\begin{array}{c} \\mathbf{a}_1^T\\mathbf{x} \\\\ \\vdots \\\\\n\\mathbf{a}_p^T\\mathbf{x} \\end{array}\\right) = 2 \\left(\\begin{array}{c}\n\\mathbf{a}_1^T \\\\ \\vdots \\\\ \\mathbf{a}_p^T\\end{array}\\right) \\mathbf{x} =\n2A\\mathbf{x}\\] So we have established \\[\\label{ch1:app2}\n\\frac{ \\partial \\mathbf{x}^TA\\mathbf{x}}{\\partial \\mathbf{x}} = 2A\\mathbf{x}\\] Note that for this result to hold, we need to know that \\(A\\) is a symmetric matrix."
  },
  {
    "objectID": "randomVectors.html#vector-mean",
    "href": "randomVectors.html#vector-mean",
    "title": "Appendix B — Random vectors",
    "section": "B.1 Vector mean",
    "text": "B.1 Vector mean\nIf we have a collection of random variables \\(x_1,\\ldots,x_p\\), we can form them into a vector \\(\\mathbf{x}=(x_1,\\ldots,x_p)^T\\). The joint distribution of the collection of random variables \\((x_1,\\ldots,x_p)\\) defines the distribution of the random vector \\(\\mathbf{x}\\).\nWe define the mean of \\(\\mathbf{x}\\) to be the vector of means of its components: \\[E(\\mathbf{x})=\\left(\\begin{array}{c} E(x_1) \\\\ \\vdots \\\\\nE(x_p)\\end{array}\\right)\\]"
  },
  {
    "objectID": "randomVectors.html#covariance-matrix",
    "href": "randomVectors.html#covariance-matrix",
    "title": "Appendix B — Random vectors",
    "section": "B.2 Covariance matrix",
    "text": "B.2 Covariance matrix\nThe obvious way to define the variance of a vector random variable would be to make it the vector of variances of its components. But it is useful also to recognize the covariances between the various components of \\(\\mathbf{x}\\), too. Accordingly, we define it to be the \\(p\\times p\\) matrix \\[Var(\\mathbf{x})=\\left( \\begin{array}{cccc} \\mbox{Var}(x_1) & \\mbox{Cov}(x_1,x_2) &\n\\cdots & \\mbox{Cov}(x_1,x_p) \\\\ \\mbox{Cov}(x_2,x_1) &\\ Var(x_2) & \\cdots &\n\\mbox{Cov}(x_2,x_p) \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\mbox{Cov}(x_p,x_1) &\n\\mbox{Cov}(x_p,x_2) & \\cdots & \\mbox{Var}(x_p) \\end{array} \\right)\\] The above is usually referred to as the variance-covariance matrix of \\(\\mathbf{x}\\), or just as the covariance matrix of \\(\\mathbf{x}\\).\nNotice that since covariance is a symmetric relation, \\(\\mbox{Cov}(x_i,x_j) = \\mbox{Cov}(x_j,x_i)\\), this is a symmetric matrix. Thus we can write \\(\\mbox{Var}(\\mathbf{x})^T=\\mbox{Var}(\\mathbf{x})\\). Also we note that \\(\\mbox{Cov}(x_i,x_i)=\\mbox{Var}(x_i)\\)."
  },
  {
    "objectID": "randomVectors.html#vector-covariance",
    "href": "randomVectors.html#vector-covariance",
    "title": "Appendix B — Random vectors",
    "section": "B.3 Vector covariance",
    "text": "B.3 Vector covariance\nMore generally, if \\(\\mathbf{x}=(x_1,\\ldots,x_p)^T\\) is a \\(p\\times 1\\) random vector and \\(\\mathbf{y}=(y_1,\\ldots,y_q)^T\\) is a \\(q\\times 1\\) random vector, we can define \\[\\mbox{Cov}(\\mathbf{x},\\mathbf{y})=\\left( \\begin{array}{cccc} \\mbox{Cov}(x_1,y_1) &\n\\mbox{Cov}(x_1,y_2) & \\cdots & \\mbox{Cov}(x_1,y_q) \\\\ \\mbox{Cov}(x_2,y_1) & \\mbox{Cov}(x_2,y_2)\n&\n\\cdots & \\mbox{Cov}(x_2,y_q) \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\mbox{Cov}(x_p,y_1) & \\mbox{Cov}(x_p,y_2) & \\cdots & \\mbox{Cov}(x_p,y_q) \\end{array}\n\\right)\\] This is not a symmetric matrix. In fact, since it is \\(p\\times q\\), it is not even square unless \\(p = q\\). Notice, however, that \\(\\mbox{Cov}(\\mathbf{y},\\mathbf{x})=\\mbox{Cov}(\\mathbf{x},\\mathbf{y})^T\\). Also we have \\(\\mbox{Cov}(\\mathbf{x},\\mathbf{x})=\\mbox{Var}(\\mathbf{x})\\).\nSometimes \\(Cov(\\mathbf{x},\\mathbf{y})\\) is referred to as the covariance of \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\)."
  },
  {
    "objectID": "randomVectors.html#some-useful-results",
    "href": "randomVectors.html#some-useful-results",
    "title": "Appendix B — Random vectors",
    "section": "B.4 Some useful results",
    "text": "B.4 Some useful results\nWe know many useful results about expectations, such as \\(E(aX+b)=aE(X)+b\\), when \\(a\\) and \\(b\\) are constants. Here are some vector generalizations.\n\n\\(E(A\\mathbf{x}+\\mathbf{b})=AE(\\mathbf{x})+\\mathbf{b}\\), when \\(A\\) is a \\(q\\times p\\) matrix of constants and \\(\\mathbf{b}\\) is a \\(q\\times 1\\) vector of constants. Notice that this expresses the mean of the \\(q\\times 1\\) random vector in terms of that of the \\(p\\times 1\\) random vector \\(\\mathbf{x}\\).\n\\(\\mbox{Var}(A\\mathbf{x}+\\mathbf{b})=A\\mbox{Var}(\\mathbf{x})A^T\\).\n\\(\\mbox{Cov}(A\\mathbf{x}+\\mathbf{b},C\\mathbf{y}+\\mathbf{d})=A\\mbox{Cov}(\\mathbf{x},\\mathbf{y})C^T\\), where the dimensions of the matrices \\(A\\) and \\(C\\) and the vectors \\(\\mathbf{b}\\) and \\(\\mathbf{d}\\) are as required to allow the matrix multiplication."
  },
  {
    "objectID": "multivariateNormal.html#definition",
    "href": "multivariateNormal.html#definition",
    "title": "Appendix C — The multivariate normal distribution",
    "section": "C.1 Definition",
    "text": "C.1 Definition\nThe normal distribution is the most important distribution for a single random variable, and its extension to a vector random variable is equally important in Statistics.\nLet us denote the \\(p\\)-dimensional random vector \\[\\mathbf{x}^T=(x_1,\\ldots,x_p)\\] where \\(x_1,\\ldots, x_p\\) are univariate random variables.\nIn the univariate case the probability density function (p.d.f.) \\[f(x)=\\frac{1}{\\sqrt{2\\pi} \\sigma} e^{ -\n\\frac{(x-\\mu)^2}{2\\sigma^2}},\\] i.e. depends on two parameters: \\(\\mu\\) and \\(\\sigma\\). Note that this formula can also be written as \\[\\label{eq5_1}\nf(x)=\\frac{1}{\\sqrt{2\\pi} \\sigma}\ne^{\\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu)}\\] where \\(\\Sigma=\\sigma^2\\). When \\(\\mathbf{x}\\) is a \\(p\\)-dimensional random vector it can be shown that the joint p.d.f. is \\[f(\\mathbf{x})=\\frac{1}{(2\\pi)^{p/2}|\\Sigma|^{1/2}}\ne^{-\\frac{1}{2}(\\mathbf{x}-\\mathbf{\\mu})^T\\Sigma^{-1}(\\mathbf{x}-\\mathbf{\\mu})}\\] where \\(\\Sigma\\) is the \\(p\\times p\\) covariance matrix of the random variables and \\(|\\Sigma|\\) is its determinant. This equation reduces to the previous one when \\(p=1\\). The p.d.f. of the MVN also has two parameters: the vector \\(\\mathbf{\\mu}\\) and the covariance matrix \\(\\Sigma\\). In statistical notation, we would write \\(\\mathbf{x} \\sim N_p(\\mathbf{\\mu},\\Sigma)\\)"
  },
  {
    "objectID": "multivariateNormal.html#linear-transformation",
    "href": "multivariateNormal.html#linear-transformation",
    "title": "Appendix C — The multivariate normal distribution",
    "section": "C.2 Linear transformation",
    "text": "C.2 Linear transformation\nThe key results about multivariate normal distributions are these.\n\nLinear transformation. If \\(A\\) is a \\(q\\times p\\) matrix of rank \\(q\\) (with \\(q\\leq p\\)) \\(\\mathbf{b}\\) is a \\(q\\)-dimensional vector, then \\[A\\mathbf{x}+\\mathbf{b}\\sim N_q(A\\mathbf{\\mu}+\\mathbf{b},A\\Sigma A^T)\\]\n(Note that the formulae for the mean and variance follow from the general results on transformations in the previous section; the extra content in this result is that Normality is preserved by linear transformations.)\nStandardization. We can derive a standardizing transformation that produces a vector with zero mean and identity variance. Since \\(\\Sigma\\) is a variance covariance matrix it is positive definite. One of the properties of positive definite matrices is that we can find a \\(p\\times p\\) matrix \\(C\\) such that \\(\\Sigma= CC^T\\). Then it follows immediately that \\[C^{-1}(\\mathbf{x}-\\mathbf{\\mu})\\sim N_p(\\mathbf{0}_p,I_p)\\] where \\(\\mathbf{0}_p\\) is the \\(p\\times 1\\) vector of zeroes and \\(I_p\\) is the \\(p\\times p\\) identity matrix. Note that \\(C\\) is not unique; there are many possible choices for \\(C\\), which can be described as a square root of \\(\\Sigma\\).\nSo we see that standardization produces a vector of random variables that are independent and identically distributed as \\(N(0,1)\\). So standardization produces independent standard normal random variables.\nIf \\(\\Sigma\\) is diagonal, i.e. \\[\\Sigma =\\left(\\begin{array}{cccc} \\sigma_{11} & 0 & \\cdots & 0\\\\ 0 &\n\\sigma_{22} & \\cdots & 0\\\\ \\vdots & \\vdots & \\ddots & \\vdots\\\\ 0 & 0\n& \\cdots & \\sigma_{pp}\\end{array}\\right)\\] then the random variables \\(x_1,\\ldots,x_p\\) are independent and of course they are uncorrelated too."
  }
]