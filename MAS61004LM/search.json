[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A Second Course on Linear Models",
    "section": "",
    "text": "Introduction\nThese notes are written for students on MAS61004. I have called this a second course because to get onto the MSc you will almost certainly have learned something about linear models already! These notes will still cover topics from the beginning, without assuming you have studied the content before, but we will work through some of the earlier topics fairly quickly.\nStudents who have taken the Graduate Certificate in Statistics to gain entry onto this MSc have studied linear models in the MAS5052 module. The Graduate Certificate lecture notes on linear models are available here. and you may find them helpful for revision.\nI will assume some knowledge of likelihood and maximum likelihood estimation, but will revise this topic in the first lecture. Specifically, you should know how to construct a likelihood function for a statistical model, and understand the rationale for using maximum likelihood to estimate model parameters. This topic is also covered in the MAS5052 module: the notes are available here.\nWe will be using R for implementing linear modelling methods, and I will not assume you have used R before. You will be learning R in the EDA with R part of this module, which is taught in parallel with this part.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "A Second Course on Linear Models",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nSome parts of the notes have been written by me, and others were written and modified by various colleagues who taught this content over the years: Eleanor Stillman, Jonathan Jordan, Kostas Triantafyllopoulos, Kevin Walters.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "basicconcepts.html",
    "href": "basicconcepts.html",
    "title": "1  Basic concepts",
    "section": "",
    "text": "1.1 Motivating example\nThe following is an extract from a data set with data on 305 individuals:\nlibrary(tidyverse)\nrespiration &lt;- read_table(\"https://oakleyj.github.io/exampledata/resp_data.txt\")\nrespiration\n\n# A tibble: 305 × 8\n     vol exercise pulse pollution asbestos  home smoke asthma\n   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1  117.     15.8 17.5     11.9          1     1     1      0\n 2  148.     23.5 28.4      1.88         1     3     1      1\n 3  214.     13.8 15.0      2.38         1     1     0      0\n 4  162.     15.6 15.7      6.34         1     3     0      1\n 5  352.     26.6 19.7      1.63         0     4     0      0\n 6  304.     23.2 14.2      2.72         0     4     0      0\n 7  157.     29.0 33.7      4.03         0     1     1      0\n 8  110.     16.6 14.7      1.24         1     2     1      0\n 9  218.     15.1  9.16     1.14         0     2     0      0\n10  246.     16.9 21.9      0.731        0     4     0      0\n# ℹ 295 more rows\nThe meanings of the column headings are:\nThe key question we are interested in is how the volume of air expelled relates to the other variables.\nThere are a number of types of variables in this data set: vol, exer, pulse and poll are continuous variables, while asbestos, home, smoke and asthma are categorical (factor) variables. Of the latter, home has multiple levels while the others are binary.\nThe variable we are trying to predict is the response variable, also referred to as the dependent variable; in this example, the response is vol. The variables that we use to predict the response are called either predictor, explanatory or independent variables.\nQuestions we might like to answer include:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basic concepts</span>"
    ]
  },
  {
    "objectID": "basicconcepts.html#motivating-example",
    "href": "basicconcepts.html#motivating-example",
    "title": "1  Basic concepts",
    "section": "",
    "text": "Example 1.1 Respiration data\n\n\n\n\n\nvol - volume of air expelled whilst breathing out\nexer - number of hours exercise per month\npulse- scaled resting pulse rate\npoll - lifetime exposure to air pollution\nasbestos - has the individual been exposed to asbestos (1=exposed, 0=unexposed)\nhome - place of residence (1=England, 2=Scotland, 3=Wales, 4=Northern Ireland)\nsmoke - have they ever smoked (1=yes, 0=no)\nasthma - do they have asthma (1=yes, 0=no)\n\n\n\n\n\n\nWhich variables are ‘best’ at explaining the variation in the volume of air and do we need all the variables? (This amounts to choosing a statistical model.)\nHow much of the variation in the volume of air can we account for with our model?\nDoes the model that we come up with satisfy the implicit assumptions of a linear model?\nHow can we perform hypothesis tests relating to parameters in our statistical model?\nAre there observations that don’t fit our model?\nAre there observations that might be exerting a lot of influence over our model parameters?\nHow can we make predictions about volumes based on new observations and how can we calculate confidence intervals for these predictions?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basic concepts</span>"
    ]
  },
  {
    "objectID": "basicconcepts.html#the-idea-of-a-linear-model",
    "href": "basicconcepts.html#the-idea-of-a-linear-model",
    "title": "1  Basic concepts",
    "section": "1.2 The idea of a linear model",
    "text": "1.2 The idea of a linear model\nIn simple linear regression, we have \\(n\\) observations of two variables, \\((x_i,y_i)\\) for \\(i=1,\\ldots,n\\). We treat \\(x_i\\) as an explanatory variable and \\(y_i\\) as the response variable (or dependent variable), and we attempt to fit a straight line \\(y=\\beta_0+\\beta_1 x\\) to our data points. We further suppose that each \\(y_i\\) is an observed value of a random variable \\(Y_i\\).\n\n\n\n\n\n\nNotation: lower and upper case letters\n\n\n\nA convention is to use an upper case letter to denote a random variable, and a lower case letter to denote the observed value of that random variable, e.g., in statistical modelling we treat some data \\(y_1,\\ldots,y_n\\) as observations of random variables \\(Y_1,\\ldots,Y_n\\), and our statistical model is a probability distribution for \\(Y_1,\\ldots,Y_n\\).\nWe will not always keep to this convention as it can become notationally difficult. You will need to be alert to the context as to whether we are considering a random variable or an observed value, though I will try to make it as clear as possible in these notes.\n\n\nA statistical model here takes the form \\(Y_i=\\beta_0+\\beta_1 x_i+\\varepsilon_i\\), where \\(\\varepsilon_i\\) is a (random) “error” term giving the difference between the straight line value and the actual value \\(y_i\\). In this section we discuss the generalization of this known as multiple regression, which allows for more than one explanatory variable.\nSuppose that for each observation \\(i\\) we have a response \\(y_i\\) and \\(r\\) explanatory variables \\(x_{i1},\\ldots,x_{ir}\\), giving data \\[\n(x_{i1},\\ldots,x_{ir},y_i), \\quad i=1,\\ldots,n.\n\\] We could propose the statistical model \\[\nY_i=\\beta_0+\\beta_1 x_{i1}+\\cdots+\\beta_rx_{ir}+\\varepsilon_i.\n\\tag{1.1}\\] This has two parts, a linear predictor \\(\\beta_0+\\beta_1 x_{i1}+\\cdots+\\beta_r x_{ir}\\) and a random error \\(\\varepsilon_i\\). The linear predictor formalizes the idea that the response is a linear combination of terms involving the explanatory variables. The error term allows for variation in the response for identical values of the explanatory variables (not necessarily measurement errors); we later impose some conditions on \\(\\varepsilon_i\\).\nWhen \\(r=2\\), giving \\(Y_i=\\beta_0+\\beta_1 x_{i1}+\\beta_2 x_{i2}+\\varepsilon_i\\), we are fitting a plane to the data; as the number of parameters increases we fit increasingly higher-dimensional hyper-planes.\nAlternatively, reverting to a single explanatory variable, we might wish to express the relationship between \\(y\\) and \\(x\\) in a quadratic way: \\[\nY_i=\\beta_0+\\beta_1 x_i+\\beta_2 x_i^2+\\varepsilon_i.\n\\tag{1.2}\\] In this model \\(\\beta_0+\\beta_1 x_i+\\beta_2 x_i^2\\) is the linear predictor. Although the relationship is quadratic, we would still call this a linear model because it is linear in the parameters \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\beta_2\\). It is a common misconception that linear models require the relationship between the response and explanatory variables to be linear, but in this terminology a linear model is one which is linear in the parameters \\(\\beta_i\\), not necessarily in the explanatory variables.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basic concepts</span>"
    ]
  },
  {
    "objectID": "basicconcepts.html#matrix-formulation",
    "href": "basicconcepts.html#matrix-formulation",
    "title": "1  Basic concepts",
    "section": "1.3 Matrix formulation",
    "text": "1.3 Matrix formulation\n\n\n\n\n\n\nNote\n\n\n\nIn these notes vectors are written in bold and scalars in normal text, so \\(\\boldsymbol{y}\\) is a vector and \\(y\\) is a single value.\n\n\nIt is convenient to express linear models using vectors and matrices. We can always collect the observed values of the variable \\(y\\) into a vector \\(\\boldsymbol{y} =(y_1,\\ldots,y_n)^T\\) and we can similarly define \\(\\boldsymbol{x} =(x_1,\\ldots,x_n)^T\\) and \\(\\boldsymbol{\\varepsilon}=(\\varepsilon_1,\\ldots,\\varepsilon_n)^T\\). If we also define \\(\\boldsymbol{1}_n=(1,,\\ldots,1)^T\\) to be a vector of \\(n\\) ones, we can write the simple regression model as \\[\\begin{equation}\\label{eq3_3}\n\\boldsymbol{Y}=\\beta_0 \\boldsymbol{1}_n+\\beta_1 \\boldsymbol{x}+\\boldsymbol{\\varepsilon}.\n\\end{equation}\\] Even more neatly, if we define the parameter vector \\(\\boldsymbol{\\beta}=(\\beta_0,\\beta_1)^T\\) and the \\(n\\times 2\\) matrix \\(X\\) is defined to have \\(\\boldsymbol{1}_n\\) as its first column and \\(\\boldsymbol{x}\\) as its second, then\n\\[\n\\boldsymbol{Y}=X\\boldsymbol{\\beta} +\\boldsymbol{\\varepsilon}.\n\\tag{1.3}\\] The form (Equation 1.3) naturally generalizes to more complicated models; for a suitable definition of the parameter vector \\(\\boldsymbol{\\beta}\\) and the matrix \\(X\\) (also called the design matrix) we can express both Equation 1.1 and Equation 1.2 in this form.\n\n\n\n\n\n\nThe importance of matrix notation\n\n\n\nMatrix notation is really useful. As well as clarifying the definition of a linear model (any model that can be expressed in the form Equation 1.3), it means that any result obtained using the matrix notation can be applied to all linear models.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basic concepts</span>"
    ]
  },
  {
    "objectID": "basicconcepts.html#sec-parameters",
    "href": "basicconcepts.html#sec-parameters",
    "title": "1  Basic concepts",
    "section": "1.4 Parameters and assumptions",
    "text": "1.4 Parameters and assumptions\nThe linear model is expressed by (Equation 1.3). In its general form,\n\n\\(\\boldsymbol{y}\\) is a \\(n\\times 1\\) vector of observed random variables.\n\\(X\\) is an \\(n\\times p\\) matrix of known coefficients (perhaps observed or controlled values of other explanatory variables, but also perhaps functions of such explanatory variables, or just constants).\n\\(\\boldsymbol{\\beta}\\) is a \\(p\\times 1\\) vector of unknown parameters.\n\\(\\boldsymbol{\\varepsilon}\\) is an \\(n \\times 1\\) vector of unobserved random variables.\n\nThe number of components, \\(p\\), of \\(\\boldsymbol{\\beta}\\) is allowed to be whatever we wish, as long as it is not too large to be sensible for the number of observations. In the multiple regression model (Equation 1.1) it would be \\(r+1\\), while in the quadratic regression model (Equation 1.2) it would be 3.\nTo fully define our model we need to make some assumptions on \\(\\boldsymbol{\\varepsilon}\\). In the general linear model, we assume that \\(\\boldsymbol{\\varepsilon}\\) has a multivariate normal distribution, whose components have zero mean, are independent, and have common variance \\(\\sigma^2\\) (that they have the same variance is referred to as homoscedasticity), which is another unknown parameter. Testing these assumptions and what to do if they are not satisfied will be a major part of this module.\nFrom these assumptions, \\[\n\\boldsymbol{\\varepsilon}\\sim N_n(\\boldsymbol{0},\\sigma^2I_n)\n\\] where \\(N_n\\) represents an \\(n\\)-dimensional multivariate normal distribution.\nSince \\(\\boldsymbol{Y}=X\\boldsymbol{\\beta}+\\boldsymbol{\\varepsilon}\\), it follows (from the linear transformation property for the multivariate normal) that \\[\n\\boldsymbol{Y}\\sim N_n(X\\boldsymbol{\\beta},\\sigma^2I_n).\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basic concepts</span>"
    ]
  },
  {
    "objectID": "basicconcepts.html#terminology",
    "href": "basicconcepts.html#terminology",
    "title": "1  Basic concepts",
    "section": "1.5 Terminology",
    "text": "1.5 Terminology\nWe can think of each column of the \\(X\\) matrix as comprising \\(n\\) values of a regressor variable, also called an independent variable. So we have in general one response variable and \\(p\\) (\\(p=r+1\\) in multiple regression, \\(p=3\\) in quadratic regression) regressor variables.\nSometimes this terminology may seem strange, because, for example, in the simple linear regression model the \\(X\\) matrix has two columns, the first of which is a column of ones. We still would say that this column defines a regressor variable, but this ‘variable’ is a constant.\nRegressor variables are also sometimes called explanatory variables, but in this course we will give this term a slightly different meaning. Consider the quadratic regression (Equation 1.2). Here there are three regressor variables (the constant, the \\(x_i\\) column and the \\(x_i^2\\) column), but there is really only one \\(x\\) variable. The three regressor variables are all functions of the \\(x\\) variable. In this course we will refer to the \\(x\\) variable as the single explanatory variable in the quadratic regression model. In general, the regressor variables will always be functions of the explanatory variables.\n\nExample 1.2 An example of the design matrix \\(X\\) - polynomial regression\n\nThe general polynomial regression with one explanatory variable, a generalization of the quadratic regression Equation 1.2, is \\[\nY_i=\\beta_0+\\beta_1x_i+\\cdots+\\beta_rx_i^r+\\varepsilon_i\n\\] which is turned into the general linear model form by \\[\n\\boldsymbol{Y}=\\left(\\begin{array}{c} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\\nY_n\\end{array}\\right), \\quad X=\\left(\\begin{array}{cccc} 1 & x_1 &\n\\cdots & x_1^r \\\\ 1 & x_2 & \\cdots & x_2^r \\\\ \\vdots & \\vdots &\n\\ddots & \\vdots \\\\ 1 & x_n & \\cdots & x_n^r\\end{array}\\right),\n\\quad \\boldsymbol{\\beta}=\\left(\\begin{array}{c} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\\n\\beta_r\\end{array}\\right), \\quad\n\\boldsymbol{\\varepsilon}=\\left(\\begin{array}{c} \\varepsilon_1 \\\\ \\varepsilon_2 \\\\\n\\vdots \\\\ \\varepsilon_n\\end{array}\\right)\n\\] and \\(p=r+1\\).\n\n\n\n\n\n\nExercise\n\n\n\n\nExercise 1.1 An alternative way to parameterise the simple linear regression model is as follows: \\[\nY_i = \\beta_0 + \\beta_1(x_i-\\bar{x}) + \\varepsilon_i.\n\\] This is sometimes referred to as mean-centring the independent variable.\n\nWrite this model in matrix notation.\nBy considering \\(E(Y_i)\\), give an interpretation of the parameter \\(\\beta_0\\). (Assume that \\(\\bar{x}\\neq 0\\), so that \\(\\beta_0\\) would not be the \\(y\\)-intercept in a regression line.)\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nIn the matrix notation \\[\\boldsymbol{Y} = X\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon},\\] we have\n\n\\[\n\\boldsymbol{Y}=\\left(\\begin{array}{c} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\\nY_n\\end{array}\\right), \\quad X=\\left(\\begin{array}{cc} 1 & (x_1-\\bar{x})\\\\\n1 & (x_2- \\bar{x})\\\\\n\\vdots & \\vdots \\\\\n1 & (x_n- \\bar{x})\n\\end{array}\\right),\n\\quad \\boldsymbol{\\beta}=\\left(\\begin{array}{c} \\beta_0 \\\\ \\beta_1 \\end{array}\\right), \\quad\n\\boldsymbol{\\varepsilon}=\\left(\\begin{array}{c} \\varepsilon_1 \\\\ \\varepsilon_2 \\\\\n\\vdots \\\\ \\varepsilon_n\\end{array}\\right).\n\\]\n\nWe note that if \\(x_i = \\bar{x}\\), then \\(E(Y_i) = \\beta_0\\). So we interpret \\(\\beta_0\\) as the expected value of the dependent variable when the independent variable is equal to its mean value.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basic concepts</span>"
    ]
  },
  {
    "objectID": "parameterestimation.html",
    "href": "parameterestimation.html",
    "title": "2  Parameter estimation for linear models",
    "section": "",
    "text": "2.1 Estimating the coefficients\nGiven a linear model as in \\[\n\\boldsymbol{Y}=X\\boldsymbol{\\beta} +\\boldsymbol{\\varepsilon},\n\\] we will want to use our observed data \\(\\boldsymbol{y}\\) to estimate the parameters \\(\\boldsymbol{\\beta}\\); for example in simple linear regression we are estimating an intercept \\(\\beta_0\\) and gradient term \\(\\beta_1\\).\nSuppose we have estimates of the parameters \\(\\hat{\\beta_0},\\hat{\\beta_1},\\ldots,\\hat{\\beta}_{p-1}\\), or in vector form \\(\\boldsymbol{\\hat{\\beta}}\\). Then the fitted value \\(\\hat{y}_i\\) for observation \\(i\\) is defined as \\[\\hat{y}_i:=\\hat{\\beta_0}+\\hat{\\beta_1}x_{i1}+\\ldots+\\hat{\\beta}_{p-1}x_{i,p-1}=\\boldsymbol{x}_i^T\\boldsymbol{\\hat{\\beta}},\\] where \\(\\boldsymbol{x}_i^T\\) is the (row) vector consisting of row \\(i\\) of \\(X\\). The matrix product \\(X\\boldsymbol{\\hat{\\beta}}\\) then gives the vector of fitted values for all observations.\nFor observation \\(i\\), the difference between the observed value \\(y_i\\) and the fitted value \\(\\hat{y}_i\\) is called the residual for that observation, denoted by \\(e_i\\).\nThe vector of residuals will be \\[\n\\boldsymbol{e}:=\\left(\\begin{array}{c} e_1\\\\ \\vdots\n\\\\ e_n\\end{array}\\right)=\\boldsymbol{y}-X\\boldsymbol{\\hat{\\beta}}.\n\\] Residuals play a key role in linear models. In later chapters we will see how they can be used to assess whether the statistical model we use to describe the relationship between our variables meets the assumptions underpinning linear model theory.\nThe sum of squares of the residuals, written as \\(s_r\\) or \\(s(\\boldsymbol{\\hat{\\beta}})\\) is nicely expressed as \\[\\begin{equation}\ns_r=s(\\boldsymbol{\\hat{\\beta}}):=\\sum_{i=1}^ne_i^2=\\boldsymbol{e}^T\\boldsymbol{e}=(\\boldsymbol{y}-X\\boldsymbol{\\hat{\\beta}})^T(\\boldsymbol{y}-X\\boldsymbol{\\hat{\\beta}}).\n\\end{equation}\\] This sum is known as residual sum of squares and plays an important role in the the analysis of linear models. To see this, consider the likelihood of \\(\\boldsymbol{\\beta}\\) and \\(\\sigma\\); this comes directly from the definition of the multivariate normal density: \\[\\begin{eqnarray*}\nL(\\boldsymbol{\\beta},\\sigma^2;\\boldsymbol{y}) &=& f(\\boldsymbol{y}|\\boldsymbol{\\beta},\\sigma^2) \\\\\n&=& \\frac{1}{(2\\pi \\sigma^2)^{n/2}}\n\\exp\\left(-\\frac{1}{2\\sigma^2}(\\boldsymbol{y} -X\\boldsymbol{\\beta})^T (\\boldsymbol{y}\n-X\\boldsymbol{\\beta}) \\right) \\\\ & \\propto & \\sigma^{-n}\n\\exp\\left(-\\frac{1}{2\\sigma^2}(\\boldsymbol{y} -X\\boldsymbol{\\beta})^T (\\boldsymbol{y}\n-X\\boldsymbol{\\beta}) \\right)\n\\end{eqnarray*}\\] In this derivation we have used the fact that \\(|\\sigma^2I_n|=(\\sigma^2)^n\\).\nThe log likelihood is thus \\[\\ell(\\boldsymbol{\\beta},\\sigma^2;\\boldsymbol{y})=-n\\log \\sigma-\\frac{1}{2\\sigma^2}(\\boldsymbol{y} -X\\boldsymbol{\\beta})^T (\\boldsymbol{y}\n-X\\boldsymbol{\\beta})+c,\\] where \\(c\\) is a constant term which does not depend on \\(\\boldsymbol{\\beta}\\) or \\(\\sigma\\) and can be ignored when maximizing.\nTo maximize this log likelihood with respect to \\(\\boldsymbol{\\beta}\\), we obviously must minimize \\((\\boldsymbol{y}-X\\boldsymbol{\\beta})^T(\\boldsymbol{y}-X\\boldsymbol{\\beta})\\), which is exactly the residual sum of squares. So the maximum likelihood estimator \\(\\hat{\\boldsymbol{\\beta}}\\) for the parameters \\(\\boldsymbol{\\beta}\\) is found by minimizing the residual sum of squares.\nMinimizing the residual sum of squares is called the method of least squares, and can also be used when the data are not assumed to be normally distributed. Intuitively, a value of \\(\\boldsymbol{\\hat{\\beta}}\\) that makes the residuals small ‘fits’ the data well, justifying the idea of the least squares estimator.\nWe give a proof in the Chapter appendix.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Parameter estimation for linear models</span>"
    ]
  },
  {
    "objectID": "parameterestimation.html#sec-leastsquares",
    "href": "parameterestimation.html#sec-leastsquares",
    "title": "2  Parameter estimation for linear models",
    "section": "",
    "text": "The ‘hat’ notation\n\n\n\nNote the difference between \\(\\boldsymbol{\\beta}\\) and \\(\\boldsymbol{\\hat{\\beta}}\\): \\(\\boldsymbol{\\beta}\\) is the vector of true parameter values, \\(\\boldsymbol{\\hat{\\beta}}\\) is the estimate of the vector of true parameter values.\n\n\n\n\n\n\n\n\n\nErrors and residuals\n\n\n\nNote the difference between the error (\\(\\varepsilon_i\\)) and the residual (\\(e_i\\)) for observation \\(i\\). The error is part of the statistical model that we fit to our data. The residual is the observed value of the difference between the response and the fitted value. As we will see later, the observed residuals, the \\(e_i\\), allow us to estimate the variance \\(\\sigma^2\\) of the error terms, the \\(\\varepsilon_i\\)\n\n\n\n\n\n\n\n\nTheorem 2.1 Assume that \\(X\\) has rank \\(p\\). Then the least squares estimate of \\(\\boldsymbol{\\beta}\\) is\n\\[\n\\boldsymbol{\\hat{\\beta}}=(X^TX)^{-1}X^T\\boldsymbol{y}.\n\\]\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\nExercise 2.1 Note the requirement that \\(X\\) is of rank \\(p\\) (full column rank), i.e. that \\(X^TX\\) is invertible. Now consider trying to fit a simple linear regression model to a data set in which the independent variable is held constant at a single value \\(x\\). Intuitively, why can we not fit the model, and how does this relate to the requirement on \\(X\\)?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIntuitively, if the independent variable is held constant, our data set might look something like this:\n\n\n\n\n\n\n\n\n\nand there’s no way we could identify the slope parameter \\(\\beta_1\\) in the model \\[Y_i = \\beta_0 + \\beta_1x_i+\\varepsilon_i,\\] from these data. The design matrix \\(X\\) would be \\[\nX = \\left(\\begin{array}{cc}1 & x\\\\ 1 & x \\\\ \\vdots & \\vdots \\\\ 1 & x\\end{array}\\right)\n\\] and we can see that column 2 is \\(x\\) multiplied by column 1: the matrix has one linearly independent column only, and so is of rank 1.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Parameter estimation for linear models</span>"
    ]
  },
  {
    "objectID": "parameterestimation.html#mean-and-variance-of-the-least-squares-estimator",
    "href": "parameterestimation.html#mean-and-variance-of-the-least-squares-estimator",
    "title": "2  Parameter estimation for linear models",
    "section": "2.2 Mean and variance of the least squares estimator",
    "text": "2.2 Mean and variance of the least squares estimator\nWith apologies for some confusing notation (though this is the convention), we are going to write both\n\\[\n\\boldsymbol{\\hat{\\beta}} = (X^TX)^{-1}X^T\\boldsymbol{y},\n\\] and \\[\n\\boldsymbol{\\hat{\\beta}} = (X^TX)^{-1}X^T\\boldsymbol{Y},\n\\] where the former is a constant, computed using the observed data \\(\\boldsymbol{y}\\), and the latter is a random variable: a function of the random vector \\(\\boldsymbol{Y}\\).\n\n\n\n\n\n\nEstimators and estimates\n\n\n\nWe refer to \\[\n(X^TX)^{-1}X^T\\boldsymbol{Y}\n\\] as an estimator (a function of a random variable) and \\[\n(X^TX)^{-1}X^T\\boldsymbol{y}\n\\] as the corresponding estimate: the observed value of the estimator.\n\n\nYou will get used to knowing which is meant from the context, but in this section, we are considering the random variable\n\\[\n\\boldsymbol{\\hat{\\beta}} = (X^TX)^{-1}X^T\\boldsymbol{Y},\n\\] so that we can consider the properties of the estimator: whether it is biased, how far our estimates might be from the true values and so on.\nFrom the assumptions about the distribution of \\(\\boldsymbol{\\varepsilon}\\) in Section 1.4, it follows that \\[\nE(\\boldsymbol{Y})=X\\boldsymbol{\\beta} \\text{ and }\nVar(\\boldsymbol{Y})=\\sigma^2I_n,\n\\] where \\(I_n\\) is the \\(n\\times n\\) identity matrix, and that the distribution of \\(\\boldsymbol{Y}\\) is multivariate normal.\nFrom the theory of transformations of multivariate distributions, the expected value of \\(\\boldsymbol{\\hat{\\beta}}\\) is \\[\nE(\\boldsymbol{\\hat{\\beta}}) = E((X^TX)^{-1}X^T\\boldsymbol{Y})=(X^TX)^{-1}X^T\nX\\boldsymbol{\\beta}=\\boldsymbol{\\beta}\n\\] and so \\(\\boldsymbol{\\hat{\\beta}}\\) is an unbiased estimator for \\(\\boldsymbol{\\beta}\\).\nThe variance properties of \\(\\boldsymbol{\\hat{\\beta}}\\) are contained in its covariance matrix. Again from the theory of the multivariate normal, \\[\\begin{eqnarray*}\nVar(\\boldsymbol{\\hat{\\beta}}) &=& Var( (X^TX)^{-1}X^T\\boldsymbol{Y} )\n\\\\ &=&\n(X^TX)^{-1}X^TVar(\\boldsymbol{y})((X^TX)^{-1}X^T)^T\n\\\\ &=& (X^TX)^{-1}X^T \\sigma^2I_nX\n(X^TX)^{-1}\\\\ &=& \\sigma^2 (X^TX)^{-1}.\n\\end{eqnarray*}\\]\n\n\n\n\n\n\nNote\n\n\n\nFor designed experiments (where we can choose the values of the independent variables), and more generally when considering different model choices, it can be useful to consider the values in the matrix \\((X^TX)^{-1}\\) and the off-diagonal elements in particular. We usually want to avoid highly correlated parameter estimators; zero or relatively small off-diagonal elements are helpful. We’ll discuss this further when we study hypothesis testing.\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\nExercise 2.2 Recall the simple linear regression model with alternative parameterisation given in Exercise 1.1: \\[\nY_i = \\beta_0 + \\beta_1(x_i-\\bar{x}) + \\varepsilon_i.\n\\] Using the matrix notation:\n\nFind the least squares estimator of \\(\\boldsymbol{\\beta}\\).\nWhat is the covariance between \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\)?\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nFrom the solution to Exercise 1.1, we have \\[\nX=\\left(\\begin{array}{cc} 1 & (x_1-\\bar{x})\\\\\n1 & (x_2- \\bar{x})\\\\\n\\vdots & \\vdots \\\\\n1 & (x_n- \\bar{x})\n\\end{array}\\right)\n\\] and so \\[\nX^TX = \\left(\\begin{array}{cc}n & 0 \\\\\n0 & \\sum_{i=1}^n(x_i- \\bar{x})^2\\end{array}\\right), \\quad (X^TX)^{-1} = \\left(\\begin{array}{cc}\\frac1n & 0 \\\\\n0 & \\frac{1}{\\sum_{i=1}^n(x_i- \\bar{x})^2}\\end{array}\\right)\n\\] as \\[\n\\sum_{i=1}^n(x_i- \\bar{x}) = \\left(\\sum_{i=1}^nx_i\\right)-n\\bar{x} = n\\bar{x} - n\\bar{x} = 0.\n\\] Hence, \\[\n\\hat{\\boldsymbol{\\beta}} = (X^TX)^{-1}X^T\\boldsymbol{Y} = \\left(\\begin{array}{c}\\bar{Y}\\\\\n\\frac{\\sum_{i=1}^n (x_i-\\bar{x})Y_i}{\\sum_{i=1}^n (x_i-\\bar{x})^2}\\end{array}\\right)\n\\]\nThe matrix \\((X^TX)^{-1}\\) is diagonal (as \\((X^TX)\\) is), and so \\(Cov(\\hat{\\beta}_0, \\hat{\\beta}_1)=0\\). Independence between parameter estimators can be desirable for computational reasons, and so it can be helpful to mean-centre the independent variable in this way.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Parameter estimation for linear models</span>"
    ]
  },
  {
    "objectID": "parameterestimation.html#sec-res_se",
    "href": "parameterestimation.html#sec-res_se",
    "title": "2  Parameter estimation for linear models",
    "section": "2.3 Estimating the error variance \\(\\sigma^2\\)",
    "text": "2.3 Estimating the error variance \\(\\sigma^2\\)\nTo construct an estimator for \\(\\sigma^2\\), we will first attempt maximum likelihood estimation. This turns out to produce a biased estimator, but it will be straightforward to apply a correction that gives an unbiased estimator.\n\n2.3.1 Maximum likelihood estimation\nWe previously noted that the log-likelihood for \\(\\boldsymbol{\\beta},\\sigma^2\\) is \\[\n\\ell(\\boldsymbol{\\beta},\\sigma^2;\\boldsymbol{y})=-n\\log \\sigma-\\frac{1}{2\\sigma^2}(\\boldsymbol{y} -X\\boldsymbol{\\beta})^T (\\boldsymbol{y}\n-X\\boldsymbol{\\beta})+c,\n\\] where \\(c\\) is a constant term.\nAfter we have maximized this with respect to \\(\\boldsymbol{\\beta}\\), we have \\[\n\\ell(\\boldsymbol{\\hat{\\beta}},\\sigma^2;\\boldsymbol{y})= -n\\log \\sigma\n-\\frac{1}{2\\sigma^2}(\\boldsymbol{y} -X\\boldsymbol{\\hat{\\beta}})^T (\\boldsymbol{y}\n-X\\boldsymbol{\\hat{\\beta}})\n\\] and we now need to maximize this with respect to \\(\\sigma^2\\). This is easily done (by differentiating with respect to \\(\\sigma^2\\) and setting the result equal to zero) and produces the MLE, \\[\n\\hat{\\sigma}^2_{MLE}=n^{-1}(\\boldsymbol{y} -X\\boldsymbol{\\hat{\\beta}})^T (\\boldsymbol{y} -X\\boldsymbol{\\hat{\\beta}})=s_r/n,\n\\] where \\(s_r\\) was introduced as the residual sum of squares in Section 2.1.\nHowever, this is a biased estimator. If we define as \\(S_r\\) as the random variable from replacing all instances of \\(\\boldsymbol{y}\\) with \\(\\boldsymbol{Y}\\) in \\(s_r\\), we show in the Chapter appendix that \\(E(S_r)=\\sigma^2(n-p)\\).\nHence, if we consider the maximum likelihood estimator \\(S_r/n\\), we have \\[\nE(S_r/n) = \\frac{n-p}{n}\\sigma^2.\n\\]\n\n\n2.3.2 An unbiased estimator of \\(\\sigma^2\\)\nTo obtain an unbiased estimator of \\(\\sigma^2\\) we simply divide \\(S_r\\) by \\(n-p\\) instead of \\(n\\), and so the estimate we compute from our data is\n\\[\n\\hat{\\sigma}^2 = \\frac{(\\boldsymbol{y} -X\\boldsymbol{\\hat{\\beta}})^T (\\boldsymbol{y} -X\\boldsymbol{\\hat{\\beta}})}{n-p}.\n\\] Note that \\(\\sqrt{\\hat{\\sigma}^2}\\) is referred to as the residual standard error (and will be reported in R output).\n\n\n\n\n\n\nNote\n\n\n\nThis division by \\(n-p\\) rather than \\(n\\) to obtain an unbiased estimator is a generalization of the division by \\(n-1\\) rather than \\(n\\) to obtain an unbiased estimator of the variance in a simple normal sample, which corresponds to the case \\(p=1\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Parameter estimation for linear models</span>"
    ]
  },
  {
    "objectID": "parameterestimation.html#distributions-of-the-estimators",
    "href": "parameterestimation.html#distributions-of-the-estimators",
    "title": "2  Parameter estimation for linear models",
    "section": "2.4 Distributions of the estimators",
    "text": "2.4 Distributions of the estimators\nConsidering the estimator \\[\n\\hat{\\boldsymbol{\\beta}} = (X^TX)^{-1}X^T\\boldsymbol{Y},\n\\] we have shown that \\[\nE(\\boldsymbol{\\hat{\\beta}}) = \\boldsymbol{\\beta}\n\\] and \\[\nVar(\\boldsymbol{\\hat{\\beta}}) = \\sigma^2 (X^TX)^{-1}.\n\\] and as \\(\\boldsymbol{\\hat{\\beta}}\\) is a linear transformation of the multivariate normal random vector \\(\\boldsymbol{Y}\\), we have the result that\n\\[\n\\boldsymbol{\\hat{\\beta}}\\sim N_p(\\boldsymbol{\\beta},\\sigma^2(X^TX)^{-1}).\n\\] Considering the estimator \\[\n\\hat{\\sigma}^2 = \\frac{1}{n-p}(\\boldsymbol{Y}-X\\boldsymbol{\\hat{\\beta}})^T(\\boldsymbol{Y}-X\\boldsymbol{\\hat{\\beta}}),\n\\] it can be shown that \\[\n(n-p)\\hat{\\sigma}^2\\sim \\sigma^2\\chi_{n-p}^2,\n\\] but we will not prove this result in this module. We do show in the Chapter appendix, however, that \\(\\hat{\\sigma}^2\\) is independent of \\(\\boldsymbol{\\hat{\\beta}}\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Parameter estimation for linear models</span>"
    ]
  },
  {
    "objectID": "parameterestimation.html#sec-beta_comps",
    "href": "parameterestimation.html#sec-beta_comps",
    "title": "2  Parameter estimation for linear models",
    "section": "2.5 Confidence intervals for components of \\(\\boldsymbol{\\beta}\\)",
    "text": "2.5 Confidence intervals for components of \\(\\boldsymbol{\\beta}\\)\nWe have just shown that \\[\n\\boldsymbol{\\hat{\\beta}}\\sim N_p(\\boldsymbol{\\beta},\\sigma^2(X^TX)^{-1}).\n\\]\nFrom the marginal distributions property of the multivariate normal distribution, if we let \\(\\hat{\\beta}_i\\) be the \\(i\\)th element of \\(\\boldsymbol{\\hat{\\beta}}\\), then \\[\n\\hat{\\beta}_i\\sim N(\\beta_i,\\sigma^2g_{ii})\n\\] where \\(\\beta_i\\) is the \\(i\\)th element of \\(\\boldsymbol{\\beta}\\) and \\(g_{ii}\\) is the \\(i\\)th diagonal element of \\(G=(X^TX)^{-1}\\).\nHence \\[\n\\frac{\\hat{\\beta}_i-\\beta_i}{\\sigma\\sqrt{g_{ii}}}\\sim N(0,1),\\]\nbut we cannot construct confidence intervals using this, as we do not know \\(\\sigma\\); instead we need to estimate it by \\(\\hat{\\sigma}\\) and use some distribution theory.\nIt can be shown that \\(\\boldsymbol{\\hat{\\beta}}\\) is independent of \\(S_r=(n-p)\\hat{\\sigma}^2\\sim \\sigma^2\\chi_{n-p}^2\\). Standard distributional theory tells us that if \\(X \\sim N(0,1)\\), \\(Y\\sim \\chi^2_{\\nu}\\) and \\(X\\) and \\(Y\\) are independent then \\(\\frac{X}{\\sqrt{Y/\\nu}} \\sim t_{\\nu}\\). It follows that if we use \\(\\hat{\\sigma}\\) instead of \\(\\sigma\\) to standardise \\(\\hat{\\beta}_i\\) we get \\[\n\\frac{\\hat{\\beta}_i-\\beta_i}{\\hat{\\sigma}\\sqrt{g_{ii}}}\\sim t_{n-p}.\n\\] From this we immediately derive a \\(100(1-\\alpha)\\%\\) confidence interval. Let \\(t_{m,\\alpha}\\) denote the upper \\(100\\alpha \\%\\) point of the \\(t_m\\) distribution. Then \\[\\begin{eqnarray*}\n1-\\alpha &=& P\\left(-t_{n-p,1-\\alpha/2}\\leq\n\\frac{\\hat{\\beta}_i-\\beta_i}{\\hat{\\sigma}\\sqrt{g_{ii}}} \\leq\nt_{n-p,1-\\alpha/2} \\right) \\\\ &=&\nP(-t_{n-p,1-\\alpha/2}\\hat{\\sigma}\\sqrt{g_{ii}} \\leq\n\\hat{\\beta}_i-\\beta_i \\leq t_{n-p,1-\\alpha/2}\n\\hat{\\sigma}\\sqrt{g_{ii}} ) \\\\ &=&\nP(\\hat{\\beta}_i-t_{n-p,1-\\alpha/2}\\hat{\\sigma}\\sqrt{g_{ii}} \\leq\n\\beta_i \\leq \\hat{\\beta}_i+t_{n-p,1-\\alpha/2}\n\\hat{\\sigma}\\sqrt{g_{ii}}  )\n\\end{eqnarray*}\\] Therefore we have the interval \\[\n\\hat{\\beta}_i\\pm t_{n-p,1-\\alpha/2} \\hat{\\sigma}\\sqrt{g_{ii}}\n\\] for \\(\\beta_i\\). This is the two-sided interval. One-sided intervals are also easily constructed. We could also similarly devise a confidence interval for any linear function of the \\(\\beta_i\\)’s.\nIt should be noted that these intervals are for individual parameters. The simultaneous confidence region for two parameters \\(\\beta_j\\) and \\(\\beta_k\\) is not a rectangle formed from individual confidence intervals: it is an ellipse where the orientation of the axes is related to the correlation of the the estimates of \\(\\beta_j\\) and \\(\\beta_k\\). Hence, although the corresponding estimates \\(b_{j0}\\) may not look unlikely for \\(\\beta_j\\) and \\(b_{k0}\\) not unlikely for \\(\\beta_k\\), the point \\((b_{j0},b_{k0})\\) may be quite unlikely for \\((\\beta_j, \\beta_k)\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Parameter estimation for linear models</span>"
    ]
  },
  {
    "objectID": "parameterestimation.html#sec-R2",
    "href": "parameterestimation.html#sec-R2",
    "title": "2  Parameter estimation for linear models",
    "section": "2.6 Model fit: coefficient of determination \\(R^2\\)",
    "text": "2.6 Model fit: coefficient of determination \\(R^2\\)\nWhen we are fitting a linear model, the estimate \\(\\boldsymbol{\\hat{\\beta}}\\) minimizes the sum of squares, and the residual sum of squares \\(S_r\\) (Section 2.1) can be thought of as a measure of fit. A model that achieves a lower residual sum of squares could be considered as giving a better fit to the data, so we could consider using \\(S_r\\) directly as a measure of model fit.\nThere are two drawbacks to doing this. The first is that \\(S_r\\) depends on the scale of the observations: a model fitted to data in which the response variable is measured in hundreds of some unit could have a larger \\(S_r\\) than a model fitted to data in which the response variable is measured in single units, even though it has a much better fit. The second is that we might prefer a measure that increases as fit improves (whereas \\(S_r\\) decreases). Based on the first drawback, it would make sense to relate \\(S_r\\) to the total variation in the response so that the scale is taken into account, and considering the second as well a better measure of model fit is \\[\nR^2=\\frac{S_{yy}-S_r}{S_{yy}}.\n\\] Here \\(S_{yy}\\) (also written \\(SS_{Total}\\)) is the total sum of squares, defined as \\((\\boldsymbol{y}-\\boldsymbol{\\bar y})^T(\\boldsymbol{y}-\\boldsymbol{\\bar y})=\\boldsymbol{y}^T\\boldsymbol{y}-n\\bar{y}^2\\), which can be thought of as the sum of the squares of residuals when fitting a model which only has a constant term. This value \\(R^2\\) is sometimes called the coefficient of determination and can be thought of the proportion of the total sum of squares that the regression model explains. The residual sum of squares \\(S_r\\) is the unexplained part of the total sum of squares of \\(y\\).\n\n\n\n\n\n\nModels without intercepts/constant term\n\n\n\nIf the model does not have an intercept or constant term, e.g.  \\[\nY_i = \\beta x_i + \\varepsilon_i,\n\\] then the comparing \\(S_r\\) and \\(S_{yy}\\) isn’t particularly helpful, as if we were to remove the regressor \\(x_i\\) from the the model, we would not be fitting a model with a constant term only. If a model is fitted in R with no constant term (see chapter 4), then \\(R^2\\) is computed as \\[\n\\frac{\\sum {y_i}^2 - S_r}{\\sum {y_i}^2},\n\\] noting that \\(\\sum {y_i}^2\\) is the residual sum of squares for a model with \\(E(Y_i) = 0\\).\n\n\nNote that in the case of the simple linear regression model, \\(R^2\\) equals \\(r^2\\), the squared sample correlation coefficient between \\(\\boldsymbol{y}\\) and \\(\\boldsymbol{x}\\), so we can see \\(R^2\\) as the generalization of \\(r^2\\) to the more general linear model. \\(R^2\\) is also called the squared multiple correlation coefficient and it always lies between 0 and 1.\nThere is also an ‘Adjusted R-squared’ value. This value takes into account the number of regressor variables used in the model. It is defined as \\[R^2(adj)=1-\\frac{S_r/(n-p)}{SS_{total}/(n-1)}\\]\nYou will see a lot more about model selection later in the course, but it does not make sense to simply pick the model with the largest \\(R^2\\) as this necessarily increases as the number of regressor variables increases. The adjusted R-squared value does not necessarily increase as the number of regressor variables increases, suggesting that using the adjusted R-squared value in model selection is more sensible.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Parameter estimation for linear models</span>"
    ]
  },
  {
    "objectID": "parameterestimation.html#chapter-appendix",
    "href": "parameterestimation.html#chapter-appendix",
    "title": "2  Parameter estimation for linear models",
    "section": "2.7 Chapter appendix",
    "text": "2.7 Chapter appendix\nYou will not be examined on your ability to derive results such as the following. However, you are encouraged to study these sections (and ask questions if you need to!) because understanding this sort of content can you help you understand other methods if you need to learn them independently.\n\n2.7.1 Deriving the least squares estimator\nTo find the least squares estimates we must solve \\[\n\\frac{d (\\boldsymbol{y}-X\\boldsymbol\\beta)^T(\\boldsymbol{y}-X\\boldsymbol\\beta)}{d \\boldsymbol\\beta} =\n\\boldsymbol {0}.\n\\] Now \\[\n(\\boldsymbol{y}-X\\boldsymbol\\beta)^T(\\boldsymbol{y}-X\\boldsymbol\\beta) = \\left(\\boldsymbol{y}^T \\boldsymbol{y}\n-\\boldsymbol\\beta^TX^T\\boldsymbol{y} -\\boldsymbol{y}^TX\\boldsymbol\\beta + \\boldsymbol\\beta^T(X^T X)\\boldsymbol\\beta\\right)\n\\] and so \\[\n\\frac{d (\\boldsymbol{y}-X\\boldsymbol\\beta)^T(\\boldsymbol{y}-X\\boldsymbol\\beta)}{d \\boldsymbol\\beta}=-2X^T\\boldsymbol{y} + 2(X^TX)\\boldsymbol\\beta.\n\\]\nWe equate the above to \\(\\boldsymbol{0}\\) for \\(\\boldsymbol\\beta=\\hat{\\boldsymbol\\beta}\\), the least squares estimator, giving \\[\n\\boldsymbol{0}=-2X^T\\boldsymbol{y} +\n2(X^TX)\\hat{\\boldsymbol\\beta},\n\\] (sometimes referred to as the normal equation) which gives us the result \\[\n\\hat{\\boldsymbol\\beta}=(X^TX)^{-1}X^T\\boldsymbol{y}\n\\]\n\n\n2.7.2 Bias of the maximum likelihood estimator\nWe define \\[\n\\boldsymbol{e}:=\\boldsymbol{Y}-X\\boldsymbol{\\hat{\\beta}}=\\boldsymbol{Y}-X(X^TX)^{-1}X^T\\boldsymbol{Y},\n\\]\nwhich we think of as a vector of random residuals. If we define \\[M:=I_n-X(X^TX)^{-1}X^T,\\] we can re-write this as \\(\\boldsymbol{e}=M\\boldsymbol{Y}\\).\nNote that \\(MX = \\boldsymbol{0}\\) and \\(M^2=M\\), i.e. \\(M\\) is an idempotent matrix.\nWe have \\[\nE(\\boldsymbol{e})=ME(\\boldsymbol{Y})=MX\\boldsymbol{\\beta}=\\boldsymbol{0},\n\\] since \\(MX=\\boldsymbol{0}\\).\nWe also have \\[\nVar(\\boldsymbol{e})=MVar(\\boldsymbol{Y})M^T=M\\sigma^2I_nM=\\sigma^2M^2=\\sigma^2M.\n\\]\nThus \\(M\\) is also related to the variance-covariance matrix of the residuals. The variance of an individual residual is \\(\\sigma^2\\) times the corresponding diagonal element of \\(M\\).\nEvery idempotent matrix except the identity is singular (non-invertible), and its rank is equal to its trace. We have (using the result that \\(\\mathrm{tr}(AB)=\\mathrm{tr}(BA)\\)) \\[\n\\mathrm{tr}(M)=\\mathrm{tr}(I_n-X(X^TX)^{-1}X^T)=n-\\mathrm{tr}((X^TX)^{-1}X^TX) =n-\\mathrm{tr}(I_p)=n-p.\n\\] and so the rank of \\(M\\) is also \\(n-p\\).\nSince \\(E(e_i)=0\\) \\((i=1,\\ldots,n)\\), we can observe that the diagonal elements of the covariance matrix \\(Var(\\boldsymbol{e})\\) are \\(E(e_1^2),\\ldots,E(e_n^2)\\) and so we have that \\(E(e_i^2)=\\sigma^2m_{ii}\\), where \\(m_{11},\\ldots,m_{nn}\\) are diagonal elements of \\(M\\), writing \\(M=(m_{ij})\\). Then we can see \\[\nE\\left(\\sum_{i=1}^ne_i^2\\right)=\\sum_{i=1}^n E(e_i^2) = \\sigma^2\n\\sum_{i=1}^n m_{ii} = \\sigma^2 tr(M) = \\sigma^2(n-p)\n\\] This shows us that the maximum likelihood estimator is biased: \\[\nE(\\hat{\\sigma}^2_{MLE}) = E\\left(\\frac{\\sum_{i=1}^ne_i^2}{n}\\right)=\\sigma^2\\frac{(n-p)}{n}.\n\\]\n\n\n2.7.3 Independence between \\(\\hat{\\sigma}^2\\) and \\(\\boldsymbol{\\hat{\\beta}}\\)\nUsing the definitions of \\(\\boldsymbol{e}\\) and \\(M\\) from above\n\\[\n\\boldsymbol{e}:=\\boldsymbol{Y}-X\\boldsymbol{\\hat{\\beta}} = M\\boldsymbol{Y}\n\\] we have \\[\n\\hat{\\sigma}^2 = \\frac{1}{n-p}\\boldsymbol{e}^T\\boldsymbol{e}\n\\] As \\(\\hat{\\sigma}^2\\) is a transformation of the random vector \\(\\boldsymbol{e}\\), we just need to show that is \\(\\boldsymbol{\\hat{\\beta}}\\) is independent of \\(\\boldsymbol{e}\\).\nBoth \\(\\boldsymbol{e}\\) and \\(\\boldsymbol{\\hat{\\beta}}\\) are linear transformations of the multivariate normal random vector \\(\\boldsymbol{Y}\\), and their joint distribution is multivariate normal.\nWe have \\[\\begin{align}\nCov(\\boldsymbol{e},\\boldsymbol{\\hat{\\beta}}) &= Cov(M\\boldsymbol{Y}, (X^TX)^{-1}X^T\\boldsymbol{Y}) \\\\\n&= MVar(\\boldsymbol{Y})X(X^TX)^{-1}\\\\\n&= M\\sigma^2 I_n X(X^TX)^{-1} \\\\\n&= \\sigma^2MX(X^TX)^{-1}\\\\\n&= \\boldsymbol{0},\n\\end{align}\\] as \\(MX = \\boldsymbol{0}\\). For multivariate normal random vectors, zero covariance implies independence.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Parameter estimation for linear models</span>"
    ]
  },
  {
    "objectID": "qualitative.html",
    "href": "qualitative.html",
    "title": "3  Qualitative independent variables",
    "section": "",
    "text": "3.1 Example: cancer survival data\nThe file cancer.csv has data from patients with advanced cancers of the stomach, bronchus, colon, ovary or breast were treated with ascorbate. We suppose that this aim is to determine if patient survival differs with respect to the organ affected by the cancer. The survival time in days was recorded for each patient.\nlibrary(tidyverse)\ncancer &lt;- read_csv(\"https://oakleyj.github.io/exampledata/cancer.csv\")\nhead(cancer)\n\n# A tibble: 6 × 2\n  survival organ  \n     &lt;dbl&gt; &lt;chr&gt;  \n1      124 Stomach\n2       42 Stomach\n3       25 Stomach\n4       45 Stomach\n5      412 Stomach\n6       51 Stomach\nWe can use a box plot to compare the survival times for each type (organ) of cancer patient:\nggplot(cancer, aes(x = organ, y = survival)) +\n  geom_boxplot()\nWe suppose that the survival time is the dependent variable of interest, which we can treat as continuous as before. But our independent variable, organ, is qualitative. Can we still use a linear model to analyse these data?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Qualitative independent variables</span>"
    ]
  },
  {
    "objectID": "qualitative.html#a-linear-model-for-the-cancer-data",
    "href": "qualitative.html#a-linear-model-for-the-cancer-data",
    "title": "3  Qualitative independent variables",
    "section": "3.2 A linear model for the cancer data",
    "text": "3.2 A linear model for the cancer data\nClearly, it would not make sense to write a model such as \\[\nY_i=\\beta_0 + \\beta_1 x_i +\\varepsilon_i,\n\\] where \\(Y_i\\) is the survival time of the \\(i\\)-th patient and \\(x_i\\) is the cancer type of the \\(i\\)-th patient, because the independent variable (cancer type) is a categorical variable (i.e. it doesn’t make sense to say “survival time \\(= 50 +3\\times\\) stomach cancer”). We could write our model as \\[\nY_i= \\beta_1 x_{i,1} + \\beta_2 x_{i,2}+\\beta_3 x_{i,3}+\\beta_4 x_{i,4}+\\beta_5 x_{i,5} +\\varepsilon_i,\n\\] where \\(x_{i,j} = 1\\) if patient had cancer type \\(j\\) (\\(j=1\\) for Breast, \\(j=2\\) for Bronchus and so on) and \\(x_{i,j} = 0\\) otherwise. We say that \\(x_{i,j}\\) is a dummy variable. The five dummy variables here ensure that the correct \\(\\beta\\) term is selected for each observation. This notation can be a little cumbersome, so we typically write these sorts of models in a different way.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Qualitative independent variables</span>"
    ]
  },
  {
    "objectID": "qualitative.html#notation-for-qualitative-independent-variables",
    "href": "qualitative.html#notation-for-qualitative-independent-variables",
    "title": "3  Qualitative independent variables",
    "section": "3.3 Notation for qualitative independent variables",
    "text": "3.3 Notation for qualitative independent variables\nEach observation is associated with a particular group, where the group is specified by the value (level) of the qualitative independent variable. The organ variable can be one of five possibilities, so we think the data as being organised in five groups.\nWe write \\(Y_{ij}\\) as the \\(j\\)-th observation within group \\(i\\). Let \\(g\\) be the total number of groups (with \\(g=5\\) in the cancer data). We then have \\(i=1,\\ldots,g\\).\n\n\n\n\n\n\nNote\n\n\n\nWhereas a quantitative independent variable is typically represented by its own letter (e.g. \\(x_i\\)), we typically represent a qualitative independent variable using an additional subscript on the dependent variable.\n\n\nWithin group \\(i\\) we let \\(n_i\\) be the total number of observations, so that we have \\(j=1,\\ldots,n_i\\). In the cancer data, if we call the group of patients with breast cancer group 1, there are 11 patients in this group, so \\(n_1=11\\), and the 11 survival times for patients with stomach cancer are denoted \\(Y_{1,1},Y_{1,2},\\ldots,Y_{1,11}\\). As usual, we think of \\(Y_{ij}\\) as a random variable, and \\(y_{ij}\\) as the observed value of that random variable.\nWe let \\(n\\) denote the total number of observations, so that \\(n=\\sum_{i=1}^g n_i\\).\nNow let \\(\\mu_i\\) denote the population mean of the dependent variable in group \\(i\\). We can now write a model for the data as follows: \\[\nY_{ij}=\\mu_i + \\varepsilon_{ij},\n\\] for \\(i=1,\\ldots,g\\), \\(j=1,\\ldots,n_i\\) and \\(\\varepsilon_{ij}\\sim N(0,\\sigma^2)\\).\n\n\n\n\n\n\nNote\n\n\n\nAnalysis of data using this model is sometimes referred to as one-way analysis of variance (ANOVA), and we will refer to the above model as the one-way ANOVA model.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Qualitative independent variables</span>"
    ]
  },
  {
    "objectID": "qualitative.html#the-one-way-anova-model-in-matrix-form",
    "href": "qualitative.html#the-one-way-anova-model-in-matrix-form",
    "title": "3  Qualitative independent variables",
    "section": "3.4 The one-way ANOVA model in matrix form",
    "text": "3.4 The one-way ANOVA model in matrix form\nThe model \\[ Y_{ij}=\\mu_i + \\varepsilon_{ij},\\] for \\(i=1,\\ldots,g\\), \\(j=1,\\ldots,n_i\\) is written in matrix form as \\[\n\\mathbf{Y} = X\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon},\n\\] with \\[\n\\mathbf{Y}=\\left(\\begin{array}{c}Y_{1,1} \\\\ \\vdots \\\\ Y_{1,n_1} \\\\ Y_{2,1}\\\\\n\\vdots \\\\ Y_{2,n_2}  \\\\ \\vdots  \\\\ Y_{g,1} \\\\\n\\vdots\n\\\\ Y_{g,n_g}\\end{array}\\right),\\quad\nX=\\left(\\begin{array}{ccccc}1& 0 & 0  & \\ldots & 0 \\\\ \\vdots & \\vdots & \\vdots & & \\vdots  \\\\\n1& 0 & 0 & \\ldots & 0 \\\\ 0& 1 & 0  & \\ldots & 0 \\\\ \\vdots & \\vdots & \\vdots & & \\vdots  \\\\\n0& 1 & 0 & \\ldots & 0 \\\\ \\vdots & \\vdots & \\vdots & & \\vdots   \\\\ 0& 0 & 0  & \\ldots & 1 \\\\ \\vdots & \\vdots & \\vdots & & \\vdots  \\\\\n0& 0 & 0 & \\ldots & 1\n\\end{array}\\right),\\quad\n\\boldsymbol{\\beta}=\\left(\\begin{array}{c}\\mu_1 \\\\ \\vdots \\\\\n\\mu_g\\end{array}\\right),\\quad\n\\boldsymbol{\\varepsilon}=\\left(\\begin{array}{c}\\varepsilon_{1,1} \\\\ \\vdots \\\\ \\varepsilon_{1,n_1} \\\\ \\varepsilon_{2,1}\\\\\n\\vdots \\\\ \\varepsilon_{2,n_2}  \\\\ \\vdots  \\\\ \\varepsilon_{g,1} \\\\\n\\vdots\n\\\\ \\varepsilon_{g,n_g}\\end{array}\\right)\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Qualitative independent variables</span>"
    ]
  },
  {
    "objectID": "qualitative.html#least-squares-estimates-for-the-one-way-anova-model",
    "href": "qualitative.html#least-squares-estimates-for-the-one-way-anova-model",
    "title": "3  Qualitative independent variables",
    "section": "3.5 Least squares estimates for the one-way ANOVA model",
    "text": "3.5 Least squares estimates for the one-way ANOVA model\nNow that we have written the model in matrix notation, we can immediately obtain least squares estimates for the unknown group means \\(\\mu_1,\\ldots,\\mu_g\\), using the formula \\(\\hat{\\boldsymbol{\\beta}}=(X^TX)^{-1}X^T\\mathbf{y}\\). Since \\[\n(X^TX)^{-1}=\\left(\\begin{array}{cccc}n_1 & 0 &  \\ldots & 0 \\\\ 0 &\nn_2  & \\ldots & 0 \\\\ \\vdots & & \\ddots &   \\vdots \\\\ 0 & 0 & \\ldots\n& n_g \\end{array} \\right)^{-1},\\quad\nX^T\\mathbf{y}=\\left(\\begin{array}{c} \\sum_{j=1}^{n_1} y_{1,j} \\\\ \\vdots \\\\\n\\sum_{j=1}^{n_g} y_{g,j} \\end{array}\\right)\n\\] we have \\[\n\\hat{\\boldsymbol{\\beta}}=\\left(\\begin{array}{c}\\hat{\\mu}_1 \\\\ \\vdots \\\\ \\hat{\\mu}_g \\end{array}\\right) = \\left(\\begin{array}{c} \\frac{1}{n_1} \\sum_{j=1}^{n_1} y_{1,j} \\\\ \\vdots \\\\\n\\frac{1}{n_g} \\sum_{j=1}^{n_g} y_{g,j} \\end{array}\\right)\n\\] This result is intuitive. For example, in group 1 we have \\(n_1\\) observations \\(y_{1,1},\\ldots,y_{1,n_1}\\), all with expected value \\(\\mu_1\\). The obvious estimate for \\(\\mu_1\\) is the sample mean \\(\\frac{1}{n_1} \\sum_{j=1}^{n_1} y_{1,j}\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Qualitative independent variables</span>"
    ]
  },
  {
    "objectID": "qualitative.html#an-alternative-parameterisation",
    "href": "qualitative.html#an-alternative-parameterisation",
    "title": "3  Qualitative independent variables",
    "section": "3.6 An alternative parameterisation",
    "text": "3.6 An alternative parameterisation\nAn alternative way of writing the one-way ANOVA model is as follows: \\[\nY_{i,j}=\\mu + \\tau_i + \\varepsilon_{i,j},\n\\] for \\(i=1,\\ldots,g\\), \\(j=1,\\ldots,n_i\\) and with \\(\\varepsilon_{i,j}\\sim N(0,\\sigma^2)\\).\nThis parameterisation is usually more convenient when it comes to investigating differences between groups, as we shall see later.\nThe intention of this parametrisation could be to think of \\(\\mu\\) as the grand mean, and \\(\\tau_i\\) as the difference between the mean of group \\(i\\) and the grand mean \\(\\mu\\). However, this model is over-parametrised.\nIn matrix notation, we would write this as \\[\n\\mathbf{Y} = X\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon},\n\\] with \\[\n\\mathbf{Y}=\\left(\\begin{array}{c}Y_{1,1} \\\\ \\vdots \\\\ Y_{1,n_1} \\\\ Y_{2,1}\\\\\n\\vdots \\\\ Y_{2,n_2} \\\\ \\vdots \\\\ Y_{g,1} \\\\\n\\vdots\n\\\\ Y_{g,n_g}\\end{array}\\right),\\quad\nX=\\left(\\begin{array}{cccccc}1&1& 0 & 0  & \\ldots & 0 \\\\ \\vdots & \\vdots & \\vdots & \\vdots & & \\vdots  \\\\\n1& 1& 0 & 0 & \\ldots & 0 \\\\ 1 & 0& 1 & 0  & \\ldots & 0 \\\\ \\vdots &  \\vdots & \\vdots & \\vdots & & \\vdots  \\\\\n1 & 0& 1 & 0 & \\ldots & 0 \\\\ \\vdots & \\vdots & \\vdots & \\vdots & & \\vdots  \\\\ 1 & 0& 0 & 0  & \\ldots & 1 \\\\ \\vdots & \\vdots & \\vdots & \\vdots & & \\vdots  \\\\\n1 & 0& 0 & 0 & \\ldots & 1\n\\end{array}\\right),\\quad\n\\boldsymbol{\\beta}=\\left(\\begin{array}{c}\\mu \\\\\\tau_1 \\\\ \\vdots \\\\\n\\tau_g\\end{array}\\right),\\quad\n\\boldsymbol{\\varepsilon}=\\left(\\begin{array}{c}\\varepsilon{1,1} \\\\ \\vdots \\\\ \\varepsilon{1,n_1} \\\\ \\varepsilon{2,1}\\\\\n\\vdots \\\\ \\varepsilon{2,n_2}  \\\\ \\vdots  \\\\ \\varepsilon{g,1} \\\\\n\\vdots\n\\\\ \\varepsilon{g,n_g}\\end{array}\\right).\n\\] For this particular design matrix \\(X\\), we find that \\[\nX^TX=\\left(\\begin{array}{ccccc}n & n_1 & n_2 & \\cdots & n_g\n\\\\ n_1 & n_1 & 0  & \\cdots & 0\\\\ n_2 & 0 & n_2 & \\cdots & 0 \\\\ \\vdots & \\vdots &\\vdots & \\ddots & \\vdots \\\\ n_g & 0 & 0 & \\cdots &\nn_g\n\\end{array}\\right).\n\\] The matrix \\(X^TX\\) cannot be inverted, as \\(\\det(X^TX)=0\\) (the first column is the sum of columns 2 to \\(g+1\\), noting that \\(\\sum_{i=1}^g n_i=n\\)). Hence it is not possible to obtain least squares parameter estimates for this model, as we cannot evaluate the expression \\(\\boldsymbol{\\beta}=(X^TX)^{-1}X^T\\mathbf{y}\\). Intuitively, this makes sense as we are trying to estimate \\(g+1\\) parameters representing group means (\\(\\mu\\) and \\(\\tau_1,\\ldots,\\tau_g\\)) with data from only \\(g\\) groups.\nThe solution is to apply constraints to the parameters. One possibility is to state that \\(\\tau_1=0\\), so that the model can be written as \\[\nY_{i,j}=\\left\\{\\begin{array}{ll}\\mu + \\varepsilon_{1,j} & \\mbox{$i=1$,\n$j=1,\\ldots n_1$}\\\\ \\mu + \\tau_i + \\varepsilon_{i,j} & \\mbox{$i=2,\\ldots,g$,\n$j=1,\\ldots n_i$} \\end{array}\\right.\n\\] For this parametrisation, \\(\\mu\\) is interpreted as the (population) mean for group 1, and \\(\\tau_i\\) gives the difference in means between group \\(i\\) and group 1, for \\(i\\neq 1\\).\nThis model is written in matrix form as \\[\n\\mathbf{Y} = X\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon},\n\\] with \\[\n\\mathbf{Y}=\\left(\\begin{array}{c}Y_{1,1} \\\\ \\vdots \\\\ Y_{1,n_1} \\\\ Y_{2,1}\\\\\n\\vdots \\\\ Y_{2,n_2}  \\\\ \\vdots  \\\\ Y_{g,1} \\\\\n\\vdots\n\\\\ Y_{g,n_g}\\end{array}\\right),\\quad\nX=\\left(\\begin{array}{ccccc}1& 0 & 0  & \\ldots & 0 \\\\ \\vdots & \\vdots & \\vdots & & \\vdots  \\\\\n1& 0 & 0 & \\ldots & 0 \\\\ 1& 1 & 0  & \\ldots & 0 \\\\ \\vdots & \\vdots & \\vdots & & \\vdots  \\\\\n1& 1 & 0 & \\ldots & 0\n\\\\ \\vdots & \\vdots & \\vdots & & \\vdots   \\\\ 1& 0 & 0  & \\ldots & 1 \\\\ \\vdots & \\vdots & \\vdots & & \\vdots  \\\\\n1& 0 & 0 & \\ldots & 1\n\\end{array}\\right),\\quad\n\\boldsymbol{\\beta}=\\left(\\begin{array}{c}\\mu \\\\ \\tau_2 \\\\ \\vdots \\\\\n\\tau_g\\end{array}\\right),\\quad\n\\boldsymbol{\\varepsilon}=\\left(\\begin{array}{c}\\varepsilon{1,1} \\\\ \\vdots \\\\ \\varepsilon{1,n_1} \\\\ \\varepsilon{2,1}\\\\\n\\vdots \\\\ \\varepsilon{2,n_2} \\\\ \\vdots  \\\\ \\varepsilon{g,1} \\\\\n\\vdots\n\\\\ \\varepsilon{g,n_g}\\end{array}\\right).\n\\] For this design matrix we find that \\[\n\\hat{\\boldsymbol{\\beta}}=\\left(\\begin{array}{c}\\frac{1}{n_1}\\sum_{j=1}^{n_1} y_{1,j} \\\\\n\\frac{1}{n_2}\\sum_{j=1}^{n_2} y_{2,j} -\n\\frac{1}{n_1}\\sum_{j=1}^{n_1} y_{1,j} \\\\ \\vdots \\\\\n\\frac{1}{n_g}\\sum_{j=1}^{n_g} y_{g,j} -\n\\frac{1}{n_1}\\sum_{j=1}^{n_1} y_{1,j}\n\\end{array}\\right),\n\\] (details omitted), hence \\(\\hat{\\mu}\\) is the sample mean of the observations in group 1, and \\(\\hat{\\tau}_i\\) is the difference between the sample means of groups \\(i\\) and 1.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Qualitative independent variables</span>"
    ]
  },
  {
    "objectID": "qualitative.html#sec-ancova",
    "href": "qualitative.html#sec-ancova",
    "title": "3  Qualitative independent variables",
    "section": "3.7 ANCOVA and models with interactions",
    "text": "3.7 ANCOVA and models with interactions\nANCOVA (Analysis of Covariance) involves regression modelling where the regression line changes between different groups in the data (with groups defined by the values of a qualitative/factor variable.)\nFor a simple example, we’ll use the built in dataset mtcars (see ?mtcars for details)\n\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\nWe’ll be using the column am which describes transmission type. To make it more readable, we’ll do\n\nmtcars2 &lt;- mtcars %&gt;%\n  mutate(transmission = factor(am, labels = c(\"automatic\",  \"manual\")))\n\nAn ANCOVA model for this data would be\n\\[\nY_{ij} = \\mu + \\tau_i + \\beta_i x_{ij} + \\varepsilon_{ij}\n\\] where \\(Y_{ij}\\) is the fuel economy mpg of the \\(j\\)th car in group \\(i\\), and \\(x_{ij}\\) is the corresponding weight wt, for \\(i=1,2\\). Group \\(i=1\\) corresponds to automatic cars, and \\(i=2\\) corresponds to manual cars. We include the constraint \\(\\tau_1 = 0\\). So the regression line for automatic cars would be \\[\ny = \\mu + \\beta_1x,\n\\] and for manual cars would be \\[\ny = \\mu +\\tau_2+ \\beta_2x.\n\\] The effect of weight on fuel economy (the ‘beta’ parameter) changes depending on whether the car is a manual or automatic: there is an interaction between transmission and weight in affecting the fuel economy.\n\n\n\n\n\n\nExercise\n\n\n\n\nExercise 3.1 Recall the respiration data:\n\nlibrary(tidyverse)\nrespiration &lt;- read_table(\"https://oakleyj.github.io/exampledata/resp_data.txt\")\nrespiration\n\n# A tibble: 305 × 8\n     vol exercise pulse pollution asbestos  home smoke asthma\n   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1  117.     15.8 17.5     11.9          1     1     1      0\n 2  148.     23.5 28.4      1.88         1     3     1      1\n 3  214.     13.8 15.0      2.38         1     1     0      0\n 4  162.     15.6 15.7      6.34         1     3     0      1\n 5  352.     26.6 19.7      1.63         0     4     0      0\n 6  304.     23.2 14.2      2.72         0     4     0      0\n 7  157.     29.0 33.7      4.03         0     1     1      0\n 8  110.     16.6 14.7      1.24         1     2     1      0\n 9  218.     15.1  9.16     1.14         0     2     0      0\n10  246.     16.9 21.9      0.731        0     4     0      0\n# ℹ 295 more rows\n\n\nThe meanings of the column headings are:\n\nvol - volume of air expelled whilst breathing out\nexer - number of hours exercise per month\npulse- scaled resting pulse rate\npoll - lifetime exposure to air pollution\nasbestos - has the individual been exposed to asbestos (1=exposed, 0=unexposed)\nhome - place of residence (1=England, 2=Scotland, 3=Wales, 4=Northern Ireland)\nsmoke - have they ever smoked (1=yes, 0=no)\nasthma - do they have asthma (1=yes, 0=no)\n\nTry writing down models with appropriate notation to represent the following, with vol the dependent variable in each case. Specify parameter constraints as needed.\n\nThere is one independent variable only: smoke .\nThere are two independent variables only: smoke and exer. There is a linear relationship between exer and vol, but the relationship changes depending on whether the individual is a smoker or not.\nThere are three independent variables only: smoke, exer and asthma. There is a linear relationship between exer and vol, but the relationship changes depending on the combination of the individual’s smoking and asthma status. Hint: you need one subscript per qualitative independent variable, and one further subscript to distinguish between individuals in the same group, with group specified by the combination of qualitative independent variables.\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nOur model is \\[\nY_{ij} = \\mu + \\tau_i + \\varepsilon_{ij},\n\\] where\n\n\n\\(Y_{ij}\\) is the observed volume of air expelled for individual \\(j\\) in group \\(i\\), with group defined by smoking status;\n\\(i=1\\) if the individual is a smoker, \\(i=2\\) otherwise,\nwe have \\(j=1,\\ldots,n_i\\) (there may be different numbers of individuals per group);\nwe constrain \\(\\tau_1=0\\).\n\n\nOur model is \\[\nY_{ij} = \\mu + \\tau_i + \\beta_ix_{ij} + \\varepsilon_{ij},\n\\]\n\n\n\\(Y_{ij}\\) is the observed volume of air expelled for individual \\(j\\) in group \\(i\\), with group defined by smoking status;\n\\(x_{ij}\\) the corresponding number of hours of exercise per month;\n\\(i=1\\) if the individual is a smoker, \\(i=2\\) otherwise,\nwe have \\(j=1,\\ldots,n_i\\);\nwe constrain \\(\\tau_1=0\\).\n\n\nWe have two qualitative variables (smoking and asthma), so we use two subscripts (\\(i\\) and \\(j\\)) to define a group. Our model is\n\n\\[\nY_{ijk} = \\mu + \\tau_{ij} + \\beta_{ij}x_{ijk} + \\varepsilon_{ijk},\n\\]\n\n\\(Y_{ijk}\\) is the observed volume of air expelled for individual \\(k\\) in group \\(ij\\), with group defined by the combination of smoking and asthma status;\n\\(x_{ijk}\\) the corresponding number of hours of exercise per month;\n\\(i=1\\) if the individual is a smoker, \\(i=2\\) otherwise,\n\\(j=1\\) if the individual has asthma and \\(j=2\\) otherwise,\nwe have \\(k=1,\\ldots,n_{ij}\\);\nwe constrain \\(\\tau_{11}=0\\).\n\nAn alternative model (for the intercept) is\n\\[\nY_{ijk} = \\mu + \\alpha_i+\\gamma_j+\\tau_{ij} + \\beta_{ij}x_{ijk} + \\varepsilon_{ijk},\n\\] now with constraints \\(\\alpha_1=\\beta_1=\\tau_{1j}=\\tau_{i1}=0\\). If we were to remove the exercise variable from this model, we would be left with the “two-way analysis of variance model”: \\[\nY_{ijk} = \\mu + \\alpha_i+\\gamma_j+\\tau_{ij} + \\varepsilon_{ijk},\n\\] which can be used to investigate whether the effects of two factors (smoking and asthma here) are additive or interact, as \\(\\tau_{ij}\\) represents an interaction effect here.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Qualitative independent variables</span>"
    ]
  },
  {
    "objectID": "fitting-in-R.html",
    "href": "fitting-in-R.html",
    "title": "4  Fitting a linear model and making predictions in R",
    "section": "",
    "text": "4.1 Fitting a simple linear regression model\nTo illustrate fitting a linear model in R, for convenience we’ll use one of R’s built in data sets: cars\nhead(cars)\n\n  speed dist\n1     4    2\n2     4   10\n3     7    4\n4     7   22\n5     8   16\n6     9   10\nOur dependent variable (dist) is the stopping distance in feet, and the independent variable is (speed) is the speed in miles per hour. The cars are very old (1920s!), but it’s a convenient data set for illustrating the linear modelling syntax.\nThe basic syntax is\nwhere the formula argument corresponds to the equation for \\(E(Y_i)\\) and uses column names from the data frame specified by the data argument. Some formula examples are as follows.\nFor example, to fit the simple linear regression model we do\nlm(dist ~ speed, cars)\n\n\nCall:\nlm(formula = dist ~ speed, data = cars)\n\nCoefficients:\n(Intercept)        speed  \n    -17.579        3.932\nand from the Coefficients output we read off \\(\\hat{\\beta}_0= -17.579\\) and \\(\\hat{\\beta}_1 = 3.932\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Fitting a linear model and making predictions in R</span>"
    ]
  },
  {
    "objectID": "fitting-in-R.html#fitting-a-simple-linear-regression-model",
    "href": "fitting-in-R.html#fitting-a-simple-linear-regression-model",
    "title": "4  Fitting a linear model and making predictions in R",
    "section": "",
    "text": "lm(formula, data )\n\n\n\n\n\n\n\n\nModel\nformula\n\n\n\n\n\\(Y_i = \\beta_0 + \\varepsilon_i\\)\ndist ~ 1\n\n\n\\(Y_i = \\beta_0 + \\beta_1 x_i+ \\varepsilon_i\\)\ndist ~ speed\n\n\n\\(Y_i = \\beta_0 + \\beta_1 x_i+ \\beta_2 x_i^2+\\varepsilon_i\\)\ndist ~ speed + I(speed^2)\n\n\n\\(Y_i = \\beta_0 + \\beta_1 \\log x_i+\\varepsilon_i\\)\ndist ~ log(speed)\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nWhen studying a topic such as this and trying out R functions, sometimes it can help you to understand what is going on if you can obtain the same results for yourself. For example:\n\n# Setup the design matrix X\nX &lt;- matrix(c(rep(1, 50), cars$speed),\n            nrow = 50,\n            ncol = 2)\n# Setup the observation vector\ny &lt;- matrix(cars$dist,\n            nrow = 50,\n            ncol = 1)\n# Compute the least squares estimates and compare with the Coefficients output above\nsolve(t(X) %*% X) %*% t(X) %*% y\n\n           [,1]\n[1,] -17.579095\n[2,]   3.932409",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Fitting a linear model and making predictions in R</span>"
    ]
  },
  {
    "objectID": "fitting-in-R.html#viewing-a-fitted-regression-model",
    "href": "fitting-in-R.html#viewing-a-fitted-regression-model",
    "title": "4  Fitting a linear model and making predictions in R",
    "section": "4.2 Viewing a fitted regression model",
    "text": "4.2 Viewing a fitted regression model\nWe can plot the fitted model and the data with the commands\n\nlibrary(tidyverse)\nggplot(cars, aes(x = speed, y = dist))+\n  geom_point()+\n  geom_smooth(method = \"lm\", formula = \"y ~ x\", se = FALSE)\n\n\n\n\n\n\n\n\nNote that the formula argument corresponds to the fitted model, but with variables x and y rather than speed and dist. We will change the argument se to TRUE when we have learned more about confidence intervals in the next chapter.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Fitting a linear model and making predictions in R</span>"
    ]
  },
  {
    "objectID": "fitting-in-R.html#the-summary-command",
    "href": "fitting-in-R.html#the-summary-command",
    "title": "4  Fitting a linear model and making predictions in R",
    "section": "4.3 The summary() command",
    "text": "4.3 The summary() command\nThe summary command will give us more information about our model fit, if we first assign the fitted model to a variable.\nFor example:\n\nlmCars &lt;- lm(dist ~ speed, cars)\nsummary(lmCars)\n\n\nCall:\nlm(formula = dist ~ speed, data = cars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-29.069  -9.525  -2.272   9.215  43.201 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -17.5791     6.7584  -2.601   0.0123 *  \nspeed         3.9324     0.4155   9.464 1.49e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 15.38 on 48 degrees of freedom\nMultiple R-squared:  0.6511,    Adjusted R-squared:  0.6438 \nF-statistic: 89.57 on 1 and 48 DF,  p-value: 1.49e-12\n\n\nWe interpret the output as follows.\n\nResiduals refer to those residuals we defined previously: \\(e_i = y_i - \\hat{y}_i\\).\nThe Estimate column gives the least squares estimate.\nThe Std. Error column gives the estimated standard error for each least squares estimate.\nThe t value and Pr(&gt;|t|) refer to a hypothesis test that the corresponding model parameter is 0. We’ll discuss this in more detail in a later chapter.\nThe residual standard error is the estimated value \\(\\hat{\\sigma}\\) of \\(\\sigma\\) (standard deviation of the errors, not the variance). Recall that this is computed using the residual sum of squares: \\(\\hat{\\sigma}^2 = \\mathbf{e}^T\\mathbf{e}/(n-p)\\)\nThe Multiple R-squared and Adjusted R-squared both report how ‘useful’ the model is for predicting the dependent variable: they report the proportion of the variation in \\(y_1,\\ldots,y_n\\) that can be explained by variation in the dependent variables. The Adjusted R-squared corrects for the number of independent variables in the model.\nThe F-statistic and p-value also refer to a hypothesis test which we will discuss later.\n\n\n\n\n\n\n\nExercise\n\n\n\n\nExercise 4.1 If you are comfortable with working with matrices and vectors in R, see if you can compute the residual standard error reported in the summary() output above using matrix notation in R.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# Setup the design matrix X\nX &lt;- matrix(c(rep(1, 50), cars$speed),\n            nrow = 50,\n            ncol = 2)\n\n# Setup the observation vector\ny &lt;- matrix(cars$dist,\n            nrow = 50,\n            ncol = 1)\n\n# Compute the least squares estimates \nbetahat &lt;- solve(t(X) %*% X) %*% t(X) %*% y\n\n# Compute the residual vector\ne &lt;- y - X %*% betahat\n\n# Residual standard error is (noting n - p = 50 - 2)\nsqrt(t(e) %*% e / 48)\n\n         [,1]\n[1,] 15.37959",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Fitting a linear model and making predictions in R</span>"
    ]
  },
  {
    "objectID": "fitting-in-R.html#models-with-factor-variables",
    "href": "fitting-in-R.html#models-with-factor-variables",
    "title": "4  Fitting a linear model and making predictions in R",
    "section": "4.4 Models with factor variables",
    "text": "4.4 Models with factor variables\nRecall the cancer example from the previous chapter\n\nlibrary(tidyverse)\ncancer &lt;- read_csv(\"https://oakleyj.github.io/exampledata/cancer.csv\")\nhead(cancer)\n\n# A tibble: 6 × 2\n  survival organ  \n     &lt;dbl&gt; &lt;chr&gt;  \n1      124 Stomach\n2       42 Stomach\n3       25 Stomach\n4       45 Stomach\n5      412 Stomach\n6       51 Stomach\n\n\nWe defined \\(Y_{ij}\\) as the \\(j\\)-th observation (survival time) in group \\(i\\), with group defined by organ (one of stomach, bronchus, colon, ovary or breast)\nWe fit the model \\[ Y_{ij}=\\mu_i + \\varepsilon_{ij},\\] for \\(i=1,\\ldots,g\\), \\(j=1,\\ldots,n_i\\) to the cancer data as follows.\n\nlmCancer &lt;- lm(survival ~ organ - 1, cancer)\n\n(the reason for the - 1 in the formula will become clearer shortly.)\nWe can now use the summary() command to get the parameter estimates\n\nsummary(lmCancer)\n\n\nCall:\nlm(formula = survival ~ organ - 1, data = cancer)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1371.91  -241.75  -111.50    87.19  2412.09 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \norganBreast     1395.9      201.9   6.915 3.77e-09 ***\norganBronchus    211.6      162.4   1.303  0.19764    \norganColon       457.4      162.4   2.817  0.00659 ** \norganOvary       884.3      273.3   3.235  0.00199 ** \norganStomach     286.0      185.7   1.540  0.12887    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 669.5 on 59 degrees of freedom\nMultiple R-squared:  0.5437,    Adjusted R-squared:  0.505 \nF-statistic: 14.06 on 5 and 59 DF,  p-value: 4.766e-09\n\n\nWe have \\(\\hat{\\beta}_1 = 1395.9\\), \\(\\hat{\\beta}_2 = 211.6\\) and so on.\n\n\n\n\n\n\nWarning\n\n\n\nBe careful with datasets where a factor/qualitative variable has been recorded numerically! You will need to make sure R understands that the variable is really a factor and not a quantitative variable.\n\n\nAs an example, suppose the cancer data were coded as 1 = Stomach, 2 = Bronchus, 3 = Colon, 4 = Ovary, 5 = Breast. We’ll set up a variable organ2 to represent this:\n\ncancer2 &lt;- cancer %&gt;%\n  mutate(organ2 = rep(1:5, times = c(13, 17, 17, 6, 11)))\n\nNow we’ll try\n\nlm(survival ~ organ2, data = cancer2)\n\n\nCall:\nlm(formula = survival ~ organ2, data = cancer2)\n\nCoefficients:\n(Intercept)       organ2  \n     -240.3        288.9  \n\n\nWhat model has been fitted, what does the coefficient organ2 represent here, and why doesn’t it make sense?\nWe can get R to interpret the organ2 variable correctly, by specifying it as a factor variable:\n\nlm(survival ~ factor(organ2) - 1, data = cancer2)\n\n\nCall:\nlm(formula = survival ~ factor(organ2) - 1, data = cancer2)\n\nCoefficients:\nfactor(organ2)1  factor(organ2)2  factor(organ2)3  factor(organ2)4  \n          286.0            211.6            457.4            884.3  \nfactor(organ2)5  \n         1395.9  \n\n\n\n4.4.1 An alternative parameterisation\nRecall the alternative parametrisation: \\[\nY_{i,j}=\\mu + \\tau_i + \\varepsilon_{i,j},\n\\] for \\(i=1,\\ldots,g\\), \\(j=1,\\ldots,n_i\\) and with \\(\\varepsilon_{i,j}\\sim N(0,\\sigma^2)\\) and the constraint \\(\\tau_1=0\\).\nThis is actually the default parametrisation in R. If we leave out the -1 from the previous command, we just do\n\nlmCancer &lt;- lm(survival ~ organ, cancer)\n\nand then use the summary() command as before.\n\nsummary(lmCancer)\n\n\nCall:\nlm(formula = survival ~ organ, data = cancer)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1371.91  -241.75  -111.50    87.19  2412.09 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     1395.9      201.9   6.915 3.77e-09 ***\norganBronchus  -1184.3      259.1  -4.571 2.53e-05 ***\norganColon      -938.5      259.1  -3.622 0.000608 ***\norganOvary      -511.6      339.8  -1.506 0.137526    \norganStomach   -1109.9      274.3  -4.046 0.000153 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 669.5 on 59 degrees of freedom\nMultiple R-squared:  0.3037,    Adjusted R-squared:  0.2565 \nF-statistic: 6.433 on 4 and 59 DF,  p-value: 0.0002295\n\n\n(Intercept) refers to \\(\\mu\\). (So we can interpret the -1 term in the formula argument as saying that we do not want an intercept.)\nWe have \\(\\hat{\\mu} = 1395.9\\), \\(\\hat{\\tau}_2 = -1184.3,\\ldots,\\hat{\\tau}_5 = -1109.9\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Fitting a linear model and making predictions in R</span>"
    ]
  },
  {
    "objectID": "fitting-in-R.html#ancova-and-models-with-interactions",
    "href": "fitting-in-R.html#ancova-and-models-with-interactions",
    "title": "4  Fitting a linear model and making predictions in R",
    "section": "4.5 ANCOVA and models with interactions",
    "text": "4.5 ANCOVA and models with interactions\nWe presented an ANCOVA model presented in Section 3.7 for the mtcars data\n\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\nand we will recap the details here. We considered the column am which describes transmission type and relabelled the elements as follows\n\nmtcars2 &lt;- mtcars %&gt;%\n  mutate(transmission = factor(am, labels = c(\"automatic\",  \"manual\")))\n\nAn ANCOVA model for this data would be\n\\[\nY_{ij} = \\mu + \\tau_i + \\beta_i x_{ij} + \\varepsilon_{ij}\n\\] where \\(Y_{ij}\\) is the fuel economy mpg of the \\(j\\)th car in group \\(i\\), and \\(x_{ij}\\) is the corresponding weight wt, for \\(i=1,2\\). Group \\(i=1\\) corresponds to automatic cars, and \\(i=2\\) corresponds to manual cars. We include the constraint \\(\\tau_1 = 0\\).\nWe noted that the effect of weight on fuel economy (the ‘beta’ parameter) changes depending on whether the car is a manual or automatic: there is an interaction between transmission and weight in affecting the fuel economy. Interactions are specified with a * in the formula argument:\n\nlmCars &lt;- lm(mpg ~ wt * transmission, mtcars2)\nsummary(lmCars)\n\n\nCall:\nlm(formula = mpg ~ wt * transmission, data = mtcars2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.6004 -1.5446 -0.5325  0.9012  6.0909 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)            31.4161     3.0201  10.402 4.00e-11 ***\nwt                     -3.7859     0.7856  -4.819 4.55e-05 ***\ntransmissionmanual     14.8784     4.2640   3.489  0.00162 ** \nwt:transmissionmanual  -5.2984     1.4447  -3.667  0.00102 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.591 on 28 degrees of freedom\nMultiple R-squared:  0.833, Adjusted R-squared:  0.8151 \nF-statistic: 46.57 on 3 and 28 DF,  p-value: 5.209e-11\n\n\nFrom this, we read off \\(\\hat{\\mu}=31.4161\\), \\(\\hat{\\beta}_1=-3.7859\\), \\(\\hat{\\tau}_2=14.8784\\), \\(\\hat{\\beta}_2=-3.7859 -5.2984\\).\nWe can plot the fitted model with the commands\n\nggplot(mtcars2, aes(x = wt, y = mpg, colour = transmission)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nUse the plot to check you have interpreted the summary output correctly. The absolute gradient is larger for the manual group, and the manual group has a higher intercept.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Fitting a linear model and making predictions in R</span>"
    ]
  },
  {
    "objectID": "fitting-in-R.html#predictions",
    "href": "fitting-in-R.html#predictions",
    "title": "4  Fitting a linear model and making predictions in R",
    "section": "4.6 Predictions",
    "text": "4.6 Predictions\nGiven a new observation \\(i\\) with regressor variables given by a row vector \\(\\boldsymbol{x}_i^T\\) (similar to a row of the design matrix \\(X\\)), there are two types of interval we may want to construct, a confidence interval for the expected value of the response, \\(\\boldsymbol{x}_i^T\\boldsymbol{\\beta}\\), and a prediction interval for the value of the response itself. We should expect the prediction interval to be wider as the response has the extra variability coming from the error term in the model.\n\n4.6.1 Confidence interval for the expected value of the response\nFor estimation of the mean we use the fact that \\[\\boldsymbol{\\hat{\\beta}} \\sim N_p(\\boldsymbol{\\beta},\\sigma^2(X^TX)^{-1})\\] to yield \\[\\boldsymbol{x}_i^T \\boldsymbol{\\hat{\\beta}} \\sim N(\\boldsymbol{x}_i^T\\boldsymbol{\\beta},\\sigma^2\\boldsymbol{x}_i^T(X^TX)^{-1}\\boldsymbol{x}_i)\\] so that \\[\\frac{\\boldsymbol{x}_i^T\\boldsymbol{\\hat{\\beta}}-\\boldsymbol{x}_i^T\\boldsymbol{\\beta}}{\\sqrt{\\sigma^2\\boldsymbol{x}_i^T(X^TX)^{-1}\\boldsymbol{x}_i}} \\sim N(0,1).\\]\nWe cannot calculate the confidence interval from this relationship because we do not know the value of \\(\\sigma^2\\), and we have to replace it with its estimate \\(\\hat \\sigma^2\\) and use the \\(t\\) distribution rather than \\(N(0,1)\\). Our standardized version of \\(\\boldsymbol{x}_i^T\\boldsymbol{\\hat{\\beta}}\\) becomes \\[T=\\frac{\\boldsymbol{x}_i^T \\boldsymbol{\\hat{\\beta}}-\\boldsymbol{x}_i^T \\boldsymbol{\\beta}}{\\sqrt{\\hat \\sigma^2\\boldsymbol{x}_i^T(X^TX)^{-1}\\boldsymbol{x}_i}}=\\frac{\\frac{\\boldsymbol{x}_i^T \\boldsymbol{\\hat{\\beta}}-\\boldsymbol{x}_i^T \\boldsymbol{\\beta}}{\\sqrt{\\sigma^2\\boldsymbol{x}_i^T(X^TX)^{-1}\\boldsymbol{x}_i}} }{\\sqrt{\\frac{\\hat\\sigma^2}{\\sigma^2}}},\\] and we have \\(T \\sim t_{n-p}\\). Hence a \\(100(1-\\alpha)\\%\\) confidence interval for the mean of the predicted value is given by \\[\\boldsymbol{x}_i^T \\boldsymbol{\\hat{\\beta}}\\pm t_{n-p,1-\\alpha/2}\\sqrt{\\hat \\sigma^2\\boldsymbol{x}_i^T(X^TX)^{-1}\\boldsymbol{x}_i}.\\]\n\n\n4.6.2 Prediction intervals for individual observations\nA similar argument leads to a \\(100(1-\\alpha)\\%\\) prediction interval for the future observation \\(y_i\\). It is given by \\[\\boldsymbol{x_i}^T \\boldsymbol{ \\hat{\\beta}}\\pm t_{n-p,1-\\alpha/2}\\sqrt{\\hat \\sigma^2(1+\\boldsymbol{x}_i^T(X^TX)^{-1}\\boldsymbol{x}_i)}.\\]\nThe prediction interval for an observation is wider than the confidence interval for the expected value of the observations because of the extra uncertainty coming from the error term (\\(\\hat \\sigma^2\\)).\n\n\n4.6.3 Obtaining and plotting the predictions and intervals in R\n\n\n\n\n\n\nNote\n\n\n\nTo get the predictions and either type of interval, we need to set up a new data frame with the column names for the independent variables the same as those in the original data frame used to fit the model.\n\n\nFor the cars simple linear regression model, we’ll obtain predictions and intervals for 30 values of speed evenly spaced between 1 and 30.\n\npredictDf &lt;- data.frame(speed = 1:30)\n\nWe fit the model, assign it to an object, and then use that object in a predict() command. For 95% confidence intervals for the mean response\n\nlmCars &lt;- lm(dist ~ speed, cars)\nciCars &lt;- predict(lmCars, predictDf,\n                           interval = \"confidence\",\n                           level = 0.95)\nhead(ciCars)\n\n         fit        lwr       upr\n1 -13.646686 -26.447265 -0.846107\n2  -9.714277 -21.733068  2.304513\n3  -5.781869 -17.026591  5.462853\n4  -1.849460 -12.329543  8.630624\n5   2.082949  -7.644150 11.810048\n6   6.015358  -2.973341 15.004056\n\n\nThe fit column gives the predictions, and lwr and upr give the confidence interval endpoints.\nTo get prediction intervals instead, we change the interval argument:\n\npiCars &lt;- predict(lmCars, predictDf,\n                           interval = \"prediction\",\n                           level = 0.95)\nhead(piCars)\n\n         fit       lwr      upr\n1 -13.646686 -47.11414 19.82076\n2  -9.714277 -42.89057 23.46202\n3  -5.781869 -38.68565 27.12192\n4  -1.849460 -34.49984 30.80092\n5   2.082949 -30.33359 34.49948\n6   6.015358 -26.18731 38.21803\n\n\n\n\n\n\n\n\nTip\n\n\n\nCheck the R output makes sense! The predicted values (the fit column) should be the same with either interval = \"confidence\" or interval = \"prediction\", but the intervals with the latter argument should be wider.\n\n\nIf we just want to plot confidence intervals for the mean response, we can get this directly with ggplot2: we use the argument se = TRUE in the geom_smooth command:\n\nggplot(cars, aes(x = speed, y = dist))+\n  geom_point()+\n  geom_smooth(method = \"lm\", formula = \"y ~ x\", se = TRUE)\n\n\n\n\n\n\n\n\nPlotting prediction intervals is more work! Recall that we created prediction intervals and stored them in piCars. We do\n\nggplot(cars, aes(x = speed, y = dist)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", formula = y ~ x, se = FALSE) +\n  geom_ribbon(data = data.frame(piCars, speed = 1:30),\n              aes(x = speed, ymin = lwr, ymax = upr, y = fit),\n              alpha = 0.1,\n              fill = \"red\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Fitting a linear model and making predictions in R</span>"
    ]
  },
  {
    "objectID": "hypothesisTesting.html",
    "href": "hypothesisTesting.html",
    "title": "5  Hypothesis testing",
    "section": "",
    "text": "5.1 The linear hypothesis",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "hypothesisTesting.html#the-linear-hypothesis",
    "href": "hypothesisTesting.html#the-linear-hypothesis",
    "title": "5  Hypothesis testing",
    "section": "",
    "text": "5.1.1 A general testing problem\nIf you have studied hypothesis testing for linear models before, e.g. for the simple linear regression model \\[\nY_i = \\beta_0+\\beta_1 + \\varepsilon_i,\n\\] you may be used to tests of the form \\[\nH_0: \\beta_1=0,\n\\] which involve computing a ``\\(T\\)-statistic’’ and comparing with the \\(t_{n-2}\\) distribution.\nHere, we present the most general linear null hypothesis, which takes the form \\[\nH_0: \\quad C\\boldsymbol{\\beta}=\\boldsymbol{c},\n\\] where \\(C\\) is a \\(q\\times p\\) matrix and \\(\\boldsymbol{c}\\) is a \\(q\\times 1\\) vector of known constants. This hypothesis simultaneously asserts specific values for \\(q\\) linear functions of \\(\\boldsymbol{\\beta}\\). Without loss of generality, we can assume that \\(C\\) has rank \\(q\\).\nThe usual alternative is simply \\[H_1: \\quad C\\boldsymbol{\\beta}\\neq\\boldsymbol{c}\\] i.e. that at least one of the \\(q\\) linear functions does not take its hypothesized value. This is a two-sided alternative; one sided alternatives are unlikely to be of interest/appropriate.\n\n\n\n\n\n\nExercise\n\n\n\n\nExercise 5.1 How do we write our hypothesis test for the simple linear regression model in this notation?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFor the test \\(H_0: \\beta_1=0\\), we have \\[\nC = (0\\quad 1), \\quad \\boldsymbol{\\beta} = \\left(\\begin{array}{c}\\beta_0\\\\\\beta_1\\end{array}\\right),\\quad \\boldsymbol{c}=(0).\n\\]\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\nExercise 5.2 For one-way ANOVA models, how do we write the test for equal group means using this notation? Try using using both parameterisations presented in Chapter 3.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIf we write the model as \\[\nY_{ij}=\\mu_i+\\varepsilon_{ij},\n\\] for \\(i=1,\\ldots,g\\), then the usual hypothesis we want to test is \\[\nH_0:\\mu_1=\\mu_2=\\ldots,\\mu_g.\n\\] This can be written as a set of \\(g-1\\) equations: \\[\\begin{align}\n\\mu_1 -\\mu_2 &=0,\\\\\n\\mu_2 -\\mu_3 &=0,\\\\\n\\cdots &\\\\\n\\mu_{g-1} -\\mu_g &=0.\n\\end{align}\\] So in our notation, we have \\[\nC = \\left(\\begin{array}{cccccc}1 & -1 & 0 & \\cdots&0 & 0\\\\\n0 & 1 & -1 & \\cdots &0 & 0\\\\\n\\vdots & \\vdots & \\vdots &  \\vdots & \\vdots & \\vdots  \\\\\n0 & 0 & 0 & \\cdots & 1 & -1\\end{array}\\right), \\quad \\boldsymbol{\\beta} = \\left(\\begin{array}{c}\\mu_1 \\\\ \\vdots \\\\\\mu_g\\end{array}\\right),\\quad \\boldsymbol{c}=\\left(\\begin{array}{c}0 \\\\ \\vdots \\\\0\\end{array}\\right).\n\\] If we write the model as \\[\nY_{ij}=\\mu+\\tau_i+\\varepsilon_{ij},\n\\] for \\(i=1,\\ldots,g\\), with the constraint \\(\\tau_1=0\\), then the usual hypothesis we want to test is \\[\nH_0:\\tau_i=0\\quad \\forall i.\n\\] So in our notation, we have \\[\nC = \\left(\\begin{array}{cccccc}0 & 1 & 0 & \\cdots&0 & 0\\\\\n0 & 0 & 1 & \\cdots &0 & 0\\\\\n\\vdots & \\vdots & \\vdots &  \\vdots & \\vdots & \\vdots  \\\\\n0 & 0 & 0 & \\cdots & 0 & 1\\end{array}\\right), \\quad \\boldsymbol{\\beta} = \\left(\\begin{array}{c}\\mu\\\\\\tau_2 \\\\ \\vdots \\\\\\tau_g\\end{array}\\right),\\quad \\boldsymbol{c}=\\left(\\begin{array}{c}0 \\\\ \\vdots \\\\0\\end{array}\\right).\n\\]\n\n\n\n\n\n5.1.2 Constructing a test\nTo construct a test, we consider the estimator \\(C\\boldsymbol{\\hat{\\beta}}\\). If the null hypothesis is true, we have that \\[\nE(C\\boldsymbol{\\hat{\\beta}})=C \\boldsymbol{\\beta}=\\boldsymbol{c},\n\\] and \\[\n\\mathrm{Var}(C \\boldsymbol{\\hat{\\beta}})=C \\mathrm{Var}(\\boldsymbol{\\hat{\\beta}})C^T=\\sigma^2C(X^TX)^{-1}C^T,\n\\] and so \\[C\\boldsymbol{\\hat{\\beta}}\\sim N_q(\\boldsymbol{c},\\sigma^2C(X^TX)^{-1}C^T).\\] But as \\(C\\boldsymbol{\\hat{\\beta}}\\) is a vector, this isn’t a result that we can easily use directly for a hypothesis test; we normally work with scalar test statistics. However, this result can be used to show that, again if \\(H_0\\) is true then \\[\n(C\\boldsymbol{\\hat{\\beta}}-\\boldsymbol{c})^T\n\\left(C(X^TX)^{-1}C^T\\right)^{-1} (C\\boldsymbol{\\hat{\\beta}}-\\boldsymbol{c})/\\sigma^2 \\sim \\chi^2_q.\n\\] We use some further results from distribution theory: if \\(X\\) and \\(Y\\) are independent random variables with \\(X\\sim \\chi^2_{\\nu_1}\\) and \\(Y\\sim \\chi^2_{\\nu_2}\\) then \\(\\frac{X/\\nu_1}{Y/\\nu_2}\\) has the \\(F_{\\nu_1,\\nu_2}\\) distribution. From the result above, and recalling that \\[\n(n-p)\\hat{\\sigma}^2\\sim \\sigma^2\\chi_{n-p}^2,\n\\] we have the result that, if \\(H_0\\) is true, then \\[\nF:=\\frac{ (C\\boldsymbol{\\hat{\\beta}}-\\boldsymbol{c})^T\n\\left(C(X^TX)^{-1}C^T\\right)^{-1} (C\\boldsymbol{\\hat{\\beta}}-\\boldsymbol{c}) }{\nq\\hat{\\sigma}^2 } \\sim F_{q,n-p}\n\\]\nSo for a \\(100 \\alpha\\%\\) significance test we will reject \\(H_0\\) if the test statistic \\(F\\) exceeds \\(F_{q,n-p,\\alpha}\\), the upper \\(100\\alpha \\%\\) point of the \\(F_{q,n-p}\\) distribution. This is a one-tailed test, even though the original alternative hypothesis was two-sided. It is one-tailed because we are using sums of squares, so that any failure of the null hypothesis leads to a larger value of the test statistic. (For \\(F\\) to be small, we need \\((C\\boldsymbol{\\hat{\\beta}}\\) similar to \\(\\boldsymbol{c}\\); the null hypothesis is that these two vectors are equal))",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "hypothesisTesting.html#equivalence-between-the-t-test-and-an-f-test",
    "href": "hypothesisTesting.html#equivalence-between-the-t-test-and-an-f-test",
    "title": "5  Hypothesis testing",
    "section": "5.2 Equivalence between the \\(t\\)-test and an \\(F\\)-test",
    "text": "5.2 Equivalence between the \\(t\\)-test and an \\(F\\)-test\nAs commented at the beginning of this chapter, you may have used a \\(t\\)-test before for testing whether single coefficients take a particular value (usually 0). This is equivalent to a particular \\(F\\)-test.\nTesting the \\(i\\)th element \\(\\beta_i\\) of \\(\\boldsymbol{\\beta}\\) is a special case of our general hypothesis, where \\(q=1\\) (only 1 hypothesis) and \\(C\\) is a row vector of zeroes except for a single 1 in the \\(i\\)-th position. Thus, we have \\(H_0: \\beta_i=c_i\\). The \\(F\\)-statistic reduces to\n\\[\nF=\\frac{(\\hat{\\beta}_i-c_i)^2}{\\hat \\sigma^2 g_{ii}} \\sim F_{1,n-p},\n\\] where \\(g_{ii}\\) is the \\(i\\)-th diagonal element of \\(G=(X^TX)^{-1}\\).\nPreviously, we explained that \\[\n\\frac{\\hat{\\beta}_i-\\beta_i}{\\hat{\\sigma}\\sqrt{g_{ii}}}\\sim t_{n-p}.\n\\] This could also be used to test the hypothesis \\(H_0: \\beta_i=c_i\\): our test statistic would be \\[\nT = \\frac{\\hat{\\beta}_i-c_i}{\\hat{\\sigma}\\sqrt{g_{ii}}}\\sim t_{n-p},\n\\] if \\(H_0\\) is true. But we see that \\(T^2=F\\), and the hypothesis tests are equivalent, because \\[\nX \\sim t_\\nu\\iff\nX^2 \\sim F_{1,\\nu}.\n\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "hypothesisTesting.html#sec-ttest-R",
    "href": "hypothesisTesting.html#sec-ttest-R",
    "title": "5  Hypothesis testing",
    "section": "5.3 The \\(t\\)-test in R",
    "text": "5.3 The \\(t\\)-test in R\nR will perform a \\(t\\)-test separately for each hypothesis \\(H_0: \\beta_i=c_i\\), for \\(i=1,\\ldots,p\\). The results of this can be seen using the summary() command. For example\n\nlmCars &lt;- lm(dist ~ speed, cars)\nsummary(lmCars)\n\n\nCall:\nlm(formula = dist ~ speed, data = cars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-29.069  -9.525  -2.272   9.215  43.201 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -17.5791     6.7584  -2.601   0.0123 *  \nspeed         3.9324     0.4155   9.464 1.49e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 15.38 on 48 degrees of freedom\nMultiple R-squared:  0.6511,    Adjusted R-squared:  0.6438 \nF-statistic: 89.57 on 1 and 48 DF,  p-value: 1.49e-12\n\n\nThe last two columns in the Coefficients table give the corresponding test statistic and \\(p\\)-values. To be clear: for the simple linear regression model (with dist the dependent variable and speed the independent variable): \\[\nY_i = \\beta_0 + \\beta_1x_i + \\varepsilon_i,\n\\] the R output has given us observed values of the test statistics \\[\nT_{0,obs} = \\frac{\\hat{\\beta}_0-0}{\\hat{\\sigma}\\sqrt{g_{11}}}=-2.601, \\quad T_{1,obs} = \\frac{\\hat{\\beta}_1-0}{\\hat{\\sigma}\\sqrt{g_{22}}}=9.464,\n\\] with \\(p\\)-values of \\(0.0123\\) and \\(1.49\\times 10^{-12}\\) respectively (although for the latter, we should not claim such a precise order of magnitude! The precision is spurious here.)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "hypothesisTesting.html#sec-summary-F",
    "href": "hypothesisTesting.html#sec-summary-F",
    "title": "5  Hypothesis testing",
    "section": "5.4 The summary() commmand and the \\(F\\)-test output",
    "text": "5.4 The summary() commmand and the \\(F\\)-test output\nWhenever we use the summary() command in R, we see the result of and \\(F\\)-test displayed at the bottom. This is a test of the hypothesis that all coefficients except the intercept term are zero.\nAs an example, consider the one-way ANOVA model for the cancer data, using the parameterisation \\[\nY_{ij}=\\mu + \\tau_i + \\varepsilon_{ij},\n\\] with \\(\\tau_1=0\\):\n\nlibrary(tidyverse)\ncancer &lt;- read_csv(\"https://oakleyj.github.io/exampledata/cancer.csv\")\nlmcancer &lt;- lm(survival ~ organ, data = cancer)\nsummary(lmcancer)\n\n\nCall:\nlm(formula = survival ~ organ, data = cancer)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1371.91  -241.75  -111.50    87.19  2412.09 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     1395.9      201.9   6.915 3.77e-09 ***\norganBronchus  -1184.3      259.1  -4.571 2.53e-05 ***\norganColon      -938.5      259.1  -3.622 0.000608 ***\norganOvary      -511.6      339.8  -1.506 0.137526    \norganStomach   -1109.9      274.3  -4.046 0.000153 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 669.5 on 59 degrees of freedom\nMultiple R-squared:  0.3037,    Adjusted R-squared:  0.2565 \nF-statistic: 6.433 on 4 and 59 DF,  p-value: 0.0002295\n\n\nA null hypothesis of interest here is that the mean survival time is the same for all groups (organs): \\[\nH_0: \\tau_i=0 \\quad \\forall i\n\\] See the exercise above for how to write this in the general notation \\(H_0: C\\boldsymbol{\\beta} = \\boldsymbol{c}\\); there are \\(n=64\\) observations, there are \\(p=5\\) parameters, and the matrix \\(\\boldsymbol{C}\\) has \\(q=4\\) rows, so under \\(H_0\\), the \\(F\\)-statistic has the \\(F_{4, 59}\\) distribution. We can see in the summary() output that the observed \\(F\\)-statistic was 6.433, R has compared this to the \\(F_{4, 59}\\) distribution, and obtained a \\(p\\)-value: 0.0002295.\n\n\n\n\n\n\nTip\n\n\n\nWe can check this \\(p\\)-value for ourself:\n\n1-pf(6.433, df1 = 4, df2 = 59)\n\n[1] 0.000229583\n\n\nand if we want to visualise it:\n\ncurve(df(x, df1 = 4, df2 = 59), from = 0, to = 10)\nabline(v = 6.433, col = \"red\")\n\n\n\n\n\n\n\n# p-value is area under the curve from the right of the vertical line\n# (very close to 0)\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIf we use the parameterisation \\[\nY_{ij}=\\mu_i  + \\varepsilon_{ij},\n\\] and fit this model in R, we do not get a sensible \\(F\\)-test!\n\nlmcancer &lt;- lm(survival ~ organ -1, data = cancer)\nsummary(lmcancer)\n\n\nCall:\nlm(formula = survival ~ organ - 1, data = cancer)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1371.91  -241.75  -111.50    87.19  2412.09 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \norganBreast     1395.9      201.9   6.915 3.77e-09 ***\norganBronchus    211.6      162.4   1.303  0.19764    \norganColon       457.4      162.4   2.817  0.00659 ** \norganOvary       884.3      273.3   3.235  0.00199 ** \norganStomach     286.0      185.7   1.540  0.12887    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 669.5 on 59 degrees of freedom\nMultiple R-squared:  0.5437,    Adjusted R-squared:  0.505 \nF-statistic: 14.06 on 5 and 59 DF,  p-value: 4.766e-09\n\n\nThere is no ‘intercept’ in this model, so the null hypothesis being tested here is \\(H_0:\\mu_i = 0 \\quad \\forall i\\). Whilst we might hypothesise that all group means are equal, is unlikely that we would hypothesise that all group means are zero.\nThis is why the parameterisation\n\\[\nY_{ij}=\\mu + \\tau_i  + \\varepsilon_{ij},\n\\] (with an appropriate constraint on the \\(\\tau\\) parameters) is more commonly used, as parameters representing differences between groups are usually the main interest.\n\n\n\n\n\n\n\n\nTip\n\n\n\nHave a look again at the summary output for a simple linear regression model:\n\nlmCars &lt;- lm(dist ~ speed, cars)\nsummary(lmCars)\n\n\nCall:\nlm(formula = dist ~ speed, data = cars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-29.069  -9.525  -2.272   9.215  43.201 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -17.5791     6.7584  -2.601   0.0123 *  \nspeed         3.9324     0.4155   9.464 1.49e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 15.38 on 48 degrees of freedom\nMultiple R-squared:  0.6511,    Adjusted R-squared:  0.6438 \nF-statistic: 89.57 on 1 and 48 DF,  p-value: 1.49e-12\n\n\nThe \\(p\\)-values for the speed coefficient and the F-statistic are the same. Make sure you understand why!",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "hypothesisTesting.html#f-tests-and-nested-models",
    "href": "hypothesisTesting.html#f-tests-and-nested-models",
    "title": "5  Hypothesis testing",
    "section": "5.5 \\(F\\)-tests and nested models",
    "text": "5.5 \\(F\\)-tests and nested models\nThe general linear hypothesis \\[\nH_0: \\quad C\\boldsymbol{\\beta}=\\boldsymbol{c},\n\\] implies a constraint on the parameter vector \\(\\beta\\) in a linear model. This constraint implies a simpler model that we refer to as being nested in our original model. We sometimes use the terms full model and reduced model to refer to the original and nested model. Here are some examples:\n\nSimple linear regression:\n\nFull model: \\[Y_i = \\beta_0 + \\beta_1x_i + \\varepsilon_i.\\]\nNull hypothesis of interest: \\[H_0: \\beta_1 = 0.\\]\nReduced model: \\[Y_i = \\beta_0 + \\varepsilon_i.\\]\nInterpretation: the null hypothesis states that there is no relationship between the dependent and independent variable.\n\nPolynomial regression:\n\nFull model: \\[Y_i = \\beta_0 + \\beta_1x_i + \\beta_2x_i^2+ \\varepsilon_i.\\]\nNull hypothesis of interest: \\[H_0: \\beta_2 = 0.\\]\nReduced model: \\[Y_i = \\beta_0 + \\beta_1x_i + \\varepsilon_i.\\]\nInterpretation: the null hypothesis states that the relationship between the dependent and independent variable is linear.\n\nOne-way analysis of variance:\n\nFull model: \\[Y_{ij} = \\mu + \\tau_{i}+ \\varepsilon_{ij}, \\quad \\tau_1=0\\]\nNull hypothesis of interest: \\[H_0: \\tau_i = 0\\quad \\forall_i.\\]\nReduced model: \\[Y_{ij} = \\mu + \\varepsilon_{ij}.\\]\nInterpretation: the expected value of the dependent variable is the same in all groups; there is no difference between the groups.\n\n\nWe can interpret the \\(F\\)-test as a comparison between the full model and the reduced model, and the \\(F\\)-statistic can be written as follows:\n\\[F=\\frac{(RSS_r-RSS_f)/(p_f-p_r)}{(RSS_f)/(n-p_f)},\\] with each term defined as follows:\n\n\\(RSS_r\\): the residual sum of squares obtained by fitting the reduced model to the data;\n\\(RSS_f\\): the residual sum of squares obtained by fitting the full model to the data;\n\\(p_r\\): the number of parameters (in the parameter vector \\(\\boldsymbol{\\beta}\\)) in the reduced model;\n\\(p_f\\): the number of parameters (in the parameter vector \\(\\boldsymbol{\\beta}\\)) in the full model;\n\\(n\\): the number of observations.\n\nThere is a nice interpretation of the \\(F\\) statistic written in this form. We are calculating the difference in how well each model fits the data \\(RSS_r - RSS_f\\), relative to the ‘goodness-of-fit’ (residual sum of squares) for the full model. We also account for the difference between the number of parameters in the two models, as we would always expect the residual sum of squares to decrease if we increase the number of parameters.\n\n5.5.1 Implementing a nested model comparison in R\nThe general approach is to fit the full and reduced models separately, and then use the anova() command. For example, suppose we have\n\\[\\begin{align}\n\\mbox{full model: } Y_i &= \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\beta x_i^3 + \\varepsilon_i,\\\\\n\\mbox{reduced model: } Y_i &= \\beta_0 + \\beta_1 x_i + \\varepsilon_i,\\\\\n\\end{align}\\] so that the null hypothesis is \\(H_0: \\beta_2 = \\beta_3 = 0\\). We test this in R as follows:\n\n# fit the full model:\nlmFull &lt;- lm(dist ~ speed + I(speed^2) + I(speed^3) , cars)\n# fit the reduced model:\nlmReduced &lt;- lm(dist ~ speed, cars)\n# perform the F-test of the null hypothesis that the reduced model is correct:\nanova(lmReduced, lmFull)\n\nAnalysis of Variance Table\n\nModel 1: dist ~ speed\nModel 2: dist ~ speed + I(speed^2) + I(speed^3)\n  Res.Df   RSS Df Sum of Sq      F Pr(&gt;F)\n1     48 11354                           \n2     46 10634  2    719.16 1.5554  0.222\n\n\nWe see that R has presented the results using an “Analysis of Variance Table”; it’s quite common to see these reported in statistical software. The entries in the table correspond to the elements of as follows (assuming we specify the reduced model as the first argument of anova()):\n\n\n\n\n\n\n\n\n\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n\n\\(n-p_r\\)\n\\(RSS_r\\)\n\n\n\n\n\n\n\\(n - p_f\\)\n\\(RSS_f\\)\n\\(p_f - p_r\\)\n\\(RSS_r - RSS_f\\)\n\\(F_{obs}\\)\n\\(P(F_{p_f - p_r ; n - p_f} &gt; F_{obs})\\)\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAnalysis of variance (ANOVA) tables are perhaps most commonly seen when testing for equality of means for models of the type \\[\nY_{ij}=\\mu+\\tau_i+\\varepsilon_{ij}\n\\] This is why the model above is called a (one-way) analysis of variance model.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "hypothesisTesting.html#help-test-overload",
    "href": "hypothesisTesting.html#help-test-overload",
    "title": "5  Hypothesis testing",
    "section": "5.6 Help! Test overload!",
    "text": "5.6 Help! Test overload!\nAt this point, you may be feeling confused by what appears to be different variants of the same test. The reason for presenting the test in different ways relates to the way tests are implemented and reported in R.\n\nWhen do we use the \\(t\\)-test?\n\nWe use this if we want to test that a single coefficient is 0. The easiest way to implement this is to use the summary() command on a fitted model, and look at the coefficients table, as shown in Section 5.3.\n\nWhen do we use the \\(F\\)-test reported in the summary() output?\n\nWe use this to test that all coefficients (apart from an intercept parameter) are zero. In the one-way ANOVA model \\[\nY_{ij} = \\mu + \\tau_i + \\varepsilon_{ij},\n\\] with \\(\\tau_1=0\\), we usually have particular interest in the null hypothesis \\(H_0:\\tau_i=0 \\quad \\forall_i\\). The easiest way to implement this is to use the summary() command on a fitted model, and look at the F-statistic output, as shown in Section 5.4.\n\nWhen do we compare ‘full and reduced’ models?\n\nTypically, the scenario here is where we have multiple independent variables, but the null model is not a sensible model to use for the null hypothesis; we might be certain a particular independent variable has an effect. For example, our full model might be\n\\[\n\\mbox{full model: }Y_i = \\beta_0 + \\beta_1x_i + \\beta_2w_i + \\beta_3 z_i + \\varepsilon_i,\n\\] and we might be certain (from other knowledge or data) that the \\(x\\)-variable has an effect, so that \\(\\beta_1\\neq 0\\), so that the main interest is in testing if \\(\\beta_2=\\beta_3=0\\). A reduced model would then be \\[\n\\mbox{reduced model: }Y_i = \\beta_0 + \\beta_1x_i + \\varepsilon_i.\n\\] The summary() command won’t perform this particular \\(F\\)-test directly. In R, we need to fit the full model and reduced models separately, and then use the anova() command, as shown in Section 5.5.1.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "cautionaryTales.html",
    "href": "cautionaryTales.html",
    "title": "6  Cautionary tales",
    "section": "",
    "text": "6.1 Multiplicity\nSuppose we have a data set with a large number of independent variables, where the aim is to investigate whether any of them are associated with the dependent variable. For example, in a multiple regression model, \\[\nY_i=\\beta_0+\\beta_1 x_{i1}+\\cdots+\\beta_rx_{ir}+\\varepsilon_i,\n\\] we might perform \\(r\\) separate tests of \\(H_0:\\beta_j=0\\) for each of \\(j=1,\\ldots,r\\). The problem here is that the more tests we do, the higher the risk of falsely rejecting a null hypothesis, assuming the number of true nulls increases. We illustrate this with a simulation experiment, in which the true and fitted models are as follows: \\[\\begin{align}\n\\mbox{True model: }Y_i &= \\beta_0 + \\varepsilon_i,\\\\\n\\mbox{Fitted model: }Y_i &= \\beta_0 + \\sum_{j=1}^{100} \\beta_j x_{ij}+\\varepsilon_i.\n\\end{align}\\]\nGiven some data, we will test 100 separate hypotheses \\(H_0:\\beta_j=0\\) for each of \\(j=1,\\ldots,100\\). By construction, we will know that \\(H_0\\) is really true in each case, but for a test of size 0.05, we’d expect to reject the null hypothesis on 5 out of the 100 occasions.\nlibrary(tidyverse)\n# set random seed for reproducibility\nset.seed(61004) \n\n# Generate and label the data. By construction, all columns are independent\ndf1 &lt;- data.frame(matrix(rnorm(1000*101), 1000, 101))\ncolnames(df1) &lt;- c(\"Y\", paste0(\"X\", 1:100))\n\n# Fit the model will all 100 independent variables\n# The Y ~ . notation is shorthand for this.\nlmFull &lt;- lm(Y ~ . , df1)\n\n# Extract the p-values of the 100 t-tests, and arrange in ascending order\ncm1 &lt;- as_tibble(summary(lmFull)$coefficients)\ncolnames(cm1)[4] &lt;- \"pvalue\"\ncm1$variable &lt;- paste0(\"beta\", 0:100)\ncm1 %&gt;%\n  arrange(pvalue)\n\n# A tibble: 101 × 5\n   Estimate `Std. Error` `t value`   pvalue variable\n      &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;   \n 1   0.124        0.0328      3.77 0.000174 beta78  \n 2   0.0823       0.0341      2.41 0.0160   beta36  \n 3   0.0802       0.0335      2.39 0.0169   beta33  \n 4  -0.0637       0.0342     -1.86 0.0629   beta47  \n 5   0.0614       0.0331      1.85 0.0640   beta70  \n 6   0.0592       0.0328      1.80 0.0718   beta68  \n 7   0.0597       0.0335      1.78 0.0753   beta15  \n 8  -0.0569       0.0322     -1.77 0.0776   beta71  \n 9   0.0587       0.0341      1.72 0.0853   beta54  \n10  -0.0558       0.0326     -1.71 0.0872   beta85  \n# ℹ 91 more rows\nIn the table above, we’ve arranged the \\(p\\)-values in ascending order. We can see three \\(p\\)-values below 0.05 (roughly what we would expect). In particular, the \\(p\\)-value for the test \\[\nH_0: \\beta_{78}=0\n\\] is 0.0002 to 4 d.p., which certainly gives the appearance of strong evidence against \\(H_0\\).\nBy construction, however, we know there is no relationship between any of the independent variables and the dependent variables. How can we guard against false results such as these? Two strategies are as follows.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Cautionary tales</span>"
    ]
  },
  {
    "objectID": "cautionaryTales.html#sec-multiplicity",
    "href": "cautionaryTales.html#sec-multiplicity",
    "title": "6  Cautionary tales",
    "section": "",
    "text": "6.1.1 Use a single \\(F\\)-test\nWe need to consider the aim of the investigation. In the example above, we might suppose that the investigator was searching for anything associated with the dependent variable, from a large set of candidate variables. The investigator didn’t single out a particular variable (e.g. the \\(x_{78}\\) variable) in advance of the experiment as the variable most likely to be associated with the dependent variable.\nIn this scenario, we might test the single hypothesis that none of the variables are associated with the dependent variable: \\[\nH_0: \\beta_1=\\beta_2=\\ldots=\\beta_{100}=0,\n\\] which we can test with an \\(F\\)-test: we compare the full model above with a reduced model \\[\nY_i = \\beta_0 + \\varepsilon_i\n\\] If we do this in R, we find that there is no evidence against \\(H_0\\):\n\nlmReduced &lt;-lm(Y ~ 1, df1)\nanova(lmReduced, lmFull)\n\nAnalysis of Variance Table\n\nModel 1: Y ~ 1\nModel 2: Y ~ X1 + X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9 + X10 + X11 + \n    X12 + X13 + X14 + X15 + X16 + X17 + X18 + X19 + X20 + X21 + \n    X22 + X23 + X24 + X25 + X26 + X27 + X28 + X29 + X30 + X31 + \n    X32 + X33 + X34 + X35 + X36 + X37 + X38 + X39 + X40 + X41 + \n    X42 + X43 + X44 + X45 + X46 + X47 + X48 + X49 + X50 + X51 + \n    X52 + X53 + X54 + X55 + X56 + X57 + X58 + X59 + X60 + X61 + \n    X62 + X63 + X64 + X65 + X66 + X67 + X68 + X69 + X70 + X71 + \n    X72 + X73 + X74 + X75 + X76 + X77 + X78 + X79 + X80 + X81 + \n    X82 + X83 + X84 + X85 + X86 + X87 + X88 + X89 + X90 + X91 + \n    X92 + X93 + X94 + X95 + X96 + X97 + X98 + X99 + X100\n  Res.Df    RSS  Df Sum of Sq      F  Pr(&gt;F)  \n1    999 998.00                               \n2    899 879.63 100    118.36 1.2097 0.08878 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n6.1.2 Adjusting for multiplicity\nWe refer to collection of tests (such as testing \\(H_0:\\beta_i=0\\) separately for \\(i=1,\\ldots,100\\)) as a family of tests, and we consider family-wise error rates. The family-wise Type I error rate (FWER) would be the probability of falsely rejecting any null hypothesis within the family, out of all cases in which the null hypothesis is true.\nThe idea is to specify the FWER (e.g. at 0.05), and then adjust the size of each test within the family to achieve (or bound) the FWER. Two possible adjustments are as follows.\n\nBonferroni correction. If we perform \\(n\\) tests, and we desire a family-wise Type I error rate of \\(\\alpha\\), we set the size of each individual test to be \\(\\alpha/n\\).\nŠidák correction. If we perform \\(n\\) tests, and we desire a family-wise Type I error rate of \\(\\alpha\\), we set the size of each individual test to be \\(\\alpha^*:=1-(1-\\alpha)^\\frac{1}{n}\\).\n\nSuppose the null hypothesis is true for all \\(n\\) tests. For the Bonferroni correction, we have \\[\nP(\\mbox{reject at least one null}) \\le \\sum_{i=1}^n P(\\mbox{reject null in $i$-th test}) = n\\frac{\\alpha}{n} = \\alpha\n\\] For the Šidák correction, assuming the test outcomes are independent \\[\nP(\\mbox{reject at least one null}) =1-P(\\mbox{do not reject any null}) = 1-(1-\\alpha^*)^n = \\alpha\n\\] Hence the Šidák correction gives an exact FWER of \\(\\alpha\\), under an assumption of independent tests. The Bonferroni correction only given an upper bound for the FWER, but does not assume independent tests.\n\n\n\n\n\n\nNote\n\n\n\nThe two corrections give very similar values. If you ever find that your result depends upon which you use, then I’d encourage you to consider the following quote from the mathematician Richard Hamming (which I think is relevant in many situations!)\n“Does anyone believe that the difference between the Lebesgue and Riemann integrals can have physical significance, and that whether say, an airplane would or would not fly could depend on this difference? If such were claimed, I should not care to fly in that plane.”\n\n\nIn the example with 100 tests, for a target FWER of 0.05, we’d look for a \\(p\\)-value smaller than 0.0005, and for a target FWER of 0.01, we’d look for a \\(p\\)-value smaller than 0.0001. The smallest \\(p\\)-value we observed was 0.00017, so we’d be more sceptical that we have found a genuine association.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Cautionary tales</span>"
    ]
  },
  {
    "objectID": "cautionaryTales.html#small-effect-sizes-significance-versus-importance",
    "href": "cautionaryTales.html#small-effect-sizes-significance-versus-importance",
    "title": "6  Cautionary tales",
    "section": "6.2 Small effect sizes: significance versus importance",
    "text": "6.2 Small effect sizes: significance versus importance\nIt’s important not to get too carried away with a significant result! The effect size may be too small for the result to have any practical importance. Continuing the example above, \\(R^2_{adj}\\) is very small, about 2%:\n\nsummary(lmFull)$adj.r.squared\n\n[1] 0.02055728\n\n\nand a scatter plot of the dependent variable against \\(x_{78}\\) shows mostly random scatter and a relatively shallow gradient in the fitted line\n\nggplot(df1, aes(x = X78, y = Y)) +\n  geom_point()+\n  geom_smooth(method = \"lm\")",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Cautionary tales</span>"
    ]
  },
  {
    "objectID": "cautionaryTales.html#sec-carquad",
    "href": "cautionaryTales.html#sec-carquad",
    "title": "6  Cautionary tales",
    "section": "6.3 Relationships between independent variables",
    "text": "6.3 Relationships between independent variables\nConsider the following example using the cars data. We’d clearly expect there to be a relationship between speed and stopping distance, and it is visible in a scatter plot:\n\nggplot(cars, aes(x = speed, y = dist)) +\n  geom_point()\n\n\n\n\n\n\n\n\nNow consider a quadratic regression model:\n\nlmQuad &lt;- lm(dist ~ speed + I(speed^2), cars)\nsummary(lmQuad)\n\n\nCall:\nlm(formula = dist ~ speed + I(speed^2), data = cars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-28.720  -9.184  -3.188   4.628  45.152 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)  2.47014   14.81716   0.167    0.868\nspeed        0.91329    2.03422   0.449    0.656\nI(speed^2)   0.09996    0.06597   1.515    0.136\n\nResidual standard error: 15.18 on 47 degrees of freedom\nMultiple R-squared:  0.6673,    Adjusted R-squared:  0.6532 \nF-statistic: 47.14 on 2 and 47 DF,  p-value: 5.852e-12\n\n\nThe F-statistic shows strong evidence against a null model \\(Y_i=\\beta_0 + \\varepsilon_i\\), but in the Coefficients table, neither the coefficients for speed or speed^2 are significant. What is going on here?\nOur quadratic regression model is \\[\nY_i=\\beta_0 + \\beta_1x_i + \\beta_2x_i^2+ \\varepsilon_i.\n\\]\nThe issue here is the relationship between \\(x_i\\) and \\(x_i^2\\), which is quite close to linear for the given set of \\(x_i\\) values:\n\ncor(cars$speed, cars$speed^2)\n\n[1] 0.9794765\n\n\nThis means we have two columns in the design matrix \\(X\\) that are close to being linearly dependent (sometimes referred to as multicollinearity). Intuitively, this suggests that, for the purpose of explaining variation in the dependent variable, we need at most one of \\(x_i\\) or \\(x_i^2\\) in our model, but not both: the two variables are, in effect, describing the same thing.\n\n6.3.1 Correlations between estimators\nContinuing the example, we have strong correlations between the estimators \\(\\hat{\\beta}_0\\), \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_2\\), and it is helpful to understand what effect this has on model fitting and hypothesis testing.\nRecall that \\[\nVar(\\hat{\\boldsymbol{\\beta}}) = \\sigma^2(X^TX)^{-1}\n\\] We can extract \\(X\\) from R using the model.matrix() command (and we also use cov2cor() to convert a covariance matrix to a correlation matrix):\n\nX &lt;- model.matrix(lmQuad)\ncov2cor(solve(t(X)%*%X))\n\n            (Intercept)      speed I(speed^2)\n(Intercept)   1.0000000 -0.9605503  0.8929849\nspeed        -0.9605503  1.0000000 -0.9794765\nI(speed^2)    0.8929849 -0.9794765  1.0000000\n\n\nThe consequence of this is that adding/removing one term (e.g. the term corresponding to \\(\\beta_1\\)) will change the estimated coefficient(s) corresponding to the other term(s).\n\n\n\n\n\n\nTip\n\n\n\nCheck this for yourself. Investigate what happens to the coefficients as terms are added or removed: try fitting models such as\nlm(dist ~ 1, cars)\nlm(dist ~ speed, cars)\nlm(dist ~ I(speed^2), cars)\nlm(dist ~ speed + I(speed^2), cars)\nand comparing the estimated coefficients in each case.\n\n\nThis gives us another way to understand why the individual \\(t\\)-tests were not significant. For example, if we consider a test of \\(\\beta_1=0\\), we can think of this as a model comparison between \\[\\begin{align}\n\\mbox{full model: }Y_i&=\\beta_0 + \\beta_1x_i + \\beta_2x_i^2+ \\varepsilon_i,\\\\\n\\mbox{reduced model: }Y_i&=\\beta_0 + \\beta_2x_i^2+ \\varepsilon_i.\n\\end{align}\\]\n\n\n\n\n\n\nNote\n\n\n\nThe fitted full model coefficients are obtained as follows:\n\nlm(dist ~ speed + I(speed^2), cars)\n\n\nCall:\nlm(formula = dist ~ speed + I(speed^2), data = cars)\n\nCoefficients:\n(Intercept)        speed   I(speed^2)  \n    2.47014      0.91329      0.09996  \n\n\nIn comparing the full and reduced model, we are not comparing \\[\\begin{align}\n\\mbox{full model: }Y_i&=2.47 + 0.91x_i + 0.10x_i^2+ \\varepsilon_i,\\\\\n\\mbox{reduced model: }Y_i&=2.47 + 0.10x_i^2+ \\varepsilon_i.\n\\end{align}\\] When we fit the reduced model to the data, the estimates of \\(\\beta_0\\) and \\(\\beta_2\\) will change, because of the correlation between the least squares estimators. In this case, the fact that the estimate change result in the reduced model fitting nearly as well as the full model. We can see this by noting the residual sum of squares for each model in the following:\n\nlmReduced &lt;- lm(dist ~ I(speed^2), cars)\nanova(lmReduced, lmQuad)\n\nAnalysis of Variance Table\n\nModel 1: dist ~ I(speed^2)\nModel 2: dist ~ speed + I(speed^2)\n  Res.Df   RSS Df Sum of Sq      F Pr(&gt;F)\n1     48 10871                           \n2     47 10825  1    46.423 0.2016 0.6555\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\nExercise 6.1 In the example above, suppose the reduced model was fitted simply by setting \\(\\hat{\\beta}_1=0\\) in the full model; the other parameter estimates don’t change. In R, lmQuad$fitted.values will extract the fitted values for the full model. Compare the residual sum of squares for the two models, if the reduced model was fitted in this way.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIn this case, we subtract \\(\\hat{\\beta}_1x_i\\) from the \\(i\\)-th fitted value in the full model\n\nyhatWrong &lt;- lmQuad$fitted.values - 0.91329 * cars$speed\n\nUsing these ‘wrong’ fitted values, the residual sum of squares would be\n\nsum((yhatWrong - cars$dist)^2)\n\n[1] 21858.17\n\n\nwhich is about double the residual sum of squares for the full model.\n\n\n\nFor designed experiments (where we can choose the dependent variable values), it can be desirable to aim for orthogonal columns in the design matrix, such that parameter estimators are independent, making it easier to isolate the effect of individual variables. (much more on this in the Sampling and Design module). In this particular example, we could make use of orthogonal polynomials. This is outside the scope of this model, but there is brief discussion in the Chapter appendix for anyone interested.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Cautionary tales</span>"
    ]
  },
  {
    "objectID": "cautionaryTales.html#correlation-and-causality",
    "href": "cautionaryTales.html#correlation-and-causality",
    "title": "6  Cautionary tales",
    "section": "6.4 Correlation and causality",
    "text": "6.4 Correlation and causality\nYou have probably heard the phase “correlation is not the same as causation”. The classic example of this is the scenario of increasing temperatures causing both an increase in ice-cream sales, and an increase in violent crime (through increased alcohol consumption). If we examine ice-cream sales and violent crime together, we might observe correlation between them, but we wouldn’t believe this to be a causal relationship.\nCausal inference is an interesting and active research topic, and connects in an intuitive way with linear modelling. A good introduction is given in\n\nPearl, J., Glymour, M. and Jewell, N.P. (2016) Causal inference in statistics: a primer. West Sussex, England: Wiley. (Ebook available from the library)\n\nWe will give a very short discussion here, in the context of hypothesis tests (and parameter estimation) for linear models.\n\n6.4.1 Graphical models\nIt can be helpful to represent relationships between variables using graphs, with nodes representing variables and directed edges between nodes representing causal relationships.\n\nlibrary(dagitty)\nlibrary(ggdag)\ndag &lt;- dagitty(\"dag{ IC &lt;- Temp -&gt; Crime}\")\ncoordinates( dag ) &lt;-\n    list( x=c(Temp = 1, IC = 0, Crime = 2),\n        y=c(Temp = 1, IC = 0, Crime = 0) )\nggdag(dag) +\n  theme_dag_blank()\n\n\n\n\n\n\n\n\nIn this plot the arrow going from Temp (temperature) to IC (ice cream sales) represents a causal relationship: the volume of ice cream sales is ‘caused’ by the value of the temperature, and potentially other unmeasured variables represented by an error term, e.g. \\[\nIC_i = \\beta_0 + \\beta_1Temp_i + \\varepsilon_i.\n\\] The diagram represents a similar causal relationship between temperature and crime, but no causal relationship between ice creams sales and crime. Ice creams sales and crime are correlated, however, due to their common dependence on temperature.\nWe’ll now consider a few examples of causal relationships versus correlations, and how they might be explored through hypothesis tests.\n\n\n6.4.2 Conditional independence\nIn the ice cream example, we might expect ice cream sales and crime to be independent conditional on temperature (the idea of conditional independence is developed more formally in the Bayesian statistics module).\nIn general, suppose we have three random variables: \\(X, Y\\) and \\(Z\\), where both \\(Y\\) and \\(Z\\) are dependent on \\(X\\), but conditionally independent given the value of \\(X\\). We visualise this as follows:\n\n\n\n\n\n\n\n\n\nAs an example, suppose that the true ‘causal’ relationships between the variables are given by \\[\\begin{align}\nY_i &= \\beta_0 + \\beta_1 X_i + \\varepsilon_i,\\\\\nZ_i &= \\alpha_0 + \\alpha_1 X_i + \\delta_i,\\\\\n\\end{align}\\] where \\(\\varepsilon_i\\) and \\(\\delta_i\\) are random error terms. We’ll first generate some random data in R:\n\nset.seed(61004)\nx &lt;- 1:100\ny &lt;- x + rnorm(100)\nz &lt;- x + rnorm(100)\n\nIf we try a simple linear regression model of \\(Y\\) on \\(Z\\), we would detect a relationship between \\(Y\\) and \\(Z\\):\n\nlmYonZ &lt;- lm(y ~  z)\nsummary(lmYonZ)\n\n\nCall:\nlm(formula = y ~ z)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.3232 -0.8955 -0.1176  0.8613  3.2355 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.010668   0.291562   0.037    0.971    \nz           0.999672   0.005003 199.811   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.449 on 98 degrees of freedom\nMultiple R-squared:  0.9976,    Adjusted R-squared:  0.9975 \nF-statistic: 3.992e+04 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n\nHowever, if we include the variable \\(X\\) in our model, the evidence for a relationship between \\(Y\\) and \\(Z\\) disappears:\n\nlmYonXandZ &lt;- lm(y ~  x + z)\nsummary(lmYonXandZ)\n\n\nCall:\nlm(formula = y ~ x + z)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.55853 -0.53880  0.09033  0.58057  2.75507 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.11805    0.19322  -0.611    0.543    \nx            1.09501    0.09720  11.266   &lt;2e-16 ***\nz           -0.09123    0.09689  -0.942    0.349    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9585 on 97 degrees of freedom\nMultiple R-squared:  0.9989,    Adjusted R-squared:  0.9989 \nF-statistic: 4.567e+04 on 2 and 97 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf we want to investigate the relationship between \\(Y\\) and \\(Z\\), and suspect they are both dependent on a third variable \\(X\\), we should adjust for that variable in our modelling: we include \\(X\\) as in independent variable in the model, and then see whether including \\(Z\\) as an additional independent variable improves our ability to predict \\(Y\\).\n\n\n\n\n6.4.3 Post-treatment variables\nA different scenario is where we a variable \\(X\\) has a causal effect on our dependent variable \\(Y\\), and the dependent variable \\(Y\\) has a causal effect on some other quantity \\(Z\\). For example, for an individual exercise (\\(X\\)) might cause weight loss (\\(Y\\)), which might then result in a cholesterol reduction (\\(Z\\)). We would refer to cholesterol reduction as a post-treatment variable in that it isn’t determined until after the treatment \\(X\\) has been applied and the effect on \\(Y\\) determined. The graphical representation would be as follows:\n\n\n\n\n\n\n\n\n\nThis has implications for what should be included in a model. We suppose that the true ‘causal’ relationships between the variables are given by \\[\\begin{align}\nY_i &= \\beta_0 + \\beta_1 X_i + \\varepsilon_i,\\\\\nZ_i &= \\alpha_0 + \\alpha_1 Y_i + \\delta_i,\\\\\n\\end{align}\\] where \\(\\varepsilon_i\\) and \\(\\delta_i\\) are random error terms. We’ll first generate some random data in R (with \\(\\beta_0=0\\) and \\(\\beta_1=1\\)):\n\nset.seed(61004)\nx &lt;- seq(from = 0, to = 10, length = 100)\ny &lt;- x + rnorm(100, 0, 1)\nz &lt;- y + rnorm(100, 0, 1)\n\nA simple linear regression of \\(Y\\) on \\(X\\) will give estimates of \\(\\beta_0\\) and \\(\\beta_1\\) close to their true values:\n\nlmYonX &lt;- lm(y ~  x)\nsummary(lmYonX)\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.5438 -0.5781  0.0856  0.6364  2.6931 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.10914    0.19015  -0.574    0.567    \nx            1.03513    0.03285  31.508   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9579 on 98 degrees of freedom\nMultiple R-squared:  0.9102,    Adjusted R-squared:  0.9092 \nF-statistic: 992.7 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n\nNow observe how including the post-treatment variable \\(Z\\) in the model changes the estimated effect of the treatment \\(X\\):\n\nlmYonXandZ &lt;- lm(y ~  x + z)\nsummary(lmYonXandZ)\n\n\nCall:\nlm(formula = y ~ x + z)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.64776 -0.47647 -0.03676  0.38458  1.62566 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.03031    0.14468  -0.209    0.835    \nx            0.52946    0.06423   8.243 8.17e-13 ***\nz            0.47666    0.05580   8.543 1.86e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7273 on 97 degrees of freedom\nMultiple R-squared:  0.9487,    Adjusted R-squared:  0.9477 \nF-statistic: 897.4 on 2 and 97 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n\n\nNote\n\n\n\nIncluding the post-treatment variable in the regression model changes the objective of the regression model: the implied objective is to find the best prediction of \\(Y\\) given both \\(X\\) and \\(Z\\), not to reveal the causal relationship between \\(Y\\) and \\((X,Z)\\). If the real purpose is to predict \\(Y\\) given \\(X\\) only, we should not include \\(Z\\) in the model.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Cautionary tales</span>"
    ]
  },
  {
    "objectID": "cautionaryTales.html#replication",
    "href": "cautionaryTales.html#replication",
    "title": "6  Cautionary tales",
    "section": "6.5 Replication",
    "text": "6.5 Replication\nAlthough outside the scope of what we can do in this module, it is important to consider replicating any study to see if the same result is obtained. The replicability of scientific findings is itself now an active area of research. See for example\n\nCamerer, C.F., Dreber, A., Holzmeister, F. et al. (2018) Evaluating the replicability of social science experiments in Nature and Science between 2010 and 2015. Nat Hum Behav 2, 637–644 . https://doi.org/10.1038/s41562-018-0399-z\nIoannidis J.P.A. (2005) Why Most Published Research Findings Are False. PLoS Med 2(8): e124. https://doi.org/10.1371/journal.pmed.0020124 pmid:16060722]\n\nIn clinical trials for new medicines, regulators such as the US Food and Drug Administration and the European Medicines Agency may require two (Phase III) studies demonstrating the effectiveness of a drug.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Cautionary tales</span>"
    ]
  },
  {
    "objectID": "cautionaryTales.html#chapter-appendix",
    "href": "cautionaryTales.html#chapter-appendix",
    "title": "6  Cautionary tales",
    "section": "6.6 Chapter appendix",
    "text": "6.6 Chapter appendix\nThe following topic is outside the scope of this module, but is included for interest.\n\n6.6.1 Orthogonal polynomials\nIn the example in Section 6.3, we can use orthogonal polynomials. The idea is to use Gram-Schmidt orthogonalisation to transform the columns of the design matrix \\(X\\) to set of linearly independent (orthogonal) columns. This makes the coefficients harder to interpret, but it is then possible to isolate a quadratic effect, if we were interested in testing for it. (It can also improve numerical stability of the results.)\nWe use the poly() command to set up orthogonal polynomials. We’ll compare quadratic regression models using both raw and orthogonal polynomials\n\n# Using raw polynomials\nlmRaw &lt;- lm(dist ~ speed + I(speed^2), cars)\n# Using orthogonal polynomials\nlmOrthogonal &lt;- lm(dist ~ poly(speed, degree = 2), cars)\n\n\nsummary(lmRaw)\n\n\nCall:\nlm(formula = dist ~ speed + I(speed^2), data = cars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-28.720  -9.184  -3.188   4.628  45.152 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)  2.47014   14.81716   0.167    0.868\nspeed        0.91329    2.03422   0.449    0.656\nI(speed^2)   0.09996    0.06597   1.515    0.136\n\nResidual standard error: 15.18 on 47 degrees of freedom\nMultiple R-squared:  0.6673,    Adjusted R-squared:  0.6532 \nF-statistic: 47.14 on 2 and 47 DF,  p-value: 5.852e-12\n\nsummary(lmOrthogonal)\n\n\nCall:\nlm(formula = dist ~ poly(speed, degree = 2), data = cars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-28.720  -9.184  -3.188   4.628  45.152 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                42.980      2.146  20.026  &lt; 2e-16 ***\npoly(speed, degree = 2)1  145.552     15.176   9.591 1.21e-12 ***\npoly(speed, degree = 2)2   22.996     15.176   1.515    0.136    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 15.18 on 47 degrees of freedom\nMultiple R-squared:  0.6673,    Adjusted R-squared:  0.6532 \nF-statistic: 47.14 on 2 and 47 DF,  p-value: 5.852e-12\n\n\nNote that the residual standard error and \\(R^2\\) statistics are the same for both versions: switching to orthogonal polynomials doesn’t improve the model fit. But in the orthogonal case, we see that the linear term is significant; the quadratic term is not.\nWe’ll extract the design matrix for each:\n\nXraw &lt;- model.matrix(lmRaw)\nXOrthogonal &lt;- model.matrix(lmOrthogonal)\n\nand compare the variance-covariance matrices (\\(\\sigma^2(X^TX)^{-1}\\)) for each:\n\noptions(digits = 4)\nsolve(t(Xraw)%*% Xraw)\n\n            (Intercept)      speed I(speed^2)\n(Intercept)     0.95326 -0.1257085  0.0037899\nspeed          -0.12571  0.0179671 -0.0005707\nI(speed^2)      0.00379 -0.0005707  0.0000189\n\nsolve(t(XOrthogonal)%*% XOrthogonal)\n\n                         (Intercept) poly(speed, degree = 2)1\n(Intercept)                2.000e-02               -2.776e-17\npoly(speed, degree = 2)1  -2.776e-17                1.000e+00\npoly(speed, degree = 2)2  -2.109e-17                2.352e-17\n                         poly(speed, degree = 2)2\n(Intercept)                            -2.109e-17\npoly(speed, degree = 2)1                2.352e-17\npoly(speed, degree = 2)2                1.000e+00\n\n\nNote that the off-diagonal elements in the orthogonal polynomial model are all very close to 0 (rounding errors): the parameter estimators are all independent. Finally, to help visualise the quadratic term using orthogonal polynomials, we’ll plot the third column of the design matrix \\(X\\) each model against speed\n\nplot(cars$speed, Xraw[, 3])\n\n\n\n\n\n\n\nplot(cars$speed, XOrthogonal[, 3])",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Cautionary tales</span>"
    ]
  },
  {
    "objectID": "assumptions.html",
    "href": "assumptions.html",
    "title": "7  Checking model assumptions",
    "section": "",
    "text": "7.1 Residuals for model checking\nAn important feature of any statistical analysis should be to check, as far as it is possible to do so, that the assumptions made in the model are valid. In the case of linear models, our assumptions concern the error terms \\(\\boldsymbol{\\varepsilon}\\). For observation \\(i\\), the model error is \\(\\varepsilon_i=y_i-\\boldsymbol{x}_i^T\\boldsymbol{\\beta}\\); the residual \\(e_i=y_i-\\boldsymbol{x}_i^T\\boldsymbol{\\hat{\\beta}}\\) can be thought of as an estimate of this.\nRecall that we assume the \\(\\varepsilon_i\\) to:\nIn principle, if we knew the actual values of the \\(\\varepsilon_i\\) then we could check these assumptions. We could look to see if \\(\\varepsilon_1,\\varepsilon_2,\\ldots,\\varepsilon_n\\) looked in all relevant respects like a sample from a normal distribution with zero mean.\nHowever, we do not know the \\(\\varepsilon_i\\) and cannot deduce them from the observed \\(y_i\\) unless we know \\(\\boldsymbol{\\beta}\\) (the true parameter vector rather than the estimate \\(\\boldsymbol{\\hat{\\beta}}\\)). In practice, we only have estimates of the \\(\\varepsilon_i\\), which are the residuals \\(e_i\\), \\[\ne_i=y_i-\\boldsymbol{x}_i^T\\boldsymbol{\\hat{\\beta}}\n\\] Therefore, the main strategy for checking the assumptions of linear models is to perform various checks using residuals.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Checking model assumptions</span>"
    ]
  },
  {
    "objectID": "assumptions.html#residuals-for-model-checking",
    "href": "assumptions.html#residuals-for-model-checking",
    "title": "7  Checking model assumptions",
    "section": "",
    "text": "have zero mean (which is equivalent to assuming that we have got the systematic part of the model represented by \\(X\\boldsymbol{\\beta}\\) right);\nbe independent;\nhave common variance (homoscedasticity);\nbe normally distributed.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Checking model assumptions</span>"
    ]
  },
  {
    "objectID": "assumptions.html#sec-stand_resids",
    "href": "assumptions.html#sec-stand_resids",
    "title": "7  Checking model assumptions",
    "section": "7.2 Standardized residuals",
    "text": "7.2 Standardized residuals\nTechnically, we should recognize that the residuals have different properties from the errors. If the model is correct, they are normally distributed and have zero mean. However, they are not independent and do not have a common variance; in Section 2.7.2 we showed that the residual vector \\(\\boldsymbol{e}\\) has variance \\[\nVar(\\boldsymbol{e})= \\sigma^2(I_n-X(X^TX)^{-1}X^T).\n\\]\nWe can correct for the unequal variances by defining the standardized residuals, which are given by \\[\ns_i=\\frac{e_i}{\\sqrt{\\widehat{Var}(e_i)}},\n\\] where \\(\\widehat{Var}\\) is element \\(i,i\\) of \\[\n\\hat{\\sigma}^2(I_n-X(X^TX)^{-1}X^T)\n\\]\nThere can be advantages to doing this, but there are disadvantages too. After standardising, the distribution of each residual is \\(t_{n-p}\\), but they are still not independent. The correlations may be small, however.\n\n\n\n\n\n\nNote\n\n\n\nAssuming \\(n-p\\) is fairly large, it’s common practice to approximate the \\(t_{n-p}\\) distribution by the \\(N(0,1)\\) distribution, and so check whether the standardised residuals are normally distributed.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Checking model assumptions</span>"
    ]
  },
  {
    "objectID": "assumptions.html#residual-plots",
    "href": "assumptions.html#residual-plots",
    "title": "7  Checking model assumptions",
    "section": "7.3 Residual plots",
    "text": "7.3 Residual plots\nThe most common diagnostic use for residuals is to plot them in various ways. We first fit our model:\n\nlmCars &lt;- lm(dist ~ speed, cars)\n\nthen use MASS::stdres() to get standardised residuals:\n\ns &lt;- MASS::stdres(lmCars)\n\n\n7.3.1 Checking normality\nThere are various plots we can try.\n\nQ-Q plot:\n\nA Q-Q (quantile-quantile) plot is a plot the quantiles of the observed data against quantiles from a reference distribution, which we specify to be the standard normal distribution. This plot is useful as it allows a visual inspection of the normality of the residuals.\n\nqqnorm(s)\nabline(0, 1)\n\n\n\n\n\n\n\n\n\nIf a sample is genuinely from a normal distribution with constant variance, then the residuals will lie close to a straight line.\nIf the plot is clearly bowed, it suggests a skew distribution (whereas the normal distribution is symmetric).\nIf the plotted points curve down at the left and up at the right, it suggests a distribution with heavier tails than the normal (and therefore one that is prone to produce outliers).\n\nIn our case, the graph is a little bowed, suggesting a skewed distribution.\n\nHistogram\n\n\nhist(s)\n\n\n\n\n\n\n\n\nThe histogram is another way of looking at the underlying assumption of normality. In a large sample, it can be a useful addition to the normal plot, but with small samples it can be difficult to make meaningful conclusions.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Checking model assumptions</span>"
    ]
  },
  {
    "objectID": "assumptions.html#checking-homoscedasticity-constant-variance",
    "href": "assumptions.html#checking-homoscedasticity-constant-variance",
    "title": "7  Checking model assumptions",
    "section": "7.4 Checking homoscedasticity (constant variance)",
    "text": "7.4 Checking homoscedasticity (constant variance)\n\n\n\n\n\n\nNote\n\n\n\n\nwe use the term homoscedasticity to mean constant variance of the errors\nwe use the term heteroscedasticity to mean non-homoscedasticity: non-constant variance\n\n\n\nIn Section 2.7.3 we showed that the residual vector \\(\\boldsymbol{e}\\) is independent of the least squares estimator \\(\\hat{\\boldsymbol{\\beta}}\\), under the assumption that the errors are independent with common variance \\(\\sigma^2\\). Hence, under this assumption, \\(\\boldsymbol{e}\\) would also be independent of the fitted values \\(X\\hat{\\boldsymbol{\\beta}}\\), and so the standardised residuals would also be independent of the fitted values.\nThis gives us a way to check the constant variance assumption: we plot standardised residuals against fitted values. If the assumption holds, we shouldn’t be able to see any pattern in the plot.\n\nplot(fitted(lmCars), s)\n\n\n\n\n\n\n\n\nThe most common form of heteroscedasticity arises when larger observations are also more variable. This often happens when the response variable is necessarily positive. In this case, if the response is just above 0 its variance is likely to be less than if the response was much larger since the response is bounded below by zero.\nWe would observe this kind of heteroscedasticity in the plot by seeing the residuals appearing to fan out as we move from left to right, and this does appear to be the case in the above example.\n\n\n\n\n\n\nTip\n\n\n\nThe fitted value is a scalar quantity, regardless of the number of independent variables. This plot may reveal problems with the model assumptions (homoscedasticity) that we can’t easily see in plots of the raw data, if we have large numbers of independent variables.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Checking model assumptions</span>"
    ]
  },
  {
    "objectID": "assumptions.html#formal-tests",
    "href": "assumptions.html#formal-tests",
    "title": "7  Checking model assumptions",
    "section": "7.5 Formal tests",
    "text": "7.5 Formal tests\nThe above residual-based plots offer an informal way of checking the fit of the linear model. We may construct more formal tests (note the difference between check and test) based on confidence intervals or hypothesis tests. Here, we just provide the most simple test, which is based on the assumption that if the model fit is good, then approximately the standardised residuals \\(s_i\\) follow a \\(t_{n-p}\\) distribution. We can then propose a test that would compare \\(s_i\\) with the quantile \\(t_{n-p, 1-\\alpha/2}\\), for some value of the significance level \\(\\alpha\\) as discussed below. If \\(|s_i|&gt;t_{n-p,1-\\alpha/2}\\), we would consider \\(s_i\\) to be too large for a \\(t_{n-p}\\) distribution and so we would consider the observation \\(y_i\\) to be an outlier.\nWe have multiple tests here: one per residual, and so the considerations from Section 6.1 apply. Noting that the standardised residuals are not independent, we use the Bonferroni correction (rather than Šidák) to give an indication of what should count as a large standardised residual.\nWithout applying the Bonferroni correction we would compare \\(|s_i|\\) with \\(t_{48;0.025}\\):\n\nqt(0.975, 48)\n\n[1] 2.010635\n\n\nIf we apply the Bonferroni correction we would have to use a significance level \\(0.05/50=0.001\\) which gives a new quantile \\(t_{48; 0.001/2}\\)\n\nqt(1-0.001/2, 48)\n\n[1] 3.505068\n\n\nIn the example, we’d note that the largest absolute standardised residual is greater than 2, but not greater than 3.5:\n\nmax(abs(s))\n\n[1] 2.91906\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\nExercise 7.1 Test the model assumptions for a one-way ANOVA model fitted to the cancer data\n\nlibrary(tidyverse)\ncancer &lt;- read_csv(\"https://oakleyj.github.io/exampledata/cancer.csv\")\nlmCancer &lt;- lm(survival ~ organ, cancer)\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nObtain the standardised residuals\n\ns &lt;- MASS::stdres(lmCancer)\n\nA Q-Q plot suggests non-normality\n\nqqnorm(s)\nabline(0, 1)\n\n\n\n\n\n\n\n\nPlotting fitted values against standardised residuals shows clear heteroscedasticity, with the error variance increasing as the fitted value increases.\n\nplot(lmCancer$fitted.values, s)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Checking model assumptions</span>"
    ]
  },
  {
    "objectID": "transformations.html",
    "href": "transformations.html",
    "title": "8  Transformations",
    "section": "",
    "text": "8.1 Transforming to restore homoscedasticity",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Transformations</span>"
    ]
  },
  {
    "objectID": "transformations.html#transforming-to-restore-homoscedasticity",
    "href": "transformations.html#transforming-to-restore-homoscedasticity",
    "title": "8  Transformations",
    "section": "",
    "text": "8.1.1 Theory\nWe usually transform the dependent variable when we suspect, either by doing diagnostic checks on residuals or for some other reason, that the model assumption of homoscedasticity does not hold. An alternative approach could be to try a Generalised Linear Model, which is often particularly suitable if there are reasons to suggest a distribution other than the Normal would be a better model; this will be covered in a later component of this module.\nSuppose that \\(Y\\) is a random variable whose variance depends on its mean, so that if \\(E(Y)=\\mu\\) then \\(Var(Y)=g(\\mu)\\), and the function \\(g\\) is known. (For example, a Poisson random variable has variance equal to its mean.) We seek a transformation from \\(Y\\) to \\(Z=f(Y)\\) such that the variance of \\(Z\\) is (approximately) constant. This is known as a variance-stabilizing transformation.\nConsider expanding \\(f\\) in a Taylor series about its mean. Keeping only the first order term, we have \\[\nz=f(y)\\approx f(\\mu)+(y-\\mu)f'(\\mu).\n\\] Now taking expectations of \\(Z=f(Y)\\), \\[\\begin{align}\nE(Z)&\\approx E(f(\\mu)+(Y-\\mu)f'(\\mu))\\\\\n&=f(\\mu)+(E(Y)-\\mu)f'(\\mu)\\\\\n&=f(\\mu)\n\\end{align}\\] and since \\(f'(\\mu)\\) is a scalar \\[\\begin{align}\nVar(Z)&\\approx Var(Yf'(\\mu))\\\\\n&=Var(Y)(f'(\\mu))^2\\\\\n&=g(\\mu)(f'(\\mu))^2.\n\\end{align}\\] We want this to be a constant, \\(k^2\\) say, so that \\(k^2=g(\\mu)f'(\\mu)^2\\). Therefore we seek a function \\(f\\) such that \\[\nf'(\\mu)=\\frac{k}{\\sqrt{g(\\mu)}}.\n\\] Therefore the solution is \\[\nf(\\mu)=k\\int (g(\\mu))^{-1/2}\\,d\\mu.\n\\]\n\nExample 8.1 Variance proportional to mean\n\nSuppose that the variance is proportional to the mean, so that \\(g(\\mu)=a\\mu\\). Then \\[\nf(\\mu)=ka^{-1/2}\\int \\mu^{-1/2}\\,d\\mu=2ka^{-1/2}\\sqrt{\\mu}+C.\n\\]\nSince \\(C\\), \\(k\\) and \\(a\\) are arbitrary constants, a variable of the form \\(Z=A\\sqrt{Y}+C\\) should have approximately constant variance, with the most obvious choice perhaps being \\(Z=\\sqrt{Y}\\). Whether it does depends on how accurate our estimation of the function \\(g\\) is (i.e. our assessment of how the variance of the response depends on the mean). If we are assessing this relationship by eye it may not be very accurate.\n\nExample 8.2 Standard deviation proportional to mean\n\nSuppose the standard deviation of \\(Y\\) is proportional to \\(\\mu\\), so that \\(g(\\mu)=a\\mu^2\\), then \\[\nf(\\mu)=ka^{-1/2}\\int \\mu^{-1}\\,d\\mu=ka^{-1/2} \\log \\mu+C.\n\\] The transformed variable \\(Z=\\log Y\\) should have approximately constant variance.\n\n\n8.1.2 Determining how the variance depends on the mean\nInspecting a plot of \\(e_i\\) against \\(\\hat{y}_i\\) is a common way to check for non-homoscedasticity. If we see a funnel shape with the spread of the residuals seeming to increase linearly with \\(\\hat{y}_i\\), then this suggests that the standard deviation of \\(y_i\\) is proportional to its mean. Then, Example 8.2 above suggests that we should use a log transformation.\nIf we see signs of increasing standard deviation, but the growth flattens out as \\(y_i\\) increases, this suggests something more like the variance being proportional to the mean. Then Example 8.1 above suggests using a square root transformation of the dependent variable.\nOther patterns might suggest other kinds of \\(g(\\mu)\\), although in practice it is hard to distinguish anything more subtle unless we have a large data set.\nRemember that in terms of the transformed variable, what may have been a linear relationship between \\(y\\) and the regressor variables becomes a nonlinear relationship between the new \\(z\\) and the same regressor variables. This can mean that although a linear model for \\(z\\) better satisfies the assumption of homoscedasticity we cannot represent \\(z\\) so well through linear combinations of regressor variables. So removing a problem of heteroscedasticity can introduce other problems.\nFortunately, in practice the reverse is often true. We frequently find that after a suitable variance-stabilizing transformation the relationship between the dependent variable and regressor variables actually simplifies. Indeed, the transformation often also improves normality.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Transformations</span>"
    ]
  },
  {
    "objectID": "transformations.html#estimating-a-transformation",
    "href": "transformations.html#estimating-a-transformation",
    "title": "8  Transformations",
    "section": "8.2 Estimating a transformation",
    "text": "8.2 Estimating a transformation\n\n8.2.1 Families of transformations\nIn a situation where we cannot easily determine how the variance of the response depends on the mean by eye, a formal statistical approach to transformations is to try to estimate what transformation to use.\nSuppose that we have a family of transformations \\(f_\\lambda\\) indexed by a parameter \\(\\lambda\\). So for any given \\(\\lambda\\) we could define the new dependent variable \\(z_\\lambda=f_\\lambda(y)\\). Suppose that we believe that there is some value of \\(\\lambda\\) for which \\(z_\\lambda\\) follows a linear model in which all the assumptions are true. Then we have a model \\[\nf_\\lambda(y_i)=\\boldsymbol{x}_i^T\\boldsymbol{\\beta}+\\varepsilon_i\n\\] where the \\(\\varepsilon_i\\)’s are independent \\(N(0,\\sigma^2)\\). This is a model with parameters \\(\\boldsymbol{\\beta}\\), \\(\\sigma^2\\) and \\(\\lambda\\).\n\n\n8.2.2 Maximum likelihood\nWe can estimate \\(\\lambda\\) by maximum likelihood. To calculate the likelihood we have to remember that we do not observe \\(f_\\lambda(y_i)\\), which depends on the unknown parameter \\(\\lambda\\). We observe the \\(y_i\\), and the likelihood should be the joint probability density function of the \\(y_i\\) given the parameters, and this entails bringing in the derivative of the transformation. The likelihood is \\[\nL(\\boldsymbol{\\beta},\\sigma^2,\\lambda;\\boldsymbol{y}) \\propto \\sigma^{-n}\n\\exp\\left\\{ -\\frac{1}{2\\sigma^2}\\sum_{i=1}^n\n\\left(f_\\lambda(y_i)-\\boldsymbol{x}_i^T\\boldsymbol{\\beta}\\right)^2\\right\\}\n\\prod_{i=1}^n f_\\lambda'(y_i).\n\\]\nWe already know how to maximize the likelihood with respect to and \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\), for fixed \\(\\lambda\\). We first choose \\(\\boldsymbol{\\hat{\\beta}}\\) to minimize the sum of squares in the exponent. The minimal value is the residual sum of squares, which we now denote by \\[\nS_\\lambda:=\\sum_{i=1}^n\n\\left(f_\\lambda(y_i)-\\boldsymbol{x}_i^T\\hat{\\boldsymbol{\\beta}}\\right)^2\n\\] to show that it depends on \\(\\lambda\\). Then the maximization with respect to \\(\\sigma^2\\) yields \\(\\hat{\\sigma}_\\lambda^2=S_\\lambda/n\\). Hence the exponent term in the likelihood, once maximised with respect to \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) is\n\\[\n\\exp\\left\\{ -\\frac{1}{2\\hat{\\sigma}^2}\\sum_{i=1}^n\n\\left(f_\\lambda(y_i)-\\boldsymbol{x}_i^T\\hat{\\boldsymbol{\\beta}}\\right)^2\\right\\} = \\exp\\left\\{-\\frac{n}{2S_{\\lambda}} S_{\\lambda}\\right\\},\n\\] which is constant for all \\(\\lambda\\).\nWe now have \\[\nL(\\boldsymbol{\\hat{\\beta}}_\\lambda,\\hat{\\sigma}_\\lambda^2,\\lambda;\\boldsymbol{y})\n\\propto S_\\lambda^{-n/2} \\prod_{i=1}^n f_\\lambda'(y_i).\n\\] The MLE of \\(\\lambda\\) is now obtained by maximising \\(L(\\boldsymbol{\\hat{\\beta}}_\\lambda,\\hat{\\sigma}_\\lambda^2,\\lambda;\\boldsymbol{y})\\). In practice it is usually easier to maximise its logarithm \\[\n\\ell(\\boldsymbol{\\hat{\\beta}}_\\lambda,\\hat{\\sigma}_\\lambda^2,\\lambda;\\boldsymbol{y})\n= c-\\frac{n}{2}\\log S_\\lambda +\\sum_{i=1}^n \\log f_\\lambda'(y_i),\n\\] where \\(c\\) is a constant (not depending on \\(\\lambda\\)) and can be ignored when maximizing.\n\n\n8.2.3 The Box-Cox family\nWhilst the above theory works for any family of transformations, by far the most popular family is the family of power transformations, also known as the Box-Cox family. Box and Cox (1964) proposed a family of transformations on the response variable to correct non-normality and/or non-constant variance. If the response is positive, the transformation is given by \\[\nf_\\lambda(y)=\\left\\{\\begin{array}{ll}(y^\\lambda-1)/\\lambda, &\n\\lambda\\neq 0 \\\\ \\log y, & \\lambda=0\\end{array}\\right.\n\\] It can be shown that \\(f_\\lambda(y)\\) is a continuous function of \\(\\lambda\\). We assume that the transformed values of \\(Y\\) are normally distributed about a linear combination of the covariates with constant variance, that is \\(f_\\lambda(Y_i)\\sim N(\\boldsymbol{x}_i^T\\boldsymbol{\\beta},\\sigma^2)\\) for some unknown value of \\(\\lambda\\). Hence, the problem is to estimate \\(\\lambda\\) as well as the parameter vector \\(\\boldsymbol{\\beta}\\).\n\n\n8.2.4 Obtaining a Box-Cox transformation\nWe now consider using maximum likelihood methods to investigate the most appropriate Box-Cox transformation to use for the simulated data set.\nWe have \\(f_\\lambda(y)=(y^\\lambda-1)/\\lambda\\) so that \\(f_\\lambda '(y)=y^{\\lambda-1}\\) and the log likelihood takes the form \\[\n\\ell(\\boldsymbol{\\hat{\\beta}}_\\lambda,\\hat{\\sigma}_\\lambda^2,\\lambda;\\boldsymbol{y})\n=c-\\frac{n}{2}\\log S_\\lambda +\\sum_{i=1}^n \\log f_\\lambda'(y_i)= c-\\frac{n}{2}\\log S_\\lambda+(\\lambda-1)ny_L,\n\\] where \\(ny_L\\) is the sum of the logarithms of the observations, \\(ny_L=\\sum_{i=1}^n \\log y_i\\) and depends only on the values of the response.\n\n\n\n\n\n\nExercise\n\n\n\n\nExercise 8.1 The residual sum of squares of a fitted model can be obtained using the anova command, e.g\n\nlmCars &lt;- lm(dist ~ speed, cars)\nanova(lmCars)\n\nAnalysis of Variance Table\n\nResponse: dist\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nspeed      1  21186 21185.5  89.567 1.49e-12 ***\nResiduals 48  11354   236.5                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nso the residual sum of squares is 11354.\nBy fitting the models\nlm(log(dist) ~ speed, cars)\nlm(sqrt(dist) ~ speed, cars)\nand using the anova() command to find the residual sum of squares, compare the likelihood values for \\(\\lambda = 0, 0.5\\) and 1, corresponding to a log transformation, a square-root transformation, and no transformation of the dependent variable.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe are given \\(S_1=11354\\) (the residual sum of squares for no transformation). In R, we do\n\nlmCarsLog &lt;- lm(log(dist) ~ speed, cars)\nlmCarsSqrt &lt;- lm(sqrt(dist) ~ speed, cars)\nanova(lmCarsLog)\n\nAnalysis of Variance Table\n\nResponse: log(dist)\n          Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nspeed      1 19.9804 19.9804   100.3 2.413e-13 ***\nResiduals 48  9.5621  0.1992                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(lmCarsSqrt)\n\nAnalysis of Variance Table\n\nResponse: sqrt(dist)\n          Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nspeed      1 142.411 142.411  117.18 1.773e-14 ***\nResiduals 48  58.334   1.215                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\ngiving \\(S_0 = 9.5621\\) and \\(S_{0.5} = 58.334\\). Computing the three log-likelihood values (ignoring constants), we find\n\n-50/2*log(11354)\n\n[1] -233.4331\n\n-50/2*log(9.5621) - sum(log(cars$dist))\n\n[1] -233.2406\n\n-50/2*log(58.334) - 0.5 * sum(log(cars$dist))\n\n[1] -190.0523\n\n\nso we have \\[\\begin{align}\n\\ell(\\boldsymbol{\\hat{\\beta}}_\\lambda,\\hat{\\sigma}_\\lambda^2,\\lambda = 1;\\boldsymbol{y})\n&= -233.4,\\\\\n\\ell(\\boldsymbol{\\hat{\\beta}}_\\lambda,\\hat{\\sigma}_\\lambda^2,\\lambda = 0;\\boldsymbol{y})\n&= -233.2,\\\\\n\\ell(\\boldsymbol{\\hat{\\beta}}_\\lambda,\\hat{\\sigma}_\\lambda^2,\\lambda = 1;\\boldsymbol{y})\n&= -190.1.\n\\end{align}\\]\nThis suggests that the square root transformation is the best out of the three, and the residual plot does look a little better in this case:\n\ns &lt;- MASS::stdres(lmCarsSqrt)\nplot(lmCarsSqrt$fitted.values, s )\n\n\n\n\n\n\n\n\n\n\n\nIn R, we can use the MASS::boxcox() function to plot the profile-likelihood for \\(\\lambda\\):\n\nMASS::boxcox(lm(dist ~ speed, cars),\n       lambda=seq(0,1,1/20),\n       plotit=TRUE,\n       xlab=expression(lambda),\n       ylab=\"Log-likelihood\")\n\n\n\n\n\n\n\n\nIf we change the plotit argument to FALSE, we can extract the values in the plot to find the maximum\n\nbc &lt;- MASS::boxcox(lm(dist ~ speed, cars),\n                   lambda = seq(0,1,1/20),\n                   plotit = FALSE)\nbc\n\n$x\n [1] 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65 0.70\n[16] 0.75 0.80 0.85 0.90 0.95 1.00\n\n$y\n [1] -56.44525 -54.44791 -52.71917 -51.25976 -50.06651 -49.13255 -48.44768\n [8] -47.99897 -47.77145 -47.74884 -47.91427 -48.25086 -48.74223 -49.37287\n[15] -50.12839 -50.99563 -51.96275 -53.01918 -54.15560 -55.36382 -56.63671\n\nbc$x[which.max(bc$y)]\n\n[1] 0.45\n\n\nsuggesting an optimal \\(\\lambda\\) around 0.45.\n\n\n\n\n\n\nExercise\n\n\n\n\nExercise 8.2 Find a suitable Box-Cox transformation for the one-way ANOVA model for the cancer data. Verify that the transformation helps with the model assumptions.\n\nlibrary(tidyverse)\ncancer &lt;- read_csv(\"https://oakleyj.github.io/exampledata/cancer.csv\")\nlmCancer &lt;- lm(survival ~ organ, cancer)\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nA bit of trial and error is needed to find an appropriate range of \\(\\lambda\\) values\n\nMASS::boxcox(lmCancer,\n             lambda = seq(-0.5, 0.5 ,1/20),\n             plotit = TRUE,\n             xlab = expression(lambda),\n             ylab = \"Log-likelihood\")\n\n\n\n\n\n\n\n\nTo get the optimal value\n\nbc &lt;- MASS::boxcox(lmCancer,\n                   lambda = seq(-0.5, 0.5 ,1/20),\n                   plotit = FALSE)\nbc\n\n$x\n [1] -0.50 -0.45 -0.40 -0.35 -0.30 -0.25 -0.20 -0.15 -0.10 -0.05  0.00  0.05\n[13]  0.10  0.15  0.20  0.25  0.30  0.35  0.40  0.45  0.50\n\n$y\n [1] -161.3377 -158.2974 -155.4732 -152.8754 -150.5141 -148.3997 -146.5426\n [8] -144.9531 -143.6412 -142.6167 -141.8887 -141.4654 -141.3543 -141.5614\n[15] -142.0913 -142.9471 -144.1297 -145.6383 -147.4701 -149.6200 -152.0811\n\nbc$x[which.max(bc$y)]\n\n[1] 0.1\n\n\nWe could use the optimal value, though it’s quite close to 0 (a log transformation), so we’ll just try that:\n\nlmCancerLog &lt;- lm(log(survival) ~ organ, cancer)\ns &lt;- MASS::stdres(lmCancerLog)\nplot(lmCancerLog$fitted.values, s)\n\n\n\n\n\n\n\n\nWe no longer have the funnel shape in the plot, as we would hope.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Transformations</span>"
    ]
  },
  {
    "objectID": "modelSelection.html",
    "href": "modelSelection.html",
    "title": "9  Model Selection",
    "section": "",
    "text": "9.1 Inference versus prediction\nWe have used hypothesis testing for inference: learning about the process that generated the data, in particular, whether or not particular variables are associated with the dependent variable of interest.\nA related, but different problem is to find the best statistical model we can for predicting the value of the dependent variable on any occasion. Rather than understanding the data we have, the interest is in how the model would perform in predicting new data. Hence for inference, we can do our analysis on the whole dataset, but for prediction, if we want to evaluate the performance of our model, we might need to hold back some of the data for testing.\nHypothesis testing can still play a role in the prediction problem: if we find a particular term in a model is not significantly different from zero, we wouldn’t expect it to be useful for the prediction task, but hypothesis testing cannot compare non-nested models\nFor example, suppose we want to know which of the two following models is best for predicting the \\(Y\\) variable: \\[\nY_i = \\beta_0 + \\beta_1 x_i+\\varepsilon_i,\n\\] or \\[\nY_i = \\alpha_0 + \\alpha_1 z_i+\\delta_i,\n\\] We might want to choose one if there is a cost to obtaining each of the \\(x\\) and \\(z\\) variables, and we want to minimise the cost. The models are not nested, so we cannot make the choice using a hypothesis test.\nWith large numbers of independent variables, it is more likely we would want to compare non-nested models. There are also computational issues if there is a large number of candidate models to compare, and if we have \\(p\\) candidate regressor variables and \\(n\\) observations, with \\(n&lt;p\\), we may not even be able to fit a model with all regressor variables included.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "modelSelection.html#bias-variance-decomposition-and-trade-offs",
    "href": "modelSelection.html#bias-variance-decomposition-and-trade-offs",
    "title": "9  Model Selection",
    "section": "9.2 Bias-variance decomposition and trade-offs",
    "text": "9.2 Bias-variance decomposition and trade-offs\n\n\n\n\n\n\nWarning\n\n\n\nIt is possible for a more complicated model to give worse predictions than a simpler one!\n\n\nConsider predicting some future observation \\(Y^*\\) given \\(\\boldsymbol{x}^*\\), which we write as \\[\nY^* = f(\\boldsymbol{x}^*) + \\varepsilon^*,\n\\] where \\(\\mathbb{E}(\\varepsilon^*)=0\\) and \\(Var(\\varepsilon^*) = 0\\). We predict \\(Y^*\\) with \\(\\hat{f}(\\boldsymbol{x}^*)\\), where \\(\\hat{f}(\\boldsymbol{x}^*)\\) is our estimate of \\(f(\\boldsymbol{x}^*)\\), and we write \\(\\hat{f}:=\\hat{f}(\\boldsymbol{x}^*)\\) and \\(f:=f(\\boldsymbol{x}^*)\\) for short. For a linear model, we would normally use least squares to obtain \\(\\hat{f}\\).\nBefore observing the data for which we will obtain \\(\\hat{f}\\), we consider the expected squared error of our estimator:\n\\[\n\\mathbb{E}((Y^*-\\hat{f})^2) = \\mathbb{E}((f+\\varepsilon^*-\\hat{f})^2).\n\\] The expectation is with respect to both the random error \\(\\varepsilon^*\\) and the random data used to obtain \\(\\hat{f}\\). We now define \\[\n\\mu_f:=\\mathbb{E}(\\hat{f})\n\\] to be the expected value of the estimator \\(\\hat{f}\\), so that if \\(\\mu_f - f\\) is the bias of the estimator \\(\\hat{f}\\). Then \\[\\begin{align}\n\\mathbb{E}((f+\\varepsilon^* - \\hat{f})^2)& = \\\\\n& = \\mathbb{E}((\\varepsilon^* + f-\\mu_f+ \\mu_f- \\hat{f})^2)\\\\\n& = \\mathbb{E}({\\varepsilon^*}*^2) + \\mathbb{E}((f-\\mu_f))^2) + \\mathbb{E}((\\hat{f}-\\mu_f)^2)\\\\\n&= \\sigma^2 + \\mbox{bias}^2 + Var(\\hat{f}).\n\\end{align}\\]\n\n\n\n\n\n\nNote\n\n\n\nTo derive this result, note that (1) the random terms in \\((f+\\varepsilon^* - \\hat{f})^2\\) are \\(\\varepsilon^*\\) and \\(\\hat{f}\\), (2) that \\(\\varepsilon^*\\) is independent of \\(\\hat{f}\\) and has expectation 0, and (3), that \\[\n\\mathbb{E}((f-\\mu_f)(\\mu_f- \\hat{f})) = (f-\\mu_f)(\\mu_f- \\mathbb{E}(\\hat{f}))  = 0,\n\\] as \\(\\mu_f:=\\mathbb{E}(\\hat{f})\\).\n\n\nHence there are three terms that contribute to the expected squared prediction error for a future observation:\n\nthe (‘irreducible’) random error term corresponding to that observation;\nthe bias (squared) in estimating \\(f\\) with \\(\\hat{f}\\);\nthe variance of the estimator \\(\\hat{f}\\).\n\nMore complex models tend to have smaller bias, but larger variance: smaller bias because the additional complexity means we can the model can get closer to the true function \\(f\\); larger variance because there will typically be more parameters to estimate and so \\(\\hat{f}\\) is more sensitive to variation in the data.\nFor prediction purposes, we should consider this trade-off between bias and variance, given that they both contribute to prediction error, and note that a simpler model can be preferable to a more complex one, if the reduction in variance outweighs the increase in bias.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "modelSelection.html#measuring-goodness-of-fit-and-penalising-for-model-complexity",
    "href": "modelSelection.html#measuring-goodness-of-fit-and-penalising-for-model-complexity",
    "title": "9  Model Selection",
    "section": "9.3 Measuring goodness of fit and penalising for model complexity",
    "text": "9.3 Measuring goodness of fit and penalising for model complexity\nFor choosing between different models, various measures of goodness of fit have been proposed, that include a penalty for the complexity of the model.\n\n9.3.1 The adjusted \\(R^2\\) statistic\nWe introduced this in Chapter 2, but will recap here. For \\(n\\) observations and \\(p\\) parameters in the model (i.e. \\(\\boldsymbol{\\beta}\\) is a \\(p\\times 1\\) vector)\n\\[\nR^2(adj):=1-\\frac{RSS}{SS_{total}}\\times\\frac{n-1}{n-p}\n\\] where \\(RSS= \\sum_{i=1}^n(y_i - \\hat{y}_i)^2\\) is the residual sum of squares, and \\(SS_{total}= \\sum_{i=1}^n(y_i - \\bar{y})^2\\) is the total sum of squares.\n\\(R^2(adj)\\) increases (to an upper bound of 1) as the goodness of fit improves. Smaller squared residuals imply a better fitting model and increase \\(R^2(adj)\\), but \\(R^2(adj)\\) is reduced as the number of parameters \\(p\\) increases.\n\n\n9.3.2 AIC and BIC\nThere are general purpose goodness of fit measures that involve the maximised log-likelihood: AIC and BIC are two of them. Informally, the likelihood is the ‘probability’ of observing the data we got, as a function of the model parameters, so the higher we can make this (by maximising the likelihood or log-likelihood), the closer the data would ‘fit’ the model.\nDenote the maximised log-likelihood by \\(\\hat{l}\\). Then Akaike’s Information Criterion (AIC) is defined as\n\\[\nAIC:= -2\\hat{l} + 2p\n\\] and the Bayesian Information Criterion (BIC) is defined as \\[\nBIC: = -2\\hat{l} + n\\log p\n\\]\nNote that the lower the AIC and BIC values, the better the model fit. We see that AIC and BIC differ in the penalty applied for model complexity: the penalty of \\(n\\log p\\) in BIC grows with sample size, so BIC is more likely to favour simpler models.\nUnder various assumptions and approximations, AIC and BIC can be interpreted as estimating different quantities. The details are outside the scope of this module, but in brief AIC approximates how well the fitted model predicts a new observation, as measured by the Kullback-Liebler divergence between the true distribution of a new observation and the distribution in the fitted model. BIC approximates the log marginal likelihood of the observed data (the likelihood, averaged over a prior distribution for the model parameters.)\nWe can obtain AIC and BIC values with the commands AIC() and BIC() on fitted models in R.\n\n\n9.3.3 Example\nWe’ll use the respiration data again, but recode the data to make it easier to understand\n\nlibrary(tidyverse)\nrespiration &lt;- read_table(\"https://oakleyj.github.io/exampledata/resp_data.txt\")\nrespiration$home &lt;- factor(respiration$home)\nrespiration$smoke &lt;- factor(respiration$smoke)\nrespiration$asthma &lt;- factor(respiration$asthma)\nrespiration$asbestos &lt;- factor(respiration$asbestos)\n\nrespiration &lt;- respiration %&gt;%\n  mutate(\n    home = fct_recode(home,\n      England = \"1\",\n      Scotland = \"2\", \n      Wales = \"3\", \n      Northern_Ireland = \"4\"\n    ),\n    smoke = fct_recode(smoke,\n                       Yes = \"1\",\n                       No = \"0\"\n                       ),\n    asbestos = fct_recode(asbestos,\n                       Yes = \"1\",\n                       No = \"0\"\n    ),\n    asthma = fct_recode(asthma,\n                       Yes = \"1\",\n                       No = \"0\"\n    ),\n    \n  )\nhead(respiration)\n\n# A tibble: 6 × 8\n    vol exercise pulse pollution asbestos home             smoke asthma\n  &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;fct&gt;    &lt;fct&gt;            &lt;fct&gt; &lt;fct&gt; \n1  117.     15.8  17.5     11.9  Yes      England          Yes   No    \n2  148.     23.5  28.4      1.88 Yes      Wales            Yes   Yes   \n3  214.     13.8  15.0      2.38 Yes      England          No    No    \n4  162.     15.6  15.7      6.34 Yes      Wales            No    Yes   \n5  352.     26.6  19.7      1.63 No       Northern_Ireland No    No    \n6  304.     23.2  14.2      2.72 No       Northern_Ireland No    No    \n\n\nIf we fit a model with all the independent variables, we can see that both smoke and asthma are significant, but the effect size is larger for smoke.\n\nlmFull &lt;- lm(vol ~ ., respiration)\nsummary(lmFull)\n\n\nCall:\nlm(formula = vol ~ ., data = respiration)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-132.420  -19.206   -0.003   18.142  178.152 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           179.7789     7.7125  23.310  &lt; 2e-16 ***\nexercise                4.0844     0.4706   8.680 2.74e-16 ***\npulse                  -0.3311     0.3754  -0.882    0.379    \npollution               0.9322     1.0816   0.862    0.389    \nasbestosYes            -5.2549     4.2059  -1.249    0.213    \nhomeScotland            5.1136     5.6024   0.913    0.362    \nhomeWales               0.8963     5.6285   0.159    0.874    \nhomeNorthern_Ireland    5.7756     5.4727   1.055    0.292    \nsmokeYes             -120.9721     3.9683 -30.485  &lt; 2e-16 ***\nasthmaYes             -49.8651     4.0839 -12.210  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 33.87 on 295 degrees of freedom\nMultiple R-squared:  0.8246,    Adjusted R-squared:  0.8192 \nF-statistic: 154.1 on 9 and 295 DF,  p-value: &lt; 2.2e-16\n\n\nIf we were to compare two models, one using smoke only and one using asthma only as the independent variable, we might expect the former to fit better as its coefficient is larger. We’ll compare \\(R^2_{adj}\\), AIC and BIC values to confirm this.\n\nlmSmoke &lt;- lm(vol ~ smoke, respiration)\nlmAsthma &lt;- lm(vol ~ asthma, respiration)\n\n\nsummary(lmSmoke)$adj.r.squared\n\n[1] 0.605248\n\nsummary(lmAsthma)$adj.r.squared\n\n[1] 0.1175858\n\n\n\nAIC(lmSmoke)\n\n[1] 3256.512\n\nAIC(lmAsthma)\n\n[1] 3501.855\n\n\n\nBIC(lmSmoke)\n\n[1] 3267.673\n\nBIC(lmAsthma)\n\n[1] 3513.016",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "modelSelection.html#constrainedpenalised-regression",
    "href": "modelSelection.html#constrainedpenalised-regression",
    "title": "9  Model Selection",
    "section": "9.4 Constrained/penalised regression",
    "text": "9.4 Constrained/penalised regression\nWe will finish with a brief mention of some alternative methods for model fitting/selection that attempt to reduce prediction errors by reducing model complexity. We will just outline some methods, and then illustrate how to implement them in the Chapter appendix.\n\n\n\n\n\n\nNote\n\n\n\nThis really is a very brief introduction to the topic! The aim is just to get you started if you want to experiment with any of these methods at some later point. You will not be assessed on any of the following.\n\n\nThe idea is to minimise the residual sum of squares, subject to a constraint on the complexity of the model. We suppose there is a ‘full model’ we are willing to consider in matrix form \\[\n\\boldsymbol{Y} = X\\boldsymbol{\\beta}+\\boldsymbol{\\varepsilon}.\n\\] Here, we write \\[\n\\boldsymbol{\\beta} = \\left(\\begin{array}{c}\\beta_0\\\\\\beta_1\\\\\\vdots\\\\ \\beta_p\\end{array}\\right)\n\\]\nand suppose that the first column of \\(X\\) a column of 1s, so that the first element \\(\\beta_0\\) of \\(\\boldsymbol{\\beta}\\) is just an intercept term. Define \\(\\boldsymbol{y}\\) to be the observed value of \\(\\boldsymbol{Y}\\).\nFor ordinary least squares, we have \\[\n\\hat{\\boldsymbol{\\beta}} = \\arg\\min_{\\boldsymbol{\\beta}}\\left((\\boldsymbol{y} - X\\boldsymbol{\\beta})^T(\\boldsymbol{y} - X\\boldsymbol{\\beta})\\right),\n\\] which we can solve analytically to get \\(\\hat{\\boldsymbol{\\beta}} = (X^TX)^{-1}X^T\\boldsymbol{y}\\).\nAn alternative approach is, given the same model \\(\\boldsymbol{Y} = X\\boldsymbol{\\beta}+\\boldsymbol{\\varepsilon}\\), place some constraint on \\(\\boldsymbol{\\beta}\\) when minimising \\((\\boldsymbol{y} - X\\boldsymbol{\\beta})^T(\\boldsymbol{y} - X\\boldsymbol{\\beta})\\). We will give three possibilities below.\n\n9.4.1 Best subset selection\nIn this approach, we find the best fitting model (using least squares) out of all models with some reduced number \\(k\\) of regressors:\n\\[\n\\hat{\\boldsymbol{\\beta}}_{S} := \\arg\\min_{\\boldsymbol{\\beta}}\\left((\\boldsymbol{y} - X\\boldsymbol{\\beta})^T(\\boldsymbol{y} - X\\boldsymbol{\\beta})\\right), \\quad\\mbox{ subject to } \\left(\\sum_{j=1}^p I(\\beta_j\\neq 0)\\right) = k.\n\\] i.e., in addition to the intercept, only \\(k&lt;p\\) elements of \\(\\boldsymbol{\\beta}\\) are allowed to be non-zero.\n\n\n9.4.2 Ridge regression\nRather than forcing elements of \\(\\boldsymbol{\\beta}\\) to be zero, we can shrink them towards zero. This introduces bias into \\(\\hat{\\boldsymbol{\\beta}}\\), but reduces the variance, potentially with an overall reduction in expected prediction error. We have\n\\[\n\\hat{\\boldsymbol{\\beta}}_{R}: = \\arg\\min_{\\boldsymbol{\\beta}}\\left((\\boldsymbol{y} - X\\boldsymbol{\\beta})^T(\\boldsymbol{y} - X\\boldsymbol{\\beta})\\right), \\quad\\mbox{ subject to } \\left(\\sum_{j=1}^p \\beta_j^2\\right)\\le t\n\\]\n\n\n\n\n\n\nWarning\n\n\n\nFor the constraint to make sense, columns 2 to \\(p\\) of \\(X\\) need to be on the same scale, e.g. transformed to be on the range \\([-1, 1]\\).\n\n\nNote that \\(\\hat{\\boldsymbol{\\beta}}_{R}\\) can be re-expressed as\n\\[\n\\hat{\\boldsymbol{\\beta}}_R = \\arg\\min_{\\boldsymbol{\\beta}}\\left( (\\boldsymbol{y} - X\\boldsymbol{\\beta})^T(\\boldsymbol{y} - X\\boldsymbol{\\beta}) +\\lambda \\sum_{j=1}^p \\beta_j^2\\right).\n\\] We can now see that ridge regression is a form of penalised least squares: we are minimising the sum of the squared errors plus an additional penalty for the size of the regression coefficients. Informally, this can also be thought of similar to a Bayesian approach, where each \\(\\beta_j\\) coefficient is given a \\(N(0,\\lambda)\\) prior.\n\n\n9.4.3 Lasso\nLasso is short for “least absolute shrinkage and selection operator”. It’s similar to ridge regression, but with a different penalty. We find\n\\[\n\\hat{\\boldsymbol{\\beta}}_{L}: = \\arg\\min_{\\boldsymbol{\\beta}}\\left((\\boldsymbol{y} - X\\boldsymbol{\\beta})^T(\\boldsymbol{y} - X\\boldsymbol{\\beta})\\right), \\quad\\mbox{ subject to } \\left(\\sum_{j=1}^p |\\beta_j|\\right)\\le t\n\\]\nwhich can be re-expressed as \\[\n\\hat{\\boldsymbol{\\beta}}_L = \\arg\\min_{\\boldsymbol{\\beta}}\\left( (\\boldsymbol{y} - X\\boldsymbol{\\beta})^T(\\boldsymbol{y} - X\\boldsymbol{\\beta}) +\\lambda \\sum_{j=1}^p |\\beta_j|\\right).\n\\] Note that the independent variables need transforming such that the columns of \\(X\\) are on the scale.\n\n\n\n\n\n\nNote\n\n\n\nLasso also ‘shrinks’ the \\(\\beta\\) coefficients towards 0, but more ‘aggressively’, as for \\(|\\beta_j|&lt;1\\) the penalty \\(\\lambda|\\beta_j|\\) is greater than the penalty \\(\\lambda\\beta_j^2\\). This has the effect of performing variable selection.\n\n\n\n\n9.4.4 Parameter choices\nFor the best subset selection method, we have to choose \\(k\\). Each value of \\(k\\) results in a particular ‘best’ model as found by least squares; we might choose the best overall \\(k\\) value by using any goodness of fit measure which includes a penalty for model complexity, such as AIC, BIC etc.\nFor ridge regression and the Lasso, we have to choose a parameter \\(\\lambda\\). One approach is to use cross-validation which involves removing a subset of observations, fitting the model to the remainder, and then predicting the observations we removed. We won’t give the details here, but this topic is covered in the Computational Methods course (second half of MAS61006)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "modelSelection.html#chapter-appendix",
    "href": "modelSelection.html#chapter-appendix",
    "title": "9  Model Selection",
    "section": "9.5 Chapter appendix",
    "text": "9.5 Chapter appendix\nWe outline how to implement each of the above penalised regression methods in R\n\n9.5.1 Best subset selection with leaps\nBest subset selection can be implemented with the leaps package 1. The following command will find the best fitting regression model for \\(k\\) taking values from 1 up to nvmax\n\nlmBestSubset &lt;- leaps::regsubsets(x = vol ~ ., data = respiration,\n                                  nvmax = 9)\n\nThe terms included for each \\(k\\) can be visualised with the following plot. We would look to see which terms are included in the top row, as this has the smallest BIC:\n\nplot(lmBestSubset)\n\n\n\n\n\n\n\n\nWe can extract the BIC values and best model as follows:\n\nbestSubsetResults &lt;- summary(lmBestSubset)\nbestSubsetResults$bic\n\n[1] -273.0611 -383.8430 -503.3554 -499.1177 -494.2794 -489.3284 -484.3395\n[8] -479.3691 -473.6750\n\nwhich.min(bestSubsetResults$bic)\n\n[1] 3\n\n\nSo the best overall model (according to BIC) is achieved with \\(k=3\\) and includes the following terms:\n\nbestSubsetResults$which[3,]\n\n         (Intercept)             exercise                pulse \n                TRUE                 TRUE                FALSE \n           pollution          asbestosYes         homeScotland \n               FALSE                FALSE                FALSE \n           homeWales homeNorthern_Ireland             smokeYes \n               FALSE                FALSE                 TRUE \n           asthmaYes \n                TRUE \n\n\n\n\n9.5.2 LASSO and Ridge Regression with glmnet\nLasso and ridge regression can be implemented with the glmnet package 2.\nWe first fit the full model and extract the design matrix, excluding the intercept column.\n\nlmFull &lt;- lm(vol ~ ., respiration)\nX &lt;- model.matrix(lmFull)[, -1]\ny &lt;- respiration$vol\n\nNote that we don’t need to standardise the columns of X: glmnet will do this for us. There is a single function we can use for both ridge regression and the lasso; an argument alpha determines which penalty is used. The function will automatically try a range of suitable \\(\\lambda\\) values.\n\nlmLasso &lt;- glmnet::glmnet(x = X, y = y, alpha = 1) # alpha = 1 for the lasso\nlmRidge &lt;- glmnet::glmnet(x = X, y = y, alpha = 0) # alpha = 0 for ridge regression\n\nWe can see the shrinkage of the coefficients towards 0 as \\(\\lambda\\) increases. The numbers at the top show the number of non-zero \\(\\beta_j\\) coefficients. For the lasso we do\n\nplot(lmLasso, xvar =  \"lambda\")\n\n\n\n\n\n\n\n\nSimilarly for ridge regression:\n\nplot(lmRidge, xvar =  \"lambda\")\n\n\n\n\n\n\n\n\nTo choose a particular value of \\(\\lambda\\), we do cross-validation. This is implemented as follows.\n\nlmLassoCV &lt;- glmnet::cv.glmnet(x = X, y = y, alpha = 1)\nlmRidgeCV &lt;- glmnet::cv.glmnet(x = X, y = y, alpha = 0)\n\nA recommendation is to choose \\(\\lambda\\) that gives a mean-squared prediction error within one standard error of the minimum. This is indicated by the right hand vertical dotted line in the following plot.\n\nplot(lmLassoCV)\n\n\n\n\n\n\n\n\nand extracted as follows:\n\nlmLassoCV$lambda.1se\n\n[1] 8.779945\n\n\nSimilarly for ridge regression.\n\nplot(lmRidgeCV)\n\n\n\n\n\n\n\n\n\nlmRidgeCV$lambda.1se\n\n[1] 20.76009\n\n\nFinally, we compare coefficients using ordinary least square, the lasso, and ridge regression.\n\ncbind(coef(lmFull),\n      coef(lmLassoCV, lmLassoCV$lambda.1se), \n      coef(lmRidgeCV, lmRidgeCV$lambda.1se))\n\n10 x 3 sparse Matrix of class \"dgCMatrix\"\n                                           s1          s1\n(Intercept)           179.7789115  187.494303 174.0328225\nexercise                4.0843780    2.604639   2.4790848\npulse                  -0.3311115    .          0.5988432\npollution               0.9322368    .          0.9345834\nasbestosYes            -5.2548552    .         -2.2433403\nhomeScotland            5.1136247    .          2.8885558\nhomeWales               0.8962507    .          0.9495367\nhomeNorthern_Ireland    5.7756180    .          4.6914726\nsmokeYes             -120.9720708 -104.327190 -96.4445966\nasthmaYes             -49.8651200  -32.166129 -40.3676352\n\n\nA . indicates a coefficient set to 0 in the lasso, which we can think of as variable selection. In this example, this is the same as the coefficients that are not significantly different from 0 in the full model. Note also how (intercept excluded), the coefficients are typically closer to 0 for the lasso and ridge regression models, compared to the full model.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "modelSelection.html#footnotes",
    "href": "modelSelection.html#footnotes",
    "title": "9  Model Selection",
    "section": "",
    "text": "Thomas Lumley based on Fortran code by Alan Miller (2020). leaps: Regression Subset Selection. R package version 3.1. https://CRAN.R-project.org/package=leaps↩︎\nFriedman J, Tibshirani R, Hastie T (2010). “Regularization Paths for Generalized Linear Models via Coordinate Descent.” Journal of Statistical Software, 33(1), 1-22. doi: 10.18637/jss.v033.i01 (URL: https://doi.org/10.18637/jss.v033.i01).↩︎",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "matrixAlgebra.html",
    "href": "matrixAlgebra.html",
    "title": "Appendix A — Matrix algebra essentials",
    "section": "",
    "text": "A.1 Rank of a matrix\nConsider the \\(p\\times q\\) matrix \\(A\\) \\[A = \\left( \\begin{array}{cccc} a_{11} & a_{12} & \\cdots & a_{1q}\n\\\\ a_{21} & a_{22} & \\cdots & a_{2q} \\\\ \\vdots & \\vdots & \\ddots &\n\\vdots \\\\ a_{p1} & a_{p2} & \\cdots & a_{pq}\n\\end{array}\\right) = \\left(\\begin{array}{cccc} \\mathbf{a}_1, & \\mathbf{a}_2, & \\ldots, &\n\\mathbf{a}_q\\end{array}\\right) = \\left(\n\\begin{array}{c} \\mathbf{b}_1 \\\\ \\mathbf{b}_2 \\\\ \\vdots \\\\ \\mathbf{b}_p\n\\end{array}\\right)\\] where \\(\\mathbf{a}_i\\) is the \\(i\\)-th column and \\(\\mathbf{b}_j\\) is the \\(j\\)-th row \\((i=1,\\ldots,q;j=1,\\ldots,p)\\).\nThe rank of \\(A\\) is a non-negative integer, written as \\(rank(A)\\), which gives the number of linearly independent columns or rows of \\(A\\). According to this \\[rank(A)=k, \\quad 0\\leq k\\leq \\min(p,q)\\] so that there are \\(k\\) independent row vectors of \\(A\\), i.e. \\[\\lambda_1 \\mathbf{b_1}+\\cdots+\\lambda_k\\mathbf{b}_k =\\mathbf{0} \\Rightarrow\n\\lambda_1=\\cdots=\\lambda_k=0\\] and there are \\(k\\) independent column vectors of \\(A\\), i.e. \\[\\mu_1 \\mathbf{a}_1+\\cdots+\\mu_k\\mathbf{a}_k=\\mathbf{0} \\Rightarrow\n\\mu_1=\\cdots=\\mu_k=0\\] It follows that there are exactly \\(k\\) linearly independent row vectors and \\(k\\) linearly independent column vectors in \\(A\\) and any \\(k+1\\) or larger row vectors or column vectors will be linearly dependent. Below we give a simple example.\nConsider the following matrix \\[A=\\left(\\begin{array}{ccc} 2 & 0 & -1 \\\\ 1 & -4 &\n1\\end{array}\\right)\\] What is the rank of \\(A\\)?\nWe have \\(p=2\\) and \\(q=3\\). There are only 2 row vectors and so \\[\\lambda_1\\mathbf{b}_1+\\lambda_2\\mathbf{b}_2=\\lambda_1\\left(\\begin{array}{c}\n2\\\\ 0\\\\ -1\\end{array}\\right) +\\lambda_2\\left(\\begin{array}{c} 1\\\\\n-4\\\\ 1\\end{array}\\right)=\\left(\\begin{array}{c} 0\\\\ 0\\\\\n0\\end{array}\\right)\\] leads to the linear system \\[\\begin{gathered}\n2\\lambda_1+\\lambda_2=0 \\\\ -4\\lambda_2=0\\\\ -\\lambda_1+\\lambda_2=0\n\\end{gathered}\\] which of course gives \\(\\lambda_1=\\lambda_2=0\\) and so the 2 row vectors of \\(A\\) are linearly independent and the rank of \\(A\\) is 2.\nNow let us consider the column vectors of \\(A\\). There are 3 column vectors and we have \\[\\mu_1\\mathbf{a}_1+\\mu_2\\mathbf{a}_2+\\mu_3\\mathbf{a}_3=\\mu_1\\left(\\begin{array}{c}\n2\\\\ 1\\end{array}\\right) +\\mu_2\\left(\\begin{array}{c} 0\\\\\n-4\\end{array}\\right) +\\mu_3\\left(\\begin{array}{c} -1\\\\\n1\\end{array}\\right)=\\left(\\begin{array}{c} 0\\\\ 0\\end{array}\\right)\\] which leads to the linear system \\[\\begin{gathered}\n2\\mu_1-\\mu_3=0 \\\\ \\mu_1-4\\mu_2+\\mu_3=0\n\\end{gathered}\\] which gives the solution \\(\\mu_1=4\\mu_2/3\\), \\(\\mu_3=8\\mu_2/3\\) and \\(\\mu_2\\) runs free in the real line. Obviously the 3 column vectors are linearly dependent and so the rank of \\(A\\) can not be 3 (something we expected from before).\nNow let’s take the first 2 column vectors and write \\[\\nu_1\\left(\\begin{array}{c} 2\\\\ 1\\end{array}\\right) + \\nu_2\\left(\\begin{array}{c} 0\\\\\n-4\\end{array}\\right) =\\left(\\begin{array}{c} 0\\\\ 0\\end{array}\\right)\\] which leads to the linear system \\[\\begin{gathered}\n2\\nu_1=0 \\\\ \\nu_1-4\\nu_2=0\n\\end{gathered}\\] and gives the solution \\(\\nu_1=\\nu_2=0\\). So there are at most 2 linearly independent column vectors and so the rank of \\(A\\) is 2, same as using the row vectors!\nWe can now state some properties of the rank of a matrix.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Matrix algebra essentials</span>"
    ]
  },
  {
    "objectID": "matrixAlgebra.html#rank",
    "href": "matrixAlgebra.html#rank",
    "title": "Appendix A — Matrix algebra essentials",
    "section": "",
    "text": "\\(0\\leq rank(A) \\leq \\min(p,q)\\). If \\(rank(A)=\\min(p,q)\\), then \\(A\\) is said to be of full rank.\nIf \\(rank(A)&lt;\\min(p,q)\\), we can find some linearly dependent columns or rows of \\(A\\).\nIf \\(A\\) is a \\(p\\times q\\) matrix with \\(p\\geq q\\) and \\(rank(A)=q\\), then the matrix \\(A^TA\\) is non-singular and the matrix \\(AA^T\\) is singular. A similar result can be stated for \\(p\\leq q\\).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Matrix algebra essentials</span>"
    ]
  },
  {
    "objectID": "matrixAlgebra.html#vector-differentiation",
    "href": "matrixAlgebra.html#vector-differentiation",
    "title": "Appendix A — Matrix algebra essentials",
    "section": "A.2 Vector differentiation",
    "text": "A.2 Vector differentiation\nSuppose that \\(\\mathbf{x}=(x_1,\\ldots,x_p)^T\\) is a \\(p\\times 1\\) vector of variables and \\(f(\\mathbf{x})\\) is a real-valued function of \\(p\\) variables \\(x_1,\\ldots,x_p\\), writing \\(f(\\mathbf{x})=f(x_1,\\ldots,x_p)\\), where the domain of \\(f(\\mathbf{x})\\) is a subset of \\(\\mathbb{R}^p\\). Let \\(\\partial\nf(\\mathbf{x})/\\partial x_i\\) denote the partial derivative of \\(f(\\mathbf{x})\\) with respect to \\(x_i\\), then by definition the partial derivative of \\(f(\\mathbf{x})\\) with respect to the vector \\(\\mathbf{x}\\) is \\[\\frac{\\partial f(\\mathbf{x}) }{ \\partial \\mathbf{x} } =\n\\left(\\begin{array}{c}\n\\partial f(\\mathbf{x}) / \\partial x_1 \\\\ \\vdots \\\\ \\partial f(\\mathbf{x}) / \\partial\nx_p \\end{array}\\right)\\] or in words: the partial derivative of \\(f(\\mathbf{x})\\) with respect to the vector \\(\\mathbf{x}\\) is the vector, in which elements are the respective partial derivatives of \\(f(\\mathbf{x})\\) with respect to the variables \\(x_1,\\ldots,x_p\\).\nSuppose now that \\(\\mathbf{c}=(c_1,\\ldots,c_p)^T\\) is a vector of constants and \\(f(\\mathbf{x})=\\mathbf{c}^T\\mathbf{x}\\). Then we have \\[\\frac{\\partial f(\\mathbf{x}) }{ \\partial \\mathbf{x} } = \\mathbf{c}\\] The proof of this is by noting that \\(\\mathbf{c}^T\\mathbf{x}=\\sum_{i=1}^p\nc_ix_i\\) with \\(\\partial f(\\mathbf{x})/\\partial x_i=c_i\\) and so \\[\\frac{\\partial f(\\mathbf{x}) }{ \\partial \\mathbf{x} } =\n\\left(\\begin{array}{c} c_1 \\\\ \\vdots \\\\ c_p\\end{array}\\right) =\n\\mathbf{c}\\] as required.\nNote that since \\(\\mathbf{c}^T\\mathbf{x}=\\mathbf{x}^T\\mathbf{c}\\), we immediately have that \\[\\label{ch1:app1}\n\\frac{ \\partial \\mathbf{c}^T\\mathbf{x} }{\\partial \\mathbf{x}} = \\frac{\n\\partial \\mathbf{x}^T \\mathbf{c}}{ \\partial \\mathbf{x}} = \\mathbf{c}\\]\nNow suppose that \\(A=(a_{ij})_{i,j=1,\\ldots,p}\\) is a \\(p\\times p\\) symmetric matrix of constants and write \\(A=(\\mathbf{a}_1,\\ldots,\\mathbf{a}_p)\\), where \\(\\mathbf{a}_i\\) is the \\(i\\)-th column of \\(A\\). Then with \\(f(\\mathbf{x})=\\mathbf{x}^TA\\mathbf{x}\\) we have \\[\\frac{\\partial f(\\mathbf{x}) }{ \\partial \\mathbf{x} } = 2A\\mathbf{x}\\] To prove this result, first we note that \\[\\mathbf{x}^TA\\mathbf{x}= \\sum_{i=1}^p\\sum_{j=1}^p a_{ij}x_ix_j= \\sum_{i=1}^p a_{ii}x_i^2 +2\\sum_{i&lt;j}^p\na_{ij}x_ix_j\\] Then \\[\\frac{ \\partial \\mathbf{x}^T A\\mathbf{x} }{\\partial x_i} = 2a_{ii}x_i\n+2\\sum_{j=2}^p a_{ij}x_j = 2 \\sum_{j=1}^p a_{ij}x_j =\n2\\mathbf{a}_i^T\\mathbf{x}\\] So we get \\[\\frac{ \\partial f(\\mathbf{x}) }{\\partial \\mathbf{x}} =\n2\\left(\\begin{array}{c} \\mathbf{a}_1^T\\mathbf{x} \\\\ \\vdots \\\\\n\\mathbf{a}_p^T\\mathbf{x} \\end{array}\\right) = 2 \\left(\\begin{array}{c}\n\\mathbf{a}_1^T \\\\ \\vdots \\\\ \\mathbf{a}_p^T\\end{array}\\right) \\mathbf{x} =\n2A\\mathbf{x}\\] So we have established \\[\\label{ch1:app2}\n\\frac{ \\partial \\mathbf{x}^TA\\mathbf{x}}{\\partial \\mathbf{x}} = 2A\\mathbf{x}\\] Note that for this result to hold, we need to know that \\(A\\) is a symmetric matrix.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Matrix algebra essentials</span>"
    ]
  },
  {
    "objectID": "randomVectors.html",
    "href": "randomVectors.html",
    "title": "Appendix B — Random vectors",
    "section": "",
    "text": "B.1 Vector mean\nIf we have a collection of random variables \\(x_1,\\ldots,x_p\\), we can form them into a vector \\(\\mathbf{x}=(x_1,\\ldots,x_p)^T\\). The joint distribution of the collection of random variables \\((x_1,\\ldots,x_p)\\) defines the distribution of the random vector \\(\\mathbf{x}\\).\nWe define the mean of \\(\\mathbf{x}\\) to be the vector of means of its components: \\[E(\\mathbf{x})=\\left(\\begin{array}{c} E(x_1) \\\\ \\vdots \\\\\nE(x_p)\\end{array}\\right)\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Random vectors</span>"
    ]
  },
  {
    "objectID": "randomVectors.html#covariance-matrix",
    "href": "randomVectors.html#covariance-matrix",
    "title": "Appendix B — Random vectors",
    "section": "B.2 Covariance matrix",
    "text": "B.2 Covariance matrix\nThe obvious way to define the variance of a vector random variable would be to make it the vector of variances of its components. But it is useful also to recognize the covariances between the various components of \\(\\mathbf{x}\\), too. Accordingly, we define it to be the \\(p\\times p\\) matrix \\[Var(\\mathbf{x})=\\left( \\begin{array}{cccc} \\mbox{Var}(x_1) & \\mbox{Cov}(x_1,x_2) &\n\\cdots & \\mbox{Cov}(x_1,x_p) \\\\ \\mbox{Cov}(x_2,x_1) &\\ Var(x_2) & \\cdots &\n\\mbox{Cov}(x_2,x_p) \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\mbox{Cov}(x_p,x_1) &\n\\mbox{Cov}(x_p,x_2) & \\cdots & \\mbox{Var}(x_p) \\end{array} \\right)\\] The above is usually referred to as the variance-covariance matrix of \\(\\mathbf{x}\\), or just as the covariance matrix of \\(\\mathbf{x}\\).\nNotice that since covariance is a symmetric relation, \\(\\mbox{Cov}(x_i,x_j) = \\mbox{Cov}(x_j,x_i)\\), this is a symmetric matrix. Thus we can write \\(\\mbox{Var}(\\mathbf{x})^T=\\mbox{Var}(\\mathbf{x})\\). Also we note that \\(\\mbox{Cov}(x_i,x_i)=\\mbox{Var}(x_i)\\).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Random vectors</span>"
    ]
  },
  {
    "objectID": "randomVectors.html#vector-covariance",
    "href": "randomVectors.html#vector-covariance",
    "title": "Appendix B — Random vectors",
    "section": "B.3 Vector covariance",
    "text": "B.3 Vector covariance\nMore generally, if \\(\\mathbf{x}=(x_1,\\ldots,x_p)^T\\) is a \\(p\\times 1\\) random vector and \\(\\mathbf{y}=(y_1,\\ldots,y_q)^T\\) is a \\(q\\times 1\\) random vector, we can define \\[\\mbox{Cov}(\\mathbf{x},\\mathbf{y})=\\left( \\begin{array}{cccc} \\mbox{Cov}(x_1,y_1) &\n\\mbox{Cov}(x_1,y_2) & \\cdots & \\mbox{Cov}(x_1,y_q) \\\\ \\mbox{Cov}(x_2,y_1) & \\mbox{Cov}(x_2,y_2)\n&\n\\cdots & \\mbox{Cov}(x_2,y_q) \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\mbox{Cov}(x_p,y_1) & \\mbox{Cov}(x_p,y_2) & \\cdots & \\mbox{Cov}(x_p,y_q) \\end{array}\n\\right)\\] This is not a symmetric matrix. In fact, since it is \\(p\\times q\\), it is not even square unless \\(p = q\\). Notice, however, that \\(\\mbox{Cov}(\\mathbf{y},\\mathbf{x})=\\mbox{Cov}(\\mathbf{x},\\mathbf{y})^T\\). Also we have \\(\\mbox{Cov}(\\mathbf{x},\\mathbf{x})=\\mbox{Var}(\\mathbf{x})\\).\nSometimes \\(Cov(\\mathbf{x},\\mathbf{y})\\) is referred to as the covariance of \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Random vectors</span>"
    ]
  },
  {
    "objectID": "randomVectors.html#some-useful-results",
    "href": "randomVectors.html#some-useful-results",
    "title": "Appendix B — Random vectors",
    "section": "B.4 Some useful results",
    "text": "B.4 Some useful results\nWe know many useful results about expectations, such as \\(E(aX+b)=aE(X)+b\\), when \\(a\\) and \\(b\\) are constants. Here are some vector generalizations.\n\n\\(E(A\\mathbf{x}+\\mathbf{b})=AE(\\mathbf{x})+\\mathbf{b}\\), when \\(A\\) is a \\(q\\times p\\) matrix of constants and \\(\\mathbf{b}\\) is a \\(q\\times 1\\) vector of constants. Notice that this expresses the mean of the \\(q\\times 1\\) random vector in terms of that of the \\(p\\times 1\\) random vector \\(\\mathbf{x}\\).\n\\(\\mbox{Var}(A\\mathbf{x}+\\mathbf{b})=A\\mbox{Var}(\\mathbf{x})A^T\\).\n\\(\\mbox{Cov}(A\\mathbf{x}+\\mathbf{b},C\\mathbf{y}+\\mathbf{d})=A\\mbox{Cov}(\\mathbf{x},\\mathbf{y})C^T\\), where the dimensions of the matrices \\(A\\) and \\(C\\) and the vectors \\(\\mathbf{b}\\) and \\(\\mathbf{d}\\) are as required to allow the matrix multiplication.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Random vectors</span>"
    ]
  },
  {
    "objectID": "multivariateNormal.html",
    "href": "multivariateNormal.html",
    "title": "Appendix C — The multivariate normal distribution",
    "section": "",
    "text": "C.1 Definition\nThe normal distribution is the most important distribution for a single random variable, and its extension to a vector random variable is equally important in Statistics.\nLet us denote the \\(p\\)-dimensional random vector \\[\\mathbf{x}^T=(x_1,\\ldots,x_p)\\] where \\(x_1,\\ldots, x_p\\) are univariate random variables.\nIn the univariate case the probability density function (p.d.f.) \\[f(x)=\\frac{1}{\\sqrt{2\\pi} \\sigma} e^{ -\n\\frac{(x-\\mu)^2}{2\\sigma^2}},\\] i.e. depends on two parameters: \\(\\mu\\) and \\(\\sigma\\). Note that this formula can also be written as \\[\\label{eq5_1}\nf(x)=\\frac{1}{\\sqrt{2\\pi} \\sigma}\ne^{\\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu)}\\] where \\(\\Sigma=\\sigma^2\\). When \\(\\mathbf{x}\\) is a \\(p\\)-dimensional random vector it can be shown that the joint p.d.f. is \\[f(\\mathbf{x})=\\frac{1}{(2\\pi)^{p/2}|\\Sigma|^{1/2}}\ne^{-\\frac{1}{2}(\\mathbf{x}-\\mathbf{\\mu})^T\\Sigma^{-1}(\\mathbf{x}-\\mathbf{\\mu})}\\] where \\(\\Sigma\\) is the \\(p\\times p\\) covariance matrix of the random variables and \\(|\\Sigma|\\) is its determinant. This equation reduces to the previous one when \\(p=1\\). The p.d.f. of the MVN also has two parameters: the vector \\(\\mathbf{\\mu}\\) and the covariance matrix \\(\\Sigma\\). In statistical notation, we would write \\(\\mathbf{x} \\sim N_p(\\mathbf{\\mu},\\Sigma)\\)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>The multivariate normal distribution</span>"
    ]
  },
  {
    "objectID": "multivariateNormal.html#linear-transformation",
    "href": "multivariateNormal.html#linear-transformation",
    "title": "Appendix C — The multivariate normal distribution",
    "section": "C.2 Linear transformation",
    "text": "C.2 Linear transformation\nThe key results about multivariate normal distributions are these.\n\nLinear transformation. If \\(A\\) is a \\(q\\times p\\) matrix of rank \\(q\\) (with \\(q\\leq p\\)) \\(\\mathbf{b}\\) is a \\(q\\)-dimensional vector, then \\[A\\mathbf{x}+\\mathbf{b}\\sim N_q(A\\mathbf{\\mu}+\\mathbf{b},A\\Sigma A^T)\\]\n(Note that the formulae for the mean and variance follow from the general results on transformations in the previous section; the extra content in this result is that Normality is preserved by linear transformations.)\nStandardization. We can derive a standardizing transformation that produces a vector with zero mean and identity variance. Since \\(\\Sigma\\) is a variance covariance matrix it is positive definite. One of the properties of positive definite matrices is that we can find a \\(p\\times p\\) matrix \\(C\\) such that \\(\\Sigma= CC^T\\). Then it follows immediately that \\[C^{-1}(\\mathbf{x}-\\mathbf{\\mu})\\sim N_p(\\mathbf{0}_p,I_p)\\] where \\(\\mathbf{0}_p\\) is the \\(p\\times 1\\) vector of zeroes and \\(I_p\\) is the \\(p\\times p\\) identity matrix. Note that \\(C\\) is not unique; there are many possible choices for \\(C\\), which can be described as a square root of \\(\\Sigma\\).\nSo we see that standardization produces a vector of random variables that are independent and identically distributed as \\(N(0,1)\\). So standardization produces independent standard normal random variables.\nIf \\(\\Sigma\\) is diagonal, i.e. \\[\\Sigma =\\left(\\begin{array}{cccc} \\sigma_{11} & 0 & \\cdots & 0\\\\ 0 &\n\\sigma_{22} & \\cdots & 0\\\\ \\vdots & \\vdots & \\ddots & \\vdots\\\\ 0 & 0\n& \\cdots & \\sigma_{pp}\\end{array}\\right)\\] then the random variables \\(x_1,\\ldots,x_p\\) are independent and of course they are uncorrelated too.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>The multivariate normal distribution</span>"
    ]
  }
]