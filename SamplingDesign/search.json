[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MPS318/4101 Sampling Theory and Design of Experiments",
    "section": "",
    "text": "Instructions\nStudents on MAS318 should study parts I and II only\nStudents on MAS4101 should study Parts I, II and III",
    "crumbs": [
      "Instructions"
    ]
  },
  {
    "objectID": "index.html#acknowledgement",
    "href": "index.html#acknowledgement",
    "title": "MPS318/4101 Sampling Theory and Design of Experiments",
    "section": "Acknowledgement",
    "text": "Acknowledgement\nSome of the content in this module has been taught by various former colleagues over the years (going back to the former Department of Probability and Statistics at Sheffield), all of whom will have developed the notes: Vic Barnett, Richard Martin, Alex Donev, David Grey and Kevin Walters.",
    "crumbs": [
      "Instructions"
    ]
  },
  {
    "objectID": "Introduction to experimental design.html",
    "href": "Introduction to experimental design.html",
    "title": "1  Introduction to Experimental Design",
    "section": "",
    "text": "1.1 Modelling and Inference\nIn previous modules, you will have met the concept of statistical inference: using statistical methods to learn about the characteristics of some population or process given suitable data. Often, we will want to make inferences about relationships between variables. You will have seen how to do this using statistical models.\nSuppose we wish to investigate the relationship between temperature and ozone concentration (ozone occurring at ground level is a pollutant with various risks to health). Some data are available in R in the built-in dataset airquality and are plotted below.\nDaily air quality measurements in New York, May to September 1973.\nWe can fit various models to these data such as \\[Y_i=\\beta_0 + \\beta_1 x_i + \\varepsilon_i\\] or \\[Y_i=\\beta_0 + \\beta_1 x_i + \\beta_2x_i^2 +  \\varepsilon_i\\] with \\(Y_i\\) defined to be and \\(x_i\\) defined to be the ozone concentration and maximum daily temperature respectively on day \\(i\\).",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Experimental Design</span>"
    ]
  },
  {
    "objectID": "Introduction to experimental design.html#observational-studies-and-designed-experiments",
    "href": "Introduction to experimental design.html#observational-studies-and-designed-experiments",
    "title": "1  Introduction to Experimental Design",
    "section": "1.2 Observational studies and designed experiments",
    "text": "1.2 Observational studies and designed experiments\nWe could describe the previous example as an observational study. Suppose the main interest is in predicting ozone concentrations. Whilst we can choose to use temperature as an explanatory variable in our model, we have no choice regarding the values of temperature we obtain in our data: we just have to use whatever is observed.\nIn a designed experiment, we can choose both what explanatory variables to use and what values they take.\n\n\n\n\n\n\nNoteExample\n\n\n\n\nExample 1.1 Suppose we want to investigate the effect of drinking alcohol on an individual’s reaction time. We could use a linear model where the dependent variable is the increase in reaction time (from some baseline value assuming no alcohol consumed), and the explanatory variable is the number of units of alcohol consumed over some period of time.\nThere is an obvious cost in obtaining observations: we have to recruit participants, it’s not quick or simple to do the experiment for each participant, and there are ethical issues in asking participants to drink alcohol! We would want to run such an experiment as efficiently (and ethically) as possible. Some statistical issues to consider are\n\nwhat values should we choose for explanatory variable (units of alcohol consumed), so what we can best learn the relationship between the explanatory and dependent variables?\nhow best to handle other sources of variation (that are not of interest in themselves), e.g. variation between participants?\n\n\n\n\nIn this part of the module, we will study various techniques to help us with these sorts of issues in designed experiments.\n\n\n\n\n\n\nImportant\n\n\n\nWe will make the assumption throughout this module that the data from the designed experiment can be analysed with a linear model.",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Experimental Design</span>"
    ]
  },
  {
    "objectID": "Introduction to experimental design.html#notation-and-definitions",
    "href": "Introduction to experimental design.html#notation-and-definitions",
    "title": "1  Introduction to Experimental Design",
    "section": "1.3 Notation and definitions",
    "text": "1.3 Notation and definitions\n\nWe consider an experiment in which we are investigating the effect of a number of explanatory variables or covariates \\(x_1, x_2, \\ldots x_m\\) on a response variable \\(y\\). We will use the term “response” variable rather than “dependent variable” in this module\nThe element of design is that we can choose the values of \\(x_1, x_2, \\ldots x_m\\) from a set known as the design region, which can be thought of as choosing a point in a \\(m\\)-dimensional space. With these design points we could then construct an experiment and measure the response \\(y\\).\nThe explanatory variable may be quantitative (e.g. the drug dose in milligrams) or qualitative (e.g. the presence/absence of a treatment). A variable may also be qualitative but with more than two levels (eg ‘diet’ consisting of vegan, vegetarian meat eater etc). We shall generally think of the response variable as quantitative and real-valued, although this will not be true of all experiments.\nWe use the term experimental subject to mean an individual item (e.g. a patient) to which we allocate values of the explanatory variables, and then observe the response variable.\nThe collection of points chosen in the design region at which to take observations is known as the design of the experiment.\nMore than one observation may be taken at the same point; this is known as replication, and may be an important feature of a design.",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Experimental Design</span>"
    ]
  },
  {
    "objectID": "Review of linear models.html",
    "href": "Review of linear models.html",
    "title": "2  A review of linear models",
    "section": "",
    "text": "2.1 The general linear model\nIn this course, we shall mostly be concerned with the general linear model which takes the form \\[\\begin{equation}\nY = \\mathbf{f}(\\mathbf{x})^T\\boldsymbol{\\beta}+\\varepsilon=\\sum_{i=1}^p f_i(\\mathbf{x})\\beta_i + \\varepsilon,\n\\end{equation}\\] where \\(\\mathbf{f}(\\mathbf{x})^T = (f_1(\\mathbf{x}) ~~ f_2(\\mathbf{x})~~\\ldots ~~f_p(\\mathbf{x})\\) is a row vector of known functions of \\(\\mathbf{x}\\). \\(\\mathbf{x}= (x_1~~x_2~~\\ldots ~~x_m)^T\\) is a vector of known explanatory variable values and \\(\\varepsilon\\) is a random variable with mean zero.\nWe assume that \\(\\varepsilon\\) is normally distributed, but the mean of \\(\\varepsilon\\) is zero by definition; constants are included in the term \\(\\mathbf{f}(\\mathbf{x})^T\\boldsymbol{\\beta}\\) as needed.\nWhen specifying a linear model, we may write \\[\nY = \\mathbf{f}(\\mathbf{x})^T\\boldsymbol{\\beta}+\\varepsilon,\n\\] or we may instead write \\[\nE(Y) = \\mathbf{f}(\\mathbf{x})^T\\boldsymbol{\\beta}\n\\]",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A review of linear models</span>"
    ]
  },
  {
    "objectID": "Review of linear models.html#the-general-linear-model",
    "href": "Review of linear models.html#the-general-linear-model",
    "title": "2  A review of linear models",
    "section": "",
    "text": "NoteExample\n\n\n\n\nExample 2.1 The simple linear regression model may be written \\[Y = \\beta_0 + \\beta_1 x + \\varepsilon\\] so that in this case \\(m=1\\), \\(p=2\\), \\(\\mathbf{x}=x\\) and \\(\\mathbf{f}(\\mathbf{x})^T=(1~~x)\\).\nThe quadratic regression model may be written \\[Y = \\beta_0 + \\beta_1 x + \\beta_{11}x^2 + \\varepsilon\\] so that in this case \\(m=1\\), \\(p=3\\), \\(\\mathbf{x}=x\\) and \\(\\mathbf{f}(\\mathbf{x})^T = (1~~x~~x^2)\\).\n\n\n\n\n\n\n\n\n\nImportantPolynomial regression models are linear models!\n\n\n\nThough the quadratic regression model above is not linear in \\(x\\), it is still classified as a linear model, because the \\(E(Y)\\) is a linear combination of the parameters \\(\\beta_0,\\beta_1,\\ldots...\\); it can be written in the matrix notation in the next section, and all the general linear model theory still applies. Hence linear models can describe complicated non-linear relationships between the explanatory variables and the response variable.\n\n\n\n\n\n\n\n\nNoteExample\n\n\n\n\nExample 2.2 With \\(m=4\\) a full quadratic model may be written \\[\\begin{align}\nY &= \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_3x_3 + \\beta_4x_4 + \\beta_{12}x_1x_2 + \\beta_{13}x_1x_3 + \\beta_{14}x_1x_4 \\\\\n&+ \\beta_{23}x_2x_3 + \\beta_{24}x_2x_4 + \\beta_{34}x_3x_4 + \\beta_{11}x_1^2 + \\beta_{22}x_2^2 + \\beta_{33}x_3^2 + \\beta_{44}x_4^2 + \\varepsilon .\n\\end{align}\\] Note how in this example the \\(\\beta\\) parameters are labelled with subscripts reflecting the explanatory variables to which they are relevant, and in what way: this is a common convention.",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A review of linear models</span>"
    ]
  },
  {
    "objectID": "Review of linear models.html#matrix-notation",
    "href": "Review of linear models.html#matrix-notation",
    "title": "2  A review of linear models",
    "section": "2.2 Matrix notation",
    "text": "2.2 Matrix notation\nSuppose that our design consists of \\(n\\) points \\(\\mathbf{x_1}, \\mathbf{x_2},\\ldots , \\mathbf{x_n}\\) (some of which will be the same if replication occurs) and the corresponding observations are \\(Y_1, Y_2, \\ldots , Y_n\\). Then we have \\[Y_j = \\mathbf{f}(\\mathbf{x_j})^T\\boldsymbol{\\beta}+\\varepsilon_j \\;\\;\\;\\ \\text{for} \\;\\;\\;\\ j=1,2,\\ldots ,n\\] say, and these \\(n\\) equations may be collected together into the single matrix equation \\[\\mathbf{Y}= \\mathbf{X}\\boldsymbol{\\beta}+\\boldsymbol{\\varepsilon}\\] where \\(\\mathbf{Y}\\) and \\(\\boldsymbol{\\varepsilon}\\) are column vectors containing \\(Y_1,Y_2, \\ldots , Y_n\\) and \\(\\varepsilon_1, \\varepsilon_2, \\ldots ,\\varepsilon_n\\) respectively, and \\(\\mathbf{X}\\) is the \\(n\\times p\\) matrix whose rows are \\[\\mathbf{f}(\\mathbf{x_1})^T, \\mathbf{f}(\\mathbf{x_2})^T, \\ldots , \\mathbf{f}(\\mathbf{x_n})^T\\] respectively. This may also be written \\[E(\\mathbf{Y})= \\mathbf{X}\\boldsymbol{\\beta}.\\]\nThe matrix \\(\\mathbf{X}\\) is known as the design matrix of the experiment. It should be noted, however, that it depends not only on the design as previously defined, but also on the model chosen, via the function \\(\\mathbf{f}\\).\n\n\n\n\n\n\nNoteExample\n\n\n\n\nExample 2.3 Suppose there is a single explanatory variable \\(x\\) and it is decided to take one observation at each of the five points \\(x=0,1,2,3\\) and \\(4\\). Then for the simple linear regression model the design matrix will be \\[\\mathbf{X}=\\left(\\begin{array}{cc} 1 & 0 \\\\ 1 & 1 \\\\ 1 & 2 \\\\ 1 & 3 \\\\ 1 & 4 \\end{array} \\right)\\] whereas for the quadratic regression model it will be \\[\\mathbf{X}=\\left(\\begin{array}{ccc} 1 & 0 & 0 \\\\ 1 & 1 & 1 \\\\ 1 & 2 & 4 \\\\ 1 & 3 & 9 \\\\ 1 & 4 & 16 \\end{array}\\right).\\]\n\n\n\nNote that under the usual assumptions that \\(\\varepsilon_1, \\varepsilon_2, \\ldots ,\\varepsilon_n\\stackrel{i.i.d}{\\sim}N(0,\\sigma^2)\\), we have\n\\[\n\\mathbf{Y}\\sim N(\\mathbf{X}\\boldsymbol{\\beta}, \\sigma^2I_n)\n\\]\n\n2.2.1 Big \\(\\mathbf{Y}\\) and little \\(\\mathbf{y}\\) notation\nWe distinguish between\n\n\\(\\mathbf{Y}\\): a vector of random variables, and\n\\(\\mathbf{y}\\): a vector of constants, representing the observed value of \\(\\mathbf{Y}\\).\n\nWe can think of \\(\\mathbf{Y}\\) as corresponding to the period before the experiment has been conducted, when we don’t yet know what values we will observe, and \\(\\mathbf{y}\\) as corresponding to the period after the experiment has been conducted, after we have observed the values of the response variables.\nIn this module, we only ever consider the design of the experiment: we won’t actually obtain any data from any experiments we design, and so we will never have an observed \\(\\mathbf{y}\\) to work with.",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A review of linear models</span>"
    ]
  },
  {
    "objectID": "Review of linear models.html#least-squares-estimation",
    "href": "Review of linear models.html#least-squares-estimation",
    "title": "2  A review of linear models",
    "section": "2.3 Least squares estimation",
    "text": "2.3 Least squares estimation\nWe estimate \\(\\boldsymbol{\\beta}\\) by minimising the sum of squares: we get an expression for the sum of squared errors, differentiate with respect to \\(\\boldsymbol{\\beta}\\) and equate to zero. The least squares estimator is \\[\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T\\mathbf{Y},\\] with the observed estimate obtain by replacing \\(\\mathbf{Y}\\) with \\(\\mathbf{y}\\).\nWe won’t review the derivation of \\(\\hat{\\boldsymbol{\\beta}}\\) in this module, but for background reference, it is available here.\n\n\n\n\n\n\nImportantWhy this formula matters\n\n\n\nWe won’t be calculating any numerical values of \\(\\hat{\\boldsymbol{\\beta}}\\) in this module but the formula is still important for two reasons:\n\na key task in experimental design is to choose the design matrix \\(\\mathbf{X}\\); we can explore how the choice of \\(\\mathbf{X}\\) affects the statistical properties of \\(\\hat{\\boldsymbol{\\beta}}\\)\nthe formula tells us we must choose \\(\\mathbf{X}\\) such that \\((\\mathbf{X}^T\\mathbf{X})\\) is invertible.\n\n\n\nFor \\((\\mathbf{X}^T\\mathbf{X})\\) to be invertible \\(\\mathbf {X}\\) must be of full rank \\(p\\) which, since \\(\\mathbf{X}\\) is an \\(n\\times p\\) matrix, requires \\(n\\geq p\\) (at least as many design points as parameters). If \\(\\mathbf{X}\\) is of full rank \\(p\\) then \\(\\mathbf{X}^T\\mathbf{X}\\) is a positive definite symmetric \\(p\\times p\\) matrix and is invertible. If it is not the case that \\(\\mathbf{X}\\) is of full rank, the model is said to be over-parametrised.",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A review of linear models</span>"
    ]
  },
  {
    "objectID": "Review of linear models.html#statistical-properties-of-the-least-squares-estimator",
    "href": "Review of linear models.html#statistical-properties-of-the-least-squares-estimator",
    "title": "2  A review of linear models",
    "section": "2.4 Statistical properties of the least squares estimator",
    "text": "2.4 Statistical properties of the least squares estimator\nWe now consider the properties of \\[\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T\\mathbf{Y},\\] noting that \\(\\mathbf{X}\\) is a constant and that \\[\n\\mathbf{Y}\\sim N(\\mathbf{X}\\boldsymbol{\\beta}, \\sigma^2I_n)\n\\] The least squares estimator is unbiased: \\[E(\\hat{\\boldsymbol{\\beta}}) = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta}=\\boldsymbol{\\beta}.\\]\nThe variance-covariance matrix 1 of \\(\\boldsymbol{\\beta}\\) is obtained as \\[\\text{Var}(\\hat{\\boldsymbol{\\beta}}) = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\sigma^2\\mathbf{I}\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\] which simplifies to \\[\\text{Var}(\\hat{\\boldsymbol{\\beta}}) =\\sigma^2 (\\mathbf{X}^T\\mathbf{X})^{-1}\\] We can now state that \\[\\hat{\\boldsymbol{\\beta}} \\sim N(\\boldsymbol{\\beta}, \\sigma^2  (\\mathbf{X}^T\\mathbf{X})^{-1}).\\] The matrix \\(\\mathbf{X}^T\\mathbf{X}\\) is called the information matrix and is very relevant to the choice of design because, as seen above, it determines the accuracy of estimation of the parameters.\n\n\n\n\n\n\nNoteExample\n\n\n\n\nExample 2.4 In the simple linear regression model, \\(E(Y)=\\beta_0+\\beta_1x\\), suppose one observation is taken at each of the points \\(x_1, x_2, \\ldots , x_n\\). Then \\[\\begin{align*}\n\\mathbf{X}^T\\mathbf{X} &= \\left(\\begin{array}{cc} n & \\sum_{j=1}^n x_j \\\\ \\sum_{j=1}^n x_j & \\sum_{j=1}^n x_j^2 \\end{array} \\right) \\\\\n&=\\left(\\begin{array}{cc} n & n\\bar{x} \\\\ n\\bar{x} & s_{xx}+n\\bar{x}^2 \\end{array} \\right)\\\\\n(\\mathbf{X}^T \\mathbf{X})^{-1} &=\\frac{1}{ns_{xx}}\\left( \\begin{array}{cc} s_{xx}+n\\bar{x}^2 & -n\\bar{x} \\\\\n-n\\bar{x} & n \\end{array} \\right)\n\\end{align*}\\] where \\(s_{xx}=\\sum_{j=1}^{n}(x_j-\\bar{x})^2=\\sum_{j=1}^{n}x_j^2-n\\bar{x}^2\\)",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A review of linear models</span>"
    ]
  },
  {
    "objectID": "Review of linear models.html#estimating-the-error-variance-sigma2",
    "href": "Review of linear models.html#estimating-the-error-variance-sigma2",
    "title": "2  A review of linear models",
    "section": "2.5 Estimating the error variance \\(\\sigma^2\\)",
    "text": "2.5 Estimating the error variance \\(\\sigma^2\\)\nWe can estimate \\(\\sigma^2\\) using the estimator \\[\n\\hat{\\sigma}^2:=\\frac{\\hat{\\boldsymbol{\\varepsilon}}^T \\hat{\\boldsymbol{\\varepsilon}}}{(n-p)}\n\\] where \\[\\hat{\\boldsymbol{\\varepsilon}}:= \\mathbf{Y}-\\hat{\\mathbf{Y}}\\] is the vector of residuals, \\[\n\\hat{\\mathbf{Y}} = \\mathbf{HY},\n\\] is the vector of fitted values, where the “hat matrix” \\(\\mathbf{H}\\) is defined as \\[\n\\mathbf{H}:=\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\n\\]\nand \\(n\\) and \\(p\\) are number of rows and columns respectively of \\(\\mathbf{X}\\). This is an unbiased estimator of \\(\\sigma ^2\\) and is statistically independent of the estimator \\(\\boldsymbol{\\beta}\\).\nAgain, we won’t be computing estimates of \\(\\sigma^2\\) in this module; we’ll just note here that we require our experimental design to have \\(n&gt;p\\) if we want to be able to estimate \\(\\sigma^2\\).\nIt can also be shown that \\(Var(\\sigma^2)\\) decreases as \\(n\\) increases, assuming \\(p\\) is fixed, but it is not straightforward to make use of this result when choosing \\(n\\), so we will not consider this further.",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A review of linear models</span>"
    ]
  },
  {
    "objectID": "Review of linear models.html#sec-qualvars",
    "href": "Review of linear models.html#sec-qualvars",
    "title": "2  A review of linear models",
    "section": "2.6 Models with qualitative variables",
    "text": "2.6 Models with qualitative variables\nQualitative explanatory variables are usually called factor variables or categorical variables. For example, in an experiment to produce some chemical compound, suppose the explanatory variables are pressure \\((x_1)\\) and temperature \\((x_2)\\) and the effect of using one of two possible catalysts. The choice of catalyst would be a factor variable, and there are two ways of writing the linear model.\nOne way is to use an additional subscript to indicate the catalyst: we write \\[\nE(Y_{ij}) = \\alpha_i + \\beta_1 x_{1, ij} + \\beta_2x_{2,ij},\n\\] where \\(Y_{ij}\\) is the \\(j\\)-th observation in which catalyst \\(i\\) was used, for \\(i=1,2\\), and \\(x_{1, ij}\\) and \\(x_{2, ij}\\) are corresponding pressure and temperature respectively for this observation.\nA second way to write such models is to use indicator variables to specify the level of the factor. A model which could be assumed is\n\\[E(Y_i) = \\beta_0 + \\alpha z_i + \\beta_1x_{1,i} + \\beta_2x_{2,i}\\]\nwhere \\(z_i\\) is an indicator variable, taking the values, say, \\(0\\) for the first catalyst and \\(1\\) for the second catalyst. This model would be appropriate if the experimenter expects the relationship between the response and the factors temperature and pressure to be the same for both catalysts, except for a possible difference in level: the regression surfaces of \\(y\\) against temperature (\\(x_1\\)) and pressure (\\(x_2\\)) for the two catalysts will be parallel, with the parameter \\(\\alpha\\) measuring the distance between them.\nIf three different catalysts are used, a possible model is \\[E(Y_i) = \\beta_0 + \\alpha_1z_{1,i} + \\alpha_2z_{2,i}+ \\alpha _3z_{3,i} + \\beta_1x_{1,i} + \\beta_2x_{2,i}\\] where \\(z_{j,i}=1\\) if catalyst \\(j\\) is used and \\(z_i=0\\) otherwise. However, provided each run of the experiment uses one catalyst, this model is over-parametrised, because \\(z_{1,i}+z_{2,i}+z_{3,i}=1\\), and so the second, third and fourth columns of the design matrix add up to the first column, and \\(X\\) is not of full rank. Another way of saying this is that \\(\\beta_0\\) is confounded with \\(\\alpha _1, \\alpha _2\\) and \\(\\alpha _3\\), in the sense that they cannot independently be estimated. Such an experiment cannot measure the absolute effects of the catalysts, but only the differences between their effects.\nThis over-parametrisation can be overcome by reducing the number of parameters by one, in various ways: for example, by removing \\(\\beta_0\\) from the model, or by imposing some linear constraint on the parameters \\(\\alpha _1, \\alpha _2\\) and \\(\\alpha _3\\) so that one of them can be expressed in terms of the other two and eliminated from the model. If, for example, we impose the constraint \\[\\alpha _1+\\alpha _2+\\alpha _3=0\\] then we may write \\(\\alpha _3=-\\alpha _1-\\alpha _2\\) and so eliminate \\(\\alpha _3\\) as a parameter of the model. Then \\(\\boldsymbol{\\beta}= (\\beta_0~~\\alpha _1~~\\alpha _2~~\\beta_1~~\\beta_2)^T\\) and for any design point \\(\\mathbf{x}\\) at which catalyst \\(3\\) is used, \\(\\mathbf{f}(\\mathbf{x})^T = (1~~-1~~-1~~x_1~~x_2 )\\).",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A review of linear models</span>"
    ]
  },
  {
    "objectID": "Review of linear models.html#sec-orthogonality",
    "href": "Review of linear models.html#sec-orthogonality",
    "title": "2  A review of linear models",
    "section": "2.7 Orthogonality",
    "text": "2.7 Orthogonality\nOrthogonality is an important concept in experimental design, as orthogonal designs have desirable statistical properties.\nSuppose that in a linear model \\[\\boldsymbol{\\beta}= \\left( \\begin{array}{c} \\boldsymbol{\\gamma}\\\\ \\boldsymbol{\\delta}\\end{array}\\right) \\quad \\text{and} \\quad \\mathbf{X}=(\\mathbf {V} \\quad \\mathbf{W})\\] where, say, \\(\\boldsymbol{\\gamma}\\) is a \\(q\\)-vector, \\(\\boldsymbol{\\delta}\\) is a \\((p-q)\\)-vector, \\(\\mathbf{V}\\) is a \\(n\\times q\\) matrix and \\(\\mathbf{W}\\) is a \\(n\\times (p-q)\\) matrix; suppose further that every column of \\(\\mathbf{V}\\) is orthogonal (perpendicular) to every column of \\(\\mathbf{W}\\), which may be written succinctly as \\[\\mathbf{V}^T\\mathbf{W}=\\mathbf{0}.\\] Then the two groups of parameters comprising the components of \\(\\boldsymbol{\\gamma}\\) and \\(\\boldsymbol{\\delta}\\) are said to be orthogonal to each other.\n\n\n\n\n\n\nImportantProperties of orthogonal parameters\n\n\n\n\nThe estimators of orthogonal parameters are independent;\nFor two orthogonal parameters, the estimate of one does not change depending on whether or not the other is included in the model\n\n\n\nTo understand the first property, note that \\[\n\\text{Var}(\\hat{\\boldsymbol{\\beta}})=\\left(\\begin{array}{cc}\\text{Var}(\\hat{\\boldsymbol{\\gamma}}) & \\text{Cov}(\\hat{\\boldsymbol{\\gamma}}, \\hat{\\boldsymbol{\\delta}}) \\\\\n\\text{Cov}(\\hat{\\boldsymbol{\\delta}}, \\hat{\\boldsymbol{\\gamma}})& \\text{Var}(\\hat{\\boldsymbol{\\delta}})\n\\end{array}\\right)=\\sigma ^2(\\mathbf{X}^T\\mathbf{X})^{-1}=\\sigma ^2\\left(\\begin{array}{cc} (\\mathbf{V}^T\\mathbf{V})^{-1} & \\mathbf{0} \\\\ \\mathbf{0} & (\\mathbf{W}^T\\mathbf{W})^{-1} \\end{array} \\right).\\]\nIn a multivariate normal distribution, zero correlation implies independence, so the estimators of \\(\\boldsymbol{\\gamma}\\) and \\(\\boldsymbol{\\delta}\\) are independent in the statistical sense.\nTo understand the second property, note that\n\\[\n\\hat{\\boldsymbol{\\beta}} =  \\left( \\begin{array}{c} \\hat{\\boldsymbol{\\gamma}}\\\\ \\hat{\\boldsymbol{\\delta}}\\end{array}\\right) = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf Y = \\left( \\begin{array}{c} (\\mathbf{V}^T\\mathbf{V})^{-1}\\mathbf{V}^T\\mathbf Y\\\\ (\\mathbf{W}^T\\mathbf{W})^{-1}\\mathbf{W}^T\\mathbf Y\\end{array}\\right).\n\\] If we were to remove the terms in the model corresponding to \\(\\boldsymbol{\\delta}\\), say, the design matrix would reduce to \\(\\mathbf{X} = \\mathbf{V}\\) and we would have the same least squares estimate for \\(\\boldsymbol{\\gamma}\\). This can be desirable if, for example, the main interest is in estimating the \\(\\boldsymbol{\\gamma}\\) parameters, but it is not straightforward to decide whether to include the \\(\\boldsymbol{\\delta}\\) parameters or not: in this case the estimated \\(\\boldsymbol{\\gamma}\\) would be the same either way.\n\n\n\n\n\n\nNoteExample\n\n\n\n\nExample 2.5 The model \\[E(Y)=\\beta_0+\\alpha z+\\beta_1x+\\beta_{11}x^2\\] is proposed for which one observation is taken at each of the values \\(x=-1,0,1\\) of the quantitative variable \\(x\\) for each of the values \\(z=-1,1\\) of the dummy variable \\(z\\). Investigate in this design which combinations of the four parameters of this model are mutually orthogonal. Ordering the parameters and the design points in an obvious way, we get \\[\\mathbf{X}=\\left( \\begin{array}{cccc} 1 & -1 & -1 & 1 \\\\ 1 & -1 & 0 & 0 \\\\\n1 & -1 & 1 & 1 \\\\ 1 & 1 & -1 & 1 \\\\ 1 & 1 & 0 & 0 \\\\ 1 & 1 & 1 & 1 \\end{array} \\right)\\] from which it follows taking sums of squares and products of columns that \\[\\mathbf{X}^T\\mathbf{X}=\\left(\\begin{array}{cccc} 6 & 0 & 0 & 4 \\\\ 0 & 6 & 0 & 0 \\\\ 0 & 0 & 4 & 0 \\\\ 4 & 0 & 0 & 4 \\end{array} \\right).\\] From this we can see that \\(\\{\\beta_0, \\beta_{11}\\} ,\\{\\alpha\\}\\) and \\(\\{\\beta_1\\}\\) are mutually orthogonal groups, but that \\(\\beta_0\\) and \\(\\beta_{11}\\) are not orthogonal to each other.\n\n\n\nNote that it’s not sufficient for two column vectors of \\(\\mathbf{X}\\) to be orthogonal to each other for the corresponding parameters to be orthogonal: we need \\(\\mathbf{X}^T\\mathbf{X}\\) to be block diagonal. To get a block diagonal matrix we may have to reorder the parameters in \\(\\boldsymbol{\\beta}\\).\nContinuing Example 2.5, if we reorder the parameters and write: \\[\\boldsymbol{\\beta}= (\\beta_0,\\beta_{11}, \\alpha, \\beta_1)^T,\\] then corresponding to this reordered parameter vector we have \\[\\mathbf{X}=\\left( \\begin{array}{cccc} 1 & 1 & -1 & -1 \\\\ 1 & 0& -1 & 0  \\\\\n1 & 1& -1 & 1  \\\\ 1 & 1 & 1 & -1  \\\\ 1& 0 & 1 & 0  \\\\ 1& 1 & 1 & 1  \\end{array} \\right),\\quad \\mathbf{X}^T\\mathbf{X}=\\left(\\begin{array}{cccc} 6 & 4 & 0 & 0 \\\\ 4& 4 & 0 & 0 \\\\ 0 & 0 & 6 & 0 \\\\ 0 & 0 & 0 & 4 \\end{array} \\right),\\] i.e. \\(\\mathbf{X}^T\\mathbf{X}\\) is block diagonal.\n\n\n\n\n\n\nNoteExample\n\n\n\n\nExample 2.6 Consider a model \\[\nE(Y) = \\beta_0 + \\beta_1 x +\\beta_2 z,\n\\] with four observations at \\((x,z)\\) pairs: \\((1, 2), (2, -1), (3, -4), (3, 4)\\). Writing the parameter vector as \\(\\boldsymbol{\\beta}= (\\beta_0,\\beta_{1}, \\beta_2)^T\\), we have \\[\n\\mathbf{X}=\\left( \\begin{array}{ccc}1 & 1 & 2\\\\\n1 & 2 & -1\\\\\n1 & 3 & -4\\\\\n1 & 3 & 4\\end{array}\\right),\\quad \\mathbf{X}^T\\mathbf{X} =  \\left( \\begin{array}{ccc}\n4 & 9 & -1\\\\\n9 & 23 & 0\\\\\n1 & 0 & 37\\end{array}\\right),\\quad (\\mathbf{X}^T\\mathbf{X})^{-1} \\simeq  \\left( \\begin{array}{rrr}2.22 & -0.87 & -0.06\\\\\n-0.86 & 0.38 & 0.02\\\\\n-0.05 & 0.02 & 0.03\\end{array}\\right)\n\\] so even though the second and third columns of \\(\\mathbf{X}\\) are orthogonal (dot product is 0), the parameters \\(\\beta_1\\) and \\(\\beta_2\\) are not orthogonal: their estimators are not independent, and removing one term from the model may change the estimate of the other.",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A review of linear models</span>"
    ]
  },
  {
    "objectID": "Review of linear models.html#prediction",
    "href": "Review of linear models.html#prediction",
    "title": "2  A review of linear models",
    "section": "2.8 Prediction",
    "text": "2.8 Prediction\nWhen designing an experiment, an objective may be to obtain suitable data for prediction: predicting the mean response at some point \\(\\mathbf{x_0}\\) in the design region which may or may not have been used in the design of the experiment. If we label this mean response as \\(Y(\\mathbf{x_0})\\), then we have \\[Y(\\mathbf{x_0})=\\mathbf{f}(\\mathbf{x_0})^T\\boldsymbol{\\beta}\\] The predicted value of the response at \\(\\mathbf{x_0}\\) is naturally estimated by \\[\\hat{y}(\\mathbf{x_0})=\\mathbf{f}(\\mathbf{x_0})^T\\hat{\\boldsymbol{\\beta}}\\] which is unbiased and has variance given by \\[\\begin{align}\n\\text{var}(\\hat{Y}(\\mathbf{x_0})) &= \\mathbf{f}(\\mathbf{x_0})^T\\text{Var}(\\hat{\\boldsymbol{\\beta}})\\mathbf{f}(\\mathbf{x_0}) \\\\\n&=\\sigma^2 \\mathbf{f}(\\mathbf{x_0})^T(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{f}(\\mathbf{x_0}).\n\\end{align}\\] So the accuracy of prediction depends on the design matrix and also the point \\(\\mathbf{x_0}\\) chosen. R example 2.6 shows how prediction variance changes with choice of prediction point.\n\n\n\n\n\n\nNoteExample\n\n\n\n\nExample 2.7 In the simple linear regression model we have \\[(\\mathbf{X}^T\\mathbf{X})^{-1}=\\frac{1}{ns_{xx}} \\left( \\begin{array}{cc} \\sum x^2 & -\\sum x \\\\ -\\sum x & n \\end{array} \\right)\\] and so \\[\\begin{align}\n\\text{var}(\\hat{Y}(x_0)) &= \\text{var}(\\hat{\\beta_0}+\\hat{\\beta_1}x_0)\\\\\n&=\\text{var} \\left(\\begin{array}{cc} 1 & x_0 \\end{array} \\right)\\left( \\begin{array}{cc} \\hat{\\beta_0} & \\hat{\\beta_1} \\end{array} \\right)^T\\\\\n&=\\frac{\\sigma^2}{ns_{xx}} \\left( \\begin{array}{cc} 1 & x_0 \\end{array} \\right) \\left( \\begin{array}{cc} \\sum x^2 & -\\sum x \\\\ -\\sum x & n \\end{array} \\right) \\left( \\begin{array}{c} 1 \\\\ x_0 \\end{array} \\right)\\\\\n&= \\sigma ^2 \\left( \\frac{1}{n}+\\frac{(x_0-\\overline{x})^2}{s_{xx}} \\right)\n\\end{align}\\] where \\(\\overline{x}=\\sum x/n\\) and \\(s_{xx} = \\sum x^2 - n\\overline{x}^2\\). Some additional steps to help you understand this derivation are given in an Appendix (Section 2.11).\n\n\n\nNote that if a model fits well within a certain region in which the design points have been chosen, there is a danger in predicting outside this region, because there is no evidence that the model continues to fit well here, and there might be good scientific reason to believe that it does not. For example, a polynomial model which happens to fit well within a limited range might become wildly unrealistic outside it – say, predicting negative values in a situation where these are impossible.\nFor future purposes it is convenient here to introduce the standardised information matrix \\[\\mathbf{M}=n^{-1} \\mathbf{X}^T\\mathbf{X}\\] and the standardised variance of prediction at a point \\(\\mathbf{x}\\) \\[d(\\mathbf{x})=n\\mathbf{f}(\\mathbf{x})^T(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{f}(\\mathbf{x})=\\mathbf{f}(\\mathbf{x})^T\\mathbf{M}^{-1}\\mathbf{f}(\\mathbf{x})\\] where the inclusion of the factor \\(n^{-1}\\) or \\(n\\) compensates for the fact that increasing the sample size inevitably increases the accuracy of estimation, and the exclusion of the factor \\(\\sigma ^2\\) from \\(d(\\mathbf{x})\\) makes this function a dimensionless and more intrinsic feature of the design. For example, if a whole design is replicated then the standardised information matrix and variance of prediction are the same for the larger experiment as for the smaller one.",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A review of linear models</span>"
    ]
  },
  {
    "objectID": "Review of linear models.html#sec-confregions",
    "href": "Review of linear models.html#sec-confregions",
    "title": "2  A review of linear models",
    "section": "2.9 Confidence regions",
    "text": "2.9 Confidence regions\nWe can obtain confidence intervals for individual parameters in a linear model, but we may be interested in the joint uncertainty of multiple parameters. In the previous section, we saw that the variance of a prediction depends on the variance-covariance matrix \\(\\text{Var}(\\hat{\\boldsymbol{\\beta}})\\). Hence we may want to consider confidence regions for the parameters jointly, as well as confidence intervals for individual parameters.\nWe will not prove this result here, but it can be shown that\n\\[p^{-1}\\hat{\\sigma}^{-2}(\\hat{\\boldsymbol{\\beta}}-\\boldsymbol{\\beta})^T\\mathbf{X}^T\\mathbf{X}(\\hat{\\boldsymbol{\\beta}}-\\boldsymbol{\\beta})\\sim F_{p,n-p},\\]\nprovided \\(n&gt;p\\).\nA \\(100(1-\\alpha)\\%\\) confidence region for \\(\\boldsymbol{\\beta}\\) will take the form \\[(\\boldsymbol{\\beta}-\\hat{\\boldsymbol{\\beta}})^T\\mathbf{X}^T\\mathbf{X}(\\boldsymbol{\\beta}-\\hat{\\boldsymbol{\\beta}}) \\leq p\\hat{\\sigma}^2F_{p, n-p, 1-\\alpha}\\] where \\(F_{p, n-p, 1-\\alpha}\\) is the \\(100(1-\\alpha)\\) percentile of the \\(F_{n,p}\\) distribution.\nIt is not particularly helpful to calculate what the confidence region is, but the formula tells us how the confidence region relates to the choice of experimental design, through the design matrix \\(\\mathbf X\\). Specifically, the confidence region will be the interior of an ellipsoid centred on \\(\\hat{\\boldsymbol{\\beta}}\\). Its size, shape and orientation depend on the covariance matrix \\((\\mathbf{X}^T\\mathbf{X})^{-1}\\). The axes of the ellipsoid point in the directions given by the eigenvectors of \\((\\mathbf{X}^T\\mathbf{X})^{-1}\\), and the length along each axis is proportional to the square root of the corresponding eigenvalue.\n\n\n\n\n\n\nImportantVolume of a confidence region\n\n\n\nGiven that the confidence region is an ellipsoid, from properties of ellipsoids we can say that the volume of the confidence region will be proportional to \\[\n\\sqrt{|(\\mathbf{X}^T\\mathbf{X})^{-1}|}.\n\\] Hence, when designing the experiment, the larger the determinant \\(|\\mathbf{X}^T\\mathbf{X}|\\) is, the smaller we should expect the confidence region for \\(\\boldsymbol{\\beta}\\) to be; the more accurate we should expect our estimate of \\(\\boldsymbol{\\beta}\\) to be.\n\n\n\n\n\n\n\n\nNoteExample\n\n\n\n\nExample 2.8 Simple linear regression with observations taken at \\(x=-1,0 \\: \\text{and}\\: 1\\), and \\(\\sigma^2\\) is unknown. In this case \\[\\mathbf{X}^T\\mathbf{X}= \\left( \\begin{array}{cc} 3 & 0 \\\\ 0 & 2 \\end{array}\\right)\\] and so the confidence region for \\(\\beta_0\\) and \\(\\beta_1\\) is the interior of an ellipse \\[3(\\beta_0-\\hat{\\beta_0})^2 + 2(\\beta_1-\\hat{\\beta_1})^2 \\leq 2\\hat{\\sigma}^2F_{2,1,1-\\alpha }\\] where \\(100(1-\\alpha )\\%\\) is the confidence level. In this case, \\((\\mathbf{X}^T\\mathbf{X})^{-1}\\) is diagonal, and has eigenvectors \\((1,0)^T\\) and \\((0,1)^T\\) and so the two parameters are orthogonal and the ellipse has its principal axes parallel to the co-ordinate axes.",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A review of linear models</span>"
    ]
  },
  {
    "objectID": "Review of linear models.html#tasks",
    "href": "Review of linear models.html#tasks",
    "title": "2  A review of linear models",
    "section": "2.10 Tasks",
    "text": "2.10 Tasks\nThese questions will help you understand the matrix notation and revise some matrix algebra.\n\nFor the example discussed in Section 2.6, suppose there are two catalysts. For each catalyst, three observations are taken, with pressure and temperature settings \\((p_1,t_1)\\), \\((p_2,t_2)\\) and \\((p_3,t_3)\\). Write down the design matrix for this experiment, assuming the indicator variable notation for the linear model.\nIf the \\(i\\)th fitted value \\(\\hat{Y}_i\\) is defined as\n\\[\n\\hat{Y}_i= \\mathbf{f}(\\mathbf{x}_i)^T\\hat{\\boldsymbol{\\beta}},\n\\] check that the vector of fitted values \\(\\hat{\\mathbf Y}\\) is given by \\(\\mathbf{HY}\\).\nFirst show that \\[\\mathbf{H}^T\\mathbf{H} = \\mathbf{H},\\] assuming \\((\\mathbf{X}^T\\mathbf{X})\\) is invertible. Then, by writing \\(\\hat{\\mathbf Y}=\\mathbf{HY}\\), show that \\[\\mathbf{Y}^T\\mathbf{Y}=\\hat{\\mathbf{Y}}^T\\hat{\\mathbf{Y}}+\\hat{\\boldsymbol{\\varepsilon}}^T\\hat{\\boldsymbol{\\varepsilon}},\\] which is a decomposition of the sum of squares of the data into the sum of squares of the fitted values and the sum of squares of the residuals. These are called, respectively, the sum of squares due to the model and the residual (or error) sum of squares.\nConsider the model and design in Example 2.5. A dataset corresponding to this design can be set up in R as follows (the y values are made up; the actual values are not important for this exercise).\n\n\nexample2_5 &lt;- data.frame(x = c(-1, 0, 1, -1, 0, 1),\n                         z = c(-1, -1, -1, 1, 1, 1),\n                         y  = c(8, 10, 12, 4, 3, 9))\n\nBy fitting suitable models in R, verify the second property of orthogonal parameters discussed in Section 2.7.\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\n\nWriting the parameter vector \\(\\boldsymbol{\\beta}=(\\beta_0, \\alpha, \\beta_1, \\beta_2)^T\\), we have \\[\n\\mathbf X = \\left(\\begin{array}{cccc}\n1 & 0 & p_1 & t_1\\\\\n1 & 0 & p_2 & t_2\\\\\n1 & 0 & p_3 & t_3\\\\\n1 & 1 & p_1 & t_1\\\\\n1 & 1 & p_2 & t_2\\\\\n1 & 1 & p_3 & t_3\n\\end{array}\\right)\n\\]\nWe have \\[\n\\hat{\\mathbf Y} =\\left(\\begin{array}{c}\n\\mathbf{f}(\\mathbf{x}_1)^T\\hat{\\boldsymbol{\\beta}}\\\\\n\\vdots \\\\\n\\mathbf{f}(\\mathbf{x}_n)^T\\hat{\\boldsymbol{\\beta}}\n\\end{array}\\right) = \\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} = \\mathbf{HY}\n\\]\nNote that if \\((\\mathbf{X}^T\\mathbf{X})\\) is invertible then \\[\n((\\mathbf{X}^T\\mathbf{X})^{-1})^T = ((\\mathbf{X}^T\\mathbf{X})^T)^{-1} = (\\mathbf{X}^T\\mathbf{X})^{-1}.\n\\]Then \\[\n\\mathbf{H}^T\\mathbf{H}=\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T = \\mathbf{H}.\n\\] Hence \\[\\begin{align}\n\\hat{\\mathbf{Y}}^T\\hat{\\mathbf{Y}}+\\hat{\\boldsymbol{\\varepsilon}}^T\\hat{\\boldsymbol{\\varepsilon}} &= (\\mathbf{HY})^T\\mathbf{HY}+(\\mathbf{Y}-\\mathbf{HY})^T(\\mathbf{Y}-\\mathbf{HY})\\\\\n&=\\mathbf{Y}^T\\mathbf{H}^T\\mathbf{HY}+\\mathbf{Y}^T\\mathbf{Y}+\\mathbf{Y}^T\\mathbf{H}^T\\mathbf{HY}-\\mathbf{Y}^T\\mathbf{H}\\mathbf{Y}-\\mathbf{Y}^T\\mathbf{H}^T\\mathbf{Y}\\\\\n&=\\mathbf{Y}^T\\mathbf{HY}+\\mathbf{Y}^T\\mathbf{Y}+\\mathbf{Y}^T\\mathbf{HY}-\\mathbf{Y}^T\\mathbf{H}\\mathbf{Y}-\\mathbf{Y}^T\\mathbf{H}^T\\mathbf{Y}\\\\\n&=\\mathbf{Y}^T\\mathbf{Y}\n\\end{align}\\] as required, as \\(\\mathbf{H}^T=\\mathbf{H}\\).\nWe fit the model specified in the example as follows:\n\n\nlm(y ~ x + z + I(x^2), example2_5)\n\n\nCall:\nlm(formula = y ~ x + z + I(x^2), data = example2_5)\n\nCoefficients:\n(Intercept)            x            z       I(x^2)  \n      6.500        2.250       -2.333        1.750  \n\n\nIt was noted that \\(\\{\\beta_0, \\beta_{11}\\} ,\\{\\alpha\\}\\) and \\(\\{\\beta_1\\}\\) are mutually orthogonal groups. If we remove the terms in the model corresponding to one group, the remaining parameter estimates should not change. We verify this as follows:\n\nlm(y ~ x  + I(x^2), example2_5) # remove z\n\n\nCall:\nlm(formula = y ~ x + I(x^2), data = example2_5)\n\nCoefficients:\n(Intercept)            x       I(x^2)  \n       6.50         2.25         1.75  \n\nlm(y ~ z + I(x^2), example2_5) # remove x\n\n\nCall:\nlm(formula = y ~ z + I(x^2), data = example2_5)\n\nCoefficients:\n(Intercept)            z       I(x^2)  \n      6.500       -2.333        1.750  \n\nlm(y ~ x + z - 1, example2_5) # remove the intercept and x^2\n\n\nCall:\nlm(formula = y ~ x + z - 1, data = example2_5)\n\nCoefficients:\n     x       z  \n 2.250  -2.333",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A review of linear models</span>"
    ]
  },
  {
    "objectID": "Review of linear models.html#sec-ch2Appendix",
    "href": "Review of linear models.html#sec-ch2Appendix",
    "title": "2  A review of linear models",
    "section": "2.11 Appendix",
    "text": "2.11 Appendix\nSome results to help with the algebra in this section. First note that we can write \\(\\sum_{i=1}^n x_i = n\\bar{x}\\). Then\n\\[\\begin{align}\ns_{xx}&:= \\sum_{i=1}^n (x_i -\\bar{x}^2)\\\\\n&= \\left(\\sum_{i=1}^n x_i^2\\right) - 2\\bar{x}\\left(\\sum_{i=1}^nx_i\\right) + \\left(\\sum_{i=1}^n \\bar{x}^2\\right)\\\\\n& = \\left(\\sum_{i=1}^n x_i^2\\right) - 2\\bar{x}n\\bar{x} + n\\bar{x}^2\\\\\n& = \\left(\\sum_{i=1}^n x_i^2\\right) - n\\bar{x}^2.\n\\end{align}\\]\nTo help with Example 2.7, we can use the result above to write \\[\n\\sum_{i=1}^n x_i^2 = s_{xx} + n\\bar{x}^2\n\\] Then we have \\[\\begin{align}\n\\left( \\begin{array}{cc} 1 & x_0 \\end{array} \\right) \\left( \\begin{array}{cc} \\sum_{i=1}^n x^2 & -\\sum_{i=1}^n x \\\\ -\\sum_{i=1}^n x & n \\end{array} \\right) \\left( \\begin{array}{c} 1 \\\\ x_0 \\end{array} \\right)&= \\left( \\begin{array}{cc} 1 & x_0 \\end{array} \\right) \\left( \\begin{array}{cc} \\sum_{i=1}^n x^2 & -n\\bar{x} \\\\ -n\\bar{x} & n \\end{array} \\right) \\left( \\begin{array}{c} 1 \\\\ x_0 \\end{array} \\right)\\\\&=\\sum_{i=1}^n x_i^2 - 2nx_0\\bar{x} + nx_0^2\\\\\n&= s_{xx} +n\\bar{x}^2 - 2nx_0\\bar{x} + nx_0^2\\\\\n&= s_{xx} + n (x_0-\\bar{x})^2,\n\\end{align}\\] from which we derive the expression for \\(\\text{var}(\\hat{Y}(x_0))\\).",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A review of linear models</span>"
    ]
  },
  {
    "objectID": "Review of linear models.html#footnotes",
    "href": "Review of linear models.html#footnotes",
    "title": "2  A review of linear models",
    "section": "",
    "text": "If you’ve forgotten the definition/properties of variance-covariance matrices, some notes are available here: https://oakleyj.github.io/MAS61004LM/randomVectors.html#covariance-matrix↩︎",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A review of linear models</span>"
    ]
  },
  {
    "objectID": "Optimality.html",
    "href": "Optimality.html",
    "title": "3  Optimality criteria",
    "section": "",
    "text": "3.1 Bounded design regions\nWe will shortly be considering examples where we maximise/minimise functions of design points. You may be used to maximising a function by differentiating and equating to zero. This approach won’t necessarily work for variables that are bounded. For example, an explanatory variable \\(x\\) may be constrained to lie in the interval \\([-1,1]\\): the design region is bounded. We will often need to consider what happens when the variable takes one of the interval limits, and some curve sketching may help.",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Optimality criteria</span>"
    ]
  },
  {
    "objectID": "Optimality.html#d-optimality",
    "href": "Optimality.html#d-optimality",
    "title": "3  Optimality criteria",
    "section": "3.2 D-optimality",
    "text": "3.2 D-optimality\nIn Section 2.9 we saw that the volume of the confidence region for \\(\\boldsymbol{\\beta}\\) decreases as the determinant of the information matrix \\(\\mathbf{M} =\\mathbf{X}^T\\mathbf{X}\\) increases.\nA design is called D-optimal if it maximises the determinant \\(|\\mathbf{M}|\\). Hence a D-optimal design is best in the sense of joint estimation of the model parameters.\n\n\n\n\n\n\nNoteExample\n\n\n\n\nExample 3.1 In simple linear regression, we noted that \\[\\begin{align}\n    \\mathbf{X}^T\\mathbf{X}&=\\left( \\begin{array}{cc} n & \\sum x \\\\\n    \\sum x & \\sum x^2 \\end{array} \\right) \\\\\n    \\text{and so } |\\mathbf{X}^T\\mathbf{X}| &= n\\sum x^2-(\\sum x)^2 \\\\\n    &= ns_{xx}\n    \\end{align}\\] so that for given sample size it is D-optimal to choose \\(s_{xx}\\) as large as possible. If the range of feasible \\(x\\) is a finite interval \\([x_{min},         x_{max}]\\) say, then this amounts to taking half the observations at \\(x=x_{min}\\) and half at \\(x=x_{max}\\), or as nearly as possible so. Although such a design is D-optimal, it has the disadvantage that it yields no evidence that the model fits well in the middle of the range of values of \\(x\\).*\n\n\n\n\n3.2.1 G-optimality\nLet \\(\\Omega\\) denote the design region: the space of possible positions of \\(k\\) design points. A design \\(\\mathbf{x_1}^*, \\mathbf{x_2}^*, \\ldots                 ,\\mathbf{x_k}^ \\in \\Omega\\) is called G-optimal if it minimises the maximum standardised variance of prediction over the whole design region. Equivalently \\[\\mathbf{x_1}^*,\\mathbf{x_2}^*,\\ldots,\\mathbf{x_k}^*=\\mathop{\\mathrm{argmin}}_{\\mathbf{x_1},\\mathbf{x_2},\\ldots,\\mathbf{x_k}\\in\\Omega}\n    \\left\\{ \\max_{\\mathbf{x_r}\\in\\Omega} d(\\mathbf{x_r})\\right\\}\\] where \\(d(\\mathbf{x})\\) is the standardised variance of prediction at a point \\(\\mathbf{x}\\). So in principle for each potential design we evaluate the worst possible variance of prediction within the design region, and then choose the design for which this is least.\n\n\n\n\n\n\nNoteExample\n\n\n\n\nExample 3.2 In the simple linear regression model, following on from Example 3.1, we get \\[d(x)=1+\\frac{n(x-\\overline{x})^2}{s_{xx}}\\] which is maximised when \\(x\\) is as far as possible from \\(\\overline{x}\\), namely whichever endpoint \\(x_{min}\\) or \\(x_{max}\\) is further away from \\(\\overline{x}\\). In particular, the maximised value of \\((x-\\overline{x})^2\\) will always be at least \\(\\frac{1}{4}(x_{max}-x_{min})^2\\). It can be seen that by taking half the observations at \\(x=x_{min}\\) and half at \\(x=x_{max}\\) we get \\(\\overline{x}=\\frac{1}{2}(x_{min}+x_{max})\\) and then we simultaneously minimise the maximum value of \\((x-\\overline{x})^2\\) and maximise the value of \\(s_{xx}\\), thereby minimising the maximum value of \\(d(x)\\) in the interval. Hence in this case the G-optimality criterion leads to the same design as the D-optimality criterion.\n\n\n\n\n\n3.2.2 V-optimality.\nA design is called V-optimal if it minimises a weighted average \\[\\int_{\\Omega } d(\\mathbf{x}) w(\\mathbf{x}) d\\mathbf{x}\\] of the standardised variance of prediction over the design region. Here \\(w\\) is some non-negative weighting function, which could be scaled to integrate to \\(1\\), although this is not necessary.\n\n\n\n\n\n\nNoteExample\n\n\n\n\nExample 3.3 Simple linear regression with \\(x\\in [-1,1]\\) and uniform weighting: we require to minimise \\[\\begin{align}\n\\int_{-1}^1 \\left(1+\\frac{n(x-\\overline{x})^2}{s_{xx}}\\right) dx &=2+\\frac{n}{s_{xx}} \\left[\\frac{(x-\\overline{x})^3}{3}\\right]_{-1}^1 \\\\\n&= 2 + \\frac{n}{3s_{xx}} \\{ (1-\\overline{x})^3- (-1-\\overline{x})^3 \\} \\\\\n&= 2 + \\frac{n}{3s_{xx}} \\{ 2 + 6\\overline{x}^2\\}.\n\\end{align}\\] Once again, we see that if there are an even number of observations, taking half of the observations at each of the end-points we simultaneously minimise \\(\\overline{x}^2\\) (as zero) and maximise \\(s_{xx}\\), and therefore minimise the above expression. So, in this case, D-optimality, G-optimality and V-optimality all yield the same design.",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Optimality criteria</span>"
    ]
  },
  {
    "objectID": "Optimality.html#sec-optimalityTasks",
    "href": "Optimality.html#sec-optimalityTasks",
    "title": "3  Optimality criteria",
    "section": "3.3 Tasks",
    "text": "3.3 Tasks\n\nTo estimate the parameters in the quadratic regression model \\[ E(Y) = \\beta_0 + \\beta_1x + \\beta_{11}x^2 \\] for \\(x\\in [-1,1]\\) it is proposed to take observations at the three points \\(x=1\\), \\(x=-1\\) and \\(x=a\\) for some \\(a\\in [-1,1]\\). Show that the D-optimal choice of \\(a\\) is \\(a=0\\). Note that if \\(\\mathbf{X}\\) is a square matrix, then \\(|\\mathbf{X}^T\\mathbf{X}|=|\\mathbf{X}^T||\\mathbf{X}|=|\\mathbf{X}|^2\\). Show also that for this design with \\(a=0\\), \\[\\max _{x\\in [-1,1]} d(x) = 3\\] and the maximum occurs at each of the three points of the design \\(x=\\pm 1\\) and \\(x=0\\). To show this you can use the result that \\[\n4-6x^2 +6x^4= \\left\\{ 6 \\left( x^2 - \\frac{1}{2} \\right) ^2 +\n\\frac{5}{2} \\right\\}\n\\]\nWhat is the V-optimal design in Example 3.3 if the weighting function is \\(w(x)=x^2\\) with an even number of observations? Hint: follow the optimisation approach in the example: don’t try to differentiate anything!\nStill using V-optimality, and Example 3.3 show that the weight function \\(w(x)\\) given by \\[\nw(x)= \\left\\{ \\begin{array}{ll} 0 & ~ \\mbox{for}~ x \\leq 0 \\\\\n1 & ~ \\mbox{for}~ x &gt;0\n\\end{array}\\right.\n\\] does not lead to the same design as in Example 3.3 with 2 observations.\n\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\n\nWe have \\[\\begin{align}\n|\\mathbf{X} | & =  \\begin{array}{|ccc|} 1 & 1 & 1 \\\\ 1 & -1 & 1 \\\\ 1 & a &\na^2 \\end{array} \\\\\n& =  2 - 2a^2. \\\\\n\\mbox{So~~~~}|\\mathbf{X} ^T\\mathbf{X}| &=4(1-a^2)^2\n\\end{align}\\] which is maximised when \\(a=0\\). In this case, \\[\\begin{align}\n\\mathbf{X} ^T\\mathbf{X} &=\\left( \\begin{array}{ccc} 3 & 0 & 2 \\\\ 0 & 2 & 0 \\\\\n2 &\n0 & 2 \\end{array} \\right) \\\\\n\\mbox{and~~~~}(\\mathbf{X} ^T\\mathbf{X} )^{-1} &=\\frac{1}{4} \\left(\n\\begin{array}{ccc} 4 & 0 & -4 \\\\ 0 & 2 & 0 \\\\ -4 & 0 & 6 \\end{array}\n\\right) . \\\\\nd(x) &=\\frac{3}{4} \\left( \\begin{array}{ccc} 1 & x & x^2\n\\end{array} \\right) \\left( \\begin{array}{ccc} 4 & 0 & -4 \\\\ 0 & 2 &\n0 \\\\ -4 & 0 & 6 \\end{array} \\right) \\left( \\begin{array}{c} 1 \\\\ x\n\\\\ x^2 \\end{array} \\right) \\\\\n&=\\frac{3}{4}(4-6x^2 +6x^4)\\mbox{~~~~after~some~algebra} \\\\\n&=\\frac{3}{4} \\left\\{ 6 \\left( x^2 - \\frac{1}{2} \\right) ^2 +\n\\frac{5}{2} \\right\\} .\n\\end{align}\\] This is maximised when \\(x^2\\) is as far as possible from \\(\\frac{1}{2}\\), namely \\(x^2=0\\mbox{~or~}1\\), and so \\(x=0\\mbox{~or~}\\pm\n1\\). Then its value is \\[\\frac{3}{4}\\left\\{ \\frac{6}{4} + \\frac{5}{2} \\right\\} = 3 . \\]\nWe have \\[\\int _{-1}^1 x^2 \\left( 1 + \\frac{n(x-\\overline{x})^2}{s_{xx}} \\right) dx  =  \\frac{2}{3} + \\frac{2n}{15s_{xx}}\\left( 3+5\\bar{x}^2\\right) \\] so we again see that with an even number of observations it is \\(V\\)-optimal to choose half at \\(x=1\\) and half at \\(x=-1\\) since \\(\\bar{x}=0\\) and \\(s_{xx}\\) is maximised.\nWe have \\[\\begin{align}\n\\int _{0}^1 \\left( 1 + \\frac{n(x-\\overline{x})^2}{s_{xx}} \\right)dx\n&=1 + \\frac{n}{s_{xx}} \\left[ \\frac{(x-\\overline{x})^3}{3} \\right] _{0}^1 \\\\\n&=1 + \\frac{n}{3s_{xx}} \\{ 1 -3\\overline{x}+3\\overline{x}^2 \\} .\n\\end{align}\\] If we take \\(x =\\{-1,1\\}\\) then \\(1 + \\frac{n}{3s_{xx}} \\{ 1 -3\\overline{x}+3\\overline{x}^2 \\}=4/3\\) whereas if \\(x =\\{-0.8,1\\}\\) then \\(1 + \\frac{n}{3s_{xx}} \\{ 1 -3\\overline{x}+3\\overline{x}^2 \\}=316/243&lt;4/3\\). So the design in Example 3.1.3 is not \\(V\\)-optimal.",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Optimality criteria</span>"
    ]
  },
  {
    "objectID": "Qualitative-single factor.html",
    "href": "Qualitative-single factor.html",
    "title": "4  Qualitative explanatory variables and blocking",
    "section": "",
    "text": "4.1 Qualitative explanatory variables\nIn Section 2.6 we reviewed how qualitative explanatory variables (e.g. which treatment is given to a patient) can be incorporated in a linear model. We will refer to a qualitative explanatory variable as a factor taking one of \\(t\\) possible levels. If designing an experiment with a single factor explanatory variables, the design problem is to choose what level of the factor variable (e.g. which treatment) to apply to each experimental subject (e.g. each patient).",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Qualitative explanatory variables and blocking</span>"
    ]
  },
  {
    "objectID": "Qualitative-single factor.html#completely-randomised-designs-crd",
    "href": "Qualitative-single factor.html#completely-randomised-designs-crd",
    "title": "4  Qualitative explanatory variables and blocking",
    "section": "4.2 Completely Randomised Designs (CRD)",
    "text": "4.2 Completely Randomised Designs (CRD)\nA typical example is where an investigator is wishing to compare \\(t\\) different drugs, one of which may be a placebo, by dividing the patients into \\(t\\) groups and administering a different drug to each group.\nA completely randomised design (CRD) is one where subjects (patients) are allocated randomly to treatments, with every possible allocation of subjects to treatments equally likely to be chosen. This helps to avoid bias (e.g. its less likely that the healthiest patients are all allocated to the same treatment). We won’t consider this in this module, but there are also particular statistical methods for analysis that can take advantage of this randomisation.\nThe model may be written \\[\nY_{ij} = \\mu _i + \\varepsilon _{ij}~~~~(i=1,2,\\ldots ,t; j=1,2,\\ldots n_i)\n\\tag{4.1}\\] where \\(n_i\\) is the size of the \\(i\\)-th group, \\(Y_{ij}\\) is the \\(j\\)-th observation in the \\(i\\)-th group, \\(\\mu _i\\) is the unknown mean for the \\(i\\)-th treatment, and \\(\\varepsilon _{ij}\\sim N(0,\\sigma^2)\\) is random error.\nWriting this model in matrix notation, we find that the parameters are orthogonal and we obtain (using the results in Chapter 2) \\[\\begin{align}\n\\hat{\\mu}_i=\\overline{Y}_{i.}\\\\\n\\text{var}(\\hat{\\mu})_i=\\frac{\\sigma ^2}{n_i}\n\\end{align}\\] where \\(\\overline{Y}_{i.}\\) is the mean of the \\(i\\)-th group.\nFrom the design point of view, the main question here is how best to choose the sizes of the groups. Obviously, as ever, the larger the experiment, the more accurate the estimators will be, and so it is of more interest to look at the problem of choice of \\(n_1, n_2, \\ldots ,n_t\\) subject to a fixed total number of observations \\(n\\) say.\nIf, for example, we adopt the \\(D\\)-optimality criterion, then, noting that in this case \\(\\mathbf{X}^T\\mathbf{X}\\) is the diagonal matrix with \\(n_1, n_2, \\ldots , n_t\\) down the diagonal, its determinant is \\(n_1n_2\\ldots n_t\\) and this is maximised subject to \\(n_1+n_2+\\ldots\n+n_t=n\\) by taking \\(n_1, n_2, \\ldots , n_t\\) as nearly as possible equal to each other.\n\n4.2.1 Estimating treatment differences\nIf there is a placebo, an alternative might be to aim to minimise the variance of the estimated treatment difference between each of the genuine treatments and the placebo. Suppose that each of the genuine treatments is administered to \\(m\\) patients, and the placebo to the remaining \\(n-(t-1)m\\) patients. If group 1 is the placebo group, then for \\(i\\neq 1\\) the effect of treatment \\(i\\) is \\(\\mu_i - \\mu_1\\), and the variance of the estimator, as a function of \\(m\\) is \\[\nv(m):=var(\\hat{\\mu}_i -\\hat{\\mu}_1) = var(\\hat{\\mu}_i)+ var(\\hat{\\mu}_1) = \\sigma ^2\\left( \\frac{1}{m} + \\frac{1}{n-(t-1)m} \\right).\\] Note that \\(\\hat{\\mu}_i\\) and \\(\\hat{\\mu}_1\\) are independent. We minimise this by treating \\(m\\) as a continuous variable and using calculus. Leaving out the constant multiplicative factor \\(\\sigma\n^2\\), \\[\\begin{align}\n\\frac{d}{dm} v(m)&=-\\frac{1}{m^2} + \\frac{t-1}{(n-(t-1)m)^2}.\\\\\n\\frac{d^2}{dm^2}v(m)&=\\frac{2}{m^3}+\\frac{2(t-1)^2}{(n-(t-1)m)^3}&gt;0.\n\\end{align}\\] Hence a minimum occurs when \\((t-1)m^2 = (n-(t-1)m)^2\\), and this is an equation which may be solved for \\(m\\). The solution will not necessarily be an integer, and so in practice it may have to be rounded up or down.\n\n\n\n\n\n\nNoteExample\n\n\n\n\nExample 4.1 Let \\(n=24\\) and \\(t=3\\). In this case the equation is\n\\[2m^2 = (24-2m)^2\\] leading to \\[m = \\frac{24}{2+\\sqrt{2}} \\simeq 7.03\\] In practice we would take \\(m=7\\), and administer each of the two genuine treatments to seven patients, and the placebo to the remaining ten.",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Qualitative explanatory variables and blocking</span>"
    ]
  },
  {
    "objectID": "Qualitative-single factor.html#reducing-variation-using-blocking",
    "href": "Qualitative-single factor.html#reducing-variation-using-blocking",
    "title": "4  Qualitative explanatory variables and blocking",
    "section": "4.3 Reducing variation using blocking",
    "text": "4.3 Reducing variation using blocking\nIn the general linear model \\[\nY = \\mathbf{f}(\\mathbf{x})^T\\boldsymbol{\\beta}+\\varepsilon,\n\\] the role of \\(\\varepsilon\\) is to account for any variation in the response variable that is not modelled (‘explained’) by the explanatory variables \\(\\mathbf{x}\\). It may also account for measurement errors when observing the value of the response variable. The more unexplained variation there is, the greater the value of \\(\\sigma^2 = Var(\\varepsilon)\\) is, which then increases the variance of the parameter estimates: \\[\\text{Var}(\\hat{\\boldsymbol{\\beta}}) =\\sigma^2 (\\mathbf{X}^T\\mathbf{X})^{-1}\\]\nWe can try to reduce variation by recording additional explanatory variables and incorporating them into the model, so that there is less ‘unexplained’ variation. For example, we could record additional physical characteristics of each patient, and include those in the model. It may be difficult to find a good model however; the relationship between these additional characteristics and the response variable may be complicated.\nAn alternative is to group the experimental subjects into blocks with similar characteristics, then simply incorporate a term in the model for the effect of each block. This can reduce unexplained variation, and hence reduce \\(Var(\\varepsilon)\\), without the need to find a complicated model.\n\n\n\n\n\n\nImportantChoosing blocks\n\n\n\nBlocks (groups) should always be chosen so that we expect less variation within blocks than between blocks. All other things being equal, we would expect smaller differences between observed responses within a block, compared with differences between observed responses in different blocks.\n\n\nContinuing the example of comparing \\(t\\) different drugs, if the patients are grouped into \\(b\\) blocks, a suitable model would be\n\\[Y_{ijk} = \\mu +\\alpha_i + \\tau_j+ \\varepsilon _{ijk}, \\tag{4.2}\\] where \\(Y_{ijk}\\) is the response for the \\(k\\)-th patient within block \\(i\\) who was allocated to treatment \\(j\\). We have \\(i=1,\\ldots,b\\); \\(j=1,\\ldots,t\\) and \\(k=1,\\ldots,n_{ij}\\). Constraints are needed on the block and treatment parameters, for example \\[\n\\sum_{i=1}^b\\alpha_i = \\sum_{j=1}^t \\tau_i = 0.\n\\]\nWe write that \\(\\varepsilon_{ijk}\\sim N(0,\\tilde{\\sigma}^2)\\) and use the notation \\(Var(\\varepsilon_{ijk})=\\tilde{\\sigma}^2\\) to make clear that we expect the error term to have a different (smaller) variance than the model in Equation 4.1, as we have explained additional variation in the response using the block effects.\n\n\n\n\n\n\nImportantThe purpose of blocking\n\n\n\nTypically, the block effects \\(\\alpha_1,\\alpha_2,\\ldots\\) are not of much interest: it’s the treatment effect parameters \\(\\tau_1,\\tau_2,\\ldots\\)that we are really interested in. The purpose of blocking is to reduce ‘unwanted variation’ (the variance of the error term), so that we can expect more accurate parameter estimates.",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Qualitative explanatory variables and blocking</span>"
    ]
  },
  {
    "objectID": "Qualitative-single factor.html#sec-RBD",
    "href": "Qualitative-single factor.html#sec-RBD",
    "title": "4  Qualitative explanatory variables and blocking",
    "section": "4.4 Randomised Block Designs (RBD)",
    "text": "4.4 Randomised Block Designs (RBD)\nHaving organised the experimental “units” (e.g. the patients in the study) into blocks, the remaining design question is how to allocate the units to treatments. If we use a randomised block design (RBD) then\n\nall of the blocks contain the same number of units;\neach treatment is used the same number of times within each block.\n\nThe simplest case is where each treatment is administered once within each block. In this case we can drop the \\(k\\) subscript in Equation 4.2 and write\n\\[Y_{ij} = \\mu +\\alpha_i + \\tau_j+ \\varepsilon _{ij},\\] with the same constraints and definitions as before.\n\n\n\n\n\n\nNoteExample\n\n\n\n\nExample 4.2 Suppose we have \\(b=2\\) blocks and \\(t=3\\) treatments. For an RBD, the model may be expressed as \\[\nE\\left( \\begin{array}{c} Y_{11} \\\\ Y_{12} \\\\ Y_{13} \\\\ Y_{21} \\\\\nY_{22} \\\\ Y_{23} \\end{array} \\right) = \\left( \\begin{array}{cccc} 1 & 1 & 1 & 0 \\\\ 1 & 1 & 0 & 1 \\\\ 1 & 1 & -1 & -1 \\\\ 1 & -1 & 1 & 0 \\\\\n1 & -1 & 0 & 1 \\\\ 1 & -1 & -1 & -1 \\end{array} \\right) \\left(\\begin{array}{c} \\mu \\\\ \\alpha _1 \\\\ \\tau _1 \\\\ \\tau _2 \\end{array}\\right)\n\\] and in this case \\[\\mathbf{X}^T\\mathbf{X}=\\left(\\begin{array}{cccc} 6 & 0 & 0 & 0 \\\\ 0 & 6 & 0 & 0 \\\\ 0 & 0 & 4 & 2 \\\\ 0 & 0 & 2 & 4 \\end{array} \\right)\n\\text{ and }(\\mathbf{X}^T\\mathbf{X})^{-1}=\\left(\\begin{array}{rrrr}\\frac{1}{6} & 0 & 0 & 0 \\\\ 0 & \\frac{1}{6} & 0 & 0 \\\\ 0 & 0 & \\frac{1}{3} &-\\frac{1}{6} \\\\ 0 & 0 & -\\frac{1}{6} & \\frac{1}{3} \\end{array} \\right).\\] Note that the three groups of parameters \\(\\{ \\mu \\} , \\{ \\alpha _i\\}\\) and \\(\\{ \\tau _j\\}\\) are mutually orthogonal.\n\n\n\nThe symmetry of the RBD makes it relatively easy to analyse. If \\(\\overline{Y}_{..}\\) denotes the overall mean of the observations, \\(\\overline{Y}_{i.}\\) denotes the mean of block \\(i\\) and \\(\\overline{Y}_{.j}\\) denotes the mean of treatment \\(j\\), then using the resultsfrom Chapter 2 we obtain \\[\\begin{align}\n\\hat{\\mu}&=\\overline{Y}_{..} \\\\\n\\text{var}(\\hat{\\mu})&=\\frac{\\tilde{\\sigma} ^2}{bt}\\\\\n\\hat{\\alpha}_i&=\\overline{Y}_{i.} - \\overline{Y}_{..}\\\\\n\\text{var}(\\hat{\\alpha}_i)&=\\frac{\\tilde{\\sigma} ^2}{t}\\left( 1 - \\frac{1}{b} \\right)\\\\\n\\text{cov}(\\hat{\\alpha}_i, \\hat{\\alpha }_{i^{\\prime }})&=-\\frac{\\tilde{\\sigma} ^2}{bt}\\text{ for }i\\neq i^{\\prime}\\\\\n\\hat{\\tau }_j&= \\overline{Y}_{.j} - \\overline{Y}_{..}\\\\\n\\text{var}(\\hat{\\tau }_j)=&\\frac{\\tilde{\\sigma}^2}{b}\\left( 1 - \\frac{1}{t} \\right)\\\\\n\\text{cov}(\\hat{\\tau}_j, \\hat{\\tau }_{j^{\\prime }})&=-\\frac{\\tilde{\\sigma}^2}{bt}\\text{ for }j\\neq j^{\\prime}\n\\end{align}\\] and the three groups of parameters are mutually orthogonal and therefore independent.",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Qualitative explanatory variables and blocking</span>"
    ]
  },
  {
    "objectID": "Qualitative-single factor.html#comparing-crds-and-rbds",
    "href": "Qualitative-single factor.html#comparing-crds-and-rbds",
    "title": "4  Qualitative explanatory variables and blocking",
    "section": "4.5 Comparing CRDs and RBDs",
    "text": "4.5 Comparing CRDs and RBDs\nAssuming the same overall sample size, which design is best for estimating a treatment effect? If we have \\(b\\) blocks and \\(m\\) treatments blocks in an RBD, the total sample size is \\(m\\times b\\). In a \\(D\\)-optimal CRD, each treatment would be used \\(b\\) times to get the same total sample size.\nThe parameters in the models Equation 4.1 and Equation 4.2 have different interpretations, so we first define a quantity that has the same meaning for both designs/models.\nWe consider the expected difference in response between two treatments \\(p\\) and \\(q\\) if given to the same patient. For a CRD, this expected difference is \\(\\mu_p-\\mu_q\\), and the variance of the estimator is \\[ \\mbox{Var}(\\hat{\\mu }_p-\\hat{\\mu }_{q}) = \\frac{\\sigma\n^2}{b} + \\frac{\\sigma ^2}{b} = \\frac{2\\sigma ^2}{b},\\] as \\(\\hat{\\mu }_p\\) and \\(\\hat{\\mu }_{q}\\) are independent.\nFor an RBD, this expected difference is \\(\\tau_p-\\tau_q\\) (the block effect cancels out) and the variance of the estimator is\n\\[\\begin{align*}\n\\mbox{Var}(\\hat{\\tau }_p-\\hat{\\tau }_{q}) & =\n\\mbox{Var}\\hat{\\tau }_p + \\mbox{Var}\\hat{\\tau }_{q}\n-2\\mbox{Cov}(\\hat{\\tau }_p, \\hat{\\tau }_q) \\\\\n& =  \\frac{2\\tilde{\\sigma}^2}{b}\\left( 1-\\frac{1}{t} \\right) +\n\\frac{2\\tilde{\\sigma} ^2}{bt} \\\\\n& =  \\frac{2\\tilde{\\sigma} ^2}{b} .\n\\end{align*}\\]\nSo the estimator will have smaller variance if \\(\\tilde{\\sigma}&lt;\\sigma\\). We would expect this to be the case, as the use of blocking should reduce the error variance. However, this benefit doesn’t come for free! For an RBD, we have fewer degrees of freedom for estimating the variance parameter, as we have more parameters in the model.",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Qualitative explanatory variables and blocking</span>"
    ]
  },
  {
    "objectID": "Qualitative-single factor.html#tasks",
    "href": "Qualitative-single factor.html#tasks",
    "title": "4  Qualitative explanatory variables and blocking",
    "section": "4.6 Tasks",
    "text": "4.6 Tasks\nQuestion 1 is a good revision exercise (you will have an exam formula sheet: you won’t need to memorise specific formulae). Question 2 is more for general interest (justifying a result in this chapter), but is not something you would be assessed on.\n\nFor the example in Example 4.2, use matrix notation and the relevant results for linear models to verify the results (estimators, variances, covariances) in Section 4.4.\nShow that, for given positive integers \\(n&gt;t\\), the product \\(n_1n_2\\ldots n_t\\) of positive integers subject to the constraint \\(n_1+n_2+\\ldots +n_t=n\\) is maximised by taking \\(n_1, n_2, \\ldots,n_t\\) as nearly as possible equal to each other, using the following approach. If two of the factors differ by more than two, say \\(n_1\\geq n_2+2\\), then the product could be increased by replacing \\(n_1\\) and \\(n_2\\) by \\(n_1-1\\) and \\(n_2+1\\); hence such a choice cannot be optimal.\n\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\n\nFor the least squares estimates we compute \\[\n\\left(\\begin{array}{c} \\hat{\\mu} \\\\ \\hat{\\alpha}_1 \\\\ \\hat{\\tau}_1 \\\\ \\hat{\\tau}_2 \\end{array}\\right) = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\n\\]\n\nand the variance covariance matrix is obtained as\n\\[\n\\left(\\begin{array}{cccc}var(\\hat{\\mu}) & cov(\\hat{\\mu}, \\hat{\\alpha}_1) & cov(\\hat{\\mu}, \\hat{\\tau}_1)  & cov(\\hat{\\mu}, \\hat{\\tau}_2) \\\\\ncov(\\hat{\\alpha}_1, \\hat{\\mu}) & var(\\hat{\\alpha}_1) & cov(\\hat{\\alpha}_1, \\hat{\\tau}_1)  & cov(\\hat{\\alpha}_1, \\hat{\\tau}_2) \\\\\ncov(\\hat{\\tau}_1, \\hat{\\mu}) & cov(\\hat{\\tau}_1, \\hat{\\alpha}_1)  & var(\\hat{\\tau}_1) & cov(\\hat{\\tau}_1, \\hat{\\tau}_2)\\\\\ncov(\\hat{\\tau}_2, \\hat{\\mu}) & cov(\\hat{\\tau}_12 \\hat{\\alpha}_1)  &   cov(\\hat{\\tau}_2, \\hat{\\tau}_1) & var(\\hat{\\tau}_2)\n\\end{array}\\right) = \\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1}.\n\\] Using the values given in the example for \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\), we get the same parameter estimates and variances/covariances as stated.\n\nIf \\(n_1\\geq n_2+2\\) then \\[\\begin{align}\n(n_1-1)(n_2+1) & =  n_1n_2 + n_1 - n_2 - 1 \\\\\n& \\geq  n_1n_2 + 2 - 1 \\\\\n& =  n_1n_2 + 1 \\\\\n& &gt;  n_1n_2\n\\end{align}\\] and therefore, since the other factors are all positive, the whole product could be increased by replacing \\(n_1\\) and \\(n_2\\) by \\(n_1-1\\) and \\(n_2+1\\), and therefore the original choice cannot be optimal. It follows that the only candidates for optimality are those where no two numbers differ by more than \\(1\\), and so they are as nearly as possible equal to each other.",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Qualitative explanatory variables and blocking</span>"
    ]
  },
  {
    "objectID": "Qualitative-single factor.html#appendix---balanced-incomplete-block-designs",
    "href": "Qualitative-single factor.html#appendix---balanced-incomplete-block-designs",
    "title": "4  Qualitative explanatory variables and blocking",
    "section": "4.7 Appendix - Balanced Incomplete Block Designs",
    "text": "4.7 Appendix - Balanced Incomplete Block Designs\nThis is included for reference only and is non-examinable.\nIn some cases, the size of a block (the number of experimental subjects in a block) may be too small to accommodate all the possible treatments. In this case, one option is to use a balanced incomplete block design (BIBD). The requirements of such a design are:\n\nall blocks are of the same size, with \\(k\\) experimental subjects per block;\nno treatment appears more than once within a block;\neach treatment appears the same number \\(r\\) times;\neach pair of treatments appears in the same number \\(\\lambda\\) of blocks;\n\nIf \\(b\\) blocks are used in total, the total number of observations of \\(bk\\). If there are \\(t\\) treatments, then we must have\n\\[\nbk = rt \\quad \\mbox{ and } r(k-1) = \\lambda(t-1),\n\\] which puts constraints on the choices of \\(b, r\\) and \\(\\lambda\\) for which BIBDs exist.\nR packages are available for generating such designs, e.g.\n\nlibrary(agricolae)\n\n# Suppose we have four treatments, \ntreatments &lt;- c(\"A\", \"B\", \"C\", \"D\")\n\n# Generate a BIBD with block sizes of k = 3 observations per block\n# and r = 6 observations for each treatment\noutdesign &lt;- design.bib(treatments, k = 3, r = 6)\n\n\nParameters BIB\n==============\nLambda     : 4\ntreatmeans : 4\nBlock size : 3\nBlocks     : 8\nReplication: 6 \n\nEfficiency factor 0.8888889 \n\n&lt;&lt;&lt; Book &gt;&gt;&gt;\n\nprint(outdesign$sketch)\n\n     [,1] [,2] [,3]\n[1,] \"D\"  \"B\"  \"C\" \n[2,] \"D\"  \"A\"  \"C\" \n[3,] \"A\"  \"B\"  \"D\" \n[4,] \"C\"  \"B\"  \"A\" \n[5,] \"B\"  \"D\"  \"C\" \n[6,] \"D\"  \"B\"  \"A\" \n[7,] \"C\"  \"A\"  \"B\" \n[8,] \"D\"  \"A\"  \"C\" \n\n\nThe function will report if a BIBD is not possible for the choice of \\(t,k, r\\). e.g, for \\(r=4\\) we should have 16 observations in total, but this is not divisible by the block size of \\(k=3\\):\n\ndesign.bib(treatments, k = 3, r = 4)\n\n\n Change r by  3, 6, 9, 12, 15, 18 ...",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Qualitative explanatory variables and blocking</span>"
    ]
  },
  {
    "objectID": "Qualitative-multiple factors.html",
    "href": "Qualitative-multiple factors.html",
    "title": "5  Latin Square designs",
    "section": "",
    "text": "5.1 Latin square designs for three factors\nThe basic Latin square design is intended for three different qualitative explanatory variables (factors), each operating at the same number \\(k\\) of levels. If we wanted to try each combination of factor levels, we would require \\(k^3\\) observations. A Latin Square design estimates the effects of each factor using using only \\(k^2\\) observations:",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Latin Square designs</span>"
    ]
  },
  {
    "objectID": "Qualitative-multiple factors.html#latin-square-designs-for-three-factors",
    "href": "Qualitative-multiple factors.html#latin-square-designs-for-three-factors",
    "title": "5  Latin Square designs",
    "section": "",
    "text": "We represent the \\(k^2\\) design points using a \\(k\\times k\\) array;\nthe row index and column index represent the levels of two of the factors;\nthe level of the third factor is specified for each of the \\(k^2\\) elements of the array, such that each level of this factor appears exactly once in each row and each column\n\n\n\n\n\n\n\nNoteExample\n\n\n\n\nExample 5.1 An experiment is designed to investigate how resistant to different materials are to wear: the response variable is the weight loss is a sample of the material following a period of motion on an abrasive surface. Four separate tests are conducted, and for each test, four samples are tested at once, with samples attached to different positions in the machine. The three factors are\n\nMaterial type (A, B, C, D)\nPosition of the material in the machine (1, 2, 3, 4)\nTest number (1, 2, 3, 4)\n\nIn this case, material type is the main factor of interest; “position” and “test number” could be thought of as blocking factors. A Latin Square design is as follows:\n\n\n\n\nPosition\nin\nmachine\n\n\n\nTest\n1\n2\n3\n4\n\n\n1\nA\nB\nD\nC\n\n\n2\nD\nC\nA\nB\n\n\n3\nC\nD\nB\nA\n\n\n4\nB\nA\nC\nD\n\n\n\nwhere the choice of material is indicated by the appropriate letter. We refer to the three factors as the “row factor” (Test), the “column factor” (Position) and “treatment factor” (material).",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Latin Square designs</span>"
    ]
  },
  {
    "objectID": "Qualitative-multiple factors.html#models-and-parameter-orthogonality",
    "href": "Qualitative-multiple factors.html#models-and-parameter-orthogonality",
    "title": "5  Latin Square designs",
    "section": "5.2 Models and parameter orthogonality",
    "text": "5.2 Models and parameter orthogonality\nThe model to be fitted to a Latin square design may be written \\[Y_{ij} = \\mu + \\alpha _i + \\beta_j + \\tau _{\\kappa (i,j)} + \\varepsilon _{ij}\\] where\n\n\\(Y_{ij}\\) is the observation in row \\(i\\) and column \\(j\\);\n\\(\\{ \\alpha _i\\}\\) and \\(\\{ \\beta_j\\}\\) are the parameters for row and column effects respectively\n\\(\\{ \\tau _{\\kappa }\\}\\) are parameters for treatment effects, with \\(\\kappa (i,j)\\) indicating the treatment which is applied in row \\(i\\) and column \\(j\\)\n\nWe choose parameter constraints \\[\n\\sum_{i=1}^k \\alpha_i = \\sum_{j=1}^k \\beta_j = \\sum_{\\kappa = 1 }^k\\tau _{\\kappa }=0.\n\\tag{5.1}\\]\nTo obtain least squares parameter estimators and check for orthogonality, we should write the model in matrix notation and inspect \\(\\mathbf{X}^T\\mathbf{X}\\) etc. This is left as a task at the end of this chapter, but we will state some results here.\nThe groups of parameters \\(\\{\\mu\\}, \\{\\alpha_i\\}, \\{\\beta_j\\},\\{tau_{\\kappa}\\}\\) are all mutually orthogonal, and least square estimators are\n\\[\\begin{align}\n\\hat{\\mu}& = \\frac{1}{k^2}\\sum_{i=1}^k\\sum_{j=1}^k Y_{ij},\\\\\n\\hat{\\alpha}_i& = \\frac{1}{k}\\sum_{j=1}^k Y_{ij} - \\hat{\\mu},\\\\\n\\hat{\\beta}_j& = \\frac{1}{k}\\sum_{i=1}^k Y_{ij}- \\hat{\\mu},\\\\\n\\hat{\\tau}_\\kappa& = \\frac{1}{k}\\sum_{i, j: \\kappa(i,j)=\\kappa}^k Y_{ij}.\n\\end{align}\\]\nGiven the Latin square design and the parameter constraints in Equation 5.1, we can understand why these are the estimators and why the parameters are orthogonal. Consider again Example 5.1. If we inspect the estimator for \\(\\hat{\\alpha}_1\\), the ‘row 1 parameter effect’, we see that \\[\nE\\left(\\sum_{j=1}^k Y_{1j}\\right) = 4\\alpha_1 +\\sum_{j=1}^4\\beta_j + \\sum_{\\kappa=1}^4\\tau_\\kappa+ 4\\mu = 4\\alpha_1 + 4\\mu\n\\] as all the column and treatment effects cancel out given the sum to zero constraints. Hence dividing the sum by 4 and subtracting \\(\\hat{\\mu}\\) gives an unbiased estimator of \\(\\alpha_1\\).\nWe can also understand why, for example, row effects and treatment effects are orthogonal. The row effect estimators are based on differences between the means of the observations in each row. But in each row, we’ve ensured each treatment is used once, and the treatment effects are specified to sum to zero: they cancel out within each row. So it makes sense that the row effect estimators would not depend on the treatment effect estimators.\n\n5.2.1 Orthogonal Latin squares\nWe can incorporate one additional factor, also with \\(k\\) levels, by superimposing a new Latin square on the existing one, such as the one represented by the Greek letters in the following diagram:\n\n\n\n\nA\\(\\alpha\\)\nB\\(\\beta\\)\nD\\(\\gamma\\)\nC\\(\\delta\\)\n\n\nD\\(\\beta\\)\nC\\(\\alpha\\)\nA\\(\\delta\\)\nB\\(\\gamma\\)\n\n\nC\\(\\gamma\\)\nD\\(\\delta\\)\nB\\(\\alpha\\)\nA\\(\\beta\\)\n\n\nB\\(\\delta\\)\nA\\(\\gamma\\)\nC\\(\\beta\\)\nD\\(\\alpha\\)\n\n\n\n\nthen it may happen, as it does above, that each combination of levels of the two treatment factors appears once in the diagram. In this case, we have a Graeco-Latin square, and the two Latin square designs, represented by the Latin letters and the Greek letters, are called orthogonal Latin squares. The parameters for the new factor are orthogonal to all the others.",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Latin Square designs</span>"
    ]
  },
  {
    "objectID": "Qualitative-multiple factors.html#tasks",
    "href": "Qualitative-multiple factors.html#tasks",
    "title": "5  Latin Square designs",
    "section": "5.3 Tasks",
    "text": "5.3 Tasks\n\nWrite down the \\(16\\) by \\(10\\) design matrix in Example 5.1.\nHence show that \\(\\tau_A\\) is orthogonal to all the non-treatment parameters but not the other treatment parameters.\nShow that \\(\\text{var}(\\hat{\\tau}_A)= \\frac{3 \\sigma^2}{16}\\) where \\(\\text{var}\\left(Y_{ij}\\right)=\\sigma^2\\).\nFor the example in Section 5.2.1, what are the dimensions of the design matrix?\n\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\n\nI have added the parameters in the top row of the matrix to make it clearer but these should obviously not be there. I have also added additional horizontal and vertical lines to make it easier to read. \\[\n\\left(\n\\begin{array}{c|ccc|ccc|ccc}\n\\mu & \\alpha_1 & \\alpha_2 & \\alpha_3 & \\beta_1 & \\beta_2 & \\beta_3 & \\tau_A & \\tau_B & \\tau_C \\\\\n\\hline\n1& 1& 0& 0& 1& 0& 0& 1& 0& 0\\\\\n1& 1& 0& 0& 0& 1& 0& 0& 1& 0\\\\\n1& 1& 0& 0& 0& 0& 1&-1&-1&-1\\\\\n1& 1& 0& 0&-1&-1&-1& 0& 0& 1\\\\\n\\hline\n1& 0& 1& 0& 1& 0& 0&-1&-1&-1\\\\\n1& 0& 1& 0& 0& 1& 0& 0& 0& 1\\\\\n1& 0& 1& 0& 0& 0& 1& 1& 0& 0\\\\\n1& 0& 1& 0&-1&-1&-1& 0& 1& 0\\\\\n\\hline\n1& 0& 0& 1& 1& 0& 0& 0& 0& 1\\\\\n1& 0& 0& 1& 0& 1& 0&-1&-1&-1\\\\\n1& 0& 0& 1& 0& 0& 1& 0& 1& 0\\\\\n1& 0& 0& 1&-1&-1&-1& 1& 0& 0\\\\\n\\hline\n1&-1&-1&-1& 1& 0& 0& 0& 1& 0\\\\\n1&-1&-1&-1& 0& 1& 0& 1& 0& 0\\\\\n1&-1&-1&-1& 0& 0& 1& 0& 0& 1\\\\\n1&-1&-1&-1&-1&-1&-1&-1&-1&-1\\\\\n\\end{array}\n\\right)\n\\]\nCalculating the column products shows that \\(\\tau_A\\) is orthogonal to all the parameters except \\(\\tau_B\\) and \\(\\tau_C\\).\nLet \\(Q\\) be the set of \\((i,j)\\) values given by \\(\\left\\{ (1,1), (2,3), (3,4), (4,2) \\right\\}\\) \\[\\begin{align}\n\\hat{\\tau}_A &= \\overline{Y}_{(\\kappa)} - \\overline{Y}_{..}\\\\\n&= \\frac{Y_{11}+Y_{23}+Y_{34}+Y_{42}}{4} - \\frac{\\sum_{i=1}^4 \\sum_{j=1}^4 Y_{ij} }{16}\\\\\n&= \\frac{3}{16}\\left( Y_{11}+Y_{23}+Y_{34}+Y_{42}\\right) - \\frac{\\sum_{(i,j) \\not\\in Q} Y_{ij}}{16}\\\\\n\\end{align}\\] Using the fact that \\(Y_{i,j}\\) is independent of \\(Y_{i^\\prime, j^\\prime}\\) for \\((i,j) \\neq (i^\\prime, j^\\prime)\\) it follows that \\[\\begin{align}\nvar({\\hat{\\tau}_A}) &= \\left(\\frac{3}{16}\\right)^2 4\\sigma^2 + \\left(\\frac{1}{16}\\right)^2 12\\sigma^2\\\\\n&= \\frac{3\\sigma^2}{16}\n\\end{align}\\]\nThe design matrix would be \\(16\\times 13\\): there are 16 observations, and 13 parameters:\n\n\nthe constant \\(\\mu\\)\nthree parameters for row effects\nthree parameters for column effects\nthree parameters for the treatment factor with levels represented by \\(A, B, C, D\\)\nthree parameters for the additional treatment factor with levels represented by \\(\\alpha,\\beta,\\gamma,\\delta\\)",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Latin Square designs</span>"
    ]
  },
  {
    "objectID": "Qualitative-multiple factors.html#appendix-existence-of-orthogonal-latin-squares",
    "href": "Qualitative-multiple factors.html#appendix-existence-of-orthogonal-latin-squares",
    "title": "5  Latin Square designs",
    "section": "5.4 Appendix: existence of orthogonal Latin squares",
    "text": "5.4 Appendix: existence of orthogonal Latin squares\nThe following is reference material and will not be examined.\nThe question whether orthogonal Latin squares exist, and, if so, how many, for a given value of \\(k\\), is of considerable combinatorial interest in its own right, and gets input from the algebraic theory of finite fields. A finite field is a finite set including \\(0\\) and \\(1\\) and in which addition, multiplication, subtraction and division by non-zero elements are possible and satisfy the usual rules. The simplest example of a finite field is “integers modulo \\(p\\)”, where \\(p\\) is a prime number. This is a field with \\(p\\) elements, \\(\\{ 0,1,\\ldots ,p-1\\}\\) say. However, the theory states that a finite field with \\(k\\) elements exists if and only if \\(k\\) is of the form \\(p^r\\), where \\(p\\) is prime and \\(r\\) is any positive integer. This is relevant to the following theorem.\n\n\n\n\n\n\nWarningTheorem\n\n\n\n\nTheorem 5.1 If \\(k\\) is of the form \\(p^r\\), where \\(p\\) is a prime number and \\(r\\) is a positive integer, then there exists a complete set of \\(k-1\\) mutually orthogonal \\(k\\times k\\) Latin squares.\n\n\n\nBy a complete set we mean that no further additions to the set are possible. We shall not prove this in general, but illustrate it by the construction of such a complete set in the case \\(k=5\\). Working in arithmetic modulo \\(5\\), suppose we label the rows and columns of any square, by the elements of the finite field: in this case \\(0,1,2,3,4\\). In the first square, in position \\((i,j)\\) we place the value of \\(i+j\\); in the second square, the value of \\(i+2j\\); in the third square, the value of \\(i+3j\\); and in the fourth square, the value of \\(i+4j\\). (In general, if we denote the elements of the field by \\(0,1,a,b,c\\) (as all finite fields must include 0 and 1), we actually want the values of \\(i+aj\\), \\(i+bj\\) and \\(i+cj\\) in the second, third and fourth squares). Then we get the following.\n\\(i+j\\) \\[\\begin{array}\n{|ccccc|} \\hline  0 & 1 & 2 & 3 & 4 \\\\ 1 & 2 &  3 & 4 & 0 \\\\ 2 & 3 & 4 & 0 & 1 \\\\ 3 & 4 & 0 & 1 & 2 \\\\ 4 & 0 & 1 & 2 & 3 \\\\ \\hline\n\\end{array}\\]\n\\(i+2j\\) \\[\\begin{array}{|ccccc|} \\hline 0 & 2 & 4 & 1 & 3 \\\\ 1 & 3 & 0 & 2 & 4 \\\\ 2 & 4 & 1 & 3 & 0 \\\\ 3 & 0 & 2 & 4 & 1 \\\\ 4 & 1 & 3 & 0 & 2 \\\\ \\hline \\end{array}\\]\n\\(i+3j\\) \\[\\begin{array}{|ccccc|} \\hline 0 & 3 & 1 & 4 & 2 \\\\ 1 & 4 & 2 & 0 & 3 \\\\ 2 & 0 & 3 & 1 & 4 \\\\ 3 & 1 & 4 & 2 & 0 \\\\ 4 & 2 & 0 & 3 & 1 \\\\ \\hline\n\\end{array}\\]\n\\(i+4j\\) \\[\\begin{array}{|ccccc|} \\hline 0 & 4 & 3 & 2 & 1 \\\\ 1 & 0 & 4 & 3 & 2 \\\\ 2 & 1 & 0 & 4 & 3 \\\\ 3 & 2 & 1 & 0 & 4 \\\\ 4 & 3 & 2 & 1 & 0 \\\\ \\hline \\end{array}\\] Clearly any two of these squares are orthogonal. This construction works because integers modulo \\(5\\) constitute a finite field.\nThe smallest value of \\(k\\) which is not of the form \\(p^r\\) is \\(k=6\\). It has been shown that there do not even exist two \\(6\\times 6\\) Latin squares which are orthogonal to each other. However, for \\(k=10\\) there do exist a complete set of orthogonal Latin squares, and so the condition that \\(k\\) takes the form \\(p^r\\) is not a necessary condition.",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Latin Square designs</span>"
    ]
  },
  {
    "objectID": "Factorial designs.html",
    "href": "Factorial designs.html",
    "title": "6  Factorial designs",
    "section": "",
    "text": "6.1 Factorial design regions\nWe now consider scenarios in which we wish to investigate the effect of several explanatory variables on the response variable, where at least some of the explanatory variables are continuous. We consider classes of designs in which the continuous explanatory variables are treated as factors, operating at a small number of different levels. For example, suppose we are investigating the effect of temperature, pressure and catalyst on the yield of some chemical process. A possible choice of factor levels is\nAlthough temperature and pressure are treated as factors here, we will treat them as continuous quantities when fitting suitable statistical models.\nThis motivates the use of factorial designs: we use the word “factorial” here due to the factorial nature of the design region. In the example above, there are \\(3\\times 2\\times2\\) possible points in the design region for which we can take observations.",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Factorial designs</span>"
    ]
  },
  {
    "objectID": "Factorial designs.html#factorial-design-regions",
    "href": "Factorial designs.html#factorial-design-regions",
    "title": "6  Factorial designs",
    "section": "",
    "text": "Explanatory variable\n\nLevels\n\n\n\n\n\n\nTemperature, \\(^{\\circ }C\\)\n\\(x_1\\)\n180\n190\n200\n\n\nPressure, psi\n\\(x_2\\)\n70\n90\n\n\n\nCatalyst\n\\(x_3\\)\nType I\nType II",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Factorial designs</span>"
    ]
  },
  {
    "objectID": "Factorial designs.html#complete-factorial-designs-cfd",
    "href": "Factorial designs.html#complete-factorial-designs-cfd",
    "title": "6  Factorial designs",
    "section": "6.2 Complete factorial designs (CFD)",
    "text": "6.2 Complete factorial designs (CFD)\nA complete factorial design (CFD) includes all possible combinations of the explanatory variables. If no replications are included, the total number of observations in a CFD is \\[n = \\prod _{i=1}^m l_i\\] where \\(m\\) is the number of explanatory variables and \\(l_i\\) is the number of levels of the \\(i\\)-th of these explanatory variables.\nIf \\(m\\) is large, even if the number of levels of each factor is small, the design may require an impractically large number of observations. Also, there may be a need to use blocks. For the moment, we shall only consider designs where each of the factors occurs at two levels and no blocking is required. The models which can be fitted using such designs will be investigated.\nIf a factor occurs at two levels, it is convenient to denote these levels by \\(-1\\) (or \\(-\\)), the low level, and \\(+1\\) (or \\(+\\)), the high level. Sometimes the factors \\(x_1, x_2, \\ldots\\) are denoted instead by \\(a,b,c,\\ldots\\) and the response corresponding to each combination is given subscripts for each of the factors which occurs at the high level, except that the response corresponding to all the factors being at the low level is given the subscript \\((1)\\). The following tables illustrate the design for \\(m=2\\) and \\(m=3\\).\n\n\n\n\n  \\(x_1 (a)\\)  \n  \\(x_2 (b)\\)  \n  Response  \n\n\n\n\n\\(-1\\)\n\\(-1\\)\n\\(Y_{(1)}\\)\n\n\n\\(1\\)\n\\(-1\\)\n\\(Y_a\\)\n\n\n\\(-1\\)\n\\(1\\)\n\\(Y_b\\)\n\n\n\\(1\\)\n\\(1\\)\n\\(Y_{ab}\\)\n\n\n\nCFD for \\(m=2\\).\n\n\n\n  \\(x_1 (a)\\)  \n  \\(x_2 (b)\\)  \n  \\(x_3 (c)\\)  \n  Response  \n\n\n\n\n\\(-1\\)\n\\(-1\\)\n\\(-1\\)\n\\(Y_{(1)}\\)\n\n\n\\(1\\)\n\\(-1\\)\n\\(-1\\)\n\\(Y_a\\)\n\n\n\\(-1\\)\n\\(1\\)\n\\(-1\\)\n\\(Y_b\\)\n\n\n\\(1\\)\n\\(1\\)\n\\(-1\\)\n\\(Y_{ab}\\)\n\n\n\\(-1\\)\n\\(-1\\)\n\\(1\\)\n\\(Y_c\\)\n\n\n\\(1\\)\n\\(-1\\)\n\\(1\\)\n\\(Y_{ac}\\)\n\n\n\\(-1\\)\n\\(1\\)\n\\(1\\)\n\\(Y_{bc}\\)\n\n\n\\(1\\)\n\\(1\\)\n\\(1\\)\n\\(Y_{abc}\\)\n\n\n\nCFD for \\(m=3\\).\n\nNote that in these tables, the rows are in standard order which means that the first column alternates between \\(-1\\) and \\(1\\), the second column has alternating pairs of \\(-1\\)’s and \\(1\\)’s, the third column has alternating groups of four, and so on. This practice ensures that every combination of levels of the factors occurs exactly once, but it should be noted that, in applying such a design, randomisation should take place in assigning factor combinations to experimental units.\nThe type of linear model which we can fit to a CFD is the polynomial model \\[E(Y)=\\beta_0+\\sum_{i=1}^m\\beta_ix_i+\\sum_{i=1}^{m-1}\\sum_{j=i+1}^m\\beta_{ij}x_ix_j+\\sum_{i=1}^{m-2}\\sum_{j=i+1}^{m-1}\\sum_{k=j+1}^m\\beta_{ijk}x_ix_jx_k+\\ldots\\] where, because we have chosen \\(\\pm 1\\) as the values taken by the \\(x_i\\)’s, there is no need to include any power of \\(x_i\\) greater than one, since \\(x_i^2\\) is identically equal to \\(1\\). Another good reason for this choice is that it makes the parameters relatively easy to interpret; for instance, in the first-order model \\[E(Y) = \\beta_0 + \\sum _{i=1}^m \\beta_ix_i\\] \\(\\beta_0\\) can be interpreted as the overall mean and, for \\(i\\geq 1\\), \\(2\\beta_i\\) can be interpreted as the difference in effect between high and low levels of factor \\(i\\).\nIn the more general model, the \\(\\beta_i\\)’s are called the main effects and the higher order parameters \\(\\beta_{ij}, \\beta_{ijk},\\ldots\\) are called the interactions. The fullest possible model is the one which includes all orders of interaction up to \\(m\\), so that the last term in the equation above is \\[\\ldots + \\beta_{12\\ldots m}x_1x_2\\ldots x_m\\] and the total number of parameters is \\[1 + {m \\choose 1} + {m \\choose 2} + \\ldots + 1 = (1+1)^m = 2^m\\] which is the same as the total number of observations. The model is said to be saturated: it fits exactly, the residuals are zero and there are no degrees of freedom for error. In practice, an experimenter will rarely expect it necessary to include any but the lowest order interactions in the model, and so the number of parameters will be much smaller, and the usual analysis of variance techniques may be applied.\nAnother property of a CFD is that, however many interactions are included in the model, the matrix \\(\\mathbf{X}^T\\mathbf{X}\\) turns out to be a multiple of the identity matrix, and so all the parameters are mutually orthogonal and it is very easy to write down their estimators.\n\n\n\n\n\n\nNoteExample\n\n\n\n\nExample 6.1 Let \\(m=3\\) and consider the model with two-factor interactions \\[E(Y) = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_3x_3 + \\beta_{12}x_1x_2 + \\beta_{13}x_1x_3 + \\beta_{23}x_2x_3 .\\] Then the design matrix is \\[\\mathbf{X}= \\left( \\begin{array}{ccccccc} 1 & -1 & -1 & -1 & 1 & 1 & 1 \\\\\n1 & 1 & -1 & -1 & -1 & -1 & 1 \\\\ 1 & -1 & 1 & -1 & -1 & 1 & -1 \\\\\n1 & 1 & 1 & -1 & 1 & -1 & -1 \\\\ 1 & -1 & -1 & 1 & 1 & -1 & -1 \\\\\n1 & 1 & -1 & 1 & -1 & 1 & -1 \\\\ 1 & -1 & 1 & 1 & -1 & -1 & 1 \\\\\n1 & 1 & 1 & 1 & 1 & 1 & 1 \\end{array} \\right) .\\] Note how here the second, third and fourth columns are lifted from the diagram of the design which we gave earlier, and the last three columns are found by multiplying together in pairs the preceding three columns. It is easy to see now that \\(\\mathbf{X}^T\\mathbf{X}=8\\mathbf{I}_7\\) where \\(\\mathbf{I}_7\\) is the \\(7\\times 7\\) identity matrix, and so \\[\\hat{\\boldsymbol{\\beta}}=\\frac{1}{8}\\mathbf{X}^T\\mathbf{Y}\\] or, more fully, \\[\\begin{align}\n\\hat{\\beta_0} &= \\frac{1}{8}\\left(Y_{(1)}+Y_a+Y_b+Y_{ab}+Y_c+Y_{ac}+Y_{bc}+Y_{abc} \\right) \\\\\n\\hat{\\beta_1} &= \\frac{1}{8}\\left(-Y_{(1)}+Y_a-Y_b+Y_{ab}-Y_c+Y_{ac}-Y_{bc}+Y_{abc} \\right) \\\\\n\\hat{\\beta_2} &= \\frac{1}{8}\\left(-Y_{(1)}-Y_a+Y_b+Y_{ab}-Y_c-Y_{ac}+Y_{bc}+Y_{abc} \\right) \\\\\n\\hat{\\beta_3} &= \\frac{1}{8}\\left(-Y_{(1)}-Y_a-Y_b-Y_{ab}+Y_c+Y_{ac}+Y_{bc}+Y_{abc} \\right) \\\\\n\\hat{\\beta_{12}} &= \\frac{1}{8}\\left(Y_{(1)}-Y_a-Y_b+Y_{ab}+Y_c-Y_{ac}-Y_{bc}+Y_{abc} \\right) \\\\\n\\hat{\\beta_{13}} &= \\frac{1}{8}\\left(Y_{(1)}-Y_a+Y_b-Y_{ab}-Y_c+Y_{ac}-Y_{bc}+Y_{abc} \\right) \\\\\n\\text{and  }\\hat{\\beta_{23}} &= \\frac{1}{8}\\left(Y_{(1)}+Y_a-Y_b-Y_{ab}-Y_c-Y_{ac}+Y_{bc}+Y_{abc} \\right).\n\\end{align}\\] It is instructive to observe the pattern of plus and minus signs in these expressions. For instance, in the expression for the estimator of \\(\\beta_{13}\\), the interaction between \\(x_1\\) and \\(x_3\\), the part inside the brackets is the difference between the sum of the observations where \\(x_1\\) and \\(x_3\\) are both acting at the same level (high or low) and the sum of the observations where they are acting at opposite levels.\n\n\n\nWe have already remarked that a CFD may be used even when the explanatory variables are quantitative, by choosing two values of each and, by suitable scaling, calling them \\(+1\\) and \\(-1\\). We can think of the design points as the corners of a cuboid in \\(m\\) dimensions. However, in this case the only model we can fit is of the kind we have just considered, where there do not exist, for example, quadratic terms in \\(x_i^2\\), because \\(x_i^2\\) is identically equal to \\(1\\) at all of the design points and so the coefficient \\(\\beta_{ii}\\), say, would be confounded with the constant term. If we wanted to fit such a model, we would have to add some other design points.",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Factorial designs</span>"
    ]
  },
  {
    "objectID": "Factorial designs.html#blocking-factorial-designs",
    "href": "Factorial designs.html#blocking-factorial-designs",
    "title": "6  Factorial designs",
    "section": "6.3 Blocking Factorial Designs",
    "text": "6.3 Blocking Factorial Designs\nIt may happen that blocking in a CFD is desirable because of the size of the experiment. This can be done provided it is not expected that the higher-order interactions will be significant, and so they can be excluded from the model, and degrees of freedom may be used to introduce blocking parameters instead.\nThere is a particularly efficient way of doing this when the number of blocks is a power of \\(2\\) and all of the blocks are the same size, necessarily also a power of \\(2\\). Consider first the case of two blocks. If we take\n\nBlock I: all observations for which \\(x_1x_2\\ldots x_m= 1\\)\nBlock II: all observations for which \\(x_1x_2\\ldots x_m=-1\\)\n\nthen we are effectively confounding blocks with the highest-order interaction which we have already assumed to be not significant. If we choose \\(\\pm 1\\) as the two values of the dummy variable for blocks, then the model is identical to the one containing that interaction, except that the parameter must now be interpreted differently.\nIn this model, \\(x_1x_2\\ldots x_m\\) is called the block generator. The choice of such a generator is not unique, but in this case it is the obvious choice since it enables all the main effects and interactions below the highest still to be included in the model, and is least likely to be harmful to the estimation of parameters.\n\n\n\n\n\n\nNoteExample\n\n\n\n\nExample 6.2 In the case \\(m=3\\), if we take \\(x_1x_2x_3\\) as the block generator then we get two blocks which may be represented as follows.\nBlock I\n\\[\\begin{array}{|ccc|} \\hline + & - & - \\\\ - & +   & - \\\\ - & - & + \\\\ + & + & + \\\\ \\hline \\end{array}\\] Block II\n\\[\\begin{array}{|ccc|} \\hline - & - & - \\\\ + & + & - \\\\ + & - & + \\\\ - & + & + \\\\ \\hline \\end{array}\\]\nSuppose we then choose to fit the first-order model without interactions \\[E(Y)=\\beta_0 + \\alpha z + \\beta_1x_1 + \\beta_2x_2 + \\beta_3x_3\\] where \\(z\\) is the dummy variable for blocks. Then we get the following design matrix \\[\\mathbf{X}=\\left( \\begin{array}{ccccc} 1 & 1 & 1 & -1 & -1 \\\\\n1 & 1 & -1 & 1 & -1 \\\\ 1 & 1 & -1 & -1 & 1 \\\\ 1 & 1 & 1 & 1 & 1 \\\\ 1\n& -1 & -1 & -1 & -1 \\\\ 1 & -1 & 1 & 1 & -1 \\\\ 1 & -1 & 1 & -1 & 1 \\\\\n1 & -1 & -1 & 1 & 1 \\end{array} \\right)\\] where the first four rows represent Block I and the last four rows represent Block II. It is easy to see that \\(\\mathbf{X}^T\\mathbf{X}\\) is a multiple of the identity matrix, and so the parameters are all orthogonal, and the estimators of those of interest are the same as they would be without blocking.\n\n\n\nNow consider the case of four blocks. We now have to choose two block generators so that blocks are determined by which of the four possible pairs of values these generators take. If we denote these block generators by \\(z_1\\) and \\(z_2\\), we might take the following.\n\nBlock I: all observations for which \\(z_1=z_2=1\\)\nBlock II: all observations for which \\(z_1=1,z_2=-1\\)\nBlock III: all observations for which \\(z_1=-1,z_2=1\\)\nBlock IV: all observations for which \\(z_1=z_2=-1\\)\n\nThe choice of two block generators is more subtle than the choice of one, the reason being that, in the full model with all interactions, not only is each of the block generators confounded with an interaction, but so is their product \\(z_1z_2\\) which, bearing in mind the algebra of these numbers (\\(x_i^2\\equiv 1\\)), may be a low-order interaction or even a main effect, which would be a bad choice. For example, in the case \\(m=5\\), it would be bad to choose \\(z_1=x_1x_2x_3x_4x_5\\) and \\(z_2=x_2x_3x_4x_5\\) because then \\[z_1z_2 = (x_1x_2x_3x_4x_5)(x_2x_3x_4x_5)=x_1\\] and so the main effect of factor 1 is confounded with blocks.\n\n\n\n\n\n\nNoteExample\n\n\n\n\nExample 6.3 When \\(m=3\\), the best we can do is to choose \\(z_1=x_1x_2\\) and \\(z_2=x_1x_3\\) so that their product is \\(x_2x_3\\), and all of the two-factor interactions are confounded with blocks. Then the model we can fit contains only main effects of the factors, say \\[E(Y) = \\beta_0 + \\alpha _1z_1 + \\alpha _2z_2 + \\alpha _{12}z_1z_2 + \\beta_1x_1 + \\beta_2x_2 + \\beta_3x_3\\] where \\(z_1=x_1x_2\\) and \\(z_2=x_1x_3\\). There is one degree of freedom left for error.",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Factorial designs</span>"
    ]
  },
  {
    "objectID": "Factorial designs.html#fractional-factorial-designs-ffd",
    "href": "Factorial designs.html#fractional-factorial-designs-ffd",
    "title": "6  Factorial designs",
    "section": "6.4 Fractional factorial designs (FFD)",
    "text": "6.4 Fractional factorial designs (FFD)\nIn cases where the experimenter would like to use a CFD but this would entail taking an impractically large number of observations, the best that can be done is to select a fraction of the possible factor combinations, giving a fractional factorial design (FFD). This must be done in a way which maintains as much efficiency as possible.\nClassical FFD’s will be discussed. They consist of \\(n=2^{m-t}\\) observations, where \\(m\\) is the number of factors and \\(2^{-t}\\) is the proportion of the possible factor combinations used.\nThe way in which the factor combinations are chosen resembles the way in which blocks were constructed in Section 6.2. Essentially, we choose just one of the \\(2^t\\) blocks which would have been constructed using \\(t\\) different block generators. Thus, we choose all possible combinations of the factor levels satisfying \\(t\\) equations known as design generators.\n\n\n\n\n\n\nNoteExample\n\n\n\n\nExample 6.4 Construct a FFD for \\(m=3\\) and \\(t=1\\) using the design generator \\(x_1x_2x_3=1\\).\n\n\n\n\\(x_1\\)\n\\(x_2\\)\n\\(x_3\\)\n\n\n\n\n\\(1\\)\n\\(-1\\)\n\\(-1\\)\n\n\n\\(-1\\)\n\\(1\\)\n\\(-1\\)\n\n\n\\(-1\\)\n\\(-1\\)\n\\(1\\)\n\n\n\\(1\\)\n\\(1\\)\n\\(1\\)\n\n\n\n\n\n\nIf a FFD is used, it is obvious that not all of the interactions can be included in the model, because the CFD was already saturated when all of the interactions were included. The pattern by which the main effects and interactions are confounded with each other in a FFD is called the alias structure. This may be found by taking the design generators and finding all the possible equations that can be derived from them by combining them together or multiplying both sides by the same quantity, remembering that in the algebra of these symbols (\\(x_i^2\\equiv 1\\)) there is simplification because any square may be deleted.\n\n\n\n\n\n\nNoteExample\n\n\n\n\nExample 6.5 Find the alias structure when \\(m=5\\), \\(t=2\\) and the design generators are \\(x_1x_2x_3=1\\) and \\(x_3x_4x_5=1\\). Discuss what models is it possible to fit using this design.\nThe design generators give \\[1 = x_1x_2x_3 = x_3x_4x_5 = x_1x_2x_4x_5\\] where the last term is found by multiplying the two design generators together. We can then multiply these equations by symbols until we have exhausted all the possibilities, as follows.\n\\[\\begin{align}\nx_1&= x_2x_3 = x_1x_3x_4x_5 = x_2x_4x_5\\\\\nx_2&= x_1x_3 = x_2x_3x_4x_5 = x_1x_4x_5\\\\\nx_3&= x_1x_2 = x_4x_5 = x_1x_2x_3x_4x_5\\\\\nx_4&= x_1x_2x_3x_4 = x_3x_5 = x_1x_2x_5\\\\\nx_5&= x_1x_2x_3x_5 = x_3x_4 = x_1x_2x_4\\\\\nx_1x_4&= x_2x_3x_4 = x_1x_3x_5 = x_2x_5\\\\\nx_1x_5&= x_2x_3x_5 = x_1x_3x_4 = x_2x_4.\n\\end{align}\\]\nThis is the alias structure. To fit a model to this design without confounding, we can only include at most one variable from each of these groups. Once we have included the main effects, this only leaves the possibility of two of the two-factor interactions such as \\(x_1x_4\\) and \\(x_1x_5\\), although including both of these would saturate the model. Note that it rarely makes sense to include a higher-order interaction when at least some of the corresponding lower-order ones are missing.\n\n\n\nTo construct a FFD for given \\(m\\) and \\(t\\) and design generators, the following is an efficient method. Construct a CFD with \\(m-t\\) of the factors which are not constrained to be related to each other by the design generators. Then express each of the remaining factors in terms of the previous ones using the design generators, and add a column to the design for each of these, by multiplying together the appropriate preceding columns.\n\n\n\n\n\n\nNoteExample\n\n\n\n\nExample 6.6 Construct the design defined in Example 6.5\n\n\n\n\\(x_1\\)\n\\(x_2\\)\n\\(x_4\\)\n\\(x_3=x_1x_2\\)\n\\(x_5=x_3x_4\\)\n\n\n\n\n\\(-1\\)\n\\(-1\\)\n\\(-1\\)\n\\(1\\)\n\\(-1\\)\n\n\n\\(1\\)\n\\(-1\\)\n\\(-1\\)\n\\(-1\\)\n\\(1\\)\n\n\n\\(-1\\)\n\\(1\\)\n\\(-1\\)\n\\(-1\\)\n\\(1\\)\n\n\n\\(1\\)\n\\(1\\)\n\\(-1\\)\n\\(1\\)\n\\(-1\\)\n\n\n\\(-1\\)\n\\(-1\\)\n\\(1\\)\n\\(1\\)\n\\(1\\)\n\n\n\\(1\\)\n\\(-1\\)\n\\(1\\)\n\\(-1\\)\n\\(-1\\)\n\n\n\\(-1\\)\n\\(1\\)\n\\(1\\)\n\\(-1\\)\n\\(-1\\)\n\n\n\\(1\\)\n\\(1\\)\n\\(1\\)\n\\(1\\)\n\\(1\\)\n\n\n\nIn this construction, we have used \\(x_1, x_2\\) and \\(x_4\\) in the initial CFD, represented by the first three columns, since the obvious first choice of \\(x_1, x_2\\) and \\(x_3\\) does not work, as these three factors are constrained by the design generator \\(x_1x_2x_3=1\\), and so their combinations of values do not range over all eight possibilities. There is no such problem with \\(x_1, x_2\\) and \\(x_4\\).\n\n\n\nThe resolution \\(R\\) of a FFD is defined as the order of the lowest order interaction which is confounded with \\(\\beta_0\\), and is traditionally denoted by a Roman numeral. Thus, in Example 6.3.2, \\(R=\\)III, since the lowest order interaction confounded with \\(\\beta_0\\) is \\(\\beta_{123}\\) or \\(\\beta_{345}\\). Resolution which is as high as possible is obviously desirable. Also, a FFD with given values of \\(m\\) and \\(t\\) is often denoted by \\(2^{m-t}_R\\), so that the design of Example 6.5 may be described as a \\(2^{5-2}_{\\text{III}}\\) design.",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Factorial designs</span>"
    ]
  },
  {
    "objectID": "Factorial designs.html#screening-experiments",
    "href": "Factorial designs.html#screening-experiments",
    "title": "6  Factorial designs",
    "section": "6.5 Screening experiments",
    "text": "6.5 Screening experiments\nIn many industrial applications, the number of variables that may affect the response variable of interest is very large. It is therefore important at an early stage of an investigation to study the relative importance of the factors. In such cases screening experiments are carried out. The aim is to reduce the number of factors included in models for future experiments. Low resolution fractional factorial designs may be used for this purpose.\nFor practical reasons it may only be feasible to fit the first-order model \\[E(Y) = \\beta_0 + \\sum _{i=1}^m \\beta_ix_i\\] and for this, the smallest possible FFD is \\(2^{m-t}\\), where \\(t\\) is the largest integer satisfying \\[2^{m-t}\\geq m+1.\\] If there is equality in the above, the design is saturated and it is not possible to perform significance tests, but it is still possible to calculate the parameter estimates and compare their magnitudes to get some idea which factors are most influential.\n\n\n\n\n\n\nNoteExample\n\n\n\n\nExample 6.7 The filtration stage in a newly constructed industrial plant had been taking almost twice as long as in other plants. To address the problem, the factors which may have been the cause were identified and a screening experiment was carried out. The factors are listed below.\n\n\n\n\n\nLevel\n\n\n\nVariable\n\n\\(-1\\)\n\\(1\\)\n\n\n1\nwater supply\ntown reservoir\nwell\n\n\n2\nraw material\non site\nother\n\n\n3\ntemperature\nlow\nhigh\n\n\n4\nrecycle\nyes\nno\n\n\n5\ncaustic soda\nfast\nslow\n\n\n6\nfilter cloth\nnew\nold\n\n\n7\nholding up time\nlow\nhigh\n\n\n\nA CFD would require \\(2^7=128\\) observations. However, the first-order model has eight parameters, and, if we are not worried about saturating the design but more worried about the size of the screening experiment, a \\(2^{7-4}\\) FFD may be used. This is what was done, using a CFD involving the factors \\(x_1, x_2\\) and \\(x_3\\) and then the generators \\(x_4=x_1x_2, x_5=x_1x_3, x_6=x_2x_3\\) and \\(x_7=x_1x_2x_3\\). The results, including the data, are given below.\n\n\n\n\\(x_1\\)\n\\(x_2\\)\n\\(x_3\\)\n\\(x_4=x_1x_2\\)\n\\(x_5=x_1x_3\\)\n\\(x_6=x_2x_3\\)\n\\(x_7=x_1x_2x_3\\)\n\\(y\\)\n\n\n\n\n\\(-1\\)\n\\(-1\\)\n\\(-1\\)\n\\(1\\)\n\\(1\\)\n\\(1\\)\n\\(-1\\)\n68.4\n\n\n\\(1\\)\n\\(-1\\)\n\\(-1\\)\n\\(-1\\)\n\\(-1\\)\n\\(1\\)\n\\(1\\)\n77.7\n\n\n\\(-1\\)\n\\(1\\)\n\\(-1\\)\n\\(-1\\)\n\\(1\\)\n\\(-1\\)\n\\(1\\)\n66.4\n\n\n\\(1\\)\n\\(1\\)\n\\(-1\\)\n\\(1\\)\n\\(-1\\)\n\\(-1\\)\n\\(-1\\)\n81.0\n\n\n\\(-1\\)\n\\(-1\\)\n\\(1\\)\n\\(1\\)\n\\(-1\\)\n\\(-1\\)\n\\(1\\)\n78.6\n\n\n\\(1\\)\n\\(-1\\)\n\\(1\\)\n\\(-1\\)\n\\(1\\)\n\\(-1\\)\n\\(-1\\)\n41.2\n\n\n\\(-1\\)\n\\(1\\)\n\\(1\\)\n\\(-1\\)\n\\(-1\\)\n\\(1\\)\n\\(-1\\)\n68.7\n\n\n\\(1\\)\n\\(1\\)\n\\(1\\)\n\\(1\\)\n\\(1\\)\n\\(1\\)\n\\(1\\)\n38.7\n\n\n\nThe regression equation fitted by a standard package (and fitting exactly, because the design is saturated) was \\[y = 65.1-5.44x_1-1.39x_2-8.29x_3+1.59x_4-11.4x_5-1.71x_6+0.262x_7.\\] The magnitudes of the coefficients suggest that \\(x_1,  x_3\\) and \\(x_5\\) are the influential factors. Note that it is legitimate to compare these magnitudes, because the explanatory variables are all on the same scale \\(\\{ -1,1\\}\\) and the estimates of the regression coefficients all have the same standard error, even though we cannot estimate it because of the saturation.",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Factorial designs</span>"
    ]
  },
  {
    "objectID": "Factorial designs.html#tasks",
    "href": "Factorial designs.html#tasks",
    "title": "6  Factorial designs",
    "section": "6.6 Tasks",
    "text": "6.6 Tasks\n\nConstruct the design matrix of a CFD for \\(m=4\\) factors, in which the model fitted contains the main effects and only the interaction between factors \\(1\\) and \\(2\\). What are the parameter estimators in this model? You may find it easier to describe them rather than writing them out in full.\nSuggest a good choice of two block generators in a CFD with \\(m=6\\) factors and four blocks, and discuss what models can be fitted to this design.\nConstruct a \\(2^{6-2}\\) FFD using the two block generators in Question 2, now as design generators. What is the resolution of this design?\nIn the screening experiment of Example 6.7, suppose that the first factor, water supply, has four levels rather than two. Suggest a way in which the experiment might be modified to take account of this.\n\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\n\nWriting the model as \\[ E(Y) = \\beta _0 + \\beta _1x_1 + \\beta _2x_2 + \\beta _3x_3 + \\beta\n_4x_4 + \\beta _{12}x_1x_2\n\\] the design matrix is \\[ \\left( \\begin{array}{cccccc} 1 & -1 & -1 & -1 & -1 & 1 \\\\ 1 & 1 & -1\n& -1 & -1 & -1 \\\\ 1 & -1 & 1 & -1 & -1 & -1 \\\\ 1 & 1 & 1 & -1 & -1 &\n1 \\\\ 1 & -1 & -1 & 1 & -1 & 1 \\\\ 1 & 1 & -1 & 1 & -1 & -1 \\\\ 1 & -1\n& 1 & 1 & -1 & -1 \\\\ 1 & 1 & 1 & 1 & -1 & 1 \\\\ 1 & -1 & -1 & -1 & 1\n& 1 \\\\ 1 & 1 & -1 & -1 & 1 & -1 \\\\ 1 & -1 & 1 & -1 & 1 & -1 \\\\ 1 & 1\n& 1 & -1 & 1 & 1 \\\\ 1 & -1 & -1 & 1 & 1 & 1 \\\\ 1 & 1 & -1 & 1 & 1 &\n-1 \\\\ 1 & -1 & 1 & 1 & 1 & -1 \\\\ 1 & 1 & 1 & 1 & 1 & 1 \\end{array}\n\\right) .\n\\] The estimator of \\(\\beta _0\\) is the overall mean; the estimator of each of the main effects is \\(\\frac{1}{16}\\) of the difference between the sum of the observations with that factor at a high level and the sum with it at a low level; the estimator of \\(\\beta _{12}\\) is \\(\\frac{1}{16}\\) of the difference between the sum of the observations with factors 1 and 2 at the same level and the sum with them at opposite levels. (This is true regardless of which interactions are included, by orthogonality.)\nA good choice of block generators is \\(x_1x_2x_3x_4\\) and \\(x_1x_2x_5x_6\\), whose product is \\(x_3x_4x_5x_6\\). This produces some confounding of four-factor interactions with blocks, but all lower-order interactions may be included in the model without confounding.\nThe design may be represented as\n\n\\[\n\\begin{array}{c|c|c|c|c|c}\nx_1 & x_2 & x_3 & x_5 &\nx_4=x_1x_2x_3 & x_6=x_1x_2x_5 \\\\ \\hline -1 & -1 & -1 &\n-1 & -1 & -1 \\\\ 1 & -1 & -1 & -1 & 1 & 1 \\\\ -1 & 1\n& -1 & -1 & 1 & 1 \\\\ 1 & 1 & -1 & -1 & -1 & -1 \\\\\n-1 & -1 & 1 & -1 & 1 & -1 \\\\ 1 & -1 & 1 & -1 & -1 & 1 \\\\\n-1 & 1 & 1 & -1 & -1 & 1 \\\\ 1 & 1 & 1 & -1 & 1 & -1 \\\\\n-1 & -1 & -1 & 1 & -1 & 1 \\\\ 1 & -1 & -1 & 1 & 1 & -1 \\\\\n-1 & 1 & -1 & 1 & 1 & -1 \\\\ 1 & 1 & -1 & 1 & -1 & 1 \\\\\n-1 & -1 & 1 & 1 & 1 & 1 \\\\ 1 & -1 & 1 & 1 & -1 & -1 \\\\\n-1 & 1 & 1 & 1 & -1 & -1 \\\\ 1 & 1 & 1 & 1 & 1 & 1 \\\\\n\\end{array}\n\\]\nThe resolution is IV as, for example, \\(\\beta _{1234}\\) is the lowest order interaction confounded with \\(\\beta _0\\).\n\nConstruct a design for factors 2 to 7 with four blocks, and allocate each level of water supply to one of the blocks. Then treat the block parameters seriously, rather than as nuisance parameters, as is usually the case. It should still be possible to find a FFD by choosing at least one design generator which does not confound itself with the block generators, but the number of observations must be at least 16 in order to fit the parameters.",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Factorial designs</span>"
    ]
  },
  {
    "objectID": "Factorial designs.html#appendix-plackett-and-burman-designs",
    "href": "Factorial designs.html#appendix-plackett-and-burman-designs",
    "title": "6  Factorial designs",
    "section": "6.7 Appendix: Plackett and Burman designs",
    "text": "6.7 Appendix: Plackett and Burman designs\nThe following is reference material and will not be examined.\nPlackett and Burman (1946) proposed a method of constructing a FFD in which the total number of observations is not necessarily a power of two, but nevertheless the columns of the design diagram are orthogonal to each other. They discovered designs for values of \\(n\\) which are multiples of 4 up to 100, excluding 92, for which a design was later discovered.\nThe method is to find a sequence of \\(\\frac{1}{2}n\\) plus signs and \\(\\frac{1}{2}n-1\\) minus signs, make this the first row of the diagram, form all the cyclical permutations of this sequence to give the next \\(n-2\\) rows, and finish the diagram off with a row of \\(-1\\)’s. If the initial sequence is chosen right, this design will have the desired orthogonality property; the difficulty is to find a sequence which does the trick. This sequence is called the generator.\n\n\n\n\n\n\nNoteExample\n\n\n\n\nExample 6.8 For \\(n=12\\), the generator takes the form \\[\\begin{array}{ccccccccccc} + & + & - & + & + & + & - & - & - & + & - \\\\ \\end{array}\\] and so the design may be represented by the following table. \\[\\begin{array}{ccccccccccc}\n+ & + & - & + & + & + & - & - & - & + & - \\\\\n- & + & + & - & + & + & + & - & - & - & + \\\\\n+ & - & + & + & - & + & + & + & - & - & - \\\\\n- & + & - & + & + & - & + & + & + & - & - \\\\\n- & - & + & - & + & + & - & + & + & + & - \\\\\n- & - & - & + & - & + & + & - & + & + & + \\\\\n+ & - & - & - & + & - & + & + & - & + & + \\\\\n+ & + & - & - & - & + & - & + & + & - & + \\\\\n+ & + & + & - & - & - & + & - & + & + & - \\\\\n- & + & + & + & - & - & - & + & - & + & + \\\\\n+ & - & + & + & + & - & - & - & + & - & + \\\\\n- & - & - & - & - & - & - & - & - & - & -\n\\end{array}\\] It may be checked that any two columns in this diagram are orthogonal to each other.\n\n\n\nA Plackett and Burman design can be used in a screening experiment with anything up to \\(n-1\\) factors, and is particularly useful if the number of factors is not close to a power of two, and the screening experiment is to be as small and economical as possible. For instance, the design in the preceding example can test 11 factors with 12 observations, whereas the nearest classical FFD would require 16 observations.\nPlackett and Burman designs are less useful in full experiments because the parameters may not be so easy to interpret.",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Factorial designs</span>"
    ]
  },
  {
    "objectID": "Factorial designs.html#appendix-composite-designs",
    "href": "Factorial designs.html#appendix-composite-designs",
    "title": "6  Factorial designs",
    "section": "6.8 Appendix: composite designs",
    "text": "6.8 Appendix: composite designs\nThe following is reference material and will not be examined.\nWe have already mentioned that CFD’s cannot be used if at least some of the factors are quantitative and it is desired to fit a quadratic regression model in such factors, so that more design points must be added. This may be done in a lot of ways, but there are certain ways which are fairly standard.\nIn a central composite design (CCD), the factors are varied at three equally spaced levels, scaled so as to take the values \\(-1, 0\\) and \\(1\\). The addition of a third level is sufficient to be able to fit the second order polynomial model \\[E(Y) = \\beta_0 + \\sum _{i=1}^m \\beta_ix_i + \\sum _{i=1}^{m-1}\\sum_{j=i+1}^m \\beta_{ij}x_ix_j + \\sum _{i=1}^m \\beta_{ii}x_i^2\\] provided the design points are chosen appropriately.\nA CCD consists of three types of points:\n\nthe points of a CFD or a FFD;\n’’star” points, namely points all of whose co-ordinates are zero except one, which is \\(\\pm \\alpha\\), where \\(\\alpha &gt;0\\), so that the point is a distance \\(\\alpha\\) from the origin;\ncentre points, all of whose co-ordinates are zero.\n\nIf for the star points we choose \\(\\alpha =1\\), we get the points in the middle of the faces of the cuboid (some of) whose corners appear in the CFD or FFD. Another common choice is \\(\\alpha =\\sqrt{m}\\) which means that the star points are the same distance from the origin as the points of the CFD or FFD, and the design region must be regarded as a spheroid. In this case, we get a rotatable composite design (RCD). This choice also means that the variables are observed at five levels (\\(\\pm 1, 0, \\pm \\alpha\\)), a consequence of which is that it is possible to test for significance of higher order models than quadratic. If we are able to add a few extra points, replication of centre points adds precision and does not destroy any of the symmetry of the design.\nIn general, if all three types of point are used in the design, and if \\(n_c\\) denotes the number of centre points used, then the total number of observations may be written \\[n = 2^{m-t} + 2m + n_c .\\]\n\n\n\n\n\n\nNoteExample\n\n\n\n\nExample 6.9 Construct a CCD for two factors and 10 observations, where it is not expected that a higher order model than quadratic will be needed.\nIn this case, \\(m=2\\) and we may choose \\(\\alpha =1\\). We may use a \\(2^2\\) CFD which takes up four observations, the star points will take up another four observations and so we may use two centre points. The design is described by the diagram below.\n\n\n\n   \\(x_1\\)   \n   \\(x_2\\)   \n\n\n\n\n\\(-1\\)\n\\(-1\\)\n\n\n1\n\\(-1\\)\n\n\n\\(-1\\)\n1\n\n\n1\n1\n\n\n1\n0\n\n\n\\(-1\\)\n0\n\n\n0\n1\n\n\n0\n\\(-1\\)\n\n\n0\n0\n\n\n0\n0\n\n\n\nThe model that can be fitted is the full quadratic model \\[E(Y) = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_{12}x_1x_2 + \\beta_{11}x_1^2 + \\beta_{22}x_2^2\\] and there are four degrees of freedom for error.",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Factorial designs</span>"
    ]
  },
  {
    "objectID": "Continuous and exact designs.html",
    "href": "Continuous and exact designs.html",
    "title": "7  Continuous and exact designs",
    "section": "",
    "text": "7.1 The General Equivalence Theorem\nConstructing an experimental design entails selecting points \\(\\mathbf{x}_1, \\mathbf{x}_2, \\ldots ,\\mathbf{x}_k\\in \\Omega\\), where \\(\\Omega\\) is the design region, and taking observations at each of these points, possibly with replication, so that, in general, we take, say, \\(n_1\\) observations at \\(\\mathbf{x}_1\\), \\(n_2\\) observations at \\(\\mathbf{x}_2\\), …, \\(n_k\\) observations at \\(\\mathbf{x}_k\\), where \\[\\sum _{i=1}^k n_i = n\\] and \\(n\\) is the total number of observations.\nThe set of design points chosen is called the support of the design. In terms of the proportions of the observations taken at the points of the support, we may denote the design symbolically by \\[\\xi = \\left( \\begin{array}{cccc} \\mathbf{x}_1 & \\mathbf{x}_2 & \\ldots & \\mathbf{x}_k\n\\\\ \\frac{n_1}{n} & \\frac{n_2}{n} & \\ldots & \\frac{n_k}{n}\n\\end{array} \\right).\\] Such a design is called an exact design, because it can be realised in practice, but for theoretical purposes it is useful more generally to consider continuous designs, where the proportions above are replaced by weights, which are non-negative real numbers \\(w_1, w_2, \\ldots , w_k\\) adding up to one, so that a continuous design may be written \\[\\xi = \\left( \\begin{array}{cccc} \\mathbf{x}_1 & \\mathbf{x}_2 & \\ldots & \\mathbf{x}\n_k \\\\ w_1 & w_2 & \\ldots & w_k \\end{array} \\right) .\\] A continuous design cannot necessarily be realised in practice, for example if some of the weights are irrational numbers, but it can be approximated by an exact design with enough observations.\nThe reason why we put an emphasis on weights rather than on numbers of replicates is that the optimality or otherwise of a design really only depends upon these weights. We can now define the information matrix of a continuous design for a given model as \\[\\mathbf{M}(\\xi)=\\sum _{i=1}^k w_i \\mathbf{f}(\\mathbf{x}_i) \\mathbf{f}(\\mathbf{x}_i)^T\\] and the variance of prediction as \\[d(\\mathbf{x},\\xi ) = \\mathbf{f}(\\mathbf{x})^T \\mathbf{M}(\\xi ) ^{-1}\\mathbf{f}(\\mathbf{x})\\] which means that, if the design is in fact an exact design, these coincide with the standardised information matrix and the standardised variance of prediction which we have mentioned earlier. To check this consider 3 design points \\(x_1,x_2,x_3\\) with \\(f(x_i)^T= \\left(1 \\; x_i \\right)\\) and with 2 observations at \\(x_2\\) and one at both \\(x_1\\) and \\(x_3\\). Verify that \\(\\mathbf{M}(\\xi )\\) is the same as the standardised information matrix in this example.\nIn other words, for continuous designs, \\(D\\)-optimality and \\(G\\)-optimality are equivalent, and an optimal design by either of these criteria can be identified by the fact that the maximum variance of prediction is equal to the number of parameters in the model. The proof of the GET is given in the appendix below. It is not examinable.",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Continuous and exact designs</span>"
    ]
  },
  {
    "objectID": "Continuous and exact designs.html#the-general-equivalence-theorem",
    "href": "Continuous and exact designs.html#the-general-equivalence-theorem",
    "title": "7  Continuous and exact designs",
    "section": "",
    "text": "WarningThe General Equivalence Theorem (GET)\n\n\n\n\nTheorem 7.1 The following three statements are equivalent.\n\nThe design \\(\\xi^*\\) maximises \\(|\\mathbf{M}(\\xi )|\\).\nThe design \\(\\xi^*\\) minimises \\(\\max_{\\mathbf{x}\\in \\Omega}d(\\mathbf{x},\\xi)\\).\n\\(\\max_{\\mathbf{x}\\in \\Omega }d(\\mathbf{x}, \\xi^*)=p\\).\n\n\n\n\n\n\n\n\n\n\n\nNoteExample\n\n\n\n\nExample 7.1 In Question 1 in Section 3.3 we showed for the quadratic regression model that the design which gives equal weights to the points \\(-1, 0\\) and 1 satisfies \\[\\max _{x \\in [-1,1] }d(x) = 3\\] which is the number of parameters in the model. We also showed that this design is \\(D\\)-optimal over a restricted range of choices, and that the maximum in the above is achieved at each of the points of support of the design. The proof of the GET now tells us that this latter is no accident, and, more importantly, that the design is both \\(D\\)-optimal and \\(G\\)-optimal over all possible choices of design for this model.\n\n\n\n\n\n\n\n\n\nNoteExample\n\n\n\n\nExample 7.2 Still with the quadratic regression model, consider the design \\[\\xi = \\left( \\begin{array}{cccc}-1&-\\frac{1}{3}&\\frac{1}{3}&1\\\\ \\frac{1}{4}&\\frac{1}{4}&\\frac{1}{4}&\\frac{1}{4}\\end{array}\\right).\\] In this case, with one observation at each of the four points, \\[\\mathbf{x}= \\left( \\begin{array}{ccc}1&-1&1\\\\1&-\\frac{1}{3}&\\frac{1}{9}\\\\1&\\frac{1}{3}&\\frac{1}{9}\\\\1&1&1\\end{array}\\right)\\] and therefore \\[\\mathbf{X}^T\\mathbf{X}=\\left(\\begin{array}{ccc}4&0&\\frac{20}{9}\\\\0&\\frac{20}{9}&0\\\\ \\frac{20}{9}&0&\\frac{164}{81}\\end{array}\\right).\\] The standardised variance of prediction is \\[\\begin{align}\nd(x)&=4\\mathbf{f}(\\mathbf{x})^T(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{f}(\\mathbf{x})\\\\\n&= 2.562 - 3.811x^2 + 5.062x^4\\\\\n&= 5.062[(x^2-0.376)^2-0.376^2]+2.562\n\\end{align}\\] which attains its maximum value 3.814 at \\(x=\\pm 1\\). This is greater than 3, and so the design is not \\(D\\)-optimal or \\(G\\)-optimal for this model.",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Continuous and exact designs</span>"
    ]
  },
  {
    "objectID": "Continuous and exact designs.html#tasks",
    "href": "Continuous and exact designs.html#tasks",
    "title": "7  Continuous and exact designs",
    "section": "7.2 Tasks",
    "text": "7.2 Tasks\n\nUse the GET to show that that the FFD in Example 6.4 is \\(D\\) and \\(G\\)-optimal\n\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\n\nThe design matrix is\n\n\\[\n\\mathbf{X}=\\left(\\begin{array}{cccc}1 & 1 & -1 & -1\\\\\n1 & -1 & 1 & -1\\\\\n1 & -1 & -1 & 1\\\\\n1 & 1 & 1 & 1\n\\end{array}\\right)\n\\]\nThe standardised information matrix \\(\\mathbf{M}\\) is the \\(4\\times 4\\) identity matrix \\(I_4\\) and so the standardised variance of prediction is \\[d(\\mathbf{x} ) = 1 + x_1^2 + x_2^2 + x_3^2 \\] Since \\(x_i\\) can only take the values -1 or 1, the maximum value of \\(d(\\mathbf{x})\\) is 4, equalling the number of model parameters. Hence by the GET, the design is both \\(D\\) and \\(G\\) optimal.",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Continuous and exact designs</span>"
    ]
  },
  {
    "objectID": "Continuous and exact designs.html#appendix-proof-of-the-get",
    "href": "Continuous and exact designs.html#appendix-proof-of-the-get",
    "title": "7  Continuous and exact designs",
    "section": "7.3 Appendix: proof of the GET",
    "text": "7.3 Appendix: proof of the GET\nThe following is reference material and will not be examined.\n\n\n\n\n\n\nWarningLemma\n\n\n\n\nLemma 7.1 For any continuous design, and model with \\(p\\) parameters, \\[ \\sum _{i=1}^k w_id(\\mathbf{x} _i,\\xi ) = p. \\]\n\n\n\nProof \\[\\begin{align}\n\\sum _{i=1}^k w_id(\\mathbf{x} _i,\\xi ) &= \\sum _{i=1}^k w_i\\mathbf{f} (\\mathbf{x}\n_i)^T\\mathbf{M} (\\xi ) ^{-1}\\mathbf{f} (\\mathbf{x} _i) \\\\\n&=  tr\\left( \\mathbf{M} (\\xi ) ^{-1}\\sum _{i=1}^k w_i\\mathbf{f} (\\mathbf{x}\n_i)\\mathbf{f} (\\mathbf{x} _i)^T \\right) \\\\\n&= tr\\left( \\mathbf{M} (\\xi ) ^{-1}\\mathbf{M} (\\xi )\\right) \\\\\n&= tr (\\mathbf{I} _p) = p\n\\end{align}\\] where we are using the fact that the trace of a scalar is itself, and the general results that \\(tr(\\lambda \\mathbf{A} +\\mathbf{B} )=\\lambda \\mbox{~tr~}\\mathbf{A}\n+\\mbox{tr~}\\mathbf{B}\\) and \\(tr(\\mathbf{A} \\mathbf{B})=tr(\\mathbf{B} \\mathbf{A})\\) whenever these equations make sense.\nIn words, this lemma states that the weighted average of the variances of prediction at the design points, with weights those of the design, is equal to the number of parameters in the model.\n\n\n\n\n\n\nWarningLemma\n\n\n\n\nLemma 7.2 If \\(\\mathbf{A} (\\alpha )=(A_{ij}(\\alpha ))\\) is a \\(p\\times p\\) positive definite matrix function of a real variable \\(\\alpha\\) and its components are differentiable with respect to \\(\\alpha\\), then \\[\n\\frac{\\partial}{\\partial \\alpha}\\ln \\left|\\mathbf{A} \\right| = tr\\left(\n\\mathbf{A}^{-1} \\frac{\\partial \\mathbf{A}}{\\partial \\alpha} \\right).\n\\]\n\n\n\nProof\n\\[\\begin{align}\n\\frac{\\partial }{\\partial \\alpha }\\ln |\\mathbf{A} | &= \\frac{1}{|\\mathbf{A}\n|}\\frac{\\partial }{\\partial \\alpha }|\\mathbf{A} | \\\\\n&= \\frac{1}{|\\mathbf{A} |} \\sum _{i=1}^p \\sum _{j=1}^p \\frac{\\partial\n|\\mathbf{A} |}{\\partial A_{ij}} \\frac{\\partial A_{ij}}{\\partial \\alpha }\\\\\n&= \\frac{1}{|\\mathbf{A} |} \\sum _{i=1}^p \\sum _{j=1}^p B_{ij}\n\\frac{\\partial A_{ij}}{\\partial \\alpha }\\\\\n& & \\mbox{where~$B_{ij}$~is~the~cofactor~of~$A_{ij}$}\\\\\n&= \\frac{1}{|\\mathbf{A} |} \\sum _{i=1}^p \\sum _{j=1}^p |\\mathbf{A} |(\\mathbf{A}\n^{-1})_{ji} \\frac{\\partial A_{ij}}{\\partial \\alpha }\\\\\n&= tr\\left( \\mathbf{A} ^{-1} \\frac{\\partial \\mathbf{A} }{\\partial \\alpha\n} \\right) .\n\\end{align}\\]\nNext, we introduce the concept of a mixture of two or more designs. If \\(\\xi _1\\) and \\(\\xi _2\\) are any two designs on the same design region, and \\(0&lt; \\alpha &lt;1\\), then the mixture \\((1-\\alpha )\\xi\n_1+\\alpha \\xi _2\\) is found by uniting the supports of the two designs and assigning weights to which one or both of \\(\\xi_1\\) and \\(\\xi_2\\) may contribute in the correct proportions \\(1-\\alpha\\) and \\(\\alpha\\). (If \\(\\alpha =0\\) or \\(\\alpha =1\\), the mixture has an obvious interpretation.)\nAs an example, consider finding \\(\\xi =\\frac{1}{4}\\xi _1+\\frac{3}{4}\\xi _2\\), where \\[ \\xi _1= \\left( \\begin{array}{ccc} -1 & 0 & 1 \\\\ \\frac{1}{3} &\n\\frac{1}{3} & \\frac{1}{3} \\end{array} \\right) \\mbox{~~~~and~~~~} \\xi\n_2= \\left( \\begin{array}{ccc} -2 & 0 & 2 \\\\ \\frac{1}{4} &\n\\frac{1}{2} & \\frac{1}{4} \\end{array} \\right) .\\] Here the support of the mixture is \\(\\{ -2,-1,0,1,2\\}\\) and the point \\(0\\) receives weight from both of the designs, whereas the other points only receive weight from one. We get \\[ \\xi = \\left( \\begin{array}{ccccc} -2 & -1 & 0 & 1 & 2 \\\\\n\\frac{3}{16} & \\frac{1}{12} & \\frac{11}{24} & \\frac{1}{12} &\n\\frac{3}{16} \\end{array} \\right) . \\]\nFrom our point of view, the important thing about mixtures is that information matrices mix in the same way. In other words, if \\(\\xi =(1-\\alpha )\\xi _1 +\\alpha\n\\xi _2\\) then \\[ \\mathbf{M} (\\xi ) = (1-\\alpha )\\mathbf{M} (\\xi _1)+\\alpha \\mathbf{M} (\\xi _2) . \\] This is obvious from the definition of the information matrix of a continuous design.\nThe following result is needed here, but we shall not prove it.\n\n\n\n\n\n\nWarningLemma\n\n\n\n\nLemma 7.3 \\(\\ln |\\mathbf{M} (\\xi )|\\) is a concave function of \\(\\xi\\), namely \\[\\ln |\\mathbf{M} ((1-\\alpha )\\xi _1+\\alpha \\xi _2)| \\geq (1-\\alpha )\\ln |\\mathbf{M}\n(\\xi_1)| + \\alpha \\ln |\\mathbf{M} (\\xi _2)|\\] for any \\(\\xi_1,\\xi _2,\\alpha\\).\n\n\n\nWe now have the results needed to prove Theorem 9.2 (GET).\nLet \\(\\xi ^*\\) be a design which maximises \\(|\\mathbf{M} (\\xi )|\\). For some other fixed design \\(\\xi _0\\), consider mixtures of the form \\[ \\xi = (1-\\alpha )\\xi ^* + \\alpha \\xi _0 . \\] Then by Lemma 7.2 \\[\\begin{align}\n\\frac{\\partial }{\\partial \\alpha }\\ln |\\mathbf{M} (\\xi )||_{\\alpha =0} & =\ntr\\left( \\mathbf{M} (\\xi )^{-1}\\frac{\\partial }{\\partial \\alpha\n}\\mathbf{M}\n(\\xi )\\right) |_{\\alpha =0} \\\\\n&= tr\\left( \\mathbf{M} (\\xi )^{-1}\\frac{\\partial }{\\partial\n\\alpha }((1-\\alpha )\\mathbf{M} (\\xi ^*) + \\alpha \\mathbf{M} (\\xi _0))\\right)\n|_{\\alpha =0} \\\\\n&= tr\\left( \\mathbf{M} (\\xi )^{-1}(\\mathbf{M} (\\xi _0)-\\mathbf{M} (\\xi\n^*))\\right ) |_{\\alpha =0} \\\\\n&= tr\\left( \\mathbf{M} (\\xi ^*)^{-1}\\mathbf{M} (\\xi _0)\\right) - p\n\\end{align}\\] since \\(\\mathbf{M} (\\xi ) = \\mathbf{M} (\\xi ^*)\\) at \\(\\alpha =0\\). As \\(\\xi ^*\\) maximises \\(|\\mathbf{M} (\\xi )|\\), the above partial derivative must be non-positive, and so we get \\[ tr\\left( \\mathbf{M} (\\xi ^*)^{-1}\\mathbf{M} (\\xi _0)\\right) \\leq p . \\] If we choose \\(\\xi _0\\) to be the design with all its weight at a point \\(\\mathbf{x}\\) in the design region, we get \\[\\begin{align}\ntr\\left( \\mathbf{M} (\\xi ^*)^{-1}\\mathbf{M} (\\xi _0)\\right) &=\ntr\\left( \\mathbf{M} (\\xi ^*)^{-1}\\mathbf{f} (\\mathbf{x} )\\mathbf{f} (\\mathbf{x} )^T\\right) \\\\\n&= \\mathbf{f} (\\mathbf{x} )^T\\mathbf{M} (\\xi ^*)^{-1}\\mathbf{f} (\\mathbf{x} ) \\\\\n&= d(\\mathbf{x}, \\xi ^*)\n\\end{align}\\] and so it follows that \\(d(\\mathbf{x} ,\\xi ^*)\\leq p\\) for all points \\(\\mathbf{x}\\) in the design region. But Lemma 7.1 tells us that a weighted average of the values of \\(d(\\mathbf{x},\\xi ^*)\\) over the points of support of \\(\\xi^*\\) is equal to \\(p\\). It follows that \\(d(\\mathbf{x} ,\\xi^*)\\) is actually equal to \\(p\\) at each of these points, and that, as a function of \\(\\mathbf{x}\\), it reaches its maximum value at each of these points. Moreover, again by Lemma 7.1, any design \\(\\xi\\) must have at least one point of support \\(\\mathbf{x}_i\\) at which \\(d(\\mathbf{x}_i,\\xi )\\geq p\\). Therefore \\[ \\max _{\\mathbf{x} \\in \\Omega }d(\\mathbf{x} ,\\xi ) \\geq p \\] for all designs \\(\\xi\\), and \\(\\xi^*\\) achieves the minimum value of the above maximum, this minimum value being \\(p\\).\nSo of the three parts of the statement of the theorem, we have shown that (1.) implies (2.) and (3.). We also know now that there exists at least one design \\(\\xi\\) for which \\(\\max_{\\mathbf{x} \\in \\Omega }d(\\mathbf{x},\\xi )=p\\), so that (2.) and (3.) are equivalent to each other. If we now take a design \\(\\xi^*\\) which satisfies (2.) or (3.), then, working backwards, we can conclude that, in the same notation as above, \\[\\frac{\\partial }{\\partial \\alpha }\\ln |\\mathbf{M} (\\xi )||_{\\alpha =0}\n\\leq 0 \\] for any choice of \\(\\xi_0\\). This would be impossible if there were a choice of \\(\\xi_0\\) for which \\(|\\mathbf{M} (\\xi_0)|&gt;|\\mathbf{M} (\\xi^*)|\\), because of the concavity property of Lemma 7.3. Hence \\(\\xi^*\\) maximises \\(|\\mathbf{M} (\\xi )|\\), and (2.) and (3.) imply (1.). This concludes the proof of the theorem.",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Continuous and exact designs</span>"
    ]
  },
  {
    "objectID": "Introduction to sampling.html",
    "href": "Introduction to sampling.html",
    "title": "8  Introduction to Sampling",
    "section": "",
    "text": "8.1 Contents\nThis topic covers statistical ideas behind sample surveys. Examples of sample surveys are opinion polls, such as those conducted by YouGov. Sample surveys are also used when it is difficult to determine quantities of interest by other means. For example:\nWe will cover the following themes:",
    "crumbs": [
      "Part II: Sampling Theory",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction to Sampling</span>"
    ]
  },
  {
    "objectID": "Introduction to sampling.html#contents",
    "href": "Introduction to sampling.html#contents",
    "title": "8  Introduction to Sampling",
    "section": "",
    "text": "Finite population methods and simple random sampling. Traditional sampling theory is based on the idea of drawing items at random, without replacement, from a finite population. Properties of estimators are derived from the sampling mechanism.\nChoosing the sample size. An important statistical question when designing any sample survey is how large the sample needs to be to provide reliable estimates of the quantities of interest.\nVariance reduction. For a fixed sample size, can we improve on “simple random sampling” and reduce the variance of our estimators?\nReliability of individual responses. Statistical issues aside, can we trust opinion polls? When might responses be biased or unreliable?\nStatistical modelling to support sample surveys. Modern sample survey methods can involve the use of statistical models to estimate quantities of interest, in contrast to simpler approaches in (1).",
    "crumbs": [
      "Part II: Sampling Theory",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction to Sampling</span>"
    ]
  },
  {
    "objectID": "Introduction to sampling.html#practicalities",
    "href": "Introduction to sampling.html#practicalities",
    "title": "8  Introduction to Sampling",
    "section": "8.2 Practicalities",
    "text": "8.2 Practicalities\nThere are various other practical issues to consider in sample surveys which we will briefly mention here, but not consider further.\n\nAdministering a survey. One choice is whether to interview participants face-to-face, or whether to ask them to complete the survey online. The latter is cheaper, but may be less reliable as participants may not engage properly with the survey. A comparison of different survey methods is reported in Asimov, A. (2025). Comparing self-administered mixed-mode and face-to-face designs in a repeated cross-sectional survey. International Journal of Social Research Methodology, 1–18.\nEthics approval. A survey conducted by staff or students at a UK university for research purposes would need ethics approval first, and other organisations may have similar requirements. A brief guide to ethics approval at Sheffield is available here.\nData management. Surveys may involve the collection of personal data. In the UK, there are legal requirements regarding the handling of personal data: the UK General Data Protection Regulation (UK GDPR) (similar to the European Union GDPR) and the Data Protection Act 2018. For an overview, see https://www.gov.uk/data-protection.",
    "crumbs": [
      "Part II: Sampling Theory",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction to Sampling</span>"
    ]
  },
  {
    "objectID": "Simple random sampling.html",
    "href": "Simple random sampling.html",
    "title": "9  Simple Random Sampling",
    "section": "",
    "text": "9.1 Finite and infinite population models\nTraditional sampling theory works through deriving properties of random samples (selections) from finite populations. This is in contrast with statistical modelling based on models of infinite populations. We first explain the difference.\nIn a study of birth weights of new-born babies, suppose we assume the weights of new-born female babies have a \\(N(\\mu,\\sigma^2)\\) distribution. This is an infinite population model:\nNow suppose we are investigating starting salaries of all Sheffield University graduates who graduated in 2025. Here, we have a finite population, as there were a finite number of graduates. For finite populations",
    "crumbs": [
      "Part II: Sampling Theory",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Simple Random Sampling</span>"
    ]
  },
  {
    "objectID": "Simple random sampling.html#finite-and-infinite-population-models",
    "href": "Simple random sampling.html#finite-and-infinite-population-models",
    "title": "9  Simple Random Sampling",
    "section": "",
    "text": "“infinite” because this doesn’t imply a limit on how many babies there are in the population; the population can include babies who will be born in the future;\na “model”: an assumption we choose to make because we think it’s useful in some way, because birth weights can’t really be normally distributed (they can’t be negative);\nwe suppose that when we observe a birth-weight, it is as if we have observed a random draw from a \\(N(\\mu,\\sigma^2)\\). If we observe lots of birth-weights and draw a histogram, we’d expect something that looks like the bell-shaped curve of the normal distribution;\nthe population mean and variance are defined as the parameters in this assumed normal distribution;\nwe can use the assumed normal distribution to derive properties of estimators of the population mean and variance.\n\n\n\nit is possible to make a list of every member of the population;\npopulation means and variances can be calculated given the values for every member on this list;\nwe draw random samples by randomly selecting members from this list;\nif we use random samples to estimate the population mean and variance, properties of the estimators can be derived by consideration of the way we randomly selected the sample;\nwe don’t need to assume any standard distribution (e.g. a normal distribution) for the population.\n\n\n\n\n\n\n\nImportantThis is confusing! When do we use each approach?\n\n\n\n\nWe can always use infinite population models: we can always assume a population can be described a particular standard probability distribution. We just have to remember that this is an assumption.\nWe can’t always use finite population methods, as it is not always possible to write down a list of everyone in the population and make random selections from it (e.g. writing down and selecting from a list of every baby including those born in the future).\nFinite population methods are best suited to small populations, when assumptions of standard probability distributions may not be suitable.",
    "crumbs": [
      "Part II: Sampling Theory",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Simple Random Sampling</span>"
    ]
  },
  {
    "objectID": "Simple random sampling.html#notation",
    "href": "Simple random sampling.html#notation",
    "title": "9  Simple Random Sampling",
    "section": "9.2 Notation",
    "text": "9.2 Notation\nConsider a finite population of \\(N\\) units, and suppose a sample of \\(n\\) units are selected from this population. We use the following notation.\n\nThe \\(N\\) population units are indexed by \\(\\{1,\\ldots,N\\}\\) and the value of interest for each member of the population are denoted by \\(X_1, \\ldots ,X_N\\).\nThe \\(n\\) sampled values are represented by \\(x_1, \\ldots ,x_n\\)\nThe proportion of the population sampled (the Sampling fraction) is \\(f=n/N\\)\nThe population mean (usual quantity of interest) is \\(\\overline{X} = \\sum_{1}^{N}X_{i}/N\\)\nThe population variance is \\[\\begin{align*}\nS^{2} &= \\frac{1}{N-1} \\sum_{1}^{N} (X_{i}-\\overline{X})^{2}\\\\\n&=\\frac{1}{N-1} \\left(\\sum_{1}^{N} X_{i}^2- N\\overline{X}^{2}\\right)\n\\end{align*}\\]\n\n\n\n\n\n\n\nImportantNote\n\n\n\n\nThe notational convention for random and non-random variables here is the opposite to that more commonly used: here capitals (\\(X_1, \\ldots ,X_N\\)) denote fixed population values and lower case letters \\(x_1, \\ldots ,x_n\\) denoting the sampled values are the random variables.\nThe formulae for the population mean and variance look like estimates but they are not! They are how we define the population mean and population variance for finite populations of size \\(N\\).",
    "crumbs": [
      "Part II: Sampling Theory",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Simple Random Sampling</span>"
    ]
  },
  {
    "objectID": "Simple random sampling.html#simple-random-sampling-definition-and-properties",
    "href": "Simple random sampling.html#simple-random-sampling-definition-and-properties",
    "title": "9  Simple Random Sampling",
    "section": "9.3 Simple random sampling: definition and properties",
    "text": "9.3 Simple random sampling: definition and properties\nA simple random sample (SRS) is the simplest form of sampling from a finite population. A list of the population members needs to be available. If all possible samples of size \\(n\\) from a population of \\(N\\) have equal probability of being drawn, then the sampling is said to be SRS. The number of such samples (the number of different selections of \\(n\\) things from \\(N\\), neglecting order) is \\[{}^NC_n=\\frac{N!}{n!(N-n)!}\\] and so the probability of each is \\(1/{N \\choose n}\\).\n\n\n\n\n\n\nImportantNotation convention\n\n\n\nSuppose there is population with \\(N=5\\) people and we have \\[\nX_1 = 12, X_2 = 8, X_3=17, X_4 = 8, X_5=20,\n\\] and we draw a simple random sample of size \\(n=1\\). We can see that \\[\nP(x_1=8) = \\frac{2}{5},\n\\] but we will read the event \\(x_1=X_2\\) to mean that for the first sampled person, we select person number \\(2\\) from the population and observe their corresponding value: we write \\[\nP(x_1 = X_2) = \\frac{1}{5}.\n\\] This simplifies the following notation, in that we don’t have to consider whether all the population values are unique or not.\n\n\nFor a SRS of size \\(n\\) from a population of size \\(N\\), the total number of possible samples (where the order matters) is \\[\nN\\times (N-1)\\times \\ldots (N-n + 1) = \\frac{N!}{n!}\n\\] so the probability of any specific sample is \\(\\frac{(N-n)!}{N!}\\).\n\n\n\n\n\n\nImportantDefining a SRS\n\n\n\nNote that the requirement for a sample to be an SRS is: for any specific sample of size \\(n\\), written in order as \\(X_{j_1},\\ldots,X_{j_n}\\), with \\(j_1,\\ldots,j_n\\in\\{1,\\ldots,N\\}\\), we have\n\\[P(x_1=X_{j_1},\\ldots ,x_n=X_{j_n})= \\frac{(N-n)!}{N!}.\\] This is a stronger requirement than the simpler statement \\[P(x_i = X_r)=1/N,\\] for all \\(i,r\\): that any sampled item is equally likely to be any member of the population. This simpler statement is a consequence of SRS and we will prove this next.\n\n\n\n\n\n\n\n\nWarningProperties of simple random samples\n\n\n\n\nTheorem 9.1 Suppose \\(x_1,\\ldots ,x_n\\) are obtained from \\(X_1,\\ldots X_N\\) by SRS. Then\n\n\\(P(x_i=X_r) = 1/N\\) for each \\(i\\) and \\(r\\) (i.e. any sample item is equally likely to be any population unit).\n\\(E(x_i)=\\overline{X}\\) for each \\(i\\).\n\\(\\text{var}(x_i) = (N-1) S^2/N\\) for each \\(i\\).\n\\(P(x_i=X_r,x_j=X_s)= 1/(N(N-1))\\) for each \\(i \\neq j\\) and \\(r \\neq s\\).\ncov\\((x_i,x_j) = -S^2/N\\), for \\(i \\neq j\\).\n\n\n\n\nProof.\n\nThe number of samples in which \\(x_i=X_r\\) is the number of ways of choosing values for the other \\(x\\)’s, which is (taking order into account) \\(\\frac{(N-1)!}{(N-n)!}\\) (the number of permutations of selecting \\(n-1\\) objects from \\(N-1\\)); hence the probability in (1) is this divided by the number of ways of choosing all of the \\(x\\)’s, \\(\\frac{N!}{(N-n)!}\\) (the number of permutations of selecting \\(n\\) objects from \\(N\\)), and so is \\(1/N\\).\nThis is included in Q1 in the tasks at the end of this chapter (solution provided).\nNote that var\\((x_i) = E(x_i^2)-(Ex_i)^2\\), and \\(E(x_i^2)=\\frac{1}{N}\\sum_1^N X_r^2\\) by (i) and \\(E(x_i)=\\overline{X}\\) by (ii), so that var\\((x_i)= \\frac{1}{N}\\sum X_r^{2} - \\overline{X}^2 = \\frac{1}{N} \\sum (X_r-\\overline{X})^2\\).\nSimilarly to (1), the number of samples in which the \\(i\\)th and \\(j\\)th elements are fixed as \\(X_r\\) and \\(X_s\\) respectively is \\(\\frac{(N-2)!}{(N-n)!}\\), and so the probability is this divided by \\(\\frac{N!}{(N-n)!}\\).\nNote that cov\\((x_i,x_j)=E(x_ix_j)-\\overline{X}^2\\) and use the fact that \\[\\begin{align}\nE(x_ix_j) &= \\frac{1}{N(N-1)}\\sum \\sum _{r\\neq s}X_rX_s \\\\\n&=\\frac{1}{N(N-1)}\\left( (\\sum X_r)^2 - \\sum X_r^2 \\right) \\\\\n&=\\frac{1}{N(N-1)}\\left( N^2\\overline{X}^2-\\{(N-1)S^2+N\\overline{X}^2 \\}\\right)\\\\\n&=\\overline{X}^2 - \\frac{S^2}{N}.\n\\end{align}\\]\n\n\n\n\n\n\n\nWarningBest Linear Unbiased Estimator (BLUE) of \\(\\overline{X}\\)\n\n\n\n\nTheorem 9.2 Under SRS, \\[\\overline{x} = \\sum_1^n x_i/n~~\\text{ is the BLUE of }\\overline{X}.\\] That is, amongst all estimators of the form \\(t = \\sum_{i=1}^n c_ix_i\\) for constants \\(c_i\\), and such that \\(E(t) = \\overline{X}\\), the one with \\(c_i = 1/n\\) has smallest variance. Moreover this minimum variance is \\[\\text{var}(\\overline{x}) = \\left(1-\\frac{n}{N}\\right)\\frac{S^2}{n}.\\]\n\n\n\nProof. For any constants \\(c_1, \\ldots,c_n\\) \\[\\begin{align}\n\\mbox{var}(t)&=\\sum c_i^2~\\mbox{var}(x_i)+\\sum \\sum_{i\\neq j}c_i c_j\\mbox{ cov }(x_i,x_j) \\\\\n&=\\frac{N-1}{N} S^2 \\sum_1^n c_i^2 + \\sum \\sum_{i \\neq j}c_i c_j\\left(-\\frac{S^2}{N}\\right) \\\\\n&=S^2 \\sum_1^n c_i^2 - \\frac{S^2}{N} \\left(\\sum _1^nc_i \\right)^{2} \\\\\n&=S^2\\left\\{\\sum_1^n\\left(c_i-\\frac{1}{n}\\right)^2+\\frac{2}{n}\\sum_1^n c_i-\\frac{1}{n} \\right\\}-\\frac{S^2}{N}\\left(\\sum_1^n c_i\\right)^2 .\n\\end{align}\\] For unbiasedness, \\(\\left(E(t) = \\overline{X}\\right)\\) we must have \\(\\sum _1^nc_i=1\\) and then the first term in the curly bracket above is a sum of squares and takes its minimum value of zero when \\(c_i=1/n\\) for each \\(i\\), which satisfies the unbiasedness constraint. Substituting back, we get \\[\\mbox{var}(t)=\\text{var}(\\overline{x})=S^2\\left\\{0+\\frac{2}{n}-\\frac{1}{n}\\right\\}-\\frac{S^2}{N}=\\left(1-\\frac{n}{N}\\right)\\frac{S^2}{n}.\\] Note that var\\((\\overline{x})\\) depends on\n\nthe population variance \\(S^2\\);\nthe sample size \\(n\\);\nthe sampling fraction \\(f=n/N\\).\n\nConsider a sample of size \\(n\\). For an infinite population we have var\\((\\overline{x})=\\sigma^2/n\\) where \\(\\sigma^2\\) is the population variance. For a finite population we have \\[\n\\text{var}(\\overline{x})=(1-f)S^{2}/n.\n\\tag{9.1}\\] \\(1-f\\) is called the finite population correction.\n\n\n\n\n\n\nImportantFinite population corrections\n\n\n\nWhen drawing samples from large populations, we can usually ignore the finite population correction, because \\(1-n/N\\) is typically close to 1. Note that also we may not actually know the population size \\(N\\).",
    "crumbs": [
      "Part II: Sampling Theory",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Simple Random Sampling</span>"
    ]
  },
  {
    "objectID": "Simple random sampling.html#estimation-of-population-variance-s2",
    "href": "Simple random sampling.html#estimation-of-population-variance-s2",
    "title": "9  Simple Random Sampling",
    "section": "9.4 Estimation of Population Variance \\(S^2\\)",
    "text": "9.4 Estimation of Population Variance \\(S^2\\)\nThe formula for var \\((\\overline{x})\\) is used:\n\nto estimate the precision obtained in the survey;\nto compare the precision obtained with that given by other sampling schemes (for example the stratified SRS considered later);\nto estimate the sample size needed to achieve a specified precision.\n\nHowever the formula depends on \\(S^2\\), which is usually unknown, so it is estimated from the sample, by the sample variance \\(s^2\\), defined as \\[s^2 = \\frac{1}{n-1} \\sum_1^n (x_{i}-\\overline{x})^{2} = \\frac{1}{n-1} \\left( \\left\\{\\sum_1^n x^2_{i}\\right\\}-n\\overline{x}^{2}\\right)\\] giving \\[\\widehat{\\text{var}}(\\overline{x})=\\left(1-\\frac{n}{N}\\right)\\frac{s^2}{n}.\\] where \\(\\widehat{\\text{var}}(\\overline{x})\\) is the estimated variance of the sample mean. Under SRS \\[E(s^2) = S^2.\\] and so the above is an unbiased estimator of var\\((\\overline{x})\\). We prove this as follows: \\[\\begin{align}\nE(n-1)s^2 &= E\\left( \\sum_{i=1}^n (x_i - \\overline{X} + \\overline{X}- \\overline{x})^2\\right) \\\\\n&=E\\left(\\sum_1^n(x_{i}-\\overline{X})^2-n(\\overline{x} - \\overline{X})^{2}\\right)\\\\\n&=\\frac{n(N-1)}{N} S^2 - n \\left(1-\\frac{n}{N}\\right)\\frac{S^2}{n}\\\\\n&=(n-1)S^2.\n\\end{align}\\]",
    "crumbs": [
      "Part II: Sampling Theory",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Simple Random Sampling</span>"
    ]
  },
  {
    "objectID": "Simple random sampling.html#sec-proportion",
    "href": "Simple random sampling.html#sec-proportion",
    "title": "9  Simple Random Sampling",
    "section": "9.5 Estimation of a Proportion \\(P\\)",
    "text": "9.5 Estimation of a Proportion \\(P\\)\nSuppose we wish to estimate the proportion (\\(P\\)) of units in the population that have a particular characteristic (\\(C\\)), for example, the proportion of a population intending to vote for a particular party at the next UK general election.\nHere, we suppose the sample is again drawn by Simple Random Sampling (SRS). We can then show that this is just a special case of using SRS to estimate a population mean \\(\\bar{X}\\). We define \\[X_i=\\left\\{\\begin{array}{ll}1 & \\mbox{if unit $i$ has characteristic $C$,}\\\\\n0 & \\mbox{if unit $i$ does not have $C$.}\\end{array}\\right.\\]\nThen the population proportion \\(P\\) with the characteristic \\(C\\) is \\[\nP=\\frac{1}{N}\\sum_{i=1}^N X_i = \\bar{X}.\n\\] The population variance \\(S^2\\) simplifies to \\[\\begin{align*}\nS^2&=\\frac{1}{N-1} \\sum_1^N (X_i-\\bar{X})^2 =\\frac{1}{N-1} \\sum_1^N (X_i-P)^2\\\\\n&=\\frac{1}{N-1}\\left(NP - 2NP^2 + NP^2 \\right) = \\frac{NP(1-P)}{N-1}\\simeq P(1-P),\n\\end{align*}\\] where for large \\(N\\), we approximate \\(N/(N-1)\\) by 1.\nFor our sample, we define \\[x_i=\\left\\{\\begin{array}{ll}1 & \\mbox{if sample member $i$ has characteristic $C$,}\\\\\n0 & \\mbox{if sample member $i$ does not have $C$.}\\end{array}\\right.\\]\nLet \\(p\\) represent the proportion of members in the sample that have characteristic \\(C\\). Then we have \\(p=\\bar{x}\\). For the sample variance \\(s^2\\) we have \\[s^2=\\frac{1}{n-1}\\sum_{i=1}^n (x_i-\\bar{x})^2 =\\frac{1}{n-1} \\sum_{i=1}^n (x_i-p)^2=\\frac{np(1-p)}{n-1},\\]\nNow, suppose we have used Simple Random Sampling (SRS) to obtain our sample. Then we have\n\nthe sample proportion is an unbiased estimator of the population proportion:\n\n\\[E(p)=E(\\overline{x})= \\overline{X} = P.\\]\n\nThe variance of the sample proportion is\n\n\\[Var(p) = Var(\\bar{x})=(1-f)S^2/n\\simeq (1-f)P(1-P)/n.\\] As this variance depends on the unknown population variance \\(S^2\\), we replace the unknown \\(S^2\\) by the sample variance \\(s^2\\) to give an estimated variance \\[\n\\widehat{Var}(p) = (1-f)s^2/n= (1-f)\\frac{p(1-p)}{n-1}.\n\\]\n\n\n\n\n\n\nImportantPopulation variances for binary variables\n\n\n\nFor a population of binary variables (e.g. where the only possible values of \\(X_i\\) are 1 or 0), we have seen that the population variance \\(S^2\\) is determined by the population proportion \\(P\\). Once we have the sample proportion \\(p\\), this is sufficient to give us both the estimate of the population proportion \\(P\\) and the estimate of the population variance \\(S^2\\); there is no independent estimation of the variance.",
    "crumbs": [
      "Part II: Sampling Theory",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Simple Random Sampling</span>"
    ]
  },
  {
    "objectID": "Simple random sampling.html#tasks",
    "href": "Simple random sampling.html#tasks",
    "title": "9  Simple Random Sampling",
    "section": "9.6 Tasks",
    "text": "9.6 Tasks\n\nShow that under SRS, \\(\\bar{x}\\) is an unbiased estimator of \\(\\bar{X}\\).\nConsider a small population of \\(N=10\\) items. Suppose 3 out of the 10 items are faulty. If an SRS of size \\(n=2\\) is drawn, give one example that shows that the number of faulty items in the sample does not have a \\(Binomial(n=10, p=0.3)\\) distribution. Briefly explain why this is the case.\nA SRS of size \\(2n\\) is chosen from a finite population of size \\(N\\) (\\(N&gt;2n\\)). The population mean and variance are \\(\\overline{X}\\) and \\(S^2\\) respectively. The sample is divided into two equal parts: the first \\(n\\) observations, and the second \\(n\\) observations. Only the two sample means are available (not the actual sample values) and are given by \\(\\overline{x}_1\\) and \\(\\overline{x}_2\\). Show that \\[\\frac{n}{2}\\left(\\overline{x}_1-\\overline{x}_2\\right)^2\\] is an unbiased estimator of \\(S^2\\). Hint: use the result that for any random variable \\(Z\\), \\[\nE(Z^2) = Var(Z) + E(Z)^2.\n\\]\n\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\n\nWe have \\[\nE(\\bar{x}) = \\frac{1}{n}\\sum_{i=1}^nE(x_i).\n\\] Now \\[\nE(x_i) = \\sum_{j=i}^N X_j P(x_i = X_j) = \\sum_{j=i}^N X_j \\frac{1}{N} = \\bar{X}.\n\\] Hence \\[\nE(\\bar{x}) = \\frac{1}{n}\\sum_{i=1}^n\\bar{X} = \\bar{X}\n\\]\nConsider the probability that both items in the sample are faulty. Ignoring the order in which items are drawn, there are \\({}^{10}C_2\\) possibly samples in total, and \\({}^3C_2\\) possible samples in which both items are faulty, so the probability both items in the sample are faulty is \\[\n\\frac{{}^3C_2}{{}^{10}C_2} = \\frac{6}{90}.\n\\] But for \\(X\\sim Binomial(n=2, p=0.3)\\), \\[\nP(X = 2) = \\left(\\frac{3}{10}\\right)^2 \\neq \\frac{6}{90}.\n\\] The difference is because in SRS, we are sampling without replacement: if the first item in the sample is faulty, the probability the second item in the sample is faulty is \\(2/9\\), not \\(3/10\\).\nWe have \\[\nE((\\bar{x}_1-\\bar{x}_2)^2) = Var(\\bar{x}_1-\\bar{x}_2) + (E(\\bar{x}_1-\\bar{x}_2))^2 = Var(\\bar{x}_1-\\bar{x}_2),\n\\] as \\(E(\\bar{x}_1) = E(\\bar{x}_2)\\).\n\nNow\n\\[\\begin{align*}Var(\\bar{x}_1-\\bar{x}_2) &=\nVar(\\bar{x}_1) + Var(\\bar{x}_2)\n-2Cov(\\bar{x}_1, \\bar{x}_2)\\\\ &=2\\left( 1-\\frac{n}{N}\\right)\\frac{S^2}{n}-2Cov(\\bar{x}_1, \\bar{x}_2).\n\\end{align*}\\] Denoting the sample in the first part by \\(x_{11},\\ldots,x_{1n}\\) and the sample in the second part by \\(x_{21},\\ldots,x_{2n}\\), we have \\[\\begin{align*}\nCov(\\overline{x}_1, \\overline{x}_2) & =  \\frac{1}{n^2} \\sum_{i=1}^n \\sum_{j=1}^n Cov(x_{1i},x_{2j})\n\\\\\n& =  \\frac{1}{n^2}n^2\\left( -\\frac{S^2}{N}\\right) \\\\\n& =  -\\frac{S^2}{N},\n\\end{align*}\\] using (5) from Theorem 9.1. Thus, substituting in the above, \\[E(\\overline{x}_1-\\overline{x}_2)^2 = 2\\left( 1-\\frac{n}{N}\\right)\n\\frac{S^2}{n} + \\frac{2S^2}{N} = \\frac{2S^2}{n}\\] and so it follows that \\[\\frac{n(\\overline{x}_1-\\overline{x}_2)^2}{2} \\] is unbiased for \\(S^2\\).",
    "crumbs": [
      "Part II: Sampling Theory",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Simple Random Sampling</span>"
    ]
  },
  {
    "objectID": "Sample sizes and confidence intervals.html",
    "href": "Sample sizes and confidence intervals.html",
    "title": "10  Confidence intervals and sample sizes",
    "section": "",
    "text": "10.1 Estimation errors and sample sizes\nIn the previous chapter, we saw that the sample mean is an unbiased estimator of the population mean (likewise for proportions). But we would always expect the sample mean to differ from the population mean. This raises two issues:\nWe address both these issues using confidence intervals:",
    "crumbs": [
      "Part II: Sampling Theory",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Confidence intervals and sample sizes</span>"
    ]
  },
  {
    "objectID": "Sample sizes and confidence intervals.html#estimation-errors-and-sample-sizes",
    "href": "Sample sizes and confidence intervals.html#estimation-errors-and-sample-sizes",
    "title": "10  Confidence intervals and sample sizes",
    "section": "",
    "text": "we need to be able to report how large the estimation error (the difference between the sample mean and the population mean) could be;\nif the estimation error is potentially too large, the sample survey won’t be very useful; we need to plan the survey to avoid this.\n\n\n\nwe report a confidence interval (typically a 95% confidence interval) as well as the point estimate from the sample mean (or sample proportion), to indicate a range of plausible values; how far the population mean could be from the sample mean;\nwe consider the relationship between the sample size and the width of the interval; we choose the sample size to ensure the width is acceptably small.",
    "crumbs": [
      "Part II: Sampling Theory",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Confidence intervals and sample sizes</span>"
    ]
  },
  {
    "objectID": "Sample sizes and confidence intervals.html#sec-CIXbar",
    "href": "Sample sizes and confidence intervals.html#sec-CIXbar",
    "title": "10  Confidence intervals and sample sizes",
    "section": "10.2 Confidence Intervals for \\(\\overline{X}\\)",
    "text": "10.2 Confidence Intervals for \\(\\overline{X}\\)\nConfidence intervals for \\(\\overline{X}\\) are usually based on the assumption that under SRS \\[\\overline{x} \\sim N(\\overline{X},(1-f)S^2/n)\\] approximately. Using this approximation, we then have \\[\\frac{\\overline{x}-\\overline{X}}{\\sqrt{(1-f)S^2/n}}\\sim N(0,1). \\tag{10.1}\\] The population variance \\(S^2\\) will be unknown, so we estimate it with the sample variance \\(s^2\\). An approximate 95% confidence interval for \\(\\overline{X}\\) can then be reported as \\[\n\\bar{x}\\pm t_{n; 0.025}\\sqrt{(1-f)s^2/n},\n\\], where \\(t_{n; 0.025}\\) is the 97.5th percentile of the \\(t\\) distribution with \\(n\\) degrees of freedom.\nMore simply, we may calculate the intervals as \\[\n\\bar{x}\\pm 1.96\\sqrt{s^2/n},\n\\tag{10.2}\\] where\n\nwe have ignored the finite population correction. We may have to if the total population size \\(N\\) is unknown; we would need to assume \\(n/N\\) is small;\nwe have assumed \\(n\\) is large enough such that there is little difference between the \\(t\\)-distribution with \\(n\\) degrees of freedom and the standard normal distribution.\n\n\n\n\n\n\n\nNoteExample\n\n\n\n\nExample 10.1 SRS is to be used in a quality control process. A batch of \\(N=200\\) components has been produced (high-specification bolts in aerospace manufacturing). For this population of 200 bolts, we want to know the mean load \\(\\bar{X}\\) the bolts can carry before they break (measured in MegaPascals MPa), but measuring for this for any single bolt will destroy the bolt: we estimate \\(\\bar{X}\\) using a small sample with \\(n=10\\).\nThe sample is shown below.\n\nx\n\n [1] 431.1 512.7 483.0 534.6 580.7 603.1 626.9 611.8 528.9 672.8\n\n\nWe compute the sample mean and sample variance as follows\n\nsum(x)/10\n\n[1] 558.56\n\n1/9 * sum((x - mean(x))^2)\n\n[1] 5393.547\n\n\nWe obtain the 97.5th percentile from the \\(t_{9}\\) distribution:\n\nqt(0.975, df = 9 )\n\n[1] 2.262157\n\n\nA 95% confidence interval is calculated as \\[\n558.56 \\pm 2.262 \\sqrt{\\left(1-\\frac{10}{200}\\right)\\frac{5393.547}{10}}\n\\] i.e. \\[\n(507.3,\\, 609.8)\\mbox{MPa}\n\\]\nIn the appendix, we use simulation to investigate the normal approximation in Equation 10.1.",
    "crumbs": [
      "Part II: Sampling Theory",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Confidence intervals and sample sizes</span>"
    ]
  },
  {
    "objectID": "Sample sizes and confidence intervals.html#sec-sampleSizeMean",
    "href": "Sample sizes and confidence intervals.html#sec-sampleSizeMean",
    "title": "10  Confidence intervals and sample sizes",
    "section": "10.3 Sample size for estimating a mean",
    "text": "10.3 Sample size for estimating a mean\nNow we have formula for the confidence interval, we can (almost!) use it to propose a sample size. We use the simpler form for the confidence interval Equation 10.2.\nThe user has to specify the desired maximum width for a confidence interval for \\(\\overline{X}\\), given a desired level of confidence (typically 95%). Using Equation 10.2 for a 95% interval, the width \\(w\\) will be \\[\nw:=2\\times 1.96 \\sqrt{s^2/n}.\n\\] If the user specifies the requirement that \\(w\\) is below some desired value, then we could solve for \\(n\\)…except that we don’t know the value of \\(s^2\\)! Two options are\n\nif we have some other data that we think is related, or perhaps data from a pilot study, use that to estimate \\(s^2\\);\nchoose a value for \\(s^2\\) based on our own judgement of what is plausible for the population standard deviation \\(S\\): estimate what we think \\(S\\) is, and then replace \\(s^2\\) by the square of our estimate of \\(S\\).\n\nTo estimate the population standard deviation \\(S\\), note that for a normal distribution, we’d expect 95% of the population to be covered by an interval of width approximately \\(4S\\). So we could estimate \\(S\\) by thinking of what sort of range of values would cover most of the population, then dividing the the width of this range by 4.\n\n\n\n\n\n\nNoteExample\n\n\n\n\nExample 10.2 A smartphone manufacturer wants to estimate the mean battery life for a particular model, assuming typical everyday use. The manufacturer will report the mean battery life for \\(n\\) different users, given a single full charge per user, and would like to report a mean within an accuracy within plus or half an hour. We interpret this as saying that a 95% confidence interval should have a width of no more than 1 hour.\nWe write\n\\[n&gt;\\frac{(2\\times1.96\\times s)^2 }{w_{max}^2},\\] where \\(w_{max}=1\\) is the maximum acceptable width of the 95% confidence interval.\nWe replace \\(s\\) with a plausible estimate of the population \\(S\\). Following the approach above, we might suppose a range of 8 hours might cover most of the population, so we estimate \\(S=8/4\\). We then replace \\(s^2\\) by 2 to get\n\\[n&gt;(2\\times1.96\\times 2)^2 = 61.5.\\]\nWe shouldn’t be too precise here: we might report that around 60 is a suitable size.\nGiven that we’ve had to estimate \\(S\\), we could plan for sample of size 60 initially, use our sample data as it comes in to re-estimate \\(S\\), then adjust the total sample size accordingly.",
    "crumbs": [
      "Part II: Sampling Theory",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Confidence intervals and sample sizes</span>"
    ]
  },
  {
    "objectID": "Sample sizes and confidence intervals.html#sec-approx-interval-prop",
    "href": "Sample sizes and confidence intervals.html#sec-approx-interval-prop",
    "title": "10  Confidence intervals and sample sizes",
    "section": "10.4 Confidence intervals for a proportion \\(P\\)",
    "text": "10.4 Confidence intervals for a proportion \\(P\\)\nWe proceed as in Section 10.2, substituting \\(p\\) for \\(\\bar{x}\\) and \\(P\\) for \\(\\bar{X}\\). We suppose\n\\[p \\sim N(P,(1-f)S^2/n)\\] approximately. Using this approximation, we then have \\[\\frac{p-P}{\\sqrt{(1-f)S^2/n}}\\sim N(0,1).\\] As before, we replace \\(S^2\\) by the sample variance \\[\ns^2 = \\frac{n}{n-1}p(1-p),\n\\] and report a 95% confidence interval \\[\np\\pm 1.96\\sqrt{(1-f)\\frac{p(1-p)}{n-1}},\n\\] One small detail: even though we have estimated a population variance \\(S^2\\) by a sample variance \\(s^2\\), we don’t make the switch from the normal distribution to the \\(t\\)-distribution. This is because, as commented earlier, in this case the population variance \\(S^2\\) is determined by the value of \\(P\\): it’s not a separate quantity that we can estimate independently.\n\n\n\n\n\n\nNoteExample\n\n\n\n\nExample 10.3 A survey is undertaken to assess what proportion of undergraduate (UG) students at Sheffield University plan to study for a postgraduate (PG) degree. A SRS of 200 of the 18,000 UGs is undertaken. Of those 200 sampled, 56 plan to go on to PG study. Calculate a 95% CI for the proportion of Sheffield UGs that plan to go on to PG study.\nWe have \\(N=18,000\\), \\(n=200\\) and \\(p=56/200=0.28\\) which gives the CI as \\[\\begin{align}\n& p \\pm z_{1-\\alpha/2} \\sqrt{\\left(1-\\frac{200}{18000}\\right)\\frac{p(1-p)}{(n-1)}} \\\\\n&=0.28 \\pm 1.96\\sqrt{\\frac{(0.28)(0.72)}{199}}\\\\\n&\\simeq(0.22, 0.34)\n\\end{align}\\]",
    "crumbs": [
      "Part II: Sampling Theory",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Confidence intervals and sample sizes</span>"
    ]
  },
  {
    "objectID": "Sample sizes and confidence intervals.html#sample-size-for-estimating-a-proportion",
    "href": "Sample sizes and confidence intervals.html#sample-size-for-estimating-a-proportion",
    "title": "10  Confidence intervals and sample sizes",
    "section": "10.5 Sample Size for estimating a proportion",
    "text": "10.5 Sample Size for estimating a proportion\nWe use the same approach as in Section 10.3, with one difference. The user again needs to specify the desired maximum width for a confidence interval for \\(P\\). For a 95% interval, ignoring the finite population correct, this width is \\[\nw:=2\\times 1.96 \\sqrt{\\frac{p(1-p)}{n-1}}\n\\] Again, the width depends on something unknown (\\(p\\)), but the maximum width occurs when \\(p(1-p)\\) is maximised, and because \\(p\\in[0,1]\\), this is maximised at 0.25 when \\(p=0.5\\). Hence to determine the minimum \\(m\\), the user specifies the maximum desired width \\(w_{max}\\), and then we solve for\n\\[\n2\\times 1.96 \\sqrt{\\frac{1}{4(n-1)}}\\le w_{max}.\n\\]\n\n\n\n\n\n\nNoteExample\n\n\n\n\nExample 10.4 A SRS is to be undertaken to assess how many UK households do not own a car. How large a sample is required to give a 95% confidence interval for \\(P\\) that has a width less than 0.05?\nWe require \\[2\\times 1.96 \\sqrt{\\frac{1}{4(n-1)}}\\le 0.05\\] so that \\(n\\ge 1537.6\\).",
    "crumbs": [
      "Part II: Sampling Theory",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Confidence intervals and sample sizes</span>"
    ]
  },
  {
    "objectID": "Sample sizes and confidence intervals.html#dependent-proportions-for-polychotomous-variables",
    "href": "Sample sizes and confidence intervals.html#dependent-proportions-for-polychotomous-variables",
    "title": "10  Confidence intervals and sample sizes",
    "section": "10.6 Dependent proportions for polychotomous variables",
    "text": "10.6 Dependent proportions for polychotomous variables\nSuppose for each member of the population, the characteristic of interest is polychotomous (more than two possibilities), rather than binary. For example, in a UK election poll, there are more than two possible parties to vote for.\nThe previous methods can be used for point estimation: we can estimate, for example, the proportion of Labour voters by redefining the population characteristic simply as “Labour voter” or “not Labour voter”. However, the analysis is a little more complex if we want to quantify uncertainty about which proportion is largest, as the proportions are not independent, and one proportion does not determine the others.\nSuppose the interest is in who is likely to win out of Conservative and Labour. One way to analyse this is as follows. For member \\(i\\) of the population, define \\[\\begin{align}\nX_i &= 1 \\mbox{ if Conservative voter, and 0 otherwise},\\\\\nY_i &= 1 \\mbox{ if Labour voter, and 0 otherwise},\\\\\nZ_i &= X_i - Y_i,\n\\end{align}\\] so the population proportions of Conservative and Labour voters are \\(\\overline{X}\\) and \\(\\overline{Y}\\) respectively, with \\(\\overline{Z}\\) the difference between these two proportions. Define the corresponding sample equivalents \\(x_i, y_i\\) and \\(z_i\\). By SRS results for the sample mean, we have \\(\\bar{z}\\) an unbiased estimator of \\(\\overline{Z}\\), with sample variance given by \\[var(\\bar{z})=(1-f)\\frac{S^2_Z}{n}.\\] As usual, we estimate the population variance \\(S^2_Z\\) by the sample variance \\(s^2_Z\\), and report a 95% confidence interval as \\[\\overline{z}\\pm 1.96\\sqrt{\\left(1-\\frac{n}{N}\\right)\\frac{s_Z^2}{n}},\\] or \\[\\overline{z}\\pm 1.96\\sqrt{\\frac{s_Z^2}{n}},\\] if we assume \\(n/N\\) is small enough to ignore the finite population correction.\nNote that \\[\\begin{align}\n(n-1)s^2_Z &= \\sum_{i=1}^n(z_i-\\overline{z})^2\\\\\n&= n(\\overline{x} + \\overline{y} - (\\overline{x} - \\overline{y})^2).\n\\end{align}\\] as \\(z_i^2 = x_i + y_i\\).\n\n\n\n\n\n\nNoteExample\n\n\n\n\nAn opinion poll in Wales will ask the question: “If there were a Senedd (Welsh Parliament) election tomorrow, which party would you vote for?”. A sample size of \\(n=1000\\) is proposed. A previous poll resulted in estimates of 37% and 23% voting for Plaid Cymru and Reform UK respectively, for the two leading parties. What width of confidence interval might we expect for the lead of Plaid Cymru over Reform UK? What would happen with a sample size of \\(n=100\\)?\nDefine \\(\\bar{x}\\) and \\(\\bar{y}\\) to be the sample proportions of Plaid Cymru and Reform UK voters respectively. Ignoring the finite population correction, the width of the confidence interval will be \\[\n2\\times 1.96 \\sqrt{s^2_Z/n}\n\\] with \\[\ns^2_Z = \\frac{n}{n-1}(\\overline{x} + \\overline{y} - (\\overline{x} - \\overline{y})^2).\n\\] We have \\(n=1000\\). Before we conduct the poll, we don’t know what the values of \\(\\bar{x}\\) and \\(\\bar{y}\\) will be. Two options here are:\n\nWe estimate \\(\\bar{x}\\) and \\(\\bar{y}\\) to be 0.37 and 0.25 respectively based on the previous poll. This gives a confidence interval width of approximately 0.1. So, for example, if the new poll again observed 37% and 25% of respondents saying they would would vote for Plaid Cymru and Reform UK respectively, the confidence interval would be \\((7\\%, 17\\%)\\) for the difference between the two parties. For a sample size of \\(n=100\\), the interval would widen to \\((-3\\%, 27\\%)\\): we might consider this to be unacceptably wide, hence a sample size of 100 is too small.\nWe could also note that maximum possible value of \\(s^2_Z\\) occurs when \\(\\bar{x}=\\bar{y}=0.5\\), giving a maximum possible width of 0.12. With a sample size of \\(n=100\\), this increases to a width of 0.4.",
    "crumbs": [
      "Part II: Sampling Theory",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Confidence intervals and sample sizes</span>"
    ]
  },
  {
    "objectID": "Sample sizes and confidence intervals.html#tasks",
    "href": "Sample sizes and confidence intervals.html#tasks",
    "title": "10  Confidence intervals and sample sizes",
    "section": "10.7 Tasks",
    "text": "10.7 Tasks\n\nA SRS of size 1000 was taken from the UK population and body weight (kg) was recorded. \\(x_1, x_2, \\ldots x_{1000}\\) are the 1000 sample weights which had a mean of 78.2 kg. Further calculation gave \\(\\sum_{i=1}^{1000}x_i^2=6,241,154\\) kg\\(^2\\). Find a 95% CI for \\(\\overline X\\), ignoring the finite population correction.\nAnother SRS of size \\(n\\) is to be taken from the current UK population. Body weight (kg) will again be recorded. A 95% CI for the population mean body weight will be formed. What is the smallest sample size (\\(n\\)) that would give a 95% CI with a width of less than 1 kg?\nA residential area has 5000 private houses. We wish to estimate the proportions of houses with:\n\nmore than three people living in them,\nmore than one car owned by the occupants of the house.\n\n\nThe estimators are required to have standard errors not exceeding 0.02 and 0.01 respectively. From other surveys it is thought that the proportions in (a) and (b) lie in the ranges \\((0.35,0.55)\\) and \\((0.10,0.20)\\) respectively. The two proportions are to be estimated from a single SRS. How large a sample is needed?\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\n\nWe have \\[s^2 = \\frac{1}{999}(6241154-1000(78.2)^2)=126.04\\] Ignoring the finite population correction, we have that a 95% CI for \\(\\overline{X}\\)is given by \\[\\begin{align}\n&\\overline{x}\\pm t_{n-1,0.025} \\sqrt{s^2/n}   \\\\\n&=78.2 \\pm 1.962 \\sqrt{126.04/1000} \\\\\n&=(77.5, 78.9) kg\n\\end{align}\\] 1.962 is the relevant \\(t_{999}\\) quantile (from R) but the degrees of freedom is so large that we could use the normal \\(z\\) quantile.\nWe can use the estimate of \\(S^2\\) from the previous sample. The population size is \\(N=68,900,000\\), the specified CI width is \\(d=1\\), the \\(z\\) quantile is \\(z_{1-\\alpha/2} = 1.96\\) and the population variance estimate is 126.04 kg\\(^2\\). Using \\[n \\geq \\frac{N}{1+N(d/(2Sz_{\\alpha/2}))^2}\\] gives \\(n \\geq 1936.7\\) so the smallest integer sample size is \\(n=1934\\).\nIf \\(P\\) is the population proportion, we require \\(n\\) such that \\[ \\sqrt{\\frac{1-f}{n}P(1-P)}~~~ \\leq ~~~0.02\\mbox{ and }0.01 \\] respectively in the two cases. The maximum of the standard error on the left hand side occurs when \\(P=0.5\\) in (a) and when \\(P=0.2\\) (the nearest believable value to \\(P=0.5\\)) in (b). This leads to the requirement that \\(n\\geq 556\\) in (a) and \\(n\\geq 1213\\) in (b). Hence we need \\(n\\geq 1213\\) to satisfy both requirements.",
    "crumbs": [
      "Part II: Sampling Theory",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Confidence intervals and sample sizes</span>"
    ]
  },
  {
    "objectID": "Sample sizes and confidence intervals.html#appendix-the-normal-approximation-for-confidence-intervals",
    "href": "Sample sizes and confidence intervals.html#appendix-the-normal-approximation-for-confidence-intervals",
    "title": "10  Confidence intervals and sample sizes",
    "section": "10.8 Appendix: the normal approximation for confidence intervals",
    "text": "10.8 Appendix: the normal approximation for confidence intervals\nConfidence intervals for population means are derived from the approximation \\[\n\\bar{x}\\sim N(\\bar{X},(1-f)S^2/n).\n\\] Here we investigate this with a simulation study, expanding on the example in Example 10.1.\nWe first make up a population of size 200, where the population distribution is clearly non-normal:\n\nset.seed(123)\nX &lt;- 400+round(rgamma(200, shape = 2, scale = 100), 1) \nhist(X, main = \"Histogram of population\")\n\n\n\n\n\n\n\n\nWe now generate 10,000 simple random samples of size 10. For each sample, we calculate the sample mean.\n\nsampleMeans &lt;- rep(0, 10000)\nfor(i in 1:10000){\n  # Use the sample() function to draw a SRS\n  # this randomly selects 10 members from the population (X),\n  # sampling without replacement\n  x &lt;- sample(X, size = 10, replace = FALSE)\n  \n  # Calculate the sample mean for the SRS\n  sampleMeans[i] &lt;- mean(x)\n}\n\nWe now compare the distribution of the sample mean with the approximate normal distribution given above (shown as the red line). Note that, because we have generated the population, we can determine the population mean and variance \\(\\bar{X}\\) and \\(S^2\\).\n\nhist(sampleMeans, prob = TRUE, main = \"Histogram of sample means\")\ncurve(dnorm(x, mean = mean(X), sd = sqrt((1-10/200)*var(X)/10)),\n      add = TRUE,\n      col = \"red\",\n      lwd = 2)\n\n\n\n\n\n\n\n\nWe can see that the histogram lines up fairly well (if not perfectly) with the normal distribution, even though the sample size is small (10) and the population distribution is skewed.",
    "crumbs": [
      "Part II: Sampling Theory",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Confidence intervals and sample sizes</span>"
    ]
  },
  {
    "objectID": "Stratified sampling.html",
    "href": "Stratified sampling.html",
    "title": "11  Stratified Sampling",
    "section": "",
    "text": "11.1 Notation\nThe requirement for simple random sampling is that all possible samples are equally likely to be drawn. In practice, this can be difficult to achieve, particularly when we are sampling people rather than, say, items in a warehouse. Some people are harder to reach and/or are less likely to participate in surveys than others. Additionally, there are often relevant groupings in a population, e.g. different regions of the country supporting different political parties. If a grouping of ‘similar’ individuals coincides with grouping of individuals that are harder to reach or less likely to take part in a survey, a sample may be biased.\nTwo strategies to deal with this are:\nWe refer to a group within a population as a stratum (plural: strata), for example, if the population is all adults in the UK, the strata could be the counties in the UK.\nSuppose there are \\(l\\) strata, with sizes \\(N_i, i=1,\\dots,l\\), so that \\(\\sum_1^lN_i=N\\). Members of \\(i\\)th stratum denoted by \\(X_{i,j}, j=1,\\dots,N_i\\).\nThe population mean of the \\(i\\)th stratum is \\(\\overline{X}_i\\) and its variance is \\[S_i^2=\\frac{1}{N_i-1} \\sum_{j=1}^{N_i}(X_{i,j}-\\overline{X}_i)^2.\\] Then the population mean is \\[\\overline{X}=\\frac{1}{N}\\sum_{i=1}^l\\sum_{j=1}^{N_i}X_{i,j}=\\sum_{i=1}^l\\frac{N_i}{N}\\overline{X}_i,\\] is a weighted mean of the stratum means \\(\\overline{X}_i\\), and \\[\nS^2 =\\frac{1}{N-1} \\sum_{i=1}^l\n\\sum_{j=1}^{N_i}(X_{i,j}-\\overline{X})^2\\]\n\\[= \\frac{1}{N-1} \\left\\{\\sum_{i=1}^l (N_i-1)S_i^2+\\sum_{i=1}^lN_i (\\overline{X}_i - \\overline{X})^2 \\right\\} \\tag{11.1}\\] where inside the curly brackets we have a decomposition of the total sum of squares of the population values about their mean into a pooled within-stratum sum of squares and a between-strata sum of squares, identical algebraically to that found in the one-way analysis of variance. We will use this result in Section 11.3.\nSuppose we draw a SRS of size \\(n_i\\) from the \\(i\\)th stratum, for \\(i=1,\\dots,l\\). Let \\(n=\\sum_1^l n_i\\) denote the size of the combined sample, and \\(f_i=n_i/N_i\\) the sampling fraction for stratum \\(i\\), \\(i=1,\\dots,l\\). Let \\(x_{ij}, j=1,\\dots,n_i\\) denote the observations from the SRS from stratum \\(i\\), and \\(\\overline{x}_i, s_i^2\\) the corresponding sample mean and variance. Initially we take \\(n\\) and the \\(n_i\\) to be fixed.",
    "crumbs": [
      "Part II: Sampling Theory",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Stratified Sampling</span>"
    ]
  },
  {
    "objectID": "Stratified sampling.html#estimation-of-overlinex",
    "href": "Stratified sampling.html#estimation-of-overlinex",
    "title": "11  Stratified Sampling",
    "section": "11.2 Estimation of \\(\\overline{X}\\)",
    "text": "11.2 Estimation of \\(\\overline{X}\\)\nThe overall sample mean \\(\\overline{x}^\\prime = \\sum_i\\sum_j x_{ij}/n = \\sum_i n_i \\overline{x}_i/n\\) has expectation \\[E(\\overline{x}^\\prime )=\\frac{1}{n} \\sum_1^l n_i \\overline{X}_i\\] evidently equal to \\(\\overline{X}\\) if \\(n_i = nN_i/N\\), and so \\(\\overline{x}^\\prime\\) in general is biased for \\(\\overline{X}\\), except if \\(f_i = \\mbox{constant} = n/N\\), the so-called proportional allocation of sampling effort (the size of the SRS from each stratum is proportional to the stratum size).\nThis suggests a better unbiased estimate, the stratified estimate using proportional allocation: \\[\\overline{x}_{st} = \\frac{1}{N} \\sum_1^l N_i \\overline{x}_i. \\tag{11.2}\\]\nIt can be proved that whatever the \\(n_i,i=1,\\dots,l\\), under SSRS \\(\\overline{x}_{st}\\) is the best linear unbiased estimator (BLUE) of \\(\\overline{X}\\), and it has variance \\[\n\\mbox{var}(\\overline{x}_{st}) = \\sum_1^l\\left(\\frac{N_i}{N}\\right)^2\\frac{1-f_i}{n_i} S_i^2.\n\\tag{11.3}\\] Deriving the variance is left as a task at the end of this chapter. The proof of BLUE is given in the appendix.\nUsually the stratum population variances \\(S_i^2\\) are unknown, and so they are estimated by the corresponding stratum sample variances \\(s_i^2\\), which by SRS are unbiased. Confidence intervals for \\(\\overline{X}\\) may then be based on a normal approximation: an approximate 95% confidence interval is\n\\[\n\\overline{x}_{st}\\pm 1.96 \\sqrt{\\widehat{var}(\\overline{x}_{st})},\n\\] where \\(\\widehat{var}(\\overline{x}_{st})\\) is obtained by estimating each \\(S_i^2\\) by the corresponding sample variance \\(s_i^2\\).",
    "crumbs": [
      "Part II: Sampling Theory",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Stratified Sampling</span>"
    ]
  },
  {
    "objectID": "Stratified sampling.html#sec-stratvsrs",
    "href": "Stratified sampling.html#sec-stratvsrs",
    "title": "11  Stratified Sampling",
    "section": "11.3 Stratification vs SRS",
    "text": "11.3 Stratification vs SRS\nWhen is \\(\\overline{x}_{st}\\) better than \\(\\overline{x}\\) from SRS? Both are unbiased, and so we compare variances. A proof is given in the appendix that \\(\\mbox{var}(\\overline{x})\\geq\\mbox{var}(\\overline{x}_{st})\\) requires \\[\n\\sum N_i(\\overline{X}_i - \\overline{X})^2 \\geq\\sum \\left(1-\\frac{N_i}{N}\\right) S_i^2.\n\\tag{11.4}\\]\nThe term on the left hand side is the between-strata sum of squares, and the one on the right hand side is a measure of the within-stratum variance. Thus \\(\\overline{x}_{st}\\) will have smaller variance than \\(\\overline{x}\\), i.e. SSRS will be preferable to SRS, when the strata are homogeneous (small within-stratum variance) but well-separated (large difference between stratum means).",
    "crumbs": [
      "Part II: Sampling Theory",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Stratified Sampling</span>"
    ]
  },
  {
    "objectID": "Stratified sampling.html#choice-of-sample-sizes-n-n_1dotsn_l",
    "href": "Stratified sampling.html#choice-of-sample-sizes-n-n_1dotsn_l",
    "title": "11  Stratified Sampling",
    "section": "11.4 Choice of Sample Sizes \\(n, n_1,\\dots,n_l\\)",
    "text": "11.4 Choice of Sample Sizes \\(n, n_1,\\dots,n_l\\)\nThis is more complicated, as we have to consider both the total sample size \\(n\\), and sample sizes within each strata. Recall that \\[\n\\mbox{var}(\\overline{x}_{st}) = \\left(\\frac{N_1}{N}\\right)^2\\frac{1-f_1}{n_1} S_1^2+\\ldots +\\left(\\frac{N_l}{N}\\right)^2\\frac{1-f_l}{n_l} S_l^2.\n\\] We see that there is a contribution to \\(\\mbox{var}(\\overline{x}_{st})\\) from each strata, which depends on\n\nthe strata sample size \\(n_i\\);\nthe strata population size \\(N_i\\);\nthe strata population variance \\(S_i^2\\).\n\nIn the appendix, methods are given for choosing sample sizes, but these depend on knowing all the strata population variances. A simpler approach is to\n\nchoose the overall sample size \\(n\\) as if we were taking a simple random sample, as in Section 10.3;\nchoose each strata sample size based on its size relative to the population: set \\[\nn_i = n \\frac{N_i}{N}.\n\\]\n\nAs before, the larger the samples within strata, the more precise the estimation. However there is usually a cost associated with sampling that inhibits use of very large samples. The best values will depend therefore on the detailed costs of sampling in relation to the value of precision of estimation. The following captures the main features of many cases, at least approximately.",
    "crumbs": [
      "Part II: Sampling Theory",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Stratified Sampling</span>"
    ]
  },
  {
    "objectID": "Stratified sampling.html#multilevel-regression-and-post-stratification-mrp",
    "href": "Stratified sampling.html#multilevel-regression-and-post-stratification-mrp",
    "title": "11  Stratified Sampling",
    "section": "11.5 Multilevel regression and post-stratification (MRP)",
    "text": "11.5 Multilevel regression and post-stratification (MRP)\nMRP is a technique used in large surveys such as UK opinion polls and the Covid infection survey. The two main purposes of MRP are to adjust for a sample that may not appropriately represent the population, and to provide estimates for individual strata, even if these have not been sampled directly. For example, in political opinion polls, MRP is used to give voting intentions within each constituency, as well as for the UK as a whole.\nAn overview of the method is as follows\n\nWe first draw a sample from the population. Whilst we may attempt to make the sample as representative as we can, the sample may under-represent some groups, e.g. because of ease of access or willingness to participate.\nIn addition to obtaining the quantity of interest for each sampled member, we also record covariates (e.g. age, home location, employment status) for the sampled member\nStratum are defined according to covariates: a particular combination of covariate values could make up a strata.\nWe use regression (typically a linear model or a logistic regression model) to estimate the relationship between the main quantity of interest and the other covariates\nThe regression model involves random effects/multilevel modelling/hierarchical modelling to help estimate quantities for particular strata, even if the sample size within that strata is small\n\n\n11.5.1 Example\nTo illustrate MRP, we will use simplified example with a made-up dataset. We suppose an opinion poll has been conducted to estimate support for the UK rejoining the European Union. We suppose a sample of 500 adults has been taken, and one covariate has been measured:\n\nregion, categorised as “South England”, “North England”, “Wales”, “Scotland” or “Northern Ireland”.\n\nWe define 5 stratum as these regions. The sizes \\(N_i\\) are given below, counted in millions:\n\nlibrary(tidyverse)\ncensus &lt;- read_csv(\"data/census.csv\")\ncensus\n\n# A tibble: 5 × 2\n  region             N_i\n  &lt;chr&gt;            &lt;dbl&gt;\n1 South England     25  \n2 North England     15  \n3 Wales              3  \n4 Scotland           5.5\n5 Northern Ireland   1.9\n\n\nSurvey data for the eight participants (out of 500) are shown below.\n\nsurvey &lt;- read_csv(\"data/survey.csv\")\nhead(survey, n = 8)\n\n# A tibble: 8 × 2\n  region        rejoin\n  &lt;chr&gt;          &lt;dbl&gt;\n1 North England      0\n2 North England      0\n3 Scotland           1\n4 Wales              0\n5 South England      0\n6 Scotland           0\n7 South England      1\n8 Scotland           1\n\n\nEach row is the observation for one participant and in column rejoin, a 1 in indicates that the participant is in favour of rejoining; 0 indicates they are not in favour.\nTo get the sample proportion in favour of rejoining we do:\n\nmean(survey$rejoin)\n\n[1] 0.646\n\n\nso about 65% in the survey are in favour of rejoining.\nWe now look at sample sizes for the five strata, and proportions in favour of rejoining within each strata.\n\nsurvey_proportions &lt;- survey %&gt;%\n  group_by(region) %&gt;%\n  summarise(\n    prop_rejoin = mean(rejoin),\n    n_i = n(), # Optional: includes the number of respondents per cell\n    .groups = \"drop\"\n  )\n\n# Join with the census data frame so we can compare strata population sizes\ncombined &lt;- inner_join(census, survey_proportions,\n                       by = \"region\")\n\n# View the result and arrange the rows in decreasing order of strata\n# population size\ncombined %&gt;% arrange(desc(N_i))\n\n# A tibble: 5 × 4\n  region             N_i prop_rejoin   n_i\n  &lt;chr&gt;            &lt;dbl&gt;       &lt;dbl&gt; &lt;int&gt;\n1 South England     25         0.440    84\n2 North England     15         0.431    51\n3 Scotland           5.5       0.733   300\n4 Wales              3         0.651    43\n5 Northern Ireland   1.9       0.727    22\n\n\nHere we can see that\n\nour sample has a high proportion (300/500) from Scotland\nin the sample, this stratum has the highest proportion (73%) in favour of rejoining\nin the population, this is one of the smaller strata.\n\nHence the estimate of 65% in favour of rejoining is likely to be an overestimate.\nWe could estimate the population proportion simply by using Equation 11.2:\n\\[\n\\bar{x}_{st} = \\sum_{i=1}^6 \\frac{N_i}{N}\\bar{x}_i\n\\] In R we do\n\nsum(combined$N_i / sum(combined$N_i) * combined$prop_rejoin)\n\n[1] 0.493078\n\n\nwhich gives a lower estimate of 49%.\n\n11.5.1.1 Using a multi-level regression model\nAn alternative to using Equation 11.2 is to model the relationship between the covariates that define the strata (region in this example) and the quantity of interest. This may be preferable when we have multiple covariates of interest; we will discuss this shortly.\nWe write that participant \\(i\\) is in region \\(j_i\\in\\{1,2,3,4,5\\}\\). We observe a binary outcome \\(\\text{rejoin}_i\\in\\{0,1\\}\\) for each participant. We use a logistic regression model with random effects for region.\n\\[\n\\text{rejoin}_i \\sim \\text{Bernoulli}(\\pi_i),\n\\] with \\[\\log \\left(\\frac{\\pi_i}{1-\\pi_i}\\right) = \\beta_0 + \\gamma_{j_i}\n\\tag{11.5}\\]\nIf you haven’t seen a logistic regression model before, the idea behind Equation 11.5 is that if we tried to specify a model such as \\(E(\\text{rejoin}_i) = \\pi_i=\\beta_0 + \\gamma_{j_i}\\), we have the problem that \\(\\pi_i\\) is bounded between 0 and 1, which then makes estimation of all the parameters on the right hand side awkward. The “logistic” transformation \\(\\log(\\pi_i/(1-\\pi_i))\\) in Equation 11.5 gives a quantity on the left hand side that is unbounded.\nThe parameters \\(\\gamma_1,\\ldots,\\gamma_5\\) describing the effects of region are modelled as random variables, not constants:\n\\[\n\\gamma_j \\sim N(0, \\sigma^2), \\quad \\text{for } j = 1, \\ldots,5.\n\\tag{11.6}\\]\nThis idea of modelling parameters as random variables goes by various different names, which all broadly mean the same thing:\n\na multi-level model\na random effects model\na (Bayesian) hierarchical model\n\n\n\n\n\n\n\nImportantRandom effects/multi-level/hierarchical models\n\n\n\nWe are not going to study these models in much detail; they are covered more in other modules. In brief:\n\nthe name “random effects” comes from treating the effect parameters (region effects in this example) as random variables.\nthe names “multi-level” and “hierarchical” both refer to the model structure. At the top level, the response variable (\\(\\text{rejoin}_i\\)) is expressed as a random variable, dependent on a particular parameter (\\(\\pi_i\\)). At the next level down, this parameter is also expressed as a random variable: a function of random variable \\(\\gamma_{j_i}\\) in this case.\nthe assumption that \\(\\gamma_1,\\ldots,\\gamma_5\\) are normally distributed with mean 0 pulls estimates of these parameters towards 0: this is referred to as “shrinkage”\nthese models can enable “borrowing strength” between strata: as we get data from some strata and learn about \\(\\beta_0\\) and \\(\\sigma^2\\), this gives information about what to expect in other strata. This can be helpful if a particular strata has a small sample size.\n\n\n\nWe can fit the model described above using the lme4 package\n\nlibrary(lme4)\nmrp &lt;- glmer(rejoin ~ (1 | region), \n               data = survey, \n               family = binomial(link = \"logit\"))\n\nThe syntax (1 | region) specifies random effects as written in Equation 11.6. Using the model fit, we now obtain the estimated proportion for each strata:\n\ncombined$estimate &lt;- predict(mrp,\n                             newdata = combined,\n                             type = \"response\")\n\nIf we inspect these estimates, they are all ‘shrunk’ slightly away from the observed proportion in that strata, towards a central value:\n\ncombined\n\n# A tibble: 5 × 5\n  region             N_i prop_rejoin   n_i estimate\n  &lt;chr&gt;            &lt;dbl&gt;       &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt;\n1 South England     25         0.440    84    0.464\n2 North England     15         0.431    51    0.469\n3 Wales              3         0.651    43    0.637\n4 Scotland           5.5       0.733   300    0.726\n5 Northern Ireland   1.9       0.727    22    0.674\n\n\nTo get the overall estimate from the MRP model, we weight the estimates in each strata by the strata size:\n\nsum(combined$N_i / sum(combined$N_i) * combined$estimate)\n\n[1] 0.5126898\n\n\nThis is slightly above the estimate from Equation 11.2, but still lower than the raw proportion from the survey\n\n\n\n\n\n\nImportantWhy bother with all this?\n\n\n\nIn the example, we had a single covariate: region, resulting in five strata only. But surveys often record multiple covariates, e.g. age group and employment status in addition to region. Each combination of covariate values would define a single strata: e.g. one strata might be all 30-40 year-olds who are currently employed and live in Wales. With many possible combinations there will be a large number of strata, but the multilevel logistic regression model can easily be extended to estimate the effects of multiple covariates, and can better handle problems with small sample sizes within strata through the effect of “borrowing strength” between strata.",
    "crumbs": [
      "Part II: Sampling Theory",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Stratified Sampling</span>"
    ]
  },
  {
    "objectID": "Stratified sampling.html#tasks",
    "href": "Stratified sampling.html#tasks",
    "title": "11  Stratified Sampling",
    "section": "11.6 Tasks",
    "text": "11.6 Tasks\n\nDerive the variance of \\(\\bar{x}_{st}\\)\nAn organisation wants to estimate the mean CO\\(_2\\) transport emissions of its staff. The organisation has 2000 employees. Three transport mode strata have been identified: car, public transport and walk/cycle. The strata sizes are 200 (car), 500 (public transport) and 1300 (walk/cycle). A stratified SRS of 100 is to be taken. Using proportional allocation, the sample sizes are 10, 25 and 65 respectively. The results of the sample are as follows:\n\n\n\n\n\nSample mean (tonnes)\nSample variance (tonnes\\(^2\\))\n\n\ncar\n0.82\n0.35\n\n\npublic transport\n0.28\n0.26\n\n\nwalk/cycle\n0.13\n0.02\n\n\n\nCalculate a 95% CI for the mean CO\\(_2\\) transport emissions of its staff based on these results. You can ignore the finite population correction.\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\n\nWe have \\[\nVar( \\bar{x}_{st}) =Var\\left(\\frac{1}{N}\\sum_1^l N_i\\bar{x}_i\\right)= \\frac{1}{N^2}\\sum_1^lVar(N_i\\bar{x}_i) = \\sum_1^l\\left(\\frac{N_i}{N}\\right)^2\\frac{1 - f_i}{n_i}S_i^2,\n\\] as \\(\\bar{x_i}\\) is the sample mean obtained from an SRS of size \\(n_i\\) drawn from a population of size \\(N_i\\) with population variance \\(S_i^2\\).\nWe have \\[\\overline{x}_{st}=0.1(0.82)+0.25(0.28)+0.65(0.13)=0.24 ~\\text{tonnes of CO}_2.\\] Using proportional allocation we have \\(f_i=n/N=100/2000=0.05\\) which is small enough to ignore. This gives the estimator variance as: \\[\\text{var}(\\overline{x}_{st}) = (0.1)^2\\frac{0.35}{10}+(0.25)^2\\frac{0.26}{25}+(0.65)^2\\frac{0.02}{65}=0.0011\\] The 95% CI is then \\[\\overline{x}_{st} \\pm 1.96 \\sqrt{\\text{var}(\\overline{x}_{st})}=0.24 \\pm 2\\sqrt{0.0011}=(0.17, 0.31)\\]",
    "crumbs": [
      "Part II: Sampling Theory",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Stratified Sampling</span>"
    ]
  },
  {
    "objectID": "Stratified sampling.html#appendix-proofs",
    "href": "Stratified sampling.html#appendix-proofs",
    "title": "11  Stratified Sampling",
    "section": "11.7 Appendix: proofs",
    "text": "11.7 Appendix: proofs\n\n11.7.1 Proof that \\(\\bar{x}_{st}\\) is the best linear unbiased estimator\nConsider a general linear estimator of \\(\\overline{X}\\) \\[ t = \\sum_{i,j} c_{ij}x_{ij} \\] where the \\(c_{ij}\\) are constants.\nUnbiasedness implies \\[\\begin{align}\nE(t) & =  \\sum_{ij} c_{ij} \\overline{X}_i \\\\\n& =  \\sum_{i=1}^l \\left(\\sum_{j=1}^{n_i} c_{ij}\\right)\n\\overline{X}_{i} \\\\ & =  \\overline{X} = \\sum_{i=1}^l\n\\left(\\frac{N_i}{N}\\right)\n\\overline{X}_i\\mbox{ whatever}\\overline{X}_i \\\\\n\\mbox{and so }\\sum_{j=1}^{n_i} c_{ij} & =  \\frac{N_i}{N}\n\\mbox{ for each }i.\n\\end{align}\\] Since the SRS’s from different strata are independent,\n\\[ Var(t) =\\sum_{i=1}^lVar\\left(\\sum_{j=1}^{n_i}\nc_{ij}x_{ij}\\right)\\] and we minimise this by minimising it for each \\(i\\) separately. But within each stratum this is essentially the same problem as for SRS , and the solution is that the \\(c_{ij}\\) should be equal to each other, which means, bearing in mind the above constraint, that \\[c_{ij} = \\frac{N_i}{Nn_i}\\mbox{ for }j=1,\\ldots ,n_i\\] and so in SSRS the BLUE of \\(\\overline X\\) is \\[\\overline{x}_{st} = \\sum_{i=1}^l \\frac{N_i}{N} \\overline{x}_i. \\]\n\n\n11.7.2 Stratification vs SRS\nWe have \\[ Var(\\overline{x}_{st(p)}) = \\sum_1^l \\left( \\frac{N_i}{N}\n\\right)^2 \\left(1-\\frac{n}{N}\\right)\\frac{S_i^2}{n_i} =\n\\frac{1-f}{nN} \\sum_1^l N_iS_i^2 \\] whereas \\[Var(\\overline{x})= \\frac{1-f}{n}S^2 = \\frac{1-f}{n(N-1)} \\left\\{\n\\sum_{i=1}^l (N_i-1)S_i^2 + \\sum_{i=1}^l N_i(\\overline{X}_i -\n\\overline{X})^2 \\right\\} . \\]\nThus \\[Var(\\overline{x})-Var(\\overline{x}_{st(p)})=\\frac{1-f}{n(N-1)}\n\\left\\{-\\sum_{i=1}^l\\frac{N-N_i}{N}S_i^2 +\n\\sum_{i=1}^lN_i(\\overline{X}_i - \\overline{X})^2 \\right\\}\n\\] which is non-negative iff \\[\\sum N_i(\\overline{X}_i - \\overline{X})^2 \\geq \\sum\n\\left(1-\\frac{N_i}{N}\\right) S_i^2. \\]",
    "crumbs": [
      "Part II: Sampling Theory",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Stratified Sampling</span>"
    ]
  },
  {
    "objectID": "Stratified sampling.html#appendix-sample-size-choices",
    "href": "Stratified sampling.html#appendix-sample-size-choices",
    "title": "11  Stratified Sampling",
    "section": "11.8 Appendix: sample size choices",
    "text": "11.8 Appendix: sample size choices\nHere, we give some methods for choosing sample sizes when stratum population variances are known.\nSuppose that the cost of the survey consists of a basic overhead cost, \\(c_0\\) say, and a cost associated with each unit measured. This unit measurement cost could differ between strata. We denote its value in stratum \\(i\\) by \\(c_i\\). Then the overall cost, \\(C\\) say, of the survey is of the form \\[C = c_0+\\sum_{i=1}^l c_in_i.\\]\nSuppose we wish to choose the \\(n_i\\) to minimise var \\(\\overline{x}_{st}\\) for a specified total cost \\(C\\). (We might equally wish to minimise cost for a specified variance: the argument in that case is similar.)\n\n11.8.1 Minimising Variance Within a Fixed Cost.\nWe want to choose the \\(n_i\\) to minimize \\[\\begin{align}\n\\mbox{var}(\\overline{x}_{st}) & =  \\sum_1^l\\left(\\frac{N_i}{N}\\right)^2\\left(1-\\frac{n_i}{N_i}\\right)\\frac{S_i^2}{n_i}\\\\\n& = \\sum_1^l \\left(\\frac{N_i}{N}\\right)^2\\frac{S_i^2}{n_i}-\\sum_1^l \\left(\\frac{N_i}{N}\\right)^2\\frac{S_i^2}{N_i}\n\\end{align}\\] subject to \\[c_0+\\sum_1^l c_in_i = C.\\] Using Lagrange multipliers, we find that the optimal size of sample from the \\(i\\)th stratum is \\[n_i \\propto \\frac{N_iS_i}{\\sqrt{c_i}}\\mbox{~~~~for~}i=1,\\dots,l.\\] As one might expect, we should take a larger sample in a given stratum if\n\nthe stratum is larger,\nthe stratum is more variable internally,\nsampling is cheaper in the stratum.\n\nPutting the formula for \\(n_i\\) in the cost constraint, we can deduce the constant of proportionality, and the optimal total number of observations for cost \\(C\\): \\[\\begin{align}\nn_i &=\\frac{(C-c_0)\\frac{N_iS_i}{\\sqrt{c_i}}}{\\sum_1^lN_iS_i\\sqrt{c_i}},\\\\\nn &= \\frac{(C-c_0)\\sum_1^l \\frac{N_iS_i}{\\sqrt{c_i}}}{\\sum_1^lN_iS_i\\sqrt{c_i}}.\n\\end{align}\\] (In practice we would round \\(n\\) and \\(n_i\\) to the nearest integers.) To use this result, we need to know the sizes \\(N_i\\) of the strata and the stratum variances \\(S_i^2\\). The latter are usually unknown and would have to be estimated from a pilot survey or from knowledge of another similar survey.\n\n\n11.8.2 Special Case: \\(c_i\\) constant.\nIn the case when measurement of a unit costs the same whichever stratum it comes from, so that the overall cost is \\(C=c_0+cn\\), then \\[\\begin{align}\nn & =  \\frac{C-c_0}{c} \\\\\n\\mbox{and}\\; n_i & =  \\frac{nN_iS_i}{\\sum_1^l N_iS_i}\\; \\mbox{for} i=1,\\dots,l.\n\\end{align}\\] This is called the Neyman allocation. For the Neyman allocation, we have \\[\\mbox{var~}\\overline{x}_{st} =  \\frac{1}{N^2} \\frac{ (\\sum N_iS_i)^2}{n} - \\frac{ \\sum N_iS_i^2 }{N^2}.\\]",
    "crumbs": [
      "Part II: Sampling Theory",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Stratified Sampling</span>"
    ]
  },
  {
    "objectID": "Population size.html",
    "href": "Population size.html",
    "title": "12  Estimating a Population Size",
    "section": "",
    "text": "12.1 A case study\nWe can use sampling methods to estimate the size of a population (\\(N\\) in the notation used so far). As a case study, we will consider an analysis conducted at the Home Office on modern slavery in the UK. In 2013, the National Crime Agency (NCA) reported that 2744 potential victims of human trafficking had been encountered. Not all victims would be known to the authorities; can the total number of (potential) victims be estimated?\n[@Silverman] describes the NCA figure as coming from different organisations, which he summarises as five lists:\nSome names appear on more than one list, for example, 54 victims are named on the LA list only, 463 victims are named on the NG list only, and 15 victims are named on both the LA and NG lists. The total number of names across the five lists is 2744, i.e., 2744 victims are named on at least one of the five lists. To estimate the total number of victims, we need to estimate how many victims there are who are not named on any of the lists.\nWe can obtain an estimate using the technique of multiple systems estimation. This is extension of the technique of capture-recapture sampling, which has its origins in environmental applications, in particular in estimating in the abundance of a particular species of animal.",
    "crumbs": [
      "Part II: Sampling Theory",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Estimating a Population Size</span>"
    ]
  },
  {
    "objectID": "Population size.html#a-case-study",
    "href": "Population size.html#a-case-study",
    "title": "12  Estimating a Population Size",
    "section": "",
    "text": "Local Authority (LA);\nNon-governmental organisation (NG);\nPolice force and National Crime Agency (PF);\nGovernment organisation (GO);\nGeneral public (GP).",
    "crumbs": [
      "Part II: Sampling Theory",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Estimating a Population Size</span>"
    ]
  },
  {
    "objectID": "Population size.html#capture-recapture-sampling",
    "href": "Population size.html#capture-recapture-sampling",
    "title": "12  Estimating a Population Size",
    "section": "12.2 Capture-recapture sampling",
    "text": "12.2 Capture-recapture sampling\nThis method for estimating a population size is as follows.\n\nDraw (‘capture’) a random sample of size \\(n\\) (without replacement) from the population. Each member of the sample is tagged (e.g. a ring may be placed on the leg of a bird).\nRelease all members of the sample back into the population.\nDraw a second random sample of size \\(m\\) (without replacement) from the population. Observe the number of tagged individuals, denoted by \\(r\\) in the second sample.\n\nA simple estimator of the population size \\(N\\), known as the Peterson estimator (or “Lincoln index”) is \\[\\widehat{N}_p = \\frac{nm}{r}.\\] This is based on the assumption that \\(n/N\\) should be similar to \\(r/m\\). If \\(N\\) is large relative to \\(m\\), we can suppose that \\[r\\sim Bin\\left(m, \\frac{n}{N}\\right)\\] Based on asymptotic properties of maximum likelihood estimators, we can derive an approximate variance for \\(\\widehat{N_p}\\) as \\[\\widehat{Var}(\\hat{N}_p)= \\frac{mn^2(m-r)}{r^3}.\\] (The proof of this result is outside the scope of this module, but for reference, the proof uses the “delta method” approximation \\(Var(g(r))\\simeq(\\frac{d}{dr}g(r))^2Var(r)\\), with \\(g(r)=1/r\\), and also uses the observed value of \\(r\\) to estimate \\(Var(r)\\).) Clearly, this will cause difficulties if we observe \\(r=0\\). Additionally, \\(\\widehat{N}_p\\) is biased, with \\[E(\\widehat{N_p})\\simeq N\\left(1+\\frac{N-n}{mn}\\right).\\] An alternative is the Chapman estimator: \\[\\widehat{N_c} = \\frac{(n+1)(m+1)}{r+1} -1\\] which can be shown to be unbiased for \\(n+m&gt;N\\) and has estimated variance \\[\\widehat{Var}(\\widehat{N}_c)= \\frac{(n+1)(m+1)(n-r)(m-r)}{(r+1)^2(r+2)}.\\] There are two assumptions with this method.\n\nThe population size \\(N\\) does not change between the first and second samples.\nAll members of the population are equally likely to be captured; there are no ‘trap shy’ or ‘trap happy’ members.\n\n\n12.2.1 The case study as a capture-recapture experiment\nReturning to the case study, suppose there were just two lists: LA and NG. From the data in [@Silverman], we have\n\n94 victims were named in total on the LA list;\n567 victims were named in total on the NG list;\n18 victims were named on both lists.\n\nThe assumptions required almost certainly won’t hold, but for illustration, we could interpret this as a capture-recapture experiment:\n\n94 members of the population were ‘tagged’;\na second sample of 567 members of the population (of victims) was drawn;\n18 members of the second sample were ‘observed to be tagged’.\n\nWe then obtain \\[\\begin{align}\n\\widehat{N}_p &= 2961\\\\\n\\widehat{N}_c &= 2839, \\\\\n\\sqrt{\\widehat{Var}(\\widehat{N}_p)} &= 627\\\\\n\\sqrt{\\widehat{Var}(\\widehat{N}_c)} &= 558.\n\\end{align}\\] The estimates do at least exceed the number of known victims that only appeared on the other three lists, but otherwise aren’t very convincing. It’s not plausible that all members of the population are equally likely to be ‘captured’, and names may not appear on the two lists independently.",
    "crumbs": [
      "Part II: Sampling Theory",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Estimating a Population Size</span>"
    ]
  },
  {
    "objectID": "Population size.html#log-linear-models-for-capture-recapture-data",
    "href": "Population size.html#log-linear-models-for-capture-recapture-data",
    "title": "12  Estimating a Population Size",
    "section": "12.3 Log-linear models for capture-recapture data",
    "text": "12.3 Log-linear models for capture-recapture data\nAnother way to think about capture-recapture data is as a contingency table with a missing observation. Continuing the example with two lists only, we have\n\n\n\n\n\n\nLA\n\n\n\n\n\nno\nyes\n\n\nNG\nno\n?\n76\n\n\n\nyes\n549\n18\n\n\n\n\nso that the estimate of the population size would be \\(76+549+18\\) plus the estimate of the number of victims not appearing on either list. We will now consider how to model these data, in such a way that the missing count can be estimated. In particular, we are going to use log-linear models, a particular case of generalised linear model. This may appear unnecessarily complex, but there are benefits:\n\nit is relatively straightforward to extend the approach to multiple lists (i.e. 5-way contingency tables);\nwith more than two lists, we can incorporate dependencies between the lists;\nwe can fit the models in R and get standard errors for the parameter estimates.\n\nWe think of the counts in each cell as independent Poisson random variables, with expectations\n\n\n\n\n\n\nLA (\\(j\\))\n\n\n\n\n\nno\nyes\n\n\nNG (\\(i\\))\nno\n\\(\\mu_{11}\\)\n\\(\\mu_{12}\\)\n\n\n\nyes\n\\(\\mu_{21}\\)\n\\(\\mu_{22}\\)\n\n\n\n\nThe interpretation of this is a little difficult, in particular the distinction between observed and expected counts, but one can imagine that a repeat of the data collection process would produce different counts, with \\(\\mu_{ij}\\) representing a mean count, assuming no trends over time. The objective is therefore to estimate \\(\\mu_{11}\\). With only two lists, we will have to assume independence between NG and LA, which we express by saying that the two ratios \\(\\mu_{12}/\\mu_{11}\\) and \\(\\mu_{22}/\\mu_{21}\\) should be equal. This gives us a way to link \\(\\mu_{11}\\) to the three observed counts.\nWe parameterise \\(\\mu_{ij}\\) as follows: \\[\n\\log \\mu_{ij} = \\phi + \\alpha_i + \\beta_j,\n\\tag{12.1}\\] with \\(\\alpha_1=\\beta_1=0\\), so the total number of parameters is three (the most we could have with only three independent observations). We call this a log-linear model, because the log of the expectation is a linear function of the parameters.\nMaximum likelihood can be used to estimate \\(\\mu\\), \\(\\alpha_2\\) and \\(\\beta_2\\). The full-likelihood is \\[L=\\frac{e^{-\\mu_{12}}\\mu_{12}^{76}}{76!}\\times\\frac{e^{-\\mu_{21}}\\mu_{21}^{549}}{549!}\\times\\frac{e^{-\\mu_{22}}\\mu_{22}^{18}}{18!},\\] with \\(\\mu_{ij}=e^{\\phi + \\alpha_i + \\beta_j}\\). Although we could do the maximisation analytically, we will set the data up in R, and get R to do the maximisation for us. Setting up the data in R:\n\ntwo.list &lt;- data.frame(count = c(76, 549, 18),\n                       LA = as.factor(c(\"yes\", \"no\", \"yes\")),\n                       NG = as.factor(c(\"no\", \"yes\", \"yes\")))\ntwo.list\n\n  count  LA  NG\n1    76 yes  no\n2   549  no yes\n3    18 yes yes\n\nglm1 &lt;- glm(count ~ LA + NG, family = poisson(log), data = two.list)\nsummary(glm1)\n\n\nCall:\nglm(formula = count ~ LA + NG, family = poisson(log), data = two.list)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   7.7485     0.2656  29.175  &lt; 2e-16 ***\nLAyes        -3.4177     0.2395 -14.268  &lt; 2e-16 ***\nNGyes        -1.4404     0.2621  -5.495 3.91e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 7.8597e+02  on 2  degrees of freedom\nResidual deviance: 7.5939e-14  on 0  degrees of freedom\nAIC: 25.055\n\nNumber of Fisher Scoring iterations: 3\n\n\nThe maximum likelihood estimates are \\(\\hat{\\phi}=7.75\\), \\(\\hat{\\alpha}_2=-1.44\\) and \\(\\hat{\\beta}_2=-3.41\\). The fitted value in the missing cell is \\[e^{\\hat{\\phi}} = 2318,\\] and adding on the other cell counts give \\(\\hat{N}=2318 + 76 + 549 + 18 = 2961\\), the same as the Peterson estimator. To get an approximate 95% confidence interval, we take the estimated standard error of \\(\\hat{\\phi}\\) which is reported as 0.2656, and do \\[\\left(76 + 549 + 18 + e^{\\hat{\\phi} -2\\times 0.2656},  76 + 549 + 18 + e^{\\hat{\\phi} +2\\times 0.2656}\\right),\\] which gives (2005, 4585). (Technically, we’ve only considered an interval for the expectation of the cell count, not the actual count. But as we are assuming a Poisson distribution, the standard deviation is the square root of the mean, and so random variation about the mean is relatively small.)\n\n12.3.1 Interaction terms\nIt is unlikely that names would appear on the two lists independently. Some agencies may share names, and others may be reluctant to do so. For example, a non-governmental organisation may offer complete anonymity to potential victims. If this were the case, we wouldn’t expect the ratios \\(\\mu_{12}/\\mu_{11}\\) and \\(\\mu_{22}/\\mu_{21}\\) to be equal; \\(\\mu_{22}\\) may be relatively small, for example. Ideally, an appropriate model would be \\[\\log \\mu_{ij} = \\phi + \\alpha_i + \\beta_j+(\\alpha\\beta)_{ij},\\] with the constraints \\((\\alpha\\beta)_{i1}=(\\alpha\\beta)_{1j}=0\\), so that the interaction parameter is \\((\\alpha_\\beta)_{22}\\). Again, with only three observations, we can’t fit a model with four parameters, but such interaction terms can be included once we introduce more lists.",
    "crumbs": [
      "Part II: Sampling Theory",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Estimating a Population Size</span>"
    ]
  },
  {
    "objectID": "Population size.html#multiple-systems-estimation",
    "href": "Population size.html#multiple-systems-estimation",
    "title": "12  Estimating a Population Size",
    "section": "12.4 Multiple systems estimation",
    "text": "12.4 Multiple systems estimation\nWith more lists, we may have better ‘coverage’ of the population (different organisations may have different means of identifying victims), and we can now account for dependencies between the lists. For simplicity, we now consider three lists: LA, NG and PF. From the data in [@Silverman], we have\n\n\n\n\nPF = no\n\nLA\n\n\n\n\n\nno\nyes\n\n\nNG\nno\n?\n57\n\n\n\nyes\n483\n16\n\n\n\n\n\n\nPF = yes\n\nLA\n\n\n\n\n\nno\nyes\n\n\nNG\nno\n1082\n19\n\n\n\nyes\n66\n2\n\n\n\n\nWith more than two lists, we don’t have the simple equivalence with capture-recapture sampling, but the log-linear modelling framework can still be used. (Note that the more complex setting with more than two lists is referred to as multiple systems estimation). Again, we suppose each observed count is an independent Poisson random variable, with expected value \\(\\mu_{ijk}\\) given by \\[\\log \\mu_{ijk} = \\phi + \\alpha_i + \\beta_j + \\gamma_k + (\\alpha\\beta)_{ij} + (\\alpha\\gamma)_{ik}+(\\beta\\gamma)_{jk},\\] with \\(i,j,k\\) corresponding to the PF, NG and LA lists respectively, each taking values 1 or 2, corresponding to a ‘no’ or ‘yes’ respectively. We apply corner point constraints: all parameters are set to 0 except \\(\\phi,\\alpha_2,\\beta_2,\\gamma_2,(\\alpha\\beta)_{22},(\\alpha\\gamma)_{22},(\\beta\\gamma)_{22}\\). This gives seven free parameters, which is the most we can fit, as we have seven observations.\nSetting up the data in R (now with a missing value for \\(i=j=k=1\\) to make it easier to input the data into R using the rep function):\n\nthree.list &lt;- data.frame(count = c(NA, 57, 483, 16, 1082, 19, 66, 2),\n                         LA = as.factor(rep(c(\"no\", \"yes\"),2)),\n                         NG = as.factor(rep(c(\"no\", \"no\", \"yes\",\"yes\"), 2)),\n                         PF = as.factor(rep(c(\"no\", \"yes\"), each = 4)))\nthree.list\n\n  count  LA  NG  PF\n1    NA  no  no  no\n2    57 yes  no  no\n3   483  no yes  no\n4    16 yes yes  no\n5  1082  no  no yes\n6    19 yes  no yes\n7    66  no yes yes\n8     2 yes yes yes\n\n\nFitting the model:\n\nglm2 &lt;- glm(count ~ LA*NG + NG*PF + LA*PF, family = poisson(log), data = three.list)\nsummary(glm2)\n\n\nCall:\nglm(formula = count ~ LA * NG + NG * PF + LA * PF, family = poisson(log), \n    data = three.list)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  7.99610    0.80673   9.912  &lt; 2e-16 ***\nLAyes       -3.95305    0.79579  -4.967 6.78e-07 ***\nNGyes       -1.81608    0.80545  -2.255   0.0241 *  \nPFyes       -1.00953    0.80616  -1.252   0.2105    \nLAyes:NGyes  0.54562    0.75413   0.724   0.4694    \nNGyes:PFyes -0.98083    0.79541  -1.233   0.2175    \nLAyes:PFyes -0.08908    0.76139  -0.117   0.9069    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 3.3068e+03  on 6  degrees of freedom\nResidual deviance: 8.6819e-14  on 0  degrees of freedom\n  (1 observation deleted due to missingness)\nAIC: 54.782\n\nNumber of Fisher Scoring iterations: 3\n\n\nThe fitted value in the missing cell is \\[e^{\\hat{\\phi}} = 2969,\\] and adding on the other cell counts give \\(\\hat{N}=4694\\). To get an approximate 95% confidence interval, we take the estimated standard error of \\(\\hat{\\mu}\\) which is reported as 0.80673, and calculate \\[\\left(1725 + e^{\\hat{\\phi} -2\\times 0.80673},  1725 + e^{\\hat{\\phi} +2\\times 0.80673}\\right),\\] which gives (2361, 16631).",
    "crumbs": [
      "Part II: Sampling Theory",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Estimating a Population Size</span>"
    ]
  },
  {
    "objectID": "Population size.html#tasks",
    "href": "Population size.html#tasks",
    "title": "12  Estimating a Population Size",
    "section": "12.5 Tasks",
    "text": "12.5 Tasks\n\nShow that Equation 12.1 satisfies the independence requirement: \\(\\mu_{12}/\\mu_{11}=\\mu_{22}/\\mu_{21}\\)\n\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\n\nWe start by noting that \\[\\begin{align*}\n\\left( \\mu + \\beta_2 \\right) - \\mu &= \\left( \\mu + \\alpha_2 + \\beta_2 \\right) - \\left( \\mu + \\alpha_2 \\right).\n\\end{align*}\\] Considering the constraints \\(\\alpha_1 = \\beta_1 = 0\\) it follows that \\[\\begin{align*}\n\\frac{e^{\\left( \\mu + \\beta_2 \\right)}}{e^{\\mu}} &= \\frac{e^{\\left( \\mu + \\alpha_2 + \\beta_2\\right)}}{e^{\\left(\\mu +\\alpha_2\\right)}}\\\\ \\Rightarrow\n\\frac{\\mu_{12}}{\\mu_{11}}&=\\frac{\\mu_{22}}{\\mu_{21}}\n\\end{align*}\\] as required.",
    "crumbs": [
      "Part II: Sampling Theory",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Estimating a Population Size</span>"
    ]
  },
  {
    "objectID": "Practicalities.html",
    "href": "Practicalities.html",
    "title": "13  Reliability of sample surveys",
    "section": "",
    "text": "13.1 Biased samples\nWe use the term “biased” sample to mean that a sample is not representative of the population, typically as a result of sampling mechanism that was never likely to access particular groups within the population. We can use post-stratification to account for biased samples, but not all published sample surveys attempt to do this! A famous example of a biased sample is given in Kellner (2018) 1.",
    "crumbs": [
      "Part II: Sampling Theory",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Reliability of sample surveys</span>"
    ]
  },
  {
    "objectID": "Practicalities.html#biased-samples",
    "href": "Practicalities.html#biased-samples",
    "title": "13  Reliability of sample surveys",
    "section": "",
    "text": "“A biased sample is a biased sample, however large it is. One celebrated example of this was the US Presidential Election in 1936. A magazine, Literary Digest, sent out 10 million post cards asking people how they would vote, received almost 2.3 million back and said that Alfred Landon was leading Franklin Roosevelt by 57-43 per cent. The Digest did not gather information that would allow it to judge the quality of its sample and correct, or”weight”, groups that were under- or over-represented. A young pollster called George Gallup employed a much smaller sample (though, at 50,000, it was much larger than those normally used today), but because he ensured that it was representative, he correctly showed Roosevelt on course to win by a landslide. In the event, Roosevelt won 60% and Landon just 37%. The Literary Digest closed down soon afterwards.”",
    "crumbs": [
      "Part II: Sampling Theory",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Reliability of sample surveys</span>"
    ]
  },
  {
    "objectID": "Practicalities.html#questionnaire-wording",
    "href": "Practicalities.html#questionnaire-wording",
    "title": "13  Reliability of sample surveys",
    "section": "13.2 Questionnaire wording",
    "text": "13.2 Questionnaire wording\nIn surveys to find about opinions held in a population, the wording of a question can influence the response. There has been extensive investigation of this: see for example Schuman and Presser (1996)2. Two of their experiments that demonstrate this effect are as follows. In each experiment, participants in a survey are split into two groups, with each group being asked a different version of a question.\nIn one experiment, the two versions of the same question were (I’ve added bold to emphasise the difference):\n\nDo you think the United States should forbid public speeches against democracy?\nDo you think the United States should allow public speeches against democracy?\n\nThree separate surveys were conducted, all showing similar effects\n\nData from Table 11.1 in Schuman and Presser (1996)\n\n\n\n\n\n\n\n\n\nSurvey 1 (1974)\nSurvey 2 (1976)\nSurvey 3 (1976)\n\n\n\n\nGroup 1: % yes to “forbid”\n28.1%\n20.1%\n21.4 %\n\n\nGroup 2: % no to “allow”\n43.9%\n45.2 %\n47.8 %\n\n\nGroup 1 sample size\n936\n586\n1475\n\n\nGroup 2 sample size\n494\n591\n1375\n\n\n\nIn another experiment, the two versions of the same question were (I’ve used italics to emphasise the difference)\n\nIf a situation like Vietnam were to develop in another part of the world, do you think the United States should or should not send troops?\nIf a situation like Vietnam were to develop in another part of the world, do you think the United States should or should not send troops to stop a communist takeover?\n\nThree separate surveys were conducted, all showing similar effects\n\nData from Table 11.5 in Schuman and Presser (1996)\n\n\n\n\n\n\n\n\n\nSurvey 1 (1974)\nSurvey 2 (1976)\nSurvey 3 (1976)\n\n\n\n\nGroup 1: should send troops\n18.3%\n15.6%\n18.3%\n\n\nGroup 2: should send troops\n33.2%\n27.9%\n36.7%\n\n\nGroup 1 sample size\n459\n575\n885\n\n\nGroup 2 sample size\n871\n549\n449\n\n\n\nI’ve been selective here in picking out experiments where the question wording did have an effect! Not all their experiments showed an effect, and Schuman and Presser wrote (p.296)\n\n“The more blatant the attempt to influence a respondent, the less likely it will succeed…Paradoxically, it may be that the most obvious examples of bias in wording—the ones that would be seized on most quickly by the survey profession as grossly improper—are the least harmful in actual practice.”",
    "crumbs": [
      "Part II: Sampling Theory",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Reliability of sample surveys</span>"
    ]
  },
  {
    "objectID": "Practicalities.html#wishful-thinking",
    "href": "Practicalities.html#wishful-thinking",
    "title": "13  Reliability of sample surveys",
    "section": "13.3 Wishful thinking",
    "text": "13.3 Wishful thinking\nSurveys sometimes ask participants how they would act in hypothetical situations, e.g. in market research to determine whether customers would take up a particular service or product if it were available. The risk is here is that the way a participant imagines they will act (and so answers the survey question) may not be the same as the way they actually act then the situation occurs for real; it may be a case of wishful thinking on the part of the participant. This has been referred to as “social desirability bias”.\nA famous example is the case of the fast food chain Wendy’s, who added fresh fruit to their menu on the basis of market research. Sales were poor, and the fresh fruit was pulled from their menu within a year. A spokesman for Wendy’s said\n\n“We listened to consumers who said they want to eat fresh fruit, but apparently they lied.”",
    "crumbs": [
      "Part II: Sampling Theory",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Reliability of sample surveys</span>"
    ]
  },
  {
    "objectID": "Practicalities.html#sec-RRT",
    "href": "Practicalities.html#sec-RRT",
    "title": "13  Reliability of sample surveys",
    "section": "13.4 Sensitive questions and the Randomized Response Technique",
    "text": "13.4 Sensitive questions and the Randomized Response Technique\nFor questions on sensitive topics, survey participants may choose not to answer truthfully; they may not accept any guarantee of confidentiality. The randomised response technique can be used in this situation.\nAs an example, suppose the aim is to estimate the proportion of adults in a population who have used cannabis. A randomised response technique would be as follows.\n\nThe participant first tosses a coin and rolls a dice. Only the participant knows the outcomes.\nIf the coin toss is a head, the participant answers the “sensitive” question of interest: “Have you ever used cannabis”?\nIf the coin toss is a tail, the participant answers a different “innocent” question: “Was the outcome of the dice roll a six?”\nThe participant does not say which question they answered: all they give is a Yes/No response.\n\nThere are variations on the method, but the key points are that it must be random whether the participant is answering the sensitive question of interest or not, and that it is impossible to know which question the participant answered given a Yes/No response only.\nDefine \\(P\\) to be the population proportion of adults who have used cannabis, and define \\(p\\) to be the sample proportion of “yes” responses. Then\n\\[\nE(p)=Pr(\\mbox{``yes\"}) = \\frac{1}{2}\\times P + \\frac{1}{2}\\times\\frac{1}{6}\n\\] An unbiased estimator of \\(P\\) is therefore \\[\n2\\left(p-\\frac{1}{12}\\right)\n\\]\n\n13.4.1 The general case\nWe can vary the probability of answering the sensitive question of interest, and the probability of a “yes” response for the alternative question. Denote these two probabilities by \\(\\pi_1\\) and \\(\\pi_2\\) respectively. The expected proportion \\(p\\) of “yes” responses is now\n\\[\nE(p)=Pr(\\mbox{``yes\"}) = \\pi_1\\times P + (1-\\pi_1)\\pi_2,\n\\] and the estimator of \\(P\\) is\n\\[\n\\hat{P}:=\\frac{p-(1-\\pi_1)\\pi_2}{\\pi_1},\n\\] and for a sample size \\(n\\), if we suppose the number of “yes” responses has the \\(Binomial(n,\\pi_1\\times P + (1-\\pi_1)\\pi_2)\\) distribution, we have \\[\nVar(\\hat{P}) = \\frac{(\\pi_1 P + (1-\\pi_1)\\pi_2)(1-\\pi_1 P - (1-\\pi_1)\\pi_2)}{n\\pi_1^2},\n\\]\nIt is possible to show that \\(Var(\\hat{P})\\) decreases as \\(\\pi_1\\) increases and is minimised for \\(\\pi_1=1\\). For \\(\\pi_1\\) we force everyone to answer the sensitive question, but this defeats the purpose of the technique!\nIt can also be shown that for fixed \\(\\pi_1\\) and \\(P&lt;0.5\\), the variance is minimised at \\(\\pi_2=0\\), and for \\(P&gt;0.5\\) the variance is minimised at \\(\\pi_2=1\\). Again, this may not be practical, for example, for \\(\\pi_2=0\\), a “yes” response must mean the participant has said “yes” to the sensitive question. This is illustrated in the tasks.\nIn summary, we can consider reducing \\(Var(\\hat{P})\\) through an appropriate design of the technique (how participants are randomised to each question, and what the innocent question is), but not to the extent that survey participants are not willing to take part.\nIntuitively, we might expect the variance to be minimised at \\(\\pi_1=1\\), as setting \\(\\pi_1=1\\) gives the most information about \\(P\\), by forcing everyone to answer the sensitive question of interest. The variance \\(Var(\\hat{P})\\) is indeed minimised at \\(\\pi_1=1\\), but the algebra is a little messy and we leave this for the appendix. But setting \\(\\pi_1\\) isn’t practical as the participants would then know their information is no longer private. All we can do is make \\(\\pi_1\\) as large as possible, without undermining the willingness of individuals to take part.",
    "crumbs": [
      "Part II: Sampling Theory",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Reliability of sample surveys</span>"
    ]
  },
  {
    "objectID": "Practicalities.html#tasks",
    "href": "Practicalities.html#tasks",
    "title": "13  Reliability of sample surveys",
    "section": "13.5 Tasks",
    "text": "13.5 Tasks\n\nSuppose in Section 13.4, the participant tosses the coin twice, and answers the sensitive question if there is at least one had. Give the estimator of the proportion \\(P\\) in this case. Without doing any calculation, comment on how this modification has effected the variance of the estimator.\nSuppose in Section 13.4, the “innocent” question in step 3 is replaced by the complement of the sensitive question: the participant is told “respond ‘yes’ if it is true that you have never taken cannabis. Otherwise you should respond ‘no’. Derive the expectation of the proportion \\(p\\) of”yes” responses in this case, and explain why the population proportion of interest \\(P\\) cannot be estimated here.\nUsing the general notation in Section 13.4.1, derive an expression for the probability a participant was answering the sensitive question, conditional on a “yes” response. Comment on what happens to this probability as \\(\\pi_2\\rightarrow 0\\).\n\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\n\nIf \\(p\\) is the sample proportion of “yes” responses, we now have\n\n\\[\nE(p)=Pr(\\mbox{``yes\"}) = \\frac{3}{4}\\times P + \\frac{1}{4}\\times\\frac{1}{6},\n\\] The estimator of \\(P\\) is now \\[\n\\hat{P} = \\frac{p-\\frac{1}{24}}{\\frac{3}{4}}\n\\] This decreases the variance of the estimator, as we have increased the probability of answering the sensitive question.\n\nWe now have \\[\nE(p)=Pr(\\mbox{``yes\"}) = \\frac{1}{2}\\times P + \\frac{1}{2}\\times(1-P)=\\frac{1}{2}\n\\] We can no longer estimate the population proportion \\(P\\) using the sample proportion \\(p\\), as the expectation of the sample proportion does not depend on the population proportion.\nDefine \\(S\\) to be the event of answering the sensitive question. We have \\[\\begin{align*}\nPr(S|\\mbox{``yes\"}) &= \\frac{Pr(S)Pr(\\mbox{``yes\"}|S)}{Pr(\\mbox{``yes\"})}\\\\\n&= \\frac{\\pi_1P}{\\pi_1P+(1-\\pi_1)\\pi_2}.\n\\end{align*}\\]\n\nWe can see (and it is obvious from the setup) that as \\(\\pi_2 \\rightarrow 0\\), we become certain that a “yes” response means the participant answered the sensitive question. Though small \\(\\pi_2\\) can be desirable in some cases (when the population proportion \\(P&lt;0.5\\)) for reducing the variance of \\(\\hat{P}\\), people may not choose to participate if \\(\\pi_2\\) is too small, as we may be able to tell which question they answered.",
    "crumbs": [
      "Part II: Sampling Theory",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Reliability of sample surveys</span>"
    ]
  },
  {
    "objectID": "Practicalities.html#footnotes",
    "href": "Practicalities.html#footnotes",
    "title": "13  Reliability of sample surveys",
    "section": "",
    "text": "Kellner, Peter. 2018. British Polling Council: A Journalist’s Guide to Opinion Polls. Available at https://publications.parliament.uk/pa/ld201719/ldselect/ldppdm/106/10616.htm (2014/03/03).↩︎\nSchuman, H. and Presser, S. (1996) Questions and answers in attitude surveys: experiments on question form, wording, and context. London: SAGE.↩︎",
    "crumbs": [
      "Part II: Sampling Theory",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Reliability of sample surveys</span>"
    ]
  },
  {
    "objectID": "Tailor-made designs.html",
    "href": "Tailor-made designs.html",
    "title": "14  Modifying designs",
    "section": "",
    "text": "14.1 Adding a point\nThis Chapter follows on from Chapter 7.\nHere, using \\(D\\)-optimality as a guiding principle, we look at the problem of either adding a new point to an existing design to accommodate an extra observation, or deleting a point from an existing design. The following result from matrix theory is of independent interest elsewhere, and particularly useful here.\nWe shall not prove this lemma, but merely explain its application. Suppose we have an existing design with design matrix \\(\\mathbf{x}\\) for a particular model, and we wish to add an extra point \\(\\mathbf{x}\\) to the design to yield an extra observation. Then, bearing in mind the definition of the information matrix, the information matrix of the new design may be written \\[\\mathbf{G}^*=\\mathbf{X}^T\\mathbf{X}+\\mathbf{f}(\\mathbf{x})\\mathbf{f}(\\mathbf{x})^T=\\mathbf{G}+\\mathbf{f}(\\mathbf{x})\\mathbf{f}(\\mathbf{x})^T\\] where \\(\\mathbf{G}\\) is the information matrix of the original design. If this result isn’t obvious to you then check it in the simple linear regression case with \\(n\\) observation in the original design matrix. If we now apply the lemma to determinants, with \\(n=1\\), \\(\\mathbf{A}=\\mathbf{G}\\), \\(\\mathbf{B}=\\mathbf{f}(\\mathbf{x})\\) and \\(\\mathbf{C}=\\mathbf{f}(\\mathbf{x})^T\\), then, noting that the second determinant on the right hand side is just that of a scalar, namely itself, we get \\[|\\mathbf{G}^*|=|\\mathbf{G}|\\left(1+\\mathbf{f}(\\mathbf{x})^T\\mathbf{G}^{-1}\\mathbf{f}(\\mathbf{x}) \\right).\\] It follows from the simple multiplicative factor on the right hand side that it is \\(D\\)-optimal to choose the point \\(\\mathbf{x}\\) for which this is as large as possible. But \\(\\mathbf{f}(\\mathbf{x})^T\\mathbf{G}^{-1}\\mathbf{f}(\\mathbf{x})\\) is just, apart from a multiplicative factor, the variance of prediction at the point \\(\\mathbf{x}\\). It follows that if a new point is to be added to the design, it is \\(D\\)-optimal to choose one with the largest variance of prediction (for the existing design). This is intuitive: such a point is in a part of the design region where we know least about the fit of the model.",
    "crumbs": [
      "Part III: self-study topics for MPS4101",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Modifying designs</span>"
    ]
  },
  {
    "objectID": "Tailor-made designs.html#adding-a-point",
    "href": "Tailor-made designs.html#adding-a-point",
    "title": "14  Modifying designs",
    "section": "",
    "text": "WarningLemma\n\n\n\n\nLemma 14.1 If \\(\\mathbf{A}\\) is a non-singular \\(p\\times p\\) matrix, \\(\\mathbf{B}\\) is a \\(p\\times n\\) matrix, \\(\\mathbf{C}\\) is a \\(n\\times p\\) matrix and \\(\\mathbf{I}\\) is the \\(n\\times n\\) identity matrix, then \\[|\\mathbf{A}+\\mathbf{B}\\mathbf{C}|=|\\mathbf{A}||\\mathbf{I}+\\mathbf{C}\\mathbf{A}^{-1}\\mathbf{B}|.\\]",
    "crumbs": [
      "Part III: self-study topics for MPS4101",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Modifying designs</span>"
    ]
  },
  {
    "objectID": "Tailor-made designs.html#deleting-a-point",
    "href": "Tailor-made designs.html#deleting-a-point",
    "title": "14  Modifying designs",
    "section": "14.2 Deleting a point",
    "text": "14.2 Deleting a point\nWe can work out a similar result for deleting an existing point \\(\\mathbf{x}\\) from a design. The plus signs may be replaced by minus signs in the lemma, and in the modification of the information matrix, and so we can say that the new information matrix has determinant \\[|\\mathbf{G}^*|=|\\mathbf{G}|\\left(1-\\mathbf{f}(\\mathbf{x})^T\\mathbf{G}^{-1} \\mathbf{f}(\\mathbf{x})\\right)\\] and so it is \\(D\\)-optimal to delete a point of the existing design at which the variance of prediction is least.",
    "crumbs": [
      "Part III: self-study topics for MPS4101",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Modifying designs</span>"
    ]
  },
  {
    "objectID": "Tailor-made designs.html#adding-two-points",
    "href": "Tailor-made designs.html#adding-two-points",
    "title": "14  Modifying designs",
    "section": "14.3 Adding two points",
    "text": "14.3 Adding two points\nIf we wish to modify a design by changing more than one point, the situation is not so clear: doing it optimally stepwise one point at a time is not obviously going to produce a design which is optimal among all choices of more than one design point, although often it will.\n\n\n\n\n\n\nNoteExample\n\n\n\n\nExample 14.1 Quadratic regression on one explanatory variable. Suppose we have taken one observation at \\(x=-1\\), one observation at \\(x=0\\) and two observations at \\(x=1\\). Then we have \\[\\mathbf{x}= \\left( \\begin{array}{ccc} 1 & -1 & 1 \\\\ 1 & 0 & 0 \\\\ 1 & 1& 1 \\\\ 1 & 1 & 1 \\end{array} \\right) ~~\\text{and}~~ \\mathbf{G}= \\left(\\begin{array}{ccc} 4 & 1 & 3 \\\\ 1 & 3 & 1 \\\\ 3 & 1 & 3 \\end{array}\\right).\\] Then \\[\\mathbf{G}^{-1}=\\frac{1}{8}\\left(\\begin{array}{ccc}8&0&-8\\\\0&3&-1\\\\-8&-1&11\\end{array}\\right)\\] and \\[\\begin{align}\n\\mathbf{f}(\\mathbf{x})^T\\mathbf{G}^{-1}\\mathbf{f}(\\mathbf{x})&=\\frac{1}{8} \\left(\\begin{array}{ccc} 1 & x & x^2 \\end{array} \\right) \\left(\n\\begin{array}  {ccc} 8 & 0 & -8 \\\\ 0 & 3 & -1 \\\\ -8 & -1 & 11\n\\end{array} \\right) \\left( \\begin{array}{c} 1 \\\\ x \\\\ x^2\n\\end{array} \\right) \\\\\n&=\\frac{1}{8}\\left( 8 - 13x^2 - 2x^3 + 11 x^4 \\right)\n\\end{align}\\] This quartic (fourth order polynomial) has a local maximum value of 1 at \\(x=0\\), as may be checked by calculus. It also takes the value 1 at \\(x=-1\\), the left-hand endpoint of the interval, and it takes a smaller value at the other endpoint, \\(x=1\\). We conclude from this that, if an extra point is to be added to the design, \\(x=0\\) and \\(x=-1\\) are equally \\(D\\)-optimal. To check this sketch the curve. So according to the \\(D\\) optimality criterion, we can add either one of these two points . The following task asks you to work out the next \\(D\\)-optimal point to add, assuming we have added \\(x=-1\\).\n\n\n\nIf the intention is merely to investigate the effect of a two-stage modification of the design, the following lemma comes in useful. Again, we state the lemma without proof.\n\n\n\n\n\n\nWarningLemma\n\n\n\n\nLemma 14.2 Under the same conditions as in Lemma 14.1, \\[(\\mathbf{A}+\\mathbf{B}\\mathbf{C})^{-1}=\\mathbf{A}^{-1}-\\mathbf{A}^{-1}\\mathbf{B}(\\mathbf{I}+\\mathbf{C}\\mathbf{A}^{-1}\\mathbf{B})^{-1} \\mathbf{C}\\mathbf{A}^{-1}\\] provided the inverses on both sides exist.\n\n\n\nThe application of this is when, say, we first add a new point \\(\\mathbf{x}\\) to the design, changing the information matrix from \\(\\mathbf{G}\\) to \\(\\mathbf{G}_1\\), say, and then add another new point \\(\\mathbf{y}\\), changing it to \\(\\mathbf{G}_2\\), say. We then have \\[\\begin{align}\n|\\mathbf{G}_2|&=|\\mathbf{G}_1|\\left(1+\\mathbf{f}(\\mathbf{y})^T\\mathbf{G}_1^{-1}\\mathbf{f}(\\mathbf{y})\\right) \\\\\n&=|\\mathbf{G}_1|\\left(1+\\mathbf{f}(\\mathbf{y})^T(\\mathbf{G}+\\mathbf{f}(\\mathbf{x})\\mathbf{f}(\\mathbf{x})^T)^{-1}\\mathbf{f}(\\mathbf{y}) \\right) \\\\\n&=|\\mathbf{G}_1|\\left( 1 + \\mathbf{f}(\\mathbf{y})^T\\mathbf{G}^{-1}\\mathbf{f}(\\mathbf{y})-\\mathbf{f}(\\mathbf{y})^T\\mathbf{G}^{-1}\\mathbf{f}(\\mathbf{x})(1 + \\mathbf{f}(\\mathbf{x})^T\\mathbf{G}^{-1}\\mathbf{f}(\\mathbf{x}))^{-1}\\mathbf{f}(\\mathbf{x})^T\\mathbf{G}^{-1}\\mathbf{f}(\\mathbf{y})\\right)\n\\end{align}\\] using Lemma 14.2 with the same interpretations of the matrices as in the application of Lemma 14.1. This expression now simplifies because the right hand side consists of scalar quantities, and also because we may substitute \\(|\\mathbf{G}_1| = |\\mathbf{G}|(1 + \\mathbf{f}(\\mathbf{x})^T\\mathbf{G}^{-1}\\mathbf{f}(\\mathbf{x}))\\). This gives \\[|\\mathbf{G}_2| = |\\mathbf{G}|\\left\\{ \\left( 1 + \\mathbf{f}(\\mathbf{x})^T\\mathbf{G}^{-1}\\mathbf{f}(\\mathbf{x}\n) \\right) \\left( 1 + \\mathbf{f}(\\mathbf{y})^T\\mathbf{G}^{-1}\\mathbf{f}(\\mathbf{y}) \\right) -\n\\left( \\mathbf{f}(\\mathbf{x})^T\\mathbf{G}^{-1}\\mathbf{f}(\\mathbf{y}) \\right) ^2 \\right\\}.\\] Using this we can assess the effect on the information matrix of adding two new points \\(x\\) and \\(y\\).",
    "crumbs": [
      "Part III: self-study topics for MPS4101",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Modifying designs</span>"
    ]
  },
  {
    "objectID": "Tailor-made designs.html#changing-a-point",
    "href": "Tailor-made designs.html#changing-a-point",
    "title": "14  Modifying designs",
    "section": "14.4 Changing a point",
    "text": "14.4 Changing a point\nSimilarly if we delete an existing point \\(\\mathbf{x}\\) from a design and then add a new point \\(\\mathbf{y}\\) we get \\[|\\mathbf{G}_2| = |\\mathbf{G}|\\left\\{ \\left( 1 - \\mathbf{f}(\\mathbf{x})^T\\mathbf{G}^{-1}\\mathbf{f}(\\mathbf{x})  \\right) \\left( 1 + \\mathbf{f}(\\mathbf{y})^T\\mathbf{G}^{-1}\\mathbf{f}(\\mathbf{y}) \\right) + \\left( \\mathbf{f}(\\mathbf{x})^T\\mathbf{G}^{-1}\\mathbf{f}(\\mathbf{y}) \\right) ^2 \\right\\} .\\]\n\n\n\n\n\n\nNoteExample\n\n\n\n\nExample 14.2 Simple linear regression with four observations at \\(x=-1, -1, 0\\) and \\(1\\). In this case, \\[\\mathbf{G}= \\left( \\begin{array}{cc} 4 & -1 \\\\ -1 & 3 \\end{array}\n\\right)~~\\text{and}~~\\mathbf{G}^{-1} = \\frac{1}{11}\\left(\\begin{array}{cc} 3 & 1 \\\\ 1& 4 \\end{array} \\right).\\] Suppose we replace one of the observations at \\(x=-1\\) with one at \\(x=1\\). Then the relevant quantities are \\[\\begin{align}\n\\mathbf{f}(-1)^T\\mathbf{G}^{-1}\\mathbf{f}(-1) &= \\frac{1}{11} \\left(\n\\begin{array}{cc} 1 & -1 \\end{array} \\right) \\left(\n\\begin{array}{cc} 3 & 1 \\\\ 1 & 4 \\end{array} \\right) \\left(\n\\begin{array}{c} 1 \\\\ -1 \\end{array} \\right) = \\frac{5}{11} \\\\\n\\mathbf{f}(1)^T\\mathbf{G}^{-1}\\mathbf{f}(1) &= \\frac{1}{11} \\left(\n\\begin{array}{cc} 1 & 1 \\end{array} \\right) \\left(\n\\begin{array}{cc} 3 & 1 \\\\ 1 & 4 \\end{array} \\right) \\left(\n\\begin{array}{c} 1 \\\\ 1 \\end{array} \\right) = \\frac{9}{11} \\\\\n\\text{and}~~\\mathbf{f}(-1)^T\\mathbf{G}^{-1}\\mathbf{f}(1) &= \\frac{1}{11} \\left(\n\\begin{array}{cc} 1 & -1 \\end{array} \\right) \\left(\n\\begin{array}{cc}  3 & 1 \\\\ 1 & 4 \\end{array} \\right) \\left(\n\\begin{array}{c} 1 \\\\ 1 \\end{array} \\right) = -\\frac{1}{11}\n\\end{align}\\] and so, substituting, \\[|\\mathbf{G}_2|=|\\mathbf{G}|\\left\\{\\left(1-\\frac{5}{11}\\right)\\left(1+\\frac{9}{11}\\right)+\\left(-\\frac{1}{11}\\right)^2\\right\\}=|\\mathbf{G}|.\\] In this case, the result is not surprising, since the design is merely being reflected about the origin, and so we would expect its essential properties not to change.",
    "crumbs": [
      "Part III: self-study topics for MPS4101",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Modifying designs</span>"
    ]
  },
  {
    "objectID": "Tailor-made designs.html#tasks",
    "href": "Tailor-made designs.html#tasks",
    "title": "14  Modifying designs",
    "section": "14.5 Tasks",
    "text": "14.5 Tasks\n\nAssume that \\(x=-1\\) is added to the design in Example 14.1. Is it true that it is now \\(D\\)-optimal to add the point \\(x=0\\) and hence restore the symmetry of the design?\nFind an optimal design with two observations for the model \\[\\begin{equation} E(Y)=\\beta_1x_1+\\beta_2x_2 \\end{equation}\\] using the points \\((-1,-1), (1,-1), (-1,1)\\) and \\((1,1)\\) as candidate points. Does it satisfy the conditions of the General Equivalence Theorem? If a third observation were to be added to the design, where would you choose to take it?\nIn the simple linear regression model \\[\\begin{equation} E(Y)=\\beta_0+\\beta_1x \\end{equation}\\] a design has three observations taken at \\(x=-1,0\\) and \\(1\\). Investigate the effect of replacing the observation at \\(x=0\\) by an observation at \\(x=a\\neq 0\\). Which value(s) of \\(a\\) give(s) the best results?\n\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\n\nIf \\(x=-1\\) is added, the design matrix is now \\[X=\n\\left(\n\\begin{array}{rrr}\n1 &-1 & 1\\\\\n1 &-1 & 1\\\\\n1 & 0 & 0\\\\\n1 & 1 & 1\\\\\n1 & 1 & 1\\\\\n\\end{array}\n\\right)\n\\] so \\[G=(X^TX)=\n\\left(\n\\begin{array}{rrr}\n5 & 0 & 4\\\\\n0 & 4 & 0\\\\\n4 & 0 & 4\\\\\n\\end{array}\n\\right)\n\\] and \\[(X^TX)^{-1}=\n\\frac{1}{4}\n\\left(\n\\begin{array}{rrr}\n4 & 0 & -4\\\\\n0 & 1 & 0\\\\\n-4 & 0 & 5\\\\\n\\end{array}\n\\right)\n.\\] So we have that \\[\\begin{align*}\nf(x)^TG^{-1}f(x)&=\\frac{1}{4}\\left(\\begin{array}{ccc} 1&x&x^2 \\end{array} \\right)\\left(\n\\begin{array}{rrr}\n4 & 0 & -4\\\\\n0 & 1 & 0\\\\\n-4 & 0 & 5\\\\\n\\end{array}\n\\right)\n\\left(\\begin{array}{c} 1\\\\x\\\\x^2 \\end{array} \\right)\\\\\n&=\\frac{1}{4}\\left(4+x^2+5x^4-8x^2\\right)\\\\\n&=\\frac{1}{4}\\left(4-7x^2+5x^4\\right)\\\\\n&=\\frac{1}{4}\\left(4-7\\left(x^2\\right)+5\\left(x^2\\right)^2\\right)\n\\end{align*}\\] Consider the quadratic function \\(h(t)=4-7t+5t^2\\). This has a minimum at \\(t=0.7\\) with \\(h(0.7)=1.55\\). When \\(t=1\\) we have \\(h(1)=2\\) and when \\(t=0\\) we have \\(h(0)=4\\). So \\(h(t)\\) is maximised over \\([0,1]\\) when \\(t=0\\). So the \\(D\\)-optimal point to add is \\(x=0\\).\nChoosing two ‘opposite’ points \\((-1,-1)\\) and \\((1,1)\\), or \\((-1,1)\\) and \\((1,-1)\\), is no good, because the design matrices \\[ \\left( \\begin{array}{cc} -1 & -1 \\\\ 1 & 1 \\end{array} \\right)\n\\mbox{ and }\\left( \\begin{array}{cc} -1 & 1 \\\\ 1 & -1\n\\end{array} \\right) \\] are square and singular, i.e. not of full rank. This only leaves choosing two ‘adjacent’ points, such as \\((-1,1)\\) and \\((1,1)\\). Obviously, by symmetry, any such pair is suitable. Then \\[ \\mathbf X ^T\\mathbf X = \\left( \\begin{array}{cc} -1 & 1 \\\\ 1 & 1 \\end{array}\n\\right) \\left( \\begin{array}{cc} -1 & 1 \\\\ 1 & 1 \\end{array} \\right)\n= \\left( \\begin{array}{cc} 2 & 0 \\\\ 0 & 2 \\end{array} \\right) \\] and so the standardised variance of prediction is \\[ 2\\left( \\begin{array}{cc} x_1 & x_2 \\end{array} \\right) \\left(\n\\begin{array}{cc} \\frac{1}{2} & 0 \\\\ 0 &\n\\frac{1}{2} \\end{array} \\right) \\left( \\begin{array}{c} x_1 \\\\ x_2\n\\end{array} \\right) = x_1^2 + x_2 ^2 \\] which is maximised when \\(x_1^2=x_2^2=1\\) and its maximum value is 2, so that the conditions of the GET are met. A third observation could be taken at any of the four candidate points, where the standardised variance of prediction is maximised, although if there is any question of whether the model is appropriate or not, it would seem better to choose one of the two points which have not already been chosen.\nThe relevant matrices are \\[ \\mathbf{X} = \\left( \\begin{array}{cc} 1 & -1 \\\\ 1 & 0 \\\\ 1 & 1\n\\end{array} \\right),\\quad \\mathbf{G} = \\mathbf{X} ^T\\mathbf{X} = \\left(\n\\begin{array}{cc} 3 & 0 \\\\ 0 & 2 \\end{array} \\right),\\quad \\mathbf G\n^{-1} = \\left( \\begin{array}{cc} \\frac{1}{3} & 0 \\\\ 0 & \\frac{1}{2}\n\\end{array} \\right) . \\] Then \\[\\begin{eqnarray*}\n\\mathbf f (0)^T\\mathbf G ^{-1}\\mathbf f (0) & = & \\left( \\begin{array}{cc} 1 & 0\n\\end{array} \\right) \\left( \\begin{array}{cc} \\frac{1}{3} & 0 \\\\ 0 &\n\\frac{1}{2} \\end{array} \\right) \\left( \\begin{array}{c} 1 \\\\ 0\n\\end{array} \\right) = \\frac{1}{3}; \\\\\n\\mathbf f (a)^T\\mathbf G ^{-1}\\mathbf f (a) & = & \\left( \\begin{array}{cc} 1 & a\n\\end{array} \\right) \\left( \\begin{array}{cc} \\frac{1}{3} & 0 \\\\ 0 &\n\\frac{1}{2} \\end{array} \\right) \\left( \\begin{array}{c} 1 \\\\ a\n\\end{array} \\right) = \\frac{1}{3} + \\frac{1}{2}a^2; \\\\\n\\mathbf f (0)^T\\mathbf G ^{-1}\\mathbf f (a) & = & \\left( \\begin{array}{cc} 1 & 0\n\\end{array} \\right) \\left( \\begin{array}{cc} \\frac{1}{3} & 0 \\\\ 0 &\n\\frac{1}{2} \\end{array} \\right) \\left( \\begin{array}{c} 1 \\\\ a\n\\end{array} \\right) = \\frac{1}{3}.\n\\end{eqnarray*}\\]\n\nSo \\[ |\\mathbf G _2| = |\\mathbf G |\\left\\{ \\left( 1 - \\frac{1}{3}\\right) \\left( 1 +\n\\frac{1}{3} + \\frac{1}{2}a^2 \\right) + \\left( \\frac{1}{3} \\right) ^2\n\\right\\} = |\\mathbf G |\\left\\{ 1 + \\frac{1}{3} a^2 \\right\\} . \\] So there is always an improvement, and the best choice is \\(a=\\pm 1\\).",
    "crumbs": [
      "Part III: self-study topics for MPS4101",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Modifying designs</span>"
    ]
  },
  {
    "objectID": "Cluster sampling.html",
    "href": "Cluster sampling.html",
    "title": "15  Cluster Sampling",
    "section": "",
    "text": "15.1 One-stage cluster sampling with equal cluster sizes\nThis Chapter follows on from Chapter 9.\nThe population is now divided into \\(l\\) strata, but, instead of sampling within each, we take a SRS of strata and measure all units in those strata selected. Strata are then called clusters.\nCluster sampling is usually chosen for its ease: it is much easier to measure all units in a cluster than a number of arbitrary units. Consider for example schools in a thinly populated part of the country: random sampling would be very expensive in travel, but maybe there are agents in a small number of towns who could quickly collect data from all schools in their towns. So the advantage is likely to be economical rather than a gain in precision.\nThe above is one-stage cluster sampling: all units in the chosen clusters are measured. A development is two-stage cluster sampling, in which there is further selection within the clusters chosen at the first stage. For example, in an educational survey, Local Education Authorities might be the primary units, schools within LEAs the secondary units, and individual children the members of the study population.\nAnalysis of cluster sampling is difficult unless we make the assumption that clusters are equal-sized. (If the clusters are not equal-sized, the total sample size will be random, even though the number of clusters sampled is fixed).\nSuppose there are \\(L\\) clusters, each of size \\(K\\): \\(LK=N\\). Take SRS of \\(l\\) of the clusters and totally enumerate members, giving \\(\\{x_{ij}\\}, i=1,\\ldots,l,\\;j=1,\\ldots,K\\). Consider \\[\\overline{x}_{cl} = \\frac{1}{lK} \\sum_{1}^{l} \\sum_{1}^{K} x_{ij}.\\]\nProperties:\nwhere \\(f=l/L=lK/LK\\) is the sampling fraction, and \\(\\overline{X}_i\\) is the mean of the \\(i\\)th cluster. We can estimate \\[S_{bc}^{2} = \\frac{1}{L-1} \\sum_1^L(\\overline{X}_i-\\overline{X})^2\\quad \\mbox{the between-cluster variance}\\] by \\[s_{bc}^2=\\frac{1}{l-1} \\sum _1^l(\\overline{x}_i-\\overline{x})^2\\] which by SRS results is unbiased for it.",
    "crumbs": [
      "Part III: self-study topics for MPS4101",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Cluster Sampling</span>"
    ]
  },
  {
    "objectID": "Cluster sampling.html#sec-onestage",
    "href": "Cluster sampling.html#sec-onestage",
    "title": "15  Cluster Sampling",
    "section": "",
    "text": "\\(E(\\overline{x}_{cl})=\\overline{X}\\).\n\\(\\mbox{var}(\\overline{x}_{cl}) = \\frac{1-f}{l} \\frac{1}{L-1}\\sum_1^L(\\overline{X}_i-\\overline{X})^2\\)",
    "crumbs": [
      "Part III: self-study topics for MPS4101",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Cluster Sampling</span>"
    ]
  },
  {
    "objectID": "Cluster sampling.html#comparison-with-srs",
    "href": "Cluster sampling.html#comparison-with-srs",
    "title": "15  Cluster Sampling",
    "section": "15.2 Comparison with SRS",
    "text": "15.2 Comparison with SRS\nConsider SRS with the same sampling fraction, so that the number sampled is \\(n = lK\\). Then the variance of the SRS estimator is \\[\\mbox{var~}\\overline{x} = \\left(1-\\frac{l}{L} \\right) \\frac{S^2}{lK}.\\] On the other hand the variance of the cluster estimator is \\[\\mbox{var}(\\overline{x}_{cl}) = \\left(1-\\frac{l}{L} \\right)\\frac{1}{l(L-1)} \\sum _1^L(\\overline{X}_i-\\overline{X})^2 = \\left(1-\\frac{l}{L}\\right) \\frac{S_{bc}^2}{l}.\\] We can express \\(S^2_{bc}\\) in terms of \\(S\\) via the following \\[\\begin{align}\n(N-1)S^2= (LK-1)S^2 & =\\sum_{i=1}^L \\sum_{j=1}^K (X_{ij}-\\overline{X})^2 \\\\\n&=\\sum_1^L \\sum_1^K (X_{ij}-\\overline{X}_i)^2 + \\sum_1^L K (\\overline{X}_i-\\overline{X})^{2}\\\\\n&=L(K-1)\\overline{S}_{wc}^2 + K(L-1)S_{bc}^2\n\\end{align}\\] where \\(\\overline{S}_{wc}^2\\) is the mean within-cluster variance. After some algebra, we can show \\[\\mbox{var}(\\overline{x}_{cl})-\\mbox{var}(\\overline{x})=\\left(\\frac{1}{lK}-\\frac{1}{LK}\\right)\\frac{L(K-1)}{L-1}(S^2 -\\overline{S}_{wc}^2)\\] and so \\(\\overline{x}\\) is more precise than \\(\\overline{x}_{cl}\\) when the average within-cluster variance \\(\\overline{S}_{wc}^2\\) is less than the population variance \\(S^2\\). Note that this is in contrast to the case for stratified random sampling, in which the stratified estimator is more precise than \\(\\overline{x}\\) when within-strata variance is smaller. The reason for the difference is that if clusters are similar, then the use of clusters - even large ones - may give a poorer estimate than SRS because little extra information is gained from measuring many observations.",
    "crumbs": [
      "Part III: self-study topics for MPS4101",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Cluster Sampling</span>"
    ]
  },
  {
    "objectID": "Cluster sampling.html#tasks",
    "href": "Cluster sampling.html#tasks",
    "title": "15  Cluster Sampling",
    "section": "15.3 Tasks",
    "text": "15.3 Tasks\n\nProve properties (1) and (2) in Section 15.1\nA survey is to be taken to estimate the mean starting wage of individuals following completion of a new training course. Two pilot studies have been conducted. In the first study, a simple random sample of size 20 was used, and the following summary statistics were observed. \\[\n\\sum_{i=1}^{20} x_i=564.9, \\hspace{2cm} \\sum_{i=1}^{20} x_i^2=16131.2,\n\\] where each \\(x_i\\) is measured in 1000. In the second study, cluster sampling was used. Courses are run at different training centres around the country, with 10 students at each centre. Two training centres were selected as the clusters. The following summary statistics were observed. \\[\\begin{align}\n\\sum_{j=1}^{10} x_{1j}&=258.7,   & \\sum_{j=1}^{10} x_{1j}^2=6714.3,\\\\\n\\sum_{j=1}^{10} x_{2j}&=305.0,   & \\sum_{j=1}^{10} x_{2j}^2=9319.2,\n\\end{align}\\] where \\(x_{ij}\\) is observation \\(j\\) within cluster \\(i\\). Based on the pilot survey data, would you recommend the use of cluster sampling or simple random sampling for the new survey?\n\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\n\nProperty 1. Because we sample the whole cluster we can replace the cluster values with the cluster mean (they contain the same information if we know the cluster size). We are then in the situation of taking an SRS of size \\(l\\) from the \\(L\\) cluster means, and we can apply the results for properties of a sample mean under SRS.\n\n\\[\\begin{align*}\nE\\left(\\bar{x}_{cl} \\right) &= E\\left(\\frac{1}{lk}\\sum_{i=1}^l \\sum_{j=1}^K x_{ij} \\right)\\\\\n&= E\\left( \\frac{1}{l}\\sum_{i=1}^l \\overline{X}_i \\right)\\\\\n&= \\frac{1}{l} \\sum_{i=1}^l  E\\left( \\overline{X}_i \\right)\\\\\n&= \\frac{1}{l} \\sum_{i=1}^l  \\left( \\overline{X} \\right)\\\\\n&= \\overline{X}\n\\end{align*}\\]\nProperty 2. We can directly use the expression for the variance of the best linear unbiased estimator of the mean from an SRS (\\(Var\\left(\\bar{x}\\right)\\)). We are taking an SRS of size \\(l\\) from \\(\\overline{X}_1, \\ldots, \\overline{X}_L\\) so \\[\\begin{align*}\nVar\\left( \\bar{x}_{cl}\\right) &= \\left( 1- \\frac{l}{L}\\right)\\frac{S^2}{l}\\\\\n&= \\left( \\frac{1- f }{l} \\right) S^2\\\\\n&= \\left( \\frac{1- f }{l} \\right) \\frac{1}{L-1}\\sum_{i=1}^L \\left( \\overline{X}_i - \\overline{X}\\right)^2\n\\end{align*}\\]\n\nCluster sampling should be better if the mean within cluster variance \\(\\bar{S}^2_{wc}\\) is greater than the population variance \\(S^2\\). Using the pilot study data, estimate of \\(S^2\\) is \\[\n\\frac{1}{19}\\left(\\sum_{i=1}^{20} x_i^2 - \\frac{(\\sum_{i=1}^{20}x_i)^2}{20}\\right)=9.24,\n\\] and the estimate of \\(\\bar{S}^2_{wc}\\) is \\[\n\\frac{0.5}{9}\\left(\\sum_{j=1}^{10} x_{1j}^2 - \\frac{(\\sum_{j=1}^{10}x_{1j})^2}{10}\\right) +\n\\frac{0.5}{9}\\left(\\sum_{j=1}^{10} x_{2j}^2 - \\frac{(\\sum_{j=1}^{10}x_{2j})^2}{10}\\right) =2.13,\n\\]\n\nso simple random sampling should be better.",
    "crumbs": [
      "Part III: self-study topics for MPS4101",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Cluster Sampling</span>"
    ]
  },
  {
    "objectID": "Introduction-computer-experiments.html",
    "href": "Introduction-computer-experiments.html",
    "title": "16  Introduction to computer experiments",
    "section": "",
    "text": "CautionAims for this chapter\n\n\n\nWe introduce the final topic for this module: computer experiments, and discuss some general problems of interest when making predictions using computer models.\n\n\nWe consider the use of computer models for conducting experiments, and some statistical tools for analysing uncertainty in such experiments. By computer model, we simply mean a mathematical model of a physical system, which we represent by a function \\(y=f(x)\\), where we use a computer to evaluate the outputs \\(y\\) given the inputs \\(x\\). A “computer experiment” refers to evaluating the output at different choices of inputs \\(x\\).\nSome models may be simple equations that we can write down; others may be much more complex, for example, systems of differential equations that are solved numerically, so that there is no closed form expression for \\(f\\). For illustrative purposes, we will consider examples where \\(f\\) is a simple function, but in general, you may think of a computer model as a ‘black box’: the model produces output \\(y\\) when given input \\(x\\), but the input-output relationship is not transparent.\nThe motivation for performing a computer experiment may be that a physical experiment would either be impossible or too costly. For example, to predict the effect of reducing CO\\(_2\\) emissions on climate, we have to rely on computer model predictions; there is no physical experiment we could do instead. In engineering applications (e.g. designing an aircraft engine), it will be too expensive to build and test prototypes of every plausible configuration; computer modelling is a much cheaper alternative. In this context, such models are sometimes referred to as “digital twins”.\nComputer models may be deterministic: evaluating \\(f(x)\\) twice at the same value of \\(x\\) produces the same value of \\(y\\). However, even when there is no randomness, we may still have uncertainty! The following are some problems where statistical methods may be helpful.\n\nUncertainty analysis/uncertainty propogation In some situations, there may be a true value of the input that should be used for the application at hand, but we don’t know what the true value is. If we denote the true, uncertain, input by \\(X\\), how do we quantify uncertainty about \\(Y=f(X)\\)?\n“Probabilistic” or “global” sensitivity analysis In an extension to (i), suppose the uncertain input is a vector \\(\\mathbf{X}=(X_1,\\ldots,X_d)\\). How do elements of \\(\\mathbf{X}\\) contribute to uncertainty in \\(Y=f(\\mathbf{X})\\)? Suppose we could pay to reduce uncertainty about any element of \\(\\mathbf{X}\\). Which element(s) should we learn about?\nCalibration/inverse problems Suppose a model has two types of inputs: control inputs \\(x_{cont}\\) that we can vary freely, and calibration inputs \\(x_{calib}\\), for which there is an unknown, true value \\(X_{calib}\\). Now suppose we have observed physical data, corresponding to different known values of the control inputs. Denote this data by \\(z(x_{cont,1}),\\ldots,z(x_{cont,n})\\). Can we infer the value of \\(X_{calib}\\), such that the computer model outputs \\(f(x_{cont,i},X_{calib})\\) match the physical data \\(z(x_{cont,i})\\) for all \\(i\\)?\nComputationally expensive models Some models take a long time to run, even with supercomputer resources: obtaining a single model ‘run’ (evaluating \\(f(x)\\) for a single choice of \\(x\\)) may take hours, days, or even months. How can we tackle problems (i) to (iii) above with limited numbers of model runs?\nReality! A computer model is only a model! How uncertain should we be about what we would observe in the real world, given what we have seen from the model?\n\nThese problems are all the focus of much current research. In this module, we will consider the first two only.",
    "crumbs": [
      "Part III: self-study topics for MPS4101",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Introduction to computer experiments</span>"
    ]
  },
  {
    "objectID": "Uncertainty-analysis.html",
    "href": "Uncertainty-analysis.html",
    "title": "17  Uncertainty analysis",
    "section": "",
    "text": "17.1 Probabilistic uncertainty analysis\nWe consider a model of the form \\(y=f(\\mathbf{x})\\), where \\(y\\) is a scalar output and \\(\\mathbf{x}=(x_1,\\ldots,x_d)\\) is a vector of \\(d\\) inputs. We suppose that there is a true value of the input \\(\\mathbf{X}\\) such that we would like to know the value of \\(Y\\) where \\(Y=f(\\mathbf{X})\\), but \\(\\mathbf{X}\\) is unknown. A simple approach might be\nThis approach is not recommended, for reasons illustrated in the following two examples.\nIn general, if \\(f\\) is a nonlinear function of the inputs, we would not expect \\(E(Y)\\) to be the same as \\(f(E(\\mathbf{X}))\\).\nSome further comments.\nThe examples in the previous section have already hinted at a probabilistic approach, but we will now consider this more formally. Given an uncertain \\(\\mathbf{X}\\), the aim is to quantify uncertainty about \\(Y=f(\\mathbf{X})\\) by deriving a probability distribution for \\(Y\\). This first requires a probability distribution for \\(\\mathbf{X}\\). If suitable data are available that inform us about \\(\\mathbf{X}\\), we may derive a (posterior) probability distribution for \\(\\mathbf{X}\\) using Bayesian inference. Without data, we may instead use expert judgement (so that we simply have a prior distribution for \\(\\mathbf{X}\\)).\nWe now have a transformation of a random variable \\(Y=f(\\mathbf{X})\\). In very simple cases, given a density function for \\(\\mathbf{X}\\), we may be able to derive the density function for \\(Y\\), but typically, this will not be possible. (At the very least, we would need a closed form expression for \\(f\\)). We can instead use a Monte Carlo technique.\nBecause each \\(Y_i\\) is a random draw from the distribution of \\(Y\\), we must have \\(E(Y_i)=E(Y)\\) and \\(P(Y_i\\le y)= P(Y \\le y)\\) (in addition to \\(Var(Y_i)=Var(Y)\\)), so the estimators above are unbiased: \\[\\begin{align}\nE\\{\\hat{E}(Y)\\}&=\\frac{1}{N}\\sum_{i=1}^NE(Y_i)\\\\&=\\frac{1}{N}\\sum_{i=1}^NE(Y)\\\\&=E(Y), \\\\\nE\\{\\hat{P}(Y\\le y )\\}&=\\frac{1}{N}\\sum_{i=1}^NE\\{I(Y_i\\le y)\\}\\\\&=\\frac{1}{N}\\sum_{i=1}^NP(Y_i\\le y)\\\\\n&=\\frac{1}{N}\\sum_{i=1}^NP(Y\\le y)\\\\&=P(Y\\le y).\n\\end{align}\\] The variances of the estimators decrease as we increase \\(N\\): \\[\\begin{align}\nVar\\{\\hat{E}(Y)\\}&=\\frac{1}{N^2}\\sum_{i=1}^NVar(Y_i)=\\frac{1}{N^2}\\sum_{i=1}^NVar(Y)=\\frac{Var(Y)}{N}, \\\\\nVar\\{\\hat{P}(Y\\le y )\\}&=\\frac{1}{N^2}\\sum_{i=1}^NVar\\{I(Y_i\\le y)\\}\\\\&=\\frac{1}{N^2}\\sum_{i=1}^NP(Y_i\\le y)\\{1-P(Y_i\\le y)\\}\\\\\n&=\\frac{1}{N^2}\\sum_{i=1}^NP(Y\\le y)\\{1-P(Y\\le y)\\}\\\\ &=\\frac{P(Y\\le y)\\{1-P(Y\\le y)\\}\n}{N}.\n\\end{align}\\] If \\(N\\) is large, we can construct approximate confidence intervals based on the central limit theorem. For example, an approximate 95% confidence interval for \\(E(Y)\\) would be \\[\\hat{E}(Y) \\pm 1.96 \\sqrt{\\frac{\\widehat{Var}(Y)}{N}},\\] where \\[\\widehat{Var}(Y)=\\frac{1}{N-1}\\sum_{i=1}^N\\{Y_i-\\hat{E}(Y)\\}^2.\\\\\\]\nset.seed(61003)\nN &lt;- 1000 \nx &lt;- rnorm(N)\ny &lt;- x^2\n# Estimate E(Y)\nmean(y)\n\n[1] 1.046482\n# Confidence interval for E(Y)\ns.e &lt;- (var(y) / N)^0.5\nc(mean(y) - 1.96 * s.e, mean(y) + 1.96 * s.e)\n\n[1] 0.9493342 1.1436297\n# Estimate  P(Y&lt;1)\nmean(y &lt; 1)\n\n[1] 0.677\n# Confidence interval for P(Y&lt;1)\ns.e &lt;- (mean(y &lt; 1) * mean(y &gt;= 1) / N)^0.5\nc(mean(y &lt; 1) - 1.96 * s.e, mean(y &lt; 1) + 1.96 * s.e)\n\n[1] 0.6480164 0.7059836\nIncreasing N will give better results: the confidence intervals will be narrower, and we expect the estimates to be closer to the true values.",
    "crumbs": [
      "Part III: self-study topics for MPS4101",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Uncertainty analysis</span>"
    ]
  },
  {
    "objectID": "Uncertainty-analysis.html#probabilistic-uncertainty-analysis",
    "href": "Uncertainty-analysis.html#probabilistic-uncertainty-analysis",
    "title": "17  Uncertainty analysis",
    "section": "",
    "text": "Generate a random sample \\(\\mathbf{X}_1,\\ldots,\\mathbf{X}_N\\) from the distribution of \\(\\mathbf{X}\\).\nEvaluate \\(Y_1=f(\\mathbf{X}_1),\\ldots,Y_N=f(\\mathbf{X}_N)\\).\nUse the sample \\(Y_1,\\ldots,Y_N\\) to estimate summaries of the distribution of \\(Y\\). For example, estimate \\(E(Y)\\) and \\(P(Y\\le y)\\) by \\[\\begin{aligned}\n\\hat{E}(Y)&=\\frac{1}{N}\\sum_{i=1}^NY_i,\\\\\n\\hat{P}(Y\\le y )&=\\frac{1}{N}\\sum_{i=1}^NI(Y_i\\le y),\\\\\n\\end{aligned}\\] where \\(I()\\) is the indicator function.\n\n\n\n\n\n\n\n\nNoteExample\n\n\n\n\nExample 17.3 Continuing Example 17.1, we show how this works in R. We estimate \\(E(Y)\\) and \\(P(Y&lt;1)\\) (which have true values of 1 and 0.683 to 3 d.p.)",
    "crumbs": [
      "Part III: self-study topics for MPS4101",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Uncertainty analysis</span>"
    ]
  },
  {
    "objectID": "Uncertainty-analysis.html#probabilistic-uncertainty-analysis-in-health-economic-evaluation",
    "href": "Uncertainty-analysis.html#probabilistic-uncertainty-analysis-in-health-economic-evaluation",
    "title": "17  Uncertainty analysis",
    "section": "17.2 Probabilistic uncertainty analysis in health economic evaluation",
    "text": "17.2 Probabilistic uncertainty analysis in health economic evaluation\nThe above technique is used in many disciplines. One area where it is (almost) a requirement is in health economic evaluations conducted for the National Institute for Health and Care Excellence (NICE). Below are some extracts from their guidance:1 (although they have used the term “probabilistic sensitivity analysis” to mean probabilistic uncertainty analysis. We will use the term sensitivity analysis to mean something else).\nSome extracts for the guidance are as follows.\n\nA third source of uncertainty comes from parameter precision, once the most appropriate sources of information have been identified […] Assign distributions to characterise the uncertainty associated with [parameter/input] values. The distributions chosen for probabilistic sensitivity analysis should not be chosen arbitrarily but chosen to represent the available evidence on the parameter of interest, and their use should be justified\n\n\nWhen doing a probabilistic analysis, enough model simulations should be used to minimise the effect of Monte Carlo error. Reviewing the variance around probabilistic model outputs […] as the number of simulations increases can provide a way of assessing if the model has been run enough times or more runs are needed.\n\n\nUsing univariate and best- or worst-case sensitivity analysis [one-at-a-time experiments] is an important way of identifying parameters that may have a substantial effect on the cost-effectiveness results and of explaining the key drivers of the model. However, such analyses become increasingly unhelpful in representing the combined effects of multiple sources of uncertainty as the number of parameters increase. Using probabilistic sensitivity analysis can allow a more comprehensive characterisation of the parameter uncertainty associated with all input parameters.",
    "crumbs": [
      "Part III: self-study topics for MPS4101",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Uncertainty analysis</span>"
    ]
  },
  {
    "objectID": "Uncertainty-analysis.html#case-study",
    "href": "Uncertainty-analysis.html#case-study",
    "title": "17  Uncertainty analysis",
    "section": "17.3 Case study",
    "text": "17.3 Case study\nWe consider a (modified version of a) cost-effectiveness model that was developed to support the National Institute for Health and Care Excellence physical activity guidance2. The model predicts the “incremental net benefit” of an intervention: exercise on prescription (e.g. from a general medical practitioner) to promote physical activity against a ‘do nothing’ scenario. The model assumes that the intervention affects health by reducing the risks of three diseases: coronary heart disease, stroke and diabetes. The health effects included in the model are those that relate to these three diseases, and the model counts costs that accrue as a result of the treatment of the three diseases, as well as those that relate to the intervention itself.\nThe incremental net benefit describes the value of the intervention per patient treated. Health benefits are converted into money, on an assumption that the NHS can afford to spend up to £20000 to achieve one extra year of life in perfect health for one patient. A positive incremental net benefit means the intervention is cost effective; a negative incremental net benefit means the money would be better spent on something else. Incremental net benefit is modelled for a cohort of patients with average age 50, at the time of the intervention.\nThe model has 24 uncertain inputs \\(X_1,\\ldots,X_{24}\\). We do not list them all here, but examples include the relative risk of a stroke for an active versus a sedentary population, and the probability of new exercise in the intervention group. Further details and R code are available on Blackboard.\nWe illustrate a Monte Carlo uncertainty analysis. 100,000 random inputs are drawn from the distribution of \\(\\mathbf{x}\\). The model is run at each input, to obtain a sample \\(y_1,\\ldots,y_{100000}\\) from the distribution of \\(Y\\). Note the interpretation of \\(Y\\) here: it is the mean net benefit per patient treated. It is a fixed, uncertain quantity, because it is a function of 24 input values that are unknown.\nWe find \\[\\begin{align}\n\\hat{E}(Y)&=\\frac{1}{100000}\\sum_{i=1}^{100000}y_i=248.5\\\\\n\\hat{P}(Y\\le 0 )&=\\frac{1}{100000}\\sum_{i=1}^{100000}I(y_i\\le 0)=0.225,\n\\end{align}\\] with 95% confidence intervals for \\(E(Y)\\) and \\(P(Y\\le 0)\\) given by (246.4, 250.6) and (0.223, 0.228) respectively, suggesting that \\(N=100000\\) is sufficiently large for estimating \\(E(Y)\\) and \\(P(Y\\le 0)\\) reliably. Hence, although we may estimate that the intervention is cost-effective, based on \\(E(Y)\\), we are fairly uncertain about this; our probability that the intervention is not cost-effective is about 20%.\nTo visualise uncertainty about \\(Y\\), we can plot a (kernel) density estimate using the sample \\(y_1,\\ldots,y_{100000}\\). We mark on the estimated 2.5th and 97.5th percentiles as vertical dashed lines.",
    "crumbs": [
      "Part III: self-study topics for MPS4101",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Uncertainty analysis</span>"
    ]
  },
  {
    "objectID": "Uncertainty-analysis.html#tasks",
    "href": "Uncertainty-analysis.html#tasks",
    "title": "17  Uncertainty analysis",
    "section": "17.4 Tasks",
    "text": "17.4 Tasks\nThe following simple model describes the concentration of pollutant at any point in a region following a release from a point source \\[\nC(x,y,z,\\boldsymbol{\\theta})=\\frac{Q}{2\\pi\nu_{10}\\sigma_z\\sigma_y}\\exp\\left[-\\frac{1}{2}\\left\\{\\frac{y^2}{\\sigma_y^2}+\\frac{(z-h)^2}{\\sigma_z^2}\\right\\}\\right],\n\\] where \\((x,y,z)\\) are the coordinates along the wind direction, cross wind and above ground respectively, and \\(\\boldsymbol{\\theta}\\) is a vector of model parameters:\n\n\\(C\\) is air concentration of the pollutant;\n\\(Q\\) is the rate of emission of the pollutant;\n\\(u_{10}\\) is the wind speed at \\(10m\\) above ground;\n\n\\(\\sigma_y\\) and \\(\\sigma_z\\) give the diffusion in the horizontal and vertical directions respectively;\n\\(h\\) is the release height.\n\nThe pollutant is released from a source 50m above ground, with an emission rate of 100 units. However, the wind speed and diffusion parameters \\(\\sigma_y\\) and \\(\\sigma_z\\) are not known exactly, and are considered to be random variables with lognormal distributions: \\[\\begin{align*}\n\\log u_{10}&\\sim  N(2,.1) \\\\ \\log \\sigma_y^2 &\\sim  N(10,0.2)\n\\\\ \\log \\sigma_z^2 &\\sim  N(5,0.05)\n\\end{align*}\\]\nIn R, write a Monte Carlo simulation to estimate the median and 95th percentile of the concentration at coordinates \\(x=100, y= 100,z=40\\). Use the quantile() function to obtain percentiles from samples. Produce a suitable plot to visualise uncertainty about the concentration.\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\nR code to implement the Monte Carlo simulation\n\n# Choose a Monte Carlo sample size \nN &lt;- 10000\n\n# sample diffusion parameters \nsy &lt;- rlnorm(N, 10, 0.2)\nsz &lt;- rlnorm(N, 5, 0.05)\n\n# sample wind speed:\nu &lt;- rlnorm(N, 2, 0.1)\n\n# Evaluate concentration at sampled parameters:\nconcentration &lt;- (100 / (2 * pi * u * (sz * sy)^0.5)*\n                    exp(-0.5 * (100^2 / sy + (40 - 50)^2 / sz)))\n# obtain median and 95th percentile\nquantile(concentration, c(0.5, 0.95))\n\n         50%          95% \n0.0006752745 0.0008125794 \n\n\nAs this the code can be run quickly, we can try much larger values of N to check that eh Monte Carlo sample size is sufficiently large.\nTo visualise uncertainty, we can use a histogram to show the distribution of concentration values:\n\nhist(concentration)",
    "crumbs": [
      "Part III: self-study topics for MPS4101",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Uncertainty analysis</span>"
    ]
  },
  {
    "objectID": "Uncertainty-analysis.html#footnotes",
    "href": "Uncertainty-analysis.html#footnotes",
    "title": "17  Uncertainty analysis",
    "section": "",
    "text": "National Institute for Health and Care Excellence (2022). NICE health technology evaluations: the manual. Available at https://www.nice.org.uk/process/pmg36. See section 4.7 Exploring Uncertainty↩︎\nNational Institute for Clinical Excellence (2006) Four commonly used methods to increase physical activity: PH2. Technical Report. National Institute for Clinical Excellence, London. (Available from http://www. nice.org.uk/PH2.) The version used in this case study is the model described in Strong, M., Oakley J. E. and Chilcott, J. (2012). Managing structural uncertainty in health economic decision models: a discrepancy approach. Journal of the Royal Statistical Society, Series C, 61(1), 25-45. Thanks to Mark Strong for providing the R code.↩︎",
    "crumbs": [
      "Part III: self-study topics for MPS4101",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Uncertainty analysis</span>"
    ]
  },
  {
    "objectID": "Sensitivity-analysis.html",
    "href": "Sensitivity-analysis.html",
    "title": "18  Sensitivity analysis",
    "section": "",
    "text": "18.1 Functional ANOVA and variance decompositions\nThe term “sensitivity analysis” can be used to mean different things, but broadly refers to investigating how predictions or inferences might change if we change particular assumptions (for example, investigating whether the prediction from a computer model changes if we change an input or parameter value). Here, we use the term sensitivity analysis to mean a specific extension of the probabilistic uncertainty analysis problem.\nWe consider a computer model with a scalar output \\(y\\) (or we investigate each element of a vector output separately) and a vector of inputs \\(\\mathbf{x}\\). We suppose that there is a true, uncertain input \\(\\mathbf{X}\\), and we are interested in the value of \\(Y=f(\\mathbf{X})\\). Writing \\(\\mathbf{X}=(X_1,\\ldots, X_d)\\), the aim of a probabilistic or global sensitivity analysis is to understand how individual elements of \\(\\mathbf{X}\\) contribute to the uncertainty in \\(Y\\). In particular, if we could commission a study to learn the true value of an input (or at least reduce uncertainty about an input), which input would be best to learn about? Is there a particular subset of inputs that we should be trying to learn about?\nWe first state a useful identity (a proof is given at the end). For any two random variables \\(A\\) and \\(B\\), \\[Var(A)=Var_B\\{E(A|B)\\} + E_B\\{Var(A|B)\\} \\tag{18.1}\\] Suppose we could learn the true value of one input. Which would be the best input to choose? One option is to consider a reduction in variance. Without any further information we have \\(Var(Y)\\) as our current variance of the output. Note the interpretation here: if \\(\\mathbf{X}\\) is a fixed quantity, so is \\(Y\\). By reducing variance, we mean we are reducing uncertainty about \\(Y\\); we are not making \\(Y\\) ‘less variable’ (though one can think in those terms if \\(\\mathbf{X}\\) is a randomly varying quantity, and there are options to reduce variability in the inputs).\nIf we knew the true value of \\(X_i\\), our variance would change to \\(Var(Y|X_i)\\), so the reduction in variance would be \\[Var(Y) - Var(Y|X_i).\\] But we don’t yet know what the true value of \\(X_i\\) is going to be; the reduction in variance is uncertain. We could instead consider the expected reduction in variance: \\[Var(Y) - E_{X_i}\\{Var(Y|X_i)\\} = Var_{X_i}\\{E(Y|X_i)\\},\\] by the variance identity (Equation 18.1). To understand the notation, note that \\[E(Y|X_i) = \\int_{\\mathcal{X}_{-i}} f(X_i,\\mathbf{x}_{-i})p_{\\mathbf{X}_{-i}|X_i}(\\mathbf{x}_{-i})d\\mathbf{x}_{-i},\\] where the subscript \\(-i\\) refers to all elements apart from the \\(i\\)th element, and \\(\\mathcal{X}_{-i}\\) is the sample space of \\(\\mathbf{X}_{-i}\\). As we have integrated out \\(\\mathbf{X}_{-i}\\), we have \\(E(Y|X_i)\\) as a function of \\(X_i\\) only, so we then evaluate \\(Var_{X_i}\\{E(Y|X_i)\\}\\), the variance of this function with respect to \\(X_i\\).\nWe don’t have to assume the inputs are independent, and we can apply the same argument to groups of inputs. The expected reduction in variance from learning a subset of inputs \\(\\mathbf{X}_u\\) is \\[Var_{\\mathbf{X}_u}\\{E(Y|\\mathbf{X}_u)\\}\\]\nWe define \\[Var_{X_i}\\{E(Y|X_i)\\},\\] to be the main effect variance of input \\(X_i\\), and \\[\\frac{Var_{X_i}\\{E(Y|X_i)\\}}{Var(Y)},\\] to be the main effect index of input \\(X_i\\). Other names for main effect index include first order sensitivity index, Sobol’ index, and correlation ratio for input \\(X_i\\).\nFor independent inputs, we can decompose \\(Var(Y)\\) into main effect variances, two-way interaction variances, three-way interaction variances and so on. We first write \\[\nf(\\mathbf{x})=f_0+\\sum_{i=1}^df_i(x_i)+\\sum_{i&lt;j}f_{i,j}(x_i,x_j)+\\\\ \\sum_{i&lt;j&lt;k}f_{i,j,k}(x_i,x_j,x_k)+\\ldots+f_{1,\\ldots,d}(x_1,\\ldots,x_d).\n\\tag{18.2}\\] This is trivial, in that the last term is a function of all the inputs, so we could choose to set \\(f_{1,\\ldots,d}(x_1,\\ldots,x_d)=f(\\mathbf{x})\\) and set all the other terms equal to zero. We now define \\[\\begin{align}\nf_0&=E\\{f(\\mathbf{X})\\},\\\\\nf_i(x_i)&=E\\{f(\\mathbf{X})|X_i=x_i\\}-f_0,\\label{fi}\\\\\nf_{i,j}(x_i,x_j)&=E\\{f(\\mathbf{X})|X_i=x_i,X_j=x_j\\}-f_i(x_i)-f_j(x_j)-f_0\\\\\nf_{i,j,k}(x_i,x_j,x_k)&=E\\{f(\\mathbf{X})|X_i=x_i,X_j=x_j,X_k=x_k\\}-f_{i,j}(x_i,x_j)-f_{i,k}(x_i,x_k)\\nonumber\\\\\n&-f_{j,k}(x_j,x_k)-f_i(x_i)-f_j(x_j)-f_k(x_k)-f_0,\n\\end{align}\\] and so on. Inspecting, for example, the definition of \\(f_{i,j}(x_i,x_j)\\), we can think of this term as representing an additional contribution to \\(f(\\mathbf{x})\\), from the interaction between \\(x_i\\) and \\(x_j\\), on top of the contributions from the single input functions \\(f_i(x_i)\\) and \\(f_j(x_j)\\). Note also that, following this construction, \\(f_{1,\\ldots,d}(x_1,\\ldots, x_d)\\) is simply \\(f(\\mathbf{x})\\), minus all the other terms on the RHS of Equation 18.2, so again, it is trivial to see that the decomposition holds.\nNow, with \\(Y=f(\\mathbf{X})\\), write \\[\\begin{align}\nY=f(\\mathbf{X})&=f_0+\\sum_{i=1}^df_i(X_i)+\\sum_{i&lt;j}f_{i,j}(X_i,X_j)\\nonumber\\\\\n&+\\sum_{i&lt;j&lt;k}f_{i,j,k}(X_i,X_j,X_k)+\\ldots+f_{1,\\ldots,d}(X_1,\\ldots,X_d).\n\\end{align}\\] If inputs in \\(\\mathbf{X}\\) are independent, we find that all the terms above are independent, and \\[\nVar\\{Y\\}=\\sum_{i=1}^dVar\\{f_i(X_i)\\}+\\sum_{i&lt;j}Var\\{f_{i,j}(X_i,X_j)\\}\\\\\n+\\sum_{i&lt;j&lt;k}Var\\{f_{i,j,k}(X_i,X_j,X_k)\\}+\\ldots\\\\\\\n+Var\\{f_{1,\\ldots,d}(X_1,\\ldots,X_d)\\}.\n\\tag{18.3}\\] Noting the definition of \\(f_i(x_i)\\), the main effect variance for \\(X_i\\) is \\[Var\\{f_i(X_i)\\}=Var_{X_i}\\{E(Y|X_i)\\}\\}\\] We can define a two-way interaction variance for \\(X_i\\) and \\(X_j\\) as \\[Var\\{f_{i,j}(X_i,X_j)\\},\\] which gives the additional contribution to \\(Var(Y)\\) from \\(X_i\\) and \\(X_j\\), above their main effect contributions. We define a total effect variance for \\(X_i\\) as the sum of all terms in \\(Var(Y)\\) involving \\(X_i\\) and the total effect index for \\(X_i\\) as its total effect variance, divided by \\(Var(Y)\\). We",
    "crumbs": [
      "Part III: self-study topics for MPS4101",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Sensitivity analysis</span>"
    ]
  },
  {
    "objectID": "Sensitivity-analysis.html#main-effect-and-total-effect-variances",
    "href": "Sensitivity-analysis.html#main-effect-and-total-effect-variances",
    "title": "18  Sensitivity analysis",
    "section": "18.2 Main effect and total effect variances",
    "text": "18.2 Main effect and total effect variances\nAn input can have a small main effect variance, but a large total effect variance. A small main effect variance doesn’t imply ‘unimportant’, but a small total effect variance does imply ‘unimportant’. We can calculate total effect variance \\(V_i^T\\) for input \\(X_i\\) via (joint) main effect variance of remaining inputs \\[Var(Y)=Var_{\\mathbf{X}_{-i}}\\{E(Y|\\mathbf{X}_{-i})\\} + V^T_i,\\] The effect of calculating \\(Var_{\\mathbf{X}_{-i}}\\{E(Y|\\mathbf{X}_{-i})\\}\\) is to sum all the terms in Equation 18.3 that do not include input \\(X_i\\). The total effect index for input \\(X_i\\) is therefore \\[1-\\frac{Var_{\\mathbf{X}_{-i}}\\{E(Y|\\mathbf{X}_{-i})\\}}{Var(Y)}.\\] We can therefore interpret the total effect index for \\(X_i\\) as the expected proportion of variance remaining if we were to learn the true value of all inputs except \\(X_i\\).\n\n\n\n\n\n\nImportant\n\n\n\nTotal effect indices are only meaningful for independent inputs; Equation 18.3 assumes independence between inputs. Main effect indices are valid (their interpretation holds) regardless of whether the inputs are independet or not.\n\n\nDependent inputs \\(\\mathbf{X}_u\\) should be treated as a single group, so that one would only calculate a total effect index for the whole group. We sum all terms in Equation 18.3 involving any elements of \\(\\mathbf{X}_u\\). This corresponds to calculating \\[1-\\frac{Var_{\\mathbf{X}_{-u}}\\{E(Y|\\mathbf{X}_{-u})\\}}{Var(Y)}.\\]\n\n\n\n\n\n\nNoteExample\n\n\n\n\nExample 18.2 Continuing Example 18.1, consider \\[Y=1.1X_1 + X_2 +X_1X_2+X_3^2\\] with \\[X_1\\sim N(0,4),\\quad X_2\\sim U[-4,4],\\quad X_3\\sim N(0,3).\\] We can partition the variance as follows.\n\n\n\n\n\\(Var\\{f_1(X_1)\\}\\)\n4.8\n\n\n\\(Var\\{f_2(X_2)\\}\\)\n5.3\n\n\n\\(Var\\{f_3(X_3)\\}\\)\n18.0\n\n\n\\(Var\\{f_{1,2}(X_1,X_2)\\}\\)\n21.3\n\n\n\\(Var\\{f_{1,3}(X_1,X_3)\\}\\)\n0.0\n\n\n\\(Var\\{f_{2,3}(X_2,X_3)\\}\\)\n0.0\n\n\n\\(Var\\{f_{1,2,3}(X_1,X_2,X_3)\\}\\)\n0.0\n\n\n\\(Var(Y)\\)\n49.5\n\n\n\n\nThe total effect variance for \\(X_1\\) is \\(4.8+21.3\\). Note that we can obtain this via \\[49.4 - Var_{X_2,X_3}\\{E(Y|X_2,X_3)\\} = 49.4 - Var(X_2 + X_3^2) = 49.4-64/12 - 18 = 26.1\\] (note \\(E(X_3^4)=27\\)).",
    "crumbs": [
      "Part III: self-study topics for MPS4101",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Sensitivity analysis</span>"
    ]
  },
  {
    "objectID": "Sensitivity-analysis.html#computation",
    "href": "Sensitivity-analysis.html#computation",
    "title": "18  Sensitivity analysis",
    "section": "18.3 Computation",
    "text": "18.3 Computation\n\n18.3.1 Using Monte Carlo\nPartition the input \\(\\mathbf{X}\\) into \\((\\mathbf{X}_u,\\mathbf{X}_{-u})\\). A Monte Carlo procedure for calculating \\(Var_{\\mathbf{X}_u}\\{E(Y|\\mathbf{X}_u)\\}\\) is as follows.\n\nSample a value \\(\\mathbf{X}_{u,j}\\) from the distribution of \\(\\mathbf{X}_u\\).\n\nSample a set of values \\(\\mathbf{X}_{-u,1},\\ldots,\\mathbf{X}_{-u,K}\\) from the distribution of \\(\\mathbf{X}_{-u}|\\mathbf{X}_u=\\mathbf{X}_{u,j}.\\)\nEstimate \\(E(Y|\\mathbf{X}_u=\\mathbf{X}_{u,j})\\) by \\[m_j=\\frac{1}{K}\\sum_{k=1}^K f(\\mathbf{X}_{u,j},\\mathbf{X}_{-u,k}).\\]\n\nRepeat for \\(j=1,\\ldots,J\\) and estimate \\(Var_{\\mathbf{X}_u}\\{E(Y|\\mathbf{X}_u)\\}\\) by \\[\\frac{1}{J-1}\\sum_{j=1}^J(m_j - \\bar{m})^2,\\] where \\(\\bar{m} = \\frac{1}{J}\\sum_{j=1}^Jm_j.\\)\n\nThis can be computationally expensive, in that large values of \\(J\\) and \\(K\\) may be needed to get accurate estimates. An R implementation for Example 18.2 is available on Blackboard.\n\n\n18.3.2 Using regression\nWe first consider estimating a main effect variance \\(Var_{X_i}\\{E(Y|X_i)\\}\\). We will shortly see how to do this, using regression, and a sample of inputs and outputs generated for a probabilistic uncertainty analysis. To illustrate this, we consider again Example 18.1. Suppose we have conducted a Monte Carlo uncertainty analysis:\n\nWe have generated a sample of \\(N\\) values from the distribution of \\(\\mathbf{X}\\). Denote the observed values of this sample by \\(\\mathbf{x}_1,\\ldots,\\mathbf{x}_N\\), with \\(\\mathbf{x}_i=(x_{1,i},x_{2,i},x_{3,i})\\).\nWe evaluate \\(y_i=f(\\mathbf{x}_i)\\) for \\(i=1,\\ldots,N\\) to get a sample from the distribution of \\(Y\\).\n\nNow suppose we want to calculate \\(Var_{X_3}\\{E(Y|X_3)\\}\\). We can think of \\(E(Y|X_3=x_3)\\) as a function of \\(x_3\\), which we will write as \\(g(x_3)\\). We could estimate \\(Var\\{g(X_3)\\}\\) using the sample variance of \\(g(x_{3,1}),\\ldots,g(x_{3,N})\\), if we could work out how to evaluate (or estimate) \\(g(x_{3,i})\\) for each \\(i\\). In this particular example, by looking at the function, we can see that \\(g(x_3)=E(Y|X_3=x_3)=x_3^2\\). Now let’s try plotting \\(y_i\\) against \\(x_{3,i}\\), together with the function \\(g(x_3^2)=x_3^2\\).\n\n\n\n\n\n\n\n\n\nCould we have used the data to estimate the function \\(g(x_3)\\)? Yes: this is a regression problem. The observed \\((y_i,x_{3,i})\\) pairs can be related via \\[y_i=g(x_{3,i})+\\varepsilon_i,\\] If we consider a random pair \\((Y,X_3)\\) pair with corresponding error term \\(\\varepsilon\\), we have \\[E(Y)=E\\{g(X_3)\\}+E(\\varepsilon),\\] and \\[E_{X_3}\\{g(X_3)\\}=E_{X_3}\\{E(Y|X_3)\\}=E(Y).\\] so that \\(E(\\varepsilon)=0\\). (Note that the variance of \\(\\varepsilon\\) is not constant for all \\(x_3\\), which ideally we would account for, though the aim here is merely to estimate \\(g\\); we are not trying to do any formal inference).\nWe can then estimate \\(g(x_{3,1}),\\ldots,g(x_{3,N})\\) by estimating the function \\(g\\), and calculating the fitted values, with the variance of the fitted values giving us an estimate of \\(Var_{X_3}\\{E(Y|X_3)\\}\\). We could use ordinary least squares regression to estimate \\(g\\), though there are other methods that can be use. (In the R code, we use a technique called generalised additive modelling (GAM), which is a more flexible regression technique. The details of GAM are outside the scope of this module).\nWe can calculate variance contributions for groups of inputs, e.g \\(X_1,X_2\\), by fitting regression models for \\(y\\) on \\(x_1\\) and \\(x_2\\), to estimate \\(E(Y|X_1=x_1,X_2=x_2)\\) as a function of \\(x_1\\) and \\(x_2\\). We can then use the sample of fitted values to estimate \\(Var_{X_1,X_2}\\{E(Y|X_1,X_2)\\}\\).\nR code implementations for Example 18.2 using generalised additive modelling are available on Blackboard.\n\n\n\n\n\n\nNoteExample\n\n\n\n\nExample 18.3 For the Case Study model \\(X_{11}, X_{12}, X_{13}\\) and \\(X_{14}\\) represent proportions of the target population taking up exercise, and maintaining their exercise, in the control and intervention groups. Below are estimates of the total effect indices for various combinations of \\(X_{11}, X_{12}, X_{13}\\) and \\(X_{14}\\)\n\n\n\n\n\\(\\mathbf{X}_u\\)\n\\(Var_{\\mathbf{X}_u}\\{E(Y|\\mathbf{X}_u)\\}/Var(Y)\\)\n\n\n\\(X_{11}\\)\n0.10\n\n\n\\(X_{12}\\)\n0.15\n\n\n\\(X_{13}\\)\n0.01\n\n\n\\(X_{14}\\)\n0.10\n\n\n\\(X_{11},X_{12}\\)\n0.49\n\n\n\\(X_{13},X_{14}\\)\n0.26\n\n\n\\(X_{11},X_{12},X_{13},X_{14}\\)\n0.78\n\n\n\n\nSo, for example, the expected reduction in the variance of the net benefit from learning \\(X_{13}\\) only is 1%, and the expected reduction in variance from learning \\(X_{14}\\) only is 10%, but the expected reduction in variance from learning both \\(X_{13}\\) and \\(X_{14}\\) is about 25%. Learning all four inputs (\\(X_{11},X_{12},X_{13},X_{14}\\)) out of the 24 is expected to reduce the variance of \\(Y\\) by about 80%. There are other uncertain inputs, such as the relative risk of developing diabetes, for exercising versus sedentary adults, that do not contribute substantially to the uncertainty in the net benefit; their total effect indices are relatively small.",
    "crumbs": [
      "Part III: self-study topics for MPS4101",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Sensitivity analysis</span>"
    ]
  },
  {
    "objectID": "Sensitivity-analysis.html#proof-of-the-variance-identity",
    "href": "Sensitivity-analysis.html#proof-of-the-variance-identity",
    "title": "18  Sensitivity analysis",
    "section": "18.4 Proof of the variance identity",
    "text": "18.4 Proof of the variance identity\nFor any two random variables \\(A\\) and \\(B\\) (with density functions \\(p_A(a)\\) and \\(p_B(b)\\) and sample spaces \\(\\mathcal{X}_A\\) and \\(\\mathcal{X}_B\\)), \\[Var(A)=Var_B\\{E(A|B)\\} + E_B\\{Var(A|B)\\}.\\label{varident2}\\] To prove this, we first prove the ‘tower property’ of expectations: \\[\\begin{align}\nE_B\\{E(A|B)\\}&=\\int_{\\mathcal{X}_B} E(A|B=b) p_B(b) db\\\\\n&=\\int_{\\mathcal{X}_B} \\int_{\\mathcal{X}_A} a p_{A|B}(a|b) p_B(b) da\\,db\\\\\n&=\\int_{\\mathcal{X}_A} a \\int_{\\mathcal{X}_B}  p_{A,B}(a,b)db\\, da\\\\\n&=\\int_{\\mathcal{X}_A} a   p_{A}(a) da\\\\\n&=E(A).\n\\end{align}\\] Now, using the tower property, \\[\\begin{align}\nVar(A)&= E(A^2) - E(A)^2\\nonumber\\\\\n&= E_B\\{E(A^2|B)\\} -E(A)^2,\\label{v1}\n\\end{align}\\] and \\[\\begin{align}\nVar_B\\{E(A|B)\\}&= E_B[\\{E(A|B)\\}^2)] - \\left[E_B\\{E(A|B)\\}\\right]^2\\nonumber\\\\\n&=E_B[\\{E(A|B)\\}^2] - E(A)^2,\\label{v2}\n\\end{align}\\] and \\[E_B\\{Var(A|B)\\}= E_B\\{E(A^2|B)\\}-E_B[\\{E(A|B)\\}^2]\\label{v3}\\] Adding these expressions for \\(E_B\\{Var(A|B)\\}\\) and \\(Var_B\\{E(A|B)\\}\\) gives \\(Var(A)\\), which proves the identity.",
    "crumbs": [
      "Part III: self-study topics for MPS4101",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Sensitivity analysis</span>"
    ]
  },
  {
    "objectID": "Sensitivity-analysis.html#tasks",
    "href": "Sensitivity-analysis.html#tasks",
    "title": "18  Sensitivity analysis",
    "section": "18.5 Tasks",
    "text": "18.5 Tasks\n\nA computer model is given by the function \\[\nY=X_1^2+ 2 X_2X_3.\n\\] The true values of the three inputs are uncertain, with \\(X_1 \\sim N(6,9)\\), \\(X_2 \\sim U[9,14]\\) and \\(X_3 \\sim N(9,1)\\), with \\(X_1,X_2,X_3\\) independent. The model user can choose to learn the true value of one of the inputs. The model user wishes to reduce the variance of the output \\(Y\\) as much as possible. Calculate the expected reduction in variance by learning each of \\(X_1\\), \\(X_2\\) and \\(X_3\\) separately and hence suggest which input the user should choose to learn and why.\nSuppose instead that \\(X_1\\) now has a gamma, Ga(\\(\\alpha\\),\\(\\beta\\)), distribution with shape and rate parameters \\(\\alpha = 12\\) and \\(\\beta = 2\\) respectively. Note that the probability density function for a random variable \\(X\\) distributed according to a gamma distribution with shape parmeter \\(\\alpha\\) and rate parameter \\(\\beta\\) is \\[\nf_X(x) = \\frac{\\beta^ \\alpha}{\\Gamma(\\alpha)}x^{\\alpha - 1}\\exp{(-\\beta x)}.\n\\] \\(X_2\\) and \\(X_3\\) keep the same distribution as in Part 1. Use the regression method in to estimate the expected reduction in variance obtained by learning \\(X_1\\).\nNow suppose that there is a single input, \\(X\\), and the output, \\(Y\\), is given by \\[\nY=2X+ 3\n\\] where \\(X\\) is uncertain with \\(X \\sim N(\\alpha, 9)\\) and \\(\\alpha\\) is a random variable distributed according to \\(\\alpha \\sim \\text{N}(6,2)\\). What is the reduction in the variance of Y from knowing the value of \\(\\alpha\\)? Why do we not need to calculate the expected reduction in the variance of Y?\n\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\n\nWe use the standard result \\(E(X^2) = \\text{var}(X)+E(X)^2\\) in many of the following calculations. We have \\[\\begin{align*}\nVar_{X_1}(E(Y|X_1))&=Var_{X_1}(X_1^2 + 2E(X_2X_3))\\\\\n&= Var_{X_1}(X_1^2)\\\\\n&=E_{X_1}(X_1^4)-E_{X_1}(X_1^2)^2\\\\\n&= 3483-(36+9)^2=1458.\\\\\nVar_{X_2}(E(Y|X_2))&=Var_{X_2}(45 + 2\\times 9 X_2)\\\\\n&=675.\\\\\nVar_{X_3}(E(Y|X_3))&=Var_{X_3}(45+2\\times 11.5 X_3)\\\\\n&= 529.\n\\end{align*}\\] The best input to learn is therefore \\(X_1\\), as it will give the largest expected reduction in variance.\nWe implement the regression method as follows.\n\n\nx1 &lt;- rgamma(100000, 12, 2) # Sample from the distribution of x1\nx2 &lt;- runif(100000, 9, 14) # Sample from the distribution of x2\nx3 &lt;- rnorm(100000, 9, 1) # Sample from the distribution of x3\ny &lt;- x1^2 + 2*x2*x3 # Evaluate the corresponding y for each sampled x1,x2,x3\ng1 &lt;- mgcv::gam(y ~ s(x1)) # Fit a GAM regression of y on x1\nvar(g1$fitted) # Calculate the variance of the fitted values\n\n[1] 533.133\n\n\n\nTo calculate Var(Y) we can use the variance identity. \\[\\begin{align*}\nVar(Y)&=E_\\alpha(Var(Y|\\alpha))+ Var_\\alpha(E(Y|\\alpha))\\\\\n&=E_\\alpha(4 Var(X))+Var_\\alpha(2\\alpha+3)\\\\\n&=4 \\times 9 +2^2 \\times Var(\\alpha)\\\\\n&=44\n\\end{align*}\\]\n\nWe can also check by simulation. We can calculate \\(\\text{var}(y)\\) via\n\nalpha &lt;- rnorm(100000, 6, sqrt(2))\nx &lt;- rnorm(100000, alpha, 3)\ny &lt;- 2 * x + 3\nvar(y)\n\n[1] 44.02133\n\n\nThe actual reduction in variance from knowing \\(\\alpha\\) is \\(Var(Y)-Var(Y|\\alpha)\\). If this reduction depended on the value of \\(\\alpha\\), we would take the expectation with respect to \\(\\alpha\\). But as \\(Var(Y|\\alpha)\\) does not depend on \\(\\alpha\\) (\\(Var(Y|\\alpha)=36\\)) so we don’t need to calculate the expected reduction in variance; we can simply calculate the actual reduction in variance.",
    "crumbs": [
      "Part III: self-study topics for MPS4101",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Sensitivity analysis</span>"
    ]
  }
]