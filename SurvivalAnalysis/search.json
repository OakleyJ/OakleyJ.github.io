[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Survival analysis",
    "section": "",
    "text": "Acknowledgements\nThese lecture notes are for students on MPS314 and MPS438: Medical Statistics. I have made a few changes to the notes this year, but the content has mainly been developed by others who have taught this content over the years: Kevin Walters, Tim Heaton and Nick Fieller.",
    "crumbs": [
      "Acknowledgements"
    ]
  },
  {
    "objectID": "background.html",
    "href": "background.html",
    "title": "1  Background and Basic Concepts",
    "section": "",
    "text": "1.1 What is survival analysis?\nIn survival analysis, we analyse time-to-event data: how long we have to wait for a particular event to occur. In medical statistics, the interest is often in how long a patient will survive on a particular treatment (e.g. a treatment for cancer), and so the “event” would be the death of a patient: in a survival analysis we analyse how long the patients live. We may have other explanatory variables for each observation, e.g. the age of each patient, and what treatment they received.\nA simple illustration of the type of data we consider in survival analysis is in Figure 1.1.\nFigure 1.1: A visualisation of a simple example dataset in survival analysis. We have two groups of observations, and for each observation in each group, we observe a “survival time”: how long it takes for some event to occur. The interest may be in whether the mean survival time is different between the two groups.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Background and Basic Concepts</span>"
    ]
  },
  {
    "objectID": "background.html#what-is-survival-analysis",
    "href": "background.html#what-is-survival-analysis",
    "title": "1  Background and Basic Concepts",
    "section": "",
    "text": "Note\n\n\n\nIn clinical trials, patients usually enter the trial at different times over some period, and so don’t all start treatment at the same time. We’ll ignore this in this module, and suppose observation of all patients starts at time 0 (or that the data have already been converted into this format).\n\n\n\n\n\n\n\n\nExample\n\n\n\nKoshiaris et al (2017) investigated how quitting smoking affects survival for patients with different cancers. They studied a group of patients who were all smokers at the time of their cancer diagnosis and survived for at least one year. For different cancer types (e.g. lung cancer), they then studied the survival times of those patients who quit smoking within their first year, and those patients who continued smoking, to see if patients tended to live longer if they gave up smoking.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Background and Basic Concepts</span>"
    ]
  },
  {
    "objectID": "background.html#terminology-survival-times-and-failure-times",
    "href": "background.html#terminology-survival-times-and-failure-times",
    "title": "1  Background and Basic Concepts",
    "section": "1.2 Terminology: survival times and failure times",
    "text": "1.2 Terminology: survival times and failure times\nIn survival analysis, we may refer to either the survival time or the failure time:\n\nfailure time: the time at which some event occurs;\nsurvival time: (a duration) how long we wait until the failure event.\n\nAs we start counting from time zero, numerically, failure time and survival time are the same, and the two terms can be used interchangeably.\n“Survival time” is more commonly used in medical statistics, as we are often studying treatments to improve patient survival. “Failure time” is more commonly used on non-medical applications, e.g. applications in engineering where we may study how long an time operates until a fault develops: the item “fails”.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Background and Basic Concepts</span>"
    ]
  },
  {
    "objectID": "background.html#censoring",
    "href": "background.html#censoring",
    "title": "1  Background and Basic Concepts",
    "section": "1.3 Censoring",
    "text": "1.3 Censoring\nIn a clinical trial, a patient may withdraw from the trial for reasons unrelated to their treatment, or they reach the end of the trial before the event has occurred (e.g. they are still alive at the end of the trial). This results in a censored observation: instead of observing the survival time, we just know that it must exceed some particular value. An illustration is given in Figure 1.2.\n\n\n\n\n\n\n\n\nFigure 1.2: Illustration of censoring. We suppose a study has been run for 6 months. For observation number 3, the patient was still alive after 2 months, but dropped out of the study. For observation number 5, the patient was still alive at the end of the study. For these two observations, rather than observing the failure times, we just know that the failure times were greater than 2 months and 6 months respectively.\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nCensoring is an important distinctive feature of survival analysis, and the reason we can’t use other methods you may be familiar (e.g. linear modelling) directly. In Figure 1.2, the observations we have to analyse are \\[\nt_1=0.5,\\,t_2=4.1,\\,t_4=2.75\\quad \\mbox{ and } \\quad t_3&gt;2, \\,t_5&gt;6\n\\] How, for example, would you estimate the mean survival time in this case?\n\n\n\n1.3.1 Types of Censoring\nThere are several forms of censoring:\n\nRight Censoring — the survival time exceeds some value. The event (e.g. death) hadn’t occurred when the individual was lost to the study or the study ended;\nLeft Censoring — the survival time is less than some value. This might occur if subjects are only observed after a fixed period of time. Here, we would know that survival time is less than the period of observation. Another example is when the survival time is the time to recurrence of a tumour, where the presence of such a tumour is only observable during surgery.\nInterval Censoring — the survival time is within some range of values. This might occur if individuals are observed at a sequence of fixed appointment times.\nType I Censoring — identical starting points and subjects are observed for a fixed time. The number of censored observations is then random.\nType II Censoring — the trial finishes after a certain number of events. Here we do not specify the end of the trial in terms of time but in terms of the number of events. This type of censoring is less common in medical studies but is widely used in electronic component testing and reliability studies. Here the number of censored observations is not random but is fixed in advance.\n\nWe typically consider Right censoring.\n\n\n1.3.2 Informative and Non-Informative Censoring\nFor all the techniques presented in this module we will assume that the censoring is random (or non-informative). This means that the censoring time is statistically independent of the failure time. This excludes situations whereby people are censored in a clinical trial if they suddenly become seriously ill and, as a consequence, are removed from the trial. Here censoring suggests that an individual is near death. It also excludes situations where they are censored/lost if they become/feel better. Here censoring would suggest that an individual is a long way from death. Examples such as these are known as informative sampling as the act of censoring gives extra information about the survival time.\nAnalysing informatively censored data is very difficult. If such informative censoring is present then the methods described here would often lead to biased estimates. In any survival analysis one should think carefully about whether the censoring is informative.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Background and Basic Concepts</span>"
    ]
  },
  {
    "objectID": "background.html#survival-time-distributions-key-definitions",
    "href": "background.html#survival-time-distributions-key-definitions",
    "title": "1  Background and Basic Concepts",
    "section": "1.4 Survival time distributions: key definitions",
    "text": "1.4 Survival time distributions: key definitions\nSuppose we have a homogeneous population of individuals, and the survival time of each individual is a continuous random variable \\(T&gt;0\\) with probability density \\(f(t)\\) and distribution function \\(F(t)\\): \\[\\begin{align}\nF(t) = P(T \\leq t) \\quad & \\textrm{and} \\quad f(t) = F'(t) \\qquad (\\textrm{so } F(t) = \\int_0^t f(u) du).\n\\end{align}\\]\nWe wish to estimate the distribution of \\(T\\) given, possibly censored, survival data. We first need to define some other functions which are all inter-related.\n\n1.4.1 Survivor Function\nWe define the survivor function \\(S(t)\\) as\n\\[\nS(t) = P(T \\geq t).\n\\]\nFor a continuous random variable \\(T\\), we have \\(P(T=t)=0\\), and so \\[\nS(t) = 1 - F(t) = \\int_t^\\infty f(u) du.\n\\]\nWe also have \\[f(t) = -S'(t).\\]\nWe tend to work with the survivor function \\(S(t)\\) rather than the distribution function \\(F(t)\\), as \\(S(t)\\) matches the form of right-censored observations where we observe a survival time \\(T\\) exceeding some value \\(t\\).\n\n\n1.4.2 Hazard function\nWe often model the lifetime through the hazard function, \\(h(t)\\), which measures the risk or proneness to death at time \\(t\\), given survival up to time \\(t\\). The hazard function represents the instantaneous death rate for an individual surviving to time \\(t\\). \\[\nh(t) := \\lim_{h \\rightarrow 0} \\frac{P(t \\leq T&lt;t+h|T \\geq t)}{h}.\n\\]\nBy the definition of conditional probability, we find that \\[\\begin{align}\nh(t) &= \\lim_{h \\rightarrow 0} \\frac{P(t\\leq T&lt;t+h, T \\geq t)}{P(T \\geq t) h} \\nonumber \\\\\n&= \\lim_{h \\rightarrow 0} \\frac{P(t&lt;T&lt;t+h)}{P(T \\geq t) h} \\nonumber \\\\\n&= \\left\\{ \\lim_{h \\rightarrow 0} \\frac{P(t&lt;T&lt;t+h)}{h}\\right\\} \\frac{1}{P(T \\geq t)} \\nonumber \\\\\n&= \\left\\{ \\lim_{h \\rightarrow 0} \\frac{F(t+h)-F(t)}{h}\\right\\} \\frac{1}{P(T \\geq t)} \\nonumber \\\\\n&= \\frac{F'(t)}{S(t)}=\\frac{f(t)}{S(t)}.\n\\end{align}\\]\nThe Cumulative (or “Integrated”) Hazard Function \\(H(t)\\) is defined as \\[\nH(t) := \\int_0^t h(u) du = -\\log{S(t)}.\n\\]\n\n\n1.4.3 Relationships between hazard, survivor and density functions\nIf we specify any one of these three functions, the other two are determined.\nSubstituting \\(f(t) = -S'(t)\\), we have \\[\nh(t) =\\frac{f(t)}{S(t)}= \\frac{-S'(t)}{S(t)}\n= - \\frac{d}{dt} \\log{S(t)}.\n\\]\nHence we can obtain the survivor function from the hazard function: \\[\nS(t) = \\exp \\left\\{ -\\int_0^t h(u) du \\right\\} = \\exp \\{ -H(t) \\}.\n\\] The density function can also be derived from the hazard function, as \\(f(t) = -S'(t)\\): \\[\nf(t) =-\\frac{d}{dt}S(t)= -\\frac{d}{dt}\\exp \\left\\{ -\\int_0^t h(u) du \\right\\}= h(t) \\exp \\{ -H(t) \\}.\n\\]\n\n\n\n\n\n\nImportant\n\n\n\nThe hazard function is a fundamental concept in survival analysis: make sure you understand the definition and these relationships well! The hazard function has an important role in modelling survival data, as it can be more intuitive to consider how a hazard function changes over time. We sometimes assess the suitability of a particular distribution/model for survival data on the basis of whether the behaviour of the hazard function is plausible.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Background and Basic Concepts</span>"
    ]
  },
  {
    "objectID": "background.html#standard-survival-time-distributions",
    "href": "background.html#standard-survival-time-distributions",
    "title": "1  Background and Basic Concepts",
    "section": "1.5 Standard survival time distributions",
    "text": "1.5 Standard survival time distributions\nVarious distributions can be used for modelling survival data. We will start by specifying the hazard function, and derive the distribution from that.\n\n1.5.1 The exponential distribution\nSuppose, for a random variable \\(T\\) with \\(T\\ge 0\\), we assume a constant hazard function \\[\nh(t) = \\lambda,\n\\] for all \\(t\\ge 0\\). This gives a cumulative hazard function \\[\nH(t) = \\int_0^t \\lambda du = \\lambda t.\n\\] We then obtain\n\\[\\begin{align}\nS(t)&= e^{-\\lambda t}  \\\\\nf(t) & = \\lambda e^{-\\lambda t} \\\\\nF(t) &=1-e^{-\\lambda t}\n\\end{align}\\]\nand we can see that \\(T\\) has the exponential distribution with rate \\(\\lambda\\).\n\n\n\n\n\n\nNote\n\n\n\nYou may recall the “lack of memory” property for exponential random variables: \\[\nP(T\\ge t+s|T\\ge s) = P(T\\ge t),\n\\] e.g. a patient surviving for an additional \\(t\\) units of time does not depend on how long the patient has survived up to that point (the value of \\(s\\) has no effect) on the probability. This is one way to understand what constant hazard means.\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhilst the exponential distribution is the simplest distribution for modelling survival data, it is not always appropriate, precisely because of the constant hazard assumption: the assumption that the risk of death never changes, regardless of how old one gets.\n\n\n\n\n1.5.2 The Weibull Distribution\nConsider a hazard function of the form \\[\nh(t) = \\lambda \\gamma (\\lambda t) ^ {\\gamma-1},\n\\] for \\(t\\ge 0\\). The hazard can now change over time. Note that\n\n\\(\\gamma &gt; 1\\) means the hazard is increasing over time;\n\\(\\gamma = 1\\) means the hazard is constant and we have an exponential distribution;\n\\(\\gamma &lt; 1\\) means the hazard is decreasing over time.\n\nThis gives a cumulative hazard function \\[\nH(t)=\\int_0^t \\lambda \\gamma (\\lambda u) ^ {\\gamma-1}du = (\\lambda t)^\\gamma,\n\\] and so\n\\[\\begin{align}\nS(t) &= \\exp\\left[ -(\\lambda t) ^ \\gamma \\right]  \\\\\nf(t) &= \\lambda \\gamma (\\lambda t) ^ {\\gamma-1} \\exp\\left[ -(\\lambda t) ^ \\gamma \\right]\\\\\nF(t) &= 1 - \\exp\\left[ -(\\lambda t) ^ \\gamma \\right].\n\\end{align}\\]\nThis is another standard distribution: the Weibull distribution. We write \\(T \\sim Weibull(\\lambda, \\gamma)\\).\n\n\n\n\n\n\nWarning\n\n\n\nTextbooks and software do not all parametrise the Weibull distribution the same way! The parametrisation above is the one used in the survreg function in R (we will use this key function extensively in later Chapters). But, confusingly, the dweibull function in R writes the density \\(f(t)\\) as \\[\nf(t) = (a/\\sigma) ( t/\\sigma) ^ {a-1} \\exp\\left[ -( t/\\sigma) ^ a \\right],\n\\] with a “scale” parameter \\(\\sigma\\) corresponding to \\(1/\\lambda\\).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Background and Basic Concepts</span>"
    ]
  },
  {
    "objectID": "background.html#an-example-dataset",
    "href": "background.html#an-example-dataset",
    "title": "1  Background and Basic Concepts",
    "section": "1.6 An example dataset",
    "text": "1.6 An example dataset\nThe survival package in R includes example datasets. These are useful for studying and experimenting with particular methods. One dataset we will use is the North Central Cancer Treatment Group lung cancer data, obtained using survival::lung. For more details about the data, type ?survival::lung in R.\nWe will simplify the data a little, extracting two columns only and recoding status (0 = censored, 1 = died):\n\nlibrary(tidyverse)\nlungData &lt;- survival::lung %&gt;%\n  select(time, status) %&gt;%\n  mutate (status = status - 1)\nhead(lungData)\n\n  time status\n1  306      1\n2  455      1\n3 1010      0\n4  210      1\n5  883      1\n6 1022      0\n\n\nIn this extract, patient 1 died after 306 days of observation, patient 3 was still alive after 1010 days of observation, and was then no longer in the study.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Background and Basic Concepts</span>"
    ]
  },
  {
    "objectID": "background.html#tasks",
    "href": "background.html#tasks",
    "title": "1  Background and Basic Concepts",
    "section": "1.7 Tasks",
    "text": "1.7 Tasks\n\nSuppose \\(T \\sim Weibull(\\lambda, \\gamma)\\).\n\n\nIf it is known that \\(P(T &lt; 1)=1/2\\) and \\(P(T \\leq 2)=15/16\\) find the values of \\(\\lambda\\) and \\(\\gamma\\) by considering the relevant survivor function.\nWith these values of \\(\\lambda\\) and \\(\\gamma\\) find the maximum value of the hazard \\(h(t)\\) in the interval \\(t \\in [0,10]\\).\n\n\nLet the random variable \\(T\\) represent the failure time of an individual. For a constant hazard function \\(h(t)=\\lambda\\) for \\(t\\ge 0\\), prove the lack of memory property: \\[\nP(T\\ge t + s | T\\ge s) = P(T\\ge t).\n\\]\nGiven the following survivor function, sketch the corresponding hazard function. By considering a cohort of 100 patients starting at time 0, the number of patients we would expect to survive for 1 year, and how many of those survivors we would expect to survive for an additional year, explain the shape in your sketch.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n1a. We form two simultaneous equations using \\(S(1)=1/2\\) and \\(S(2)=1/16\\). These give \\[\\begin{align*}\n    \\exp(-\\lambda^\\gamma) &=1/2  \\\\\n    \\exp(-(2\\lambda)^\\gamma) &=1/16\n  \\end{align*}\\] or equivalently \\[\\begin{align*}\n    \\lambda^\\gamma &=\\ln 2  \\\\\n    (2\\lambda)^\\gamma &=\\ln 16\n  \\end{align*}\\] Dividing these two equations gives \\(2^\\gamma=4\\) so \\(\\gamma=2\\). Substituting gives \\(\\lambda=\\sqrt{\\ln 2}\\)\n1b. \\(h(t) \\propto t\\) so the hazard is an increasing function and the maximum hazard in the interval \\(t \\in [0,10]\\) occurs at \\(t=10\\).\n\nWe have\n\n\\[\\begin{align*}\nP(T\\ge t + s | T\\ge s) & = \\frac{P(T\\ge t+s \\mbox{ and } T\\ge s)}{P(T\\ge s)}\\\\\n& = \\frac{P(T\\ge t+s)}{P(T\\ge s)}\n\\end{align*}\\] because if \\(T\\ge t+s\\) then is must also be true that \\(T\\ge s\\). Then \\[\n\\frac{P(T\\ge t+s)}{P(T\\ge s)} = \\frac{\\exp(-\\lambda(t+s))}{\\exp(-\\lambda s)} =\\exp(-\\lambda t) = P(T\\ge t).\n\\] Hence \\[\nP(T\\ge t + s | T\\ge s) = P(T\\ge t).\n\\] 3. The hazard function is obtained from the survivor function via\n\\[\nh(t) = - \\frac{d}{dt} \\log S(t)\n\\]\nWe have a linear survivor function that is 0 at \\(t=2\\), so the shape of \\(-\\log S(t)\\) is\n\n\n\n\n\n\n\n\n\nThe gradient increases with \\(t\\), so the shape of the hazard is\n\n\n\n\n\n\n\n\n\nTo explain this shape, if we consider a cohort of 100 patients, we have \\(S(1)=0.5\\), so we expect 50 of the patients to survive the first year. We have \\(S(2)=0\\), so of these 50, we expect none of them survive a whole second year. This is why the hazard increases over time, and tends to infinity as \\(t\\) tends to 2, as the probability of surviving beyond two years is 0.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Background and Basic Concepts</span>"
    ]
  },
  {
    "objectID": "survivor.html",
    "href": "survivor.html",
    "title": "2  Estimating survivor functions: life-tables and Kaplan-Meier",
    "section": "",
    "text": "2.1 Preliminaries\nYou may have seen before how to estimate a cumulative distribution function \\(F(t)\\) using the empirical distribution function. We can obviously use the same idea to estimate a survivor function \\(S(t)=1-F(t)\\). The method is",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Estimating survivor functions: life-tables and Kaplan-Meier</span>"
    ]
  },
  {
    "objectID": "survivor.html#preliminaries",
    "href": "survivor.html#preliminaries",
    "title": "2  Estimating survivor functions: life-tables and Kaplan-Meier",
    "section": "",
    "text": "obtain a sample of values \\(t_1,\\ldots,t_n\\) from the distribution of \\(T\\)\nestimate the survivor function \\(S(t) = P(T\\ge t)\\) for any value of \\(t\\) by counting the proportion of values in the sample \\(t_1,\\ldots,t_n\\) that are greater than or equal to \\(t\\).\n\n\n\n\n\n\n\nExample\n\n\n\nWe will test the method above in R to estimate \\(S(1)=P(T\\ge 1)\\) where \\(T\\sim exp(rate = 1)\\):\n\n# fix the random seed so you can reproduce this\nset.seed(123) \n\n# generate a sample of 1000 random observations from the exp(rate = 1) distribution\nt_sample &lt;- rexp(n = 1000, rate = 1)\n\n# count how many observations in the sample are greater than or equal to 1\n# and divide by 1000 to get the proportion\nsum(t_sample &gt;= 1) / 1000 \n\n[1] 0.393\n\n# compare with the true value of the survivor function:\n1 - pexp(1, rate = 1)\n\n[1] 0.3678794\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis approach, on its own, won’t work if the sample of observations includes censored data!\n\n\n\n2.1.1 The factorisation method\nWe will see later that it can be convenient to estimate a survival probability via a factorisation. As an example, if we want to estimate \\(S(2)\\), we can write\n\\[\nS(2)=P(T\\ge2) = P(T\\ge1) P(T\\ge 2|T\\ge 1) = S(1)P(T\\ge 2|T\\ge 1)\n\\]\n\nSuppose we have a sample of survival times \\(t_1,\\ldots t_n\\) from the distribution of \\(T\\)\nWe estimate \\(S(1)\\) exactly as before\nWe estimate \\(P(T\\ge 2|T\\ge 1)\\) by counting the proportion of observations in our sample greater than or equal to 2, out of those observations which are greater than or equal to 1.\nWe multiply the estimates from steps 2 and 3 to get an estimate of \\(S(2)\\)\n\nThis may seem unnecessarily complicated, but it will be useful for handling censored observations in our sample.\n\n\n\n\n\n\nExample\n\n\n\nWe illustrate the above approach in R, again with \\(T\\sim exp(rate = 1)\\) for illustration.\n\n# fix the random seed so you can reproduce this\nset.seed(123) \n\n# generate a sample of 1000 random observations from the exp(rate=1) distribution\nt_sample &lt;- rexp(1000) \n\n# count how many observations in the sample are greater than or equal to 1\n# and divide by 100 to get the proportion\np1 &lt;- sum(t_sample &gt;= 1) / 1000\n\n# Extract observations in the sample are greater than or equal to 1\nt_sample_above_1 &lt;- t_sample[t_sample &gt;=1]\n\n# count how many observations in the reduced are greater than or equal to 2\n# and divide by the sample size to get the proportion\np2 &lt;- sum(t_sample_above_1 &gt;= 2) / length(t_sample_above_1)\n\n# Multiply to estimate the desired probability\np1 * p2\n\n[1] 0.137\n\n# Compare with the true value of the probability\n1 - pexp(2, rate = 1)\n\n[1] 0.1353353\n\n\n\n\n\n\n2.1.2 Terminology: “at risk”\nWe use the term “at risk” to mean that the individual has not yet had the failure event (e.g. death): they are still “at risk” of the event occurring. The number of patients at risk during at some interval is the maximum number of events that could occur in that interval\nContinuing the R example above, we started with a sample of 1000 patients, so there were 1000 patients at risk for \\(t\\ge 0\\). At time \\(t=1\\), if we count how many patients have had their event by this time:\n\nsum(t_sample&lt; 1)\n\n[1] 607\n\n\nwe say that there are \\(1000-607\\) patients at risk at time \\(t\\ge 1\\). We estimate \\[\nP(T\\ge 2 | T\\ge 1)\n\\] by computing \\[\n\\frac{\\mbox{number of patients with survival times }\\ge 2}{\\mbox{number of patients at risk during the period time }t\\ge1}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Estimating survivor functions: life-tables and Kaplan-Meier</span>"
    ]
  },
  {
    "objectID": "survivor.html#life-table-estimate-of-the-survivor-function",
    "href": "survivor.html#life-table-estimate-of-the-survivor-function",
    "title": "2  Estimating survivor functions: life-tables and Kaplan-Meier",
    "section": "2.2 Life table estimate of the survivor function",
    "text": "2.2 Life table estimate of the survivor function\n\n\n\n\n\n\nNote\n\n\n\nThe life table method (also known as the “actuarial method”) involves grouping the data: this loses information, and the estimate will depend on the choice of grouping. The life-table method can, however, handled censored observations. Note that the data may have been collated in a grouped format to start with, so that exact survival times/censoring times are not available. Note also that this method is not actually estimating the entire function \\(S(t)\\) for all \\(t\\ge 0\\); it estimates \\(S(t)\\) at a finite set of \\(t\\) values.\n\n\nWe will use the lung dataset as an example.\n\nlibrary(tidyverse)\nlungData &lt;- survival::lung %&gt;%\n  select(time, status) %&gt;%\n  mutate (status = status - 1)\n\n\nChoose a set of time intervals. The end point for the last interval needs to exceed the longest survival/censoring time in the data. We have\n\n\nmax(lungData$time)\n\n[1] 1022\n\n\nand we choose the intervals to be \\([0, 200), [200, 400), [400, 600), [600, 800), [800, 1000), [1000, 1200)\\):\n\nbreaks &lt;- seq(0, 1200, by = 200)\n\nThis means we will obtain estimates of \\(S(200), S(400), S(600), S(800), S(1000)\\). If we want to estimate \\(S(t)\\) at more values of \\(t\\), we would have to choose more intervals. (If the data were grouped to begin with, we would just use the intervals provided with the data.)\n\nCount the number of deaths and the number of censored observations in each interval. Count also the number of patients at risk at the start of each interval: the total sample size, minus all deaths and censored observations that have occurred prior to the start of the interval.\n\nThis code assigns each observation to an interval\n\nlung_binned &lt;- lungData %&gt;%\n  mutate(\n    # Use cut to assign each 'time' to a 200-day interval\n    time_interval = cut(time,\n                        breaks = breaks,\n                        right = FALSE, # Intervals are [start, end)\n                        include.lowest = TRUE, # Include the starting point (0)\n                        labels = paste0(breaks[-length(breaks)], \"-\", breaks[-1]) # Custom labels\n    )\n  )\nhead(lung_binned)\n\n  time status time_interval\n1  306      1       200-400\n2  455      1       400-600\n3 1010      0     1000-1200\n4  210      1       200-400\n5  883      1      800-1000\n6 1022      0     1000-1200\n\n\nand this code does the counting:\n\nsurvival_summary_table &lt;- lung_binned %&gt;%\n  group_by(time_interval) %&gt;%\n  summarise(\n    N_Deaths = sum(status == 1), # Count observations where status is 1 (Dead)\n    N_Censored = sum(status == 0), # Count observations where status is 0 (Censored)\n  ) %&gt;%\n  mutate(at_risk = nrow(lungData) - c(0,\n                                      cumsum(N_Deaths)[1:5] +\n                                      cumsum(N_Censored)[1:5]))  \n\n\nknitr::kable(survival_summary_table)\n\n\n\n\ntime_interval\nN_Deaths\nN_Censored\nat_risk\n\n\n\n\n0-200\n72\n12\n228\n\n\n200-400\n54\n33\n144\n\n\n400-600\n22\n11\n57\n\n\n600-800\n15\n1\n24\n\n\n800-1000\n2\n4\n8\n\n\n1000-1200\n0\n2\n2\n\n\n\n\n\n\nNow we deal with the censoring. Considering \\(S(200)\\), for example, we could estimate this by \\[\n\\frac{228 - 72}{228}\n\\] as there were 228 patients at risk for the period \\(0\\le t &lt; 200\\), and there were 72 deaths during this period. But 12 patients were not observed for the full 200 days, as they were censored.\n\n\n\n\n\n\n\nThe actuarial assumption\n\n\n\nWe make the “actuarial assumption” that censoring within an interval occurs at a uniform rate over that interval. So if there were 12 censored observations occurring at a uniform rate over the interval \\([0, 200)\\), we treat this as equivalent to \\(0.5 \\times 12\\) patients being at risk for the whole interval. The “adjusted” number at risk is calculated as \\(228 - 0.5\\times 12\\)\n\n\n\nsurvival_summary_table &lt;- survival_summary_table %&gt;%\n  mutate(at_risk_adjusted = at_risk - 0.5 * N_Censored)  \n\n\nknitr::kable(survival_summary_table)\n\n\n\n\ntime_interval\nN_Deaths\nN_Censored\nat_risk\nat_risk_adjusted\n\n\n\n\n0-200\n72\n12\n228\n222.0\n\n\n200-400\n54\n33\n144\n127.5\n\n\n400-600\n22\n11\n57\n51.5\n\n\n600-800\n15\n1\n24\n23.5\n\n\n800-1000\n2\n4\n8\n6.0\n\n\n1000-1200\n0\n2\n2\n1.0\n\n\n\n\n\nWe can now calculate estimates of the survivor function through repeated use of the factorisation method in Section 2.1.1\n\n\n\n\n\n\n\n\n\n\\(j\\)\n\\(t_j\\)\n\\(\\hat{P}(T\\ge t_j | T \\ge t_{j-1})\\)\n\\(\\hat{S}(t_j) =\\hat{S}(t_{j-1})\\times \\hat{P}(T\\ge t_j | T \\ge t_{j-1})\\)\n\n\n\n\n1\n0\n-\n1\n\n\n2\n200\n\\(\\frac{222-72}{72}\\)\n\\(\\frac{222-72}{222}\\)\n\n\n3\n400\n\\(\\frac{127.5-54}{127.5}\\)\n\\(\\frac{222-72}{222}\\times \\frac{127.5-54}{127.5}\\)\n\n\n4\n600\n\\(\\frac{51.5-22}{51.5}\\)\n\\(\\frac{222-72}{222}\\times \\frac{127.5-54}{127.5}\\times \\frac{51.5-22}{51.5}\\)\n\n\n5\n800\n\\(\\frac{23.5-15}{23.5}\\)\n\\(\\frac{222-72}{222}\\times \\frac{127.5-54}{127.5}\\times \\frac{51.5-22}{51.5}\\times \\frac{23.5-15}{23.5}\\)\n\n\n6\n1000\n\\(\\frac{6-2}{6}\\)\n\\(\\frac{222-72}{222} \\times \\ldots \\times \\frac{6-2}{6}\\)\n\n\n7\n1200\n\\(\\frac{1-0}{1}\\)\n\\(\\frac{222-72}{222} \\times \\ldots \\times \\frac{1-0}{1}\\)\n\n\n\nNote that the estimates would be less reliable as time increases, as the number at risk decreases; we don’t expect the statement \\(P(T\\ge 1200 | T\\ge 1000)=1\\) to be true, for example.\nTo do the calculations in R:\n\nP &lt;- (survival_summary_table$at_risk_adjusted - survival_summary_table$N_Deaths)/\n  survival_summary_table$at_risk_adjusted\n\n# Take the cumulative product of P to get the survivor function estimates with cumprod(P)\n\n# This is just to format things nicely:\nknitr::kable(data.frame(`time t` = breaks, `S(t)` = round(c(1, cumprod(P)), 3),\n                        check.names = FALSE))\n\n\n\n\ntime t\nS(t)\n\n\n\n\n0\n1.000\n\n\n200\n0.676\n\n\n400\n0.390\n\n\n600\n0.223\n\n\n800\n0.081\n\n\n1000\n0.054\n\n\n1200\n0.054\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe have just shown here the “life table estimate” of the survivor function. Life tables such as those produced by the Office for National Statistics typically present more information. For example, a life table may consider a cohort of size 100,000, and then list the number expected to be alive at the start of each period (obtained by multiplying \\(S(t)\\) by 100,000 for each \\(t\\)),",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Estimating survivor functions: life-tables and Kaplan-Meier</span>"
    ]
  },
  {
    "objectID": "survivor.html#kaplan-meier-estimate-of-st",
    "href": "survivor.html#kaplan-meier-estimate-of-st",
    "title": "2  Estimating survivor functions: life-tables and Kaplan-Meier",
    "section": "2.3 Kaplan-Meier estimate of \\(S(t)\\)",
    "text": "2.3 Kaplan-Meier estimate of \\(S(t)\\)\nThis is the most commonly reported estimate of a survivor function; you’ll see a Kaplan-Meier plot in just about any publication of clinical trial results involving survival data.\nThe Kaplan-Meier estimate requires individual survival/censoring times; it doesn’t work with grouped data, but, unlike the life table method, it provides an estimate of the function \\(S(t)\\) for \\(t\\ge 0\\).\nWe will first show how to construct the Kaplan-Meier estimate using an example. Suppose we have five patients and observe survival times of \\(t=3, 4, 10, 12\\) for four of them, and have one censored observation at \\(t=7\\) (so for the fifth patient, we only know their survival time exceeds 7).\n\n\n\n\n\n\n\n\n\n\nFor \\(0\\le t &lt; 3\\), everyone who is at risk is still alive, so we estimate \\(\\hat{S}(t) = 1\\) for \\(0\\le t &lt; 3\\).\nOne of the five patients is not alive at time \\(t=3\\), so we estimate \\(\\hat{S}(3) = 4/5\\).\nFor \\(3\\le t &lt; 4\\), everyone who is at risk is still alive, so for \\(3\\le t &lt; 4\\) we use the factorisation approach in Section 2.1.1 and estimate \\[\n\\hat{S}(t) = \\hat{S}(3)\\hat{P}(T\\ge t|T\\ge 3) = \\frac{4}{5}\\times 1\n\\]\n\n\\(\\hat{S}(t) = 1\\) for \\(0\\le t &lt; 3\\).\n\nFor \\(t=4\\) we again use the factorisation approach and write \\[\nS(4) = S(3)P(T\\ge 4|T\\ge 3).\n\\] We have \\(\\hat{S}(3)=4/5\\) as before, but now we note that, of the four patients alive at time \\(t=3\\), one was not alive at time \\(t=4\\), so we have the estimate \\[\n\\hat{S}(4) = \\hat{S}(3)\\hat{P}(T\\ge 4|T\\ge 3) = \\frac{4}{5}\\times \\frac{3}{4} = \\frac{3}{5}\n\\]\nFor \\(4\\le t &lt; 10\\), we do not observe any deaths out of those patients at risk (though we have observed one censoring event), so for \\(4\\le t &lt; 10\\) we have \\[\n\\hat{S}(t) = \\hat{S}(4)\\hat{P}(T\\ge t|T\\ge 4) = \\frac{3}{5}\\times 1\n\\]\nFor \\(t=10\\), we could follow the same process as before and write \\[\nS(10) = S(4)P(T\\ge 10|T\\ge 4),\n\\] but this is awkward because of the censoring event at time \\(t=7\\). But in the factorisation method, we can condition on any time we like up to time \\(t=10\\). If we instead write \\[\nS(10) = S(7)P(T\\ge 10|T\\ge 7),\n\\] we can now handle the censoring more easily. We have \\(\\hat{S}(7)=3/5\\) from previously. From time \\(t=7\\) onwards, there were two patients at risk, and one was alive at time \\(t=10\\). So we estimate \\[\n\\hat{S}(10) = \\hat{S}(7)\\hat{P}(T\\ge 10|T\\ge 7) = \\frac{3}{5}\\times \\frac{1}{2} = \\frac{3}{10}.\n\\]\nContinuing this approach, we estimate \\(\\hat{S}(t) = 3/10\\) for \\(10 \\le t &lt; 12\\) and \\(\\hat{S}(t) = 0\\) for \\(t\\ge 12\\).\n\nTabulating the Kaplan-Meier estimate, we have\n\n\n\n\\(t\\)\n\\(\\hat{S}(t)\\)\n\n\n\n\n1.0\n\\(0\\le t &lt; 3\\)\n\n\n0.8\n\\(3\\le t &lt; 4\\)\n\n\n0.6\n\\(4\\le t &lt; 10\\)\n\n\n0.3\n\\(10\\le t &lt; 12\\)\n\n\n0.0\n\\(12\\le t\\)\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe Kaplan-Meier estimate is a step function:\n\nit decreases by some amount at every observed failure event\nthere is no decrease in the function at any censoring event, but we reduce the number of patients at risk in subsequent calculations.\n\n\n\n\n2.3.1 General notation and formula\nWe define\n\n\\(t_{(1)} &lt;t_{(2)} &lt; \\ldots &lt;t_{(k)}\\) to be the \\(k\\) distinct lifetimes with \\(d_j\\) individuals failing at time \\(t_{(j)}\\). Note that these are times when we observe failure events only, not censoring;\n\\(I_j\\) to be the number of individuals who were censored in the interval \\(t_{(j-1)} \\leq t &lt; t_{(j)}\\)\n\\(n\\) to be the total number of patients at the beginning of the study;\n\\(r_j\\) to be the number of patients at risk just before time \\(t_{(j)}\\): the number of patients we are monitoring just before time \\(t_{(j)}\\)\n\nWe then compute\n\\[\\begin{align}\nr_1 &= n - I_1 \\\\\nr_{j} &= r_{j-1} - d_{j-1} - I_{j} \\\\\n&= n - (d_1 + d_2 + \\ldots +d_{j-1})-(I_1+I_2+\\ldots+I_{j}) \\quad \\textrm{for } j \\geq 2\n\\end{align}\\] and the Kaplan-Meier estimate of \\(S(t)\\) is given by \\[\n\\hat{S}(t) = \\prod_{j=1}^s \\left( 1 - \\frac{d_j}{r_j} \\right) \\quad \\textrm{for }  t_{(s)} \\leq t  &lt; t_{(s+1)}\n\\]\nIf \\(I_{k+1} &gt; 0\\) then \\[\\hat{S}(t) = \\prod_{j=1}^s \\left( 1 - \\frac{d_j}{r_j} \\right) &gt; 0\\] since \\(r_k &gt; d_k\\). Hence if there are still individuals left in the trial after the last observed death (possibly as the observation period has ended) then \\(\\hat{S}(t)\\) will not tend to 0. However we know, by definition that as \\(t \\rightarrow \\infty\\) then \\(S(t) \\rightarrow 0\\) since everyone will fail eventually, so the Kaplan-Meier estimate is biased if the maximum observation is censored.\nWe can also estimate \\(H(t)\\) by \\(\\hat{H}(t)= -\\log{\\hat{S}(t)}\\) or we can use the simpler approximation \\[\n\\tilde{H}(t)= \\sum_{j=1}^s \\frac{d_j}{r_j} \\quad \\textrm{for }  t_{(s)} \\leq t  &lt; t_{(s+1)}\n\\]\n\n\n\n\n\n\nExample: Tumour Remission Times\n\n\n\nA study investigates the remission times for 10 patients with tumours. During the study:\n\nsix relapse after 3.0, 6.5, 6.5, 10, 12, and 15 months\nand one was lost to follow-up at 8.4 months;\nthe other three were still in remission at end of study after 4.0, 5.7, 10.1 months respectively.\n\nWe show this data in Table 2.1 along with the Kaplan-Meier estimate of the survivor function.\n\n\n\nTable 2.1: Kaplan-Meier Estimate of the Survivor Function for the Tumour Remission Times Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(j\\)\n\\(t_{(j)}\\)\n\\(I_j\\)\n\\(r_j\\)\n\\(d_j\\)\n\\(\\left(1-\\frac{d_j}{r_j}\\right)\\)\n\\(\\hat{S}(t)\\)\n\\(t\\)\nCalculation of \\(\\hat{S}(t)\\)\n\n\n\n\n0\n0\n0\n\n\n\n1.00\n\\(0 \\leq t &lt;3.0\\)\n\n\n\n1\n3.00\n0\n10\n1\n9/10\n0.90\n\\(3.0 \\leq t&lt;6.5\\)\n9/10\n\n\n2\n6.50\n2\n7\n2\n5/7\n0.64\n\\(6.5\\leq t&lt;10.0\\)\n\\(9/10 \\times 5/7\\)\n\n\n3\n10.00\n1\n4\n1\n3/4\n0.48\n\\(10.0\\leq t&lt;12.0\\)\n\\(9/10 \\times 5/7 \\times 3/4\\)\n\n\n4\n12.00\n1\n2\n1\n1/2\n0.24\n\\(12.0\\leq t&lt;15.0\\)\n\\(9/10 \\times 5/7 \\times 3/4 \\times 1/2\\)\n\n\n5\n15.00\n0\n1\n1\n0\n0.00\n\\(15\\leq t\\)\n\n\n\n\n\n\n\n\n\n\n\n2.3.2 Finding the Median Survival Time from KM plots\nThe median, \\(M\\), is defined as \\[\nM = \\mathop{\\mathrm{argmin}}_t \\{ S(t) \\leq 0.5\\},\n\\] i.e. it is the smallest value of \\(t\\) where the survivor function takes a value of 0.5 or less.\n\n\n\n\n\n\nExample: Tumour Remission Times continued\n\n\n\nWe can see from Table 2.1 that \\(\\hat{S}(t) &gt; 0.5\\) for \\(0 \\leq t &lt; 10\\) and \\(\\hat{S}(10)\\leq 0.5\\) so that the median tumour remission time is 10 months.\n\n\n\n\n2.3.3 Variance of the Kaplan-Meier estimator\n\\(\\hat{S}(t)\\) is subject to sampling error. The Greenwood estimate of the variance is\n\\[\nvar\\left( \\hat{S}(t) \\right) \\simeq \\left( \\hat{S}(t) \\right)^2 \\sum_{j=1}^{s} \\frac{d_j}{r_j(r_j-d_j)} \\quad \\textrm{for }  t_{(s)} \\leq t  &lt; t_{(s+1)}\n\\]\n\n\n2.3.4 Calculating Kaplan-Meier Estimates in R\n\nR functions for analysing survival data are in the survival package. You shouldn’t need to install it as it is included in the basic installation of R.\nThe first step is to create a survival object with the function Surv() (note the capital S). It contains information on which observations are censored (coded with a 0) and which are observed events (coded with a 1).\nThe next step is to estimate the survivor curve with the function survfit().\nTo produce flexible Kaplan-Meier plots use the ggsurvplot function in the survminer package (install and load it as above). The function summary() will give the estimate of the survivor function and other details of the model fitting.\n\nWe will create the data frame from scratch. The analysis can be performed as described below.\n\ntumour &lt;- data.frame(time = c(3, 6.5, 6.5, 10, 12, 15, 8.4, 4, 5.7, 10.1),\n                     censor = c(1, 1, 1, 1, 1, 1, 0, 0, 0, 0))\n\nlibrary(survival)\nlibrary(survminer)\ntumour # 0 means censored, 1 means observed\n\n   time censor\n1   3.0      1\n2   6.5      1\n3   6.5      1\n4  10.0      1\n5  12.0      1\n6  15.0      1\n7   8.4      0\n8   4.0      0\n9   5.7      0\n10 10.1      0\n\ntumour_sv &lt;- Surv(tumour$time, tumour$censor, type = \"right\")\ntumourSurv &lt;-survfit(tumour_sv ~ 1, data=tumour)\nsummary(tumourSurv)\n\nCall: survfit(formula = tumour_sv ~ 1, data = tumour)\n\n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n  3.0     10       1    0.900  0.0949       0.7320            1\n  6.5      7       2    0.643  0.1679       0.3852            1\n 10.0      4       1    0.482  0.1877       0.2248            1\n 12.0      2       1    0.241  0.1946       0.0496            1\n 15.0      1       1    0.000     NaN           NA           NA\n\nggsurvplot(tumourSurv, \n           data = tumour_sv,\n           surv.median.line = 'hv',\n           conf.int = TRUE, \n           conf.int.style = \"ribbon\",\n           conf.int.alpha = 0.4,\n           xlab = \"Time (months)\", \n           risk.table = T,\n           break.time.by = 1,\n           ncensor.plot = T,\n           color = \"black\", \n           censor = T,\n           censor.shape = 3,\n           censor.size = 5,\n           tables.height = 0.25)\n\n\n\n\nKaplan-Meier Estimate of the survivor function for the Tumour Remission Times data\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThere are lots of arguments you can specify in the ggsurvplot function. The minimum you need in the above example would be\n\nggsurvplot(tumourSurv, data = tumour_sv)\n\nNote also that this creates a ggplot2 object that you can customise with other ggplot2 commands. To access this you would do\n\nKMplot &lt;- ggsurvplot(tumourSurv, data = tumour_sv)\nKM$plot",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Estimating survivor functions: life-tables and Kaplan-Meier</span>"
    ]
  },
  {
    "objectID": "survivor.html#tasks",
    "href": "survivor.html#tasks",
    "title": "2  Estimating survivor functions: life-tables and Kaplan-Meier",
    "section": "2.4 Tasks",
    "text": "2.4 Tasks\n\nDerive the life table estimate of the survival function for patients with angina pectoris, using the data given below. (To shorten the question, data are only given for the first five years of survival.)\n\n\n\n\n\n\n\n\n\nSurvival time (years)\nNumber of patients known to survive at beginning of interval\nNumber of patients lost to follow up\n\n\n\n\n0 - 1\n2418\n0\n\n\n1 - 2\n1962\n39\n\n\n2 - 3\n1697\n22\n\n\n3 - 4\n1523\n23\n\n\n4 - 5\n1329\n24\n\n\n\n\nThe data below give the time to relapse (in weeks), following remission, for a group of leukaemia patients treated with a particular drug: (* indicates a right censored value).\n\n\\[6^*, 6, 6, 6, 7, 9^*, 10^*, 10, 11^*, 13, 16,\\] \\[17^*, 19^*, 20^*, 22, 23, 25^*, 32^*, 34^*, 35^*\\]\n\nTabulate the Kaplan-Meier estimate of the survivor function. Sketch the estimated survivor function.\nEstimate the median time to relapse.\nEstimate the proportion of patients that will not relapse within 6 weeks of starting remission.\nUse R to verify your table and plot. Note that the following code will set up the data in R:\n\n\nleukemia &lt;- data.frame(time = c(6, 6, 6, 6, 7, 9, 10,\n                                10, 11, 13, 16, 17, 19,\n                                20, 22, 23, 25, 32, 32,\n                                34, 35),\n                       censor = c(0, 1, 1, 1, 1, 0, 0, 1,\n                                  0, 1, 1, 0, 0, 0, 1, 1, 0, 0,\n                                  0, 0, 0)\n)\n\n\nComment on the estimated survivor function for \\(t&gt;25\\) weeks: whether you think the estimate is reliable or not.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nWe proceed by finding the number who died in the first interval \\(2418-1962 = 456\\); in the second interval \\(1962-39-1697=226\\) and so on. We tabulate and add in the adjusted number at risk\n\n\n\n\n\n\n\n\n\n\n\nSurvival time (years)\nNumber of patients known to survive at beginning of interval\nNumber of patients lost to follow up\nNumber of deaths\nAdjusted number at risk\n\n\n\n\n0 - 1\n2418\n0\n456\n2418.0\n\n\n1 - 2\n1962\n39\n226\n1942.5\n\n\n2 - 3\n1697\n22\n152\n1686.0\n\n\n3 - 4\n1523\n23\n171\n1511.5\n\n\n4 - 5\n1329\n24\n135\n1317.0\n\n\n\nThe life-table estimate is computed as\n\n\n\n\n\n\n\n\n\n\\(j\\)\n\\(t_j\\)\n\\(\\hat{P}(T\\ge t_j | T\\ge t_{j-1})\\)\n\\(\\hat{S}(t_j) = \\hat{S}(t_{j-1})\\times\\hat{P}(T\\ge t_j | T\\ge t_{j-1})\\)\n\n\n\n\n1\n0\n-\n1\n\n\n2\n1\n\\((2418-456)/2418\\)\n\\(0.81\\)\n\n\n3\n2\n\\((1942.5-226)/1942.5\\)\n\\(0.72\\)\n\n\n4\n3\n\\((1686.0-152)/1686.0\\)\n\\(0.64\\)\n\n\n5\n4\n\\((1511.5-171)/1511.5\\)\n\\(0.57\\)\n\n\n6\n5\n\\((1317.0-135)/1317.0\\)\n\\(0.50\\)\n\n\n\n\nThe R code below is the solution to part d, and also gives the Kaplan-Meier estimate and plot for part a.\n\n\nleukemia &lt;- data.frame(time = c(6, 6, 6, 6, 7, 9, 10,\n                                10, 11, 13, 16, 17, 19,\n                                20, 22, 23, 25, 32, 32,\n                                34, 35),\n                       censor = c(0, 1, 1, 1, 1, 0, 0, 1,\n                                  0, 1, 1, 0, 0, 0, 1, 1, 0, 0,\n                                  0, 0, 0)\n)\nlibrary(survival)\nlibrary(survminer)\n\nleukemia_sv &lt;- Surv(leukemia$time, leukemia$censor, type = \"right\")\nleukemiaSurv &lt;-survfit(leukemia_sv ~ 1, data=leukemia)\nsummary(leukemiaSurv)\n\nCall: survfit(formula = leukemia_sv ~ 1, data = leukemia)\n\n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    6     21       3    0.857  0.0764        0.720        1.000\n    7     17       1    0.807  0.0869        0.653        0.996\n   10     15       1    0.753  0.0963        0.586        0.968\n   13     12       1    0.690  0.1068        0.510        0.935\n   16     11       1    0.627  0.1141        0.439        0.896\n   22      7       1    0.538  0.1282        0.337        0.858\n   23      6       1    0.448  0.1346        0.249        0.807\n\n\nTo get the plot we do\n\nggsurvplot(leukemiaSurv, data = leukemia,\n            surv.median.line = 'hv',\n            xlab = \"Remission Time (Weeks)\")\n\n\n\n\n\n\n\n\n\nFor the median we need the smallest \\(t\\) for which the estimated survivor function \\(S(t)\\le0.5\\). We can read this off the table as \\(t=23\\).\nThe the proportion of patients that will not relapse within 6 weeks of starting remission is \\(S(6)\\). From the table, this is estimated as 0.857.\nFor \\(t&gt;25\\) we see in the plot that the confidence intervals for \\(\\hat{S}(t)\\) are wide, and we also note all observations for \\(t&gt;25\\) are censored. The estimate of \\(S(t)\\) may not very reliable here. A ‘flat’ survivor function as seen for \\(t&gt;23\\) may not be plausible, unless it is the case that if remission has not occurred after a certain number of weeks, it is unlikely to occur at all.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Estimating survivor functions: life-tables and Kaplan-Meier</span>"
    ]
  },
  {
    "objectID": "parametric.html",
    "href": "parametric.html",
    "title": "3  Parametric one-sample models",
    "section": "",
    "text": "3.1 Exponential models\nSuppose we have a non-negative failure time \\(T\\) with p.d.f. \\(f(t)\\); distribution function \\(F(t)\\); survival function \\(S(t)=1 - F(t)\\) ; and hazard function \\(h(t)\\). Typically the pdf depends on an unknown parameter \\(\\theta\\) that needs to be estimated from the data. There are many methods of estimation but we concentrate on maximum likelihood estimation (m.l.e.) and then perform tests/inference relying on its nice asymptotic (large sample) properties.\nTo find the mle, we first find the likelihood \\(L(\\theta)\\) of a parameter \\(\\theta\\) for data \\(x_1, \\ldots ,x_n\\). We can now maximize \\(L(\\theta)\\) wrt \\(\\theta\\) to find the maximum likelihood estimate of \\(\\theta\\). To do this we usually maximise the log-likelihood as this is generally simpler to work with.\nRefer to Section 1.5.1 for the density and survivor functions.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Parametric one-sample models</span>"
    ]
  },
  {
    "objectID": "parametric.html#exponential-models",
    "href": "parametric.html#exponential-models",
    "title": "3  Parametric one-sample models",
    "section": "",
    "text": "3.1.1 Uncensored Data\nSuppose we observe (non-censored) failure times \\(t_1,t_2, \\ldots,t_n\\), then \\[\\begin{align}\nL(\\lambda;t_1,t_2,\\ldots,t_n) &= \\prod_{i=1}^n f(t_i) = \\lambda^n e^{-\\lambda \\sum_{i=1}^n t_i} \\\\\n\\log(L)= l(\\lambda) &= n \\log(\\lambda)-\\lambda \\sum_{i=1}^n t_i \\\\\n\\Rightarrow \\hat{\\lambda} = \\frac{n}{\\sum_{i=1}^n t_i}.\n\\end{align}\\] We can also find a confidence interval for \\(\\lambda\\) since \\(Y = \\sum_{i=1}^n T_i \\sim \\Gamma(n , \\lambda)\\) and \\[Z= 2\\lambda Y \\sim \\chi^2_{2n}.\\] Hence \\[P\\left(\\chi^2_{2n; \\alpha/2} &lt; 2\\lambda \\sum_{i=1}^n T_i &lt; \\chi^2_{2n; 1- \\alpha/2}\\right)= 1- \\alpha,\\] and so a \\(100(1-\\alpha)\\%\\) confidence interval for \\(\\lambda\\) is given by \\[\n\\left(\\frac{\\chi^2_{2n; \\alpha/2}}{2 \\sum_{i=1}^n t_i}, \\frac{\\chi^2_{2n; 1- \\alpha/2}}{2 \\sum_{i=1}^n t_i}  \\right).\n\\] Finally we can find the mle of \\(S(t)\\) as \\[\\hat{S}(t) = e^{-\\hat{\\lambda} t}\\]\n\n\n3.1.2 Censored Data\nNow suppose that we have right censored data. Specifically, let us consider \\(n\\) patients, with potential i.i.d. lifetimes \\(T_i \\sim Exp(\\lambda)\\). We observe either the lifetime \\(t_i\\) or the fact that \\(t_i&gt;c_i\\) for individual \\(i\\). The simplest case is to assume that the \\(c_i\\) are fixed and known for all individuals, i.e. non-random. For individuals where we observe the failure time (where \\(t_i \\leq c_i\\)), the contribution of each individual to the likelihood is \\[f(t_i) = \\lambda e^{-\\lambda t_i}.\\] For individuals where we observe the right-censored value (where \\(t_i &gt; c_i\\)), using the fact that \\(P(T_i &gt; c_i) = S(c_i)\\), the contribution to the likelihood is \\[S(c_i)=e^{-\\lambda c_i}.\\] If we define \\[\n\\delta_i= \\left\\{ \\begin{array}{ll} 1 & \\textrm{if } t_i \\leq c_i \\textrm{ (i.e. uncensored)} \\\\\n                   0 & \\textrm{if } t_i &gt; c_i \\textrm{ (i.e. censored)} \\end{array} \\right.\n\\] then \\[\nL(\\lambda) = \\prod_{i=1}^n  \\left[f(t_i) \\right]^{\\delta_i} \\left[ S(c_i)\\right]^{1- \\delta_i} =  \\prod_{i=1}^n  \\left[\\lambda e^{-\\lambda t_i} \\right]^{\\delta_i} \\left[ e^{-\\lambda c_i} \\right]^{1- \\delta_i}.\n\\] It follows that \\[\\begin{align}\nl(\\lambda) &= \\log{\\lambda} \\sum_{i=1}^n \\delta_i  - \\lambda \\sum_{i=1}^n t_i\\delta_i - \\lambda \\sum_{i=1}^n (1-\\delta_i)c_i \\\\\n\\frac{\\partial l}{\\partial \\lambda} &= \\frac{\\sum_{i=1}^n \\delta_i}{\\lambda}  - \\sum_{i=1}^n \\left( t_i\\delta_i + (1-\\delta_i)c_i \\right) \\\\\n\\Rightarrow \\hat{\\lambda} &= \\frac{\\sum_{i=1}^n \\delta_i}{\\sum_{i=1}^n \\left(  t_i\\delta_i + (1-\\delta_i)c_i \\right)}.\n\\end{align}\\]\nWe can use the asymptotic properties of maximum likelihood estimators to find confidence intervals for \\(\\hat{\\lambda}\\). We have \\[\n\\hat{\\lambda} \\xrightarrow{d} N \\left( \\lambda, I^{-1} \\right),\n\\] where \\(I^{-1}=var(\\hat{\\lambda})\\) is the variance-covariance matrix of \\(\\hat{\\lambda}\\). If we use the Fisher information then \\(I\\) is given by \\[\nI = E \\left[ - \\frac{\\partial^2 l}{\\partial \\lambda^2} \\right].\n\\] Now, in our case \\[\\frac{\\partial^2 l}{\\partial \\lambda^2} = -\\frac{\\sum \\delta_i}{\\lambda^2}\\] and (implicitly assuming that the \\(c_i\\) are considered non-random) \\[\\begin{align}\nE[\\delta_i] &=1 P(T_i \\leq c_i) + 0 P(T_i&gt;c_i) \\\\\n& = (1-e^{-\\lambda c_i}).\n\\end{align}\\] Replacing \\(\\lambda\\) with \\(\\hat{\\lambda}\\) we have that \\[\nvar(\\hat{\\lambda}) \\approx \\frac{\\hat{\\lambda}^2}{\\sum_{i=1}^n (1-e^{-\\hat{\\lambda} c_i})}.\n\\] Alternatively, we can use the observed information to specify \\(I\\). In this case we have \\[\nI= - \\frac{\\partial^2 l}{\\partial \\lambda^2} \\Bigr|_{\\lambda = \\hat \\lambda}\n\\] giving \\[\nvar(\\hat{\\lambda}) \\approx \\frac{\\hat{\\lambda}^2}{\\sum_{i=1}^n \\delta_i}.\n\\] To find a \\(100(1- \\alpha)\\%\\) confidence interval for \\(\\lambda\\) we use the fact that the distribution of \\(\\hat{\\lambda}\\) is normal giving us the interval \\[\n\\hat{\\lambda} \\pm 1.96 \\times \\sqrt{var(\\hat{\\lambda})}\n\\]\n\n\n3.1.3 Interest in Other Parameters\nOur interest may be in other aspects, e.g. the mean lifetime \\(\\mu= E[T]=\\lambda^{-1}\\); or the age \\(S_\\alpha\\) beyond which \\(100\\alpha\\%\\) survive. To estimate these we use the result that for any differentiable, monotonic function \\(g(\\lambda)\\), we have \\[\nvar(g(\\hat{\\lambda})) \\approx \\{g'(\\lambda)\\}^2  \\mid_{\\lambda = \\hat{\\lambda}}var(\\hat \\lambda).\n\\] Suppose we want to find the variance of the m.l.e of the mean lifetime \\(var(\\hat{\\mu})\\). Since \\(\\hat{\\mu} = 1/\\hat{\\lambda}\\) we have \\(g(\\lambda) = 1/\\lambda\\). Using the fisher information we get that \\[\nvar(\\hat{\\mu}) \\approx  \\frac{\\hat{\\mu}^2}{\\sum_{i=1}^n (1-e^{-\\hat{\\lambda} c_i})}.\n\\] If using the observed information we replace \\(\\sum_{i=1}^n (1-e^{-\\hat{\\lambda} c_i})\\) with \\(\\sum_{i=1}^n \\delta_i\\). Suppose we want to find the variance of the estimator of the time beyond which \\(100 \\alpha\\%\\) survive, \\(var(\\hat{S}_\\alpha)\\). We have \\(\\alpha = P(T \\geq S_\\alpha) = S(S_\\alpha) = e^{-\\lambda S_\\alpha}\\) and so \\[\\begin{align}\nS_\\alpha &= - \\lambda^{-1} \\log \\alpha  \\\\\n\\Rightarrow \\hat{S}_\\alpha &= - \\hat{\\lambda}^{-1} \\log \\alpha\n\\end{align}\\] and to find the variance, \\[\\begin{align}\nvar(\\hat{S}_\\alpha) &= var(- \\hat{\\lambda}^{-1} \\log \\alpha)  \\\\\n&= \\left[ -\\log \\alpha \\right]^2  var(\\hat{\\lambda}^{-1}) .\n\\end{align}\\]\n\n\n3.1.4 An Example: Lung Cancer Survival\nThe survival times (in days) of 10 patients with advanced lung cancer are shown in Table 3.1. The study terminated after 90 days and any patient still alive at that point was no longer observed. Patients joined the study at different times.\n\n\n\nTable 3.1: 90 day study of 10 patients with lung cancer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPatient.no.\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\nEntry time\n9\n18\n20\n30\n49\n59\n59\n60\n61\n69\n\n\nMax. possible \\(c_i\\)\n81\n72\n70\n60\n41\n31\n31\n30\n29\n21\n\n\nSurvival time \\(t_i\\)\n2\n\\(&gt;72\\)\n51\n\\(&gt;60\\)\n33\n27\n14\n24\n4\n\\(&gt;21\\)\n\n\n\\(\\delta_i\\)\n1\n0\n1\n0\n1\n1\n1\n1\n1\n0\n\n\n\n\n\n\nThere were \\(\\sum \\delta_i = 7\\) deaths during the study with \\[\\sum \\delta_i t_i = 155 \\quad \\textrm{and } \\sum (1- \\delta_i) c_i = 153.\\] Thus our estimate of the expected number of deaths per day (\\(\\hat{\\lambda}\\)) is 0.0227 and the m.l.e. of the mean lifetime is \\(\\hat{\\mu} = 1/\\hat{\\lambda} = 44.0\\) days. We can find a 95% confidence interval for \\(\\hat{\\lambda}\\) as (0.00586, 0.0395).\n\n\n3.1.5 Allowing Random Censoring (only examinable in MPS438)\nA full treatment of this topic is beyond the scope of this module but we’ll consider the simplest approaches and state some results. The simplest case is the where observations are collected over a fixed time interval of length \\(T_{max}\\). To introduce the randomness of the censoring in a simple way suppose subjects arrive uniformly over the interval \\((0, T_{max})\\) and that there is no other loss to follow up, so the censoring is caused only because the study ends before the subject experiences the event.\nLet \\(\\delta_i\\) be the censoring indicator. It can be shown (you should verify it) that \\[\nE[\\delta_i] = 1 - \\frac{1-e^{-\\lambda T_{max}}}{\\lambda T_{max}},\n\\] and so analogously to the previous section \\[\ns.e.(\\hat{\\lambda}) \\approx \\frac{\\hat{\\lambda}}{\\sqrt{n\\left(1 - \\frac{1-e^{-\\lambda T_{max}}}{\\lambda T_{max}}\\right)}}.\n\\] If recruitment is over a shorter period \\(R\\) but the study lasts for a time \\(T_{max}\\) so the arrival times are uniformly spread over \\((0, R)\\) then we have \\[\nE[\\delta_i] = 1 - \\frac{1-e^{-\\lambda (T_{max} - R)} - e^{-\\lambda T_{max}}}{\\lambda R}\n\\] with obvious modification to the standard error.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Parametric one-sample models</span>"
    ]
  },
  {
    "objectID": "parametric.html#survreg",
    "href": "parametric.html#survreg",
    "title": "3  Parametric one-sample models",
    "section": "3.2 Fitting Parametric models in R",
    "text": "3.2 Fitting Parametric models in R\nThe basic function to perform estimation using these models is survreg() and one of the arguments specifies which distribution to use from the options weibull, exponential, gaussian, logistic, lognormal and loglogistic. The default is weibull. It is also possible to use other distributions if the distribution function and density function are specified (see the help system if needed).\nCare needs to be taken with the parameterization in survreg() since it models the parameters in what may not initially seem the most intuitive way. It generally presents value for log of the parameter and these will need interpreting/transforming back into the actual survival times themselves. The reason for this is that survreg can be adapted to more complicated models (as we will later) where this parameterisation is more natural.\n\nFor the exponential model with rate parameter \\(\\lambda\\) (or mean \\(\\lambda^{-1}\\)), the MLE is \\(\\hat \\lambda =\\exp(-\\beta_0)\\), where \\(\\beta_0\\) is the survreg() intercept. It follows that \\(\\beta_0\\) represents the log(mean survival time). It is easiest to calculate a CI for \\(-\\beta_0\\), and then exponentiate the CI limits (base e) to get a confidence interval for the true value of \\(\\lambda\\).\nFor the Weibull model, the MLE of \\(\\lambda\\) is \\(\\hat \\lambda=\\exp(-\\beta_0)\\) as for the exponential case. \\(\\hat \\gamma\\) is the reciprocal of the ‘scale’ parameter provided directly in the survreg() summary.\n\n\n3.2.1 Fitting an Exponential model to the lung cancer data\nWe illustrate fitting a distribution to the lung cancer survival times. As with calculation of the non-parametric Kaplan-Meier estimate, we need to use the Surv() function to create a survival object which contains the information on censoring. This object can then be used in fitting any of the available survival regression models.\nWe fit an exponential model as follows:\n\nlibrary(survival)\nlcancer &lt;- data.frame(time = c(2, 72, 51, 60, 33, 27, 14, 24, 4, 21),\n                      censor = c(1, 0, 1, 0, 1, 1, 1, 1, 1, 0))\nlcancer_sv &lt;- Surv(lcancer$time, lcancer$censor, type = \"right\")\nlcancer_regexp &lt;- survreg(lcancer_sv ~ 1, dist = \"exponential\")\nsummary(lcancer_regexp)\n\n\nCall:\nsurvreg(formula = lcancer_sv ~ 1, dist = \"exponential\")\n            Value Std. Error  z      p\n(Intercept) 3.784      0.378 10 &lt;2e-16\n\nScale fixed at 1 \n\nExponential distribution\nLoglik(model)= -33.5   Loglik(intercept only)= -33.5\nNumber of Newton-Raphson Iterations: 4 \nn= 10 \n\n\nThus \\[\\hat{\\lambda}=\\frac{1}{\\text{exp(intercept)}} = \\frac{1}{\\text{exp}(3.78)} = 0.0228\\] and a 95% confidence interval for \\(\\lambda\\) is \\[\\frac{1}{\\exp( 3.78 \\pm 1.96 \\times 0.378)} = (0.0108, 0.0479)\\] The standard errors in the R output are not the same as the standard errors calculated above. That would give an approximate standard error of \\(\\hat \\lambda\\) as \\[\\frac{1}{\\sqrt{7}\\exp(3.78)} = 0.008626 \\hspace{1cm} \\left[ s.e.(\\hat{\\lambda}) \\approx \\frac{\\hat{\\lambda}}{\\sqrt{\\sum \\delta_i}} \\right]\\] and a confidence interval of (0.00589, 0.0397). These illustrate that calculations of standard errors and confidence intervals depend on the approach being used although the differences may often not be very large.\n\n\n3.2.2 Fitting a Weibull model to the tumour remission data\nThe default distribution for survreg is the Weibull distribution so you don’t need to specify a distribution argument to fit a Weibull distribution.\n\ntumour &lt;- data.frame(time = c(3, 6.5, 6.5, 10, 12, 15, 8.4, 4, 5.7, 10.1),\n                     censor = c(1, 1, 1, 1, 1, 1, 0, 0, 0, 0))\ntumour_sv &lt;- Surv(tumour$time, tumour$censor, type = \"right\")\ntumourSurvWeib &lt;- survreg(tumour_sv ~ 1, data = tumour)\nsummary(tumourSurvWeib)\n\n\nCall:\nsurvreg(formula = tumour_sv ~ 1, data = tumour)\n             Value Std. Error     z      p\n(Intercept)  2.420      0.147 16.41 &lt;2e-16\nLog(scale)  -1.018      0.312 -3.26 0.0011\n\nScale= 0.361 \n\nWeibull distribution\nLoglik(model)= -18.3   Loglik(intercept only)= -18.3\nNumber of Newton-Raphson Iterations: 6 \nn= 10 \n\n\nAs mentioned in Section 1.5.2, there are different parameterisations of the Weibull distribution, in particular, survreg() uses a different parameterisation to dweibull, pweibull etc in R. We relate each to the summary output above as follows:\n\n\n\n\n\n\n\nSection 1.5.2 parameterisation\nR dweibull parameterisation\n\n\n\n\n\\(\\hat{\\lambda} = \\exp(-\\)(Intercept))\n\\(\\hat{\\sigma} = \\exp(\\)(Intercept))\n\n\n\\(\\hat{\\gamma} = \\exp(-\\)Log(scale))\n\\(\\hat{a} = \\exp(-\\)Log(scale))\n\n\n\nFrom the R output we see that \\(\\hat \\lambda = \\exp(-2.42)=0.089\\) and \\(\\hat \\gamma = 1/0.361 = 2.8\\) . Remember that forcing the \\(\\gamma\\) parameter of the Weibull distribution to be 1 reduces the Weibull model to an exponential model.\n\n\n3.2.3 Checking goodness of fit\nA visual way to check goodness of fit is to plot the fitted survivor function on top of a Kaplan-Meier plot. Here is an example in R for the lung cancer data. Recall that we obtained \\(\\hat{\\lambda}=0.0228\\) for the exponential model.\n\nlibrary(survival)\nlibrary(survminer)\nlcancer &lt;- data.frame(time = c(2, 72, 51, 60, 33, 27, 14, 24, 4, 21),\n                      censor = c(1, 0, 1, 0, 1, 1, 1, 1, 1, 0))\nlcancer_sv &lt;- Surv(lcancer$time, lcancer$censor, type = \"right\")\nlcancerSurv &lt;- survfit(lcancer_sv ~ 1)\n\n# use ggsurvplot to obtain the Kaplan-Meier estimate...\n# ...but assign the result to a variable (KMplot) instead of displaying\n\nKMplot &lt;- ggsurvplot(lcancerSurv, data = lcancer_sv,\n           conf.int = TRUE, conf.int.style = \"ribbon\",\n           conf.int.alpha = 0.4, xlab = \"Time (months)\",\n           legend = \"none\"\n           )\n\n## KMplot$plot is a ggplot2 object that we can add to\n## We use stat_function to add the exponential fit\n## The survivor function is 1 - pexp(x, rate = 0.023)\n## as the m.l.e of lambda was 0.023\n\n## This will draw the Kaplan-Meier estimate and add the exponential fit:\nKMplot$plot +\n  stat_function(fun = function(x) 1-pexp(x, rate = 0.023), color = \"blue\")\n\n\n\n\n\n\n\n\nWe will do the same for the Weibull example:\n\ntumour &lt;- data.frame(time = c(3, 6.5, 6.5, 10, 12, 15, 8.4, 4, 5.7, 10.1),\n                     censor = c(1, 1, 1, 1, 1, 1, 0, 0, 0, 0))\ntumour_sv &lt;- Surv(tumour$time, tumour$censor, type = \"right\")\ntumourSurv &lt;- survfit(tumour_sv ~ 1)\n\nKMplot &lt;- ggsurvplot(tumourSurv, data = tumour,\n                conf.int = TRUE, conf.int.style = \"ribbon\",\n                conf.int.alpha = 0.4, xlab = \"Time (months)\",\n                legend = \"none\"\n)\nKMplot$plot +\n  stat_function(fun = function(x) 1 - pweibull(x, scale = exp(2.42), \n                                             shape = 1/exp(-1.018)), color = \"blue\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Parametric one-sample models</span>"
    ]
  },
  {
    "objectID": "parametric.html#tasks",
    "href": "parametric.html#tasks",
    "title": "3  Parametric one-sample models",
    "section": "3.3 Tasks",
    "text": "3.3 Tasks\nThe data below give the time to relapse (in weeks), following remission, for a group of leukaemia patients treated with a particular drug “Drug-6-MP”: (* indicates a right censored value).\n\\[6^*, 6, 6, 6, 7, 9^*, 10^*, 10, 11^*, 13, 16,\\] \\[17^*, 19^*, 20^*, 22, 23, 25^*, 32^*, 34^*, 35^*\\] These are the data used in the Chapter 2 Tasks. Here, we will suppose the remission times follow an exponential distribution with rate parameter \\(\\lambda\\).\n\nEstimate the MLE of the rate parameter \\(\\lambda\\).\nCalculate the median relapse time.\nCalculate the inter-quartile range of the relapse time.\nCalculate the probability that a leukaemia patient will not relapse within 6 weeks of remission.\nEstimate the density function of relapse time at 6 weeks.\nEstimate the hazard function of relapse time at 6 weeks.\nCalculate a 95% confidence interval for \\(\\lambda\\).\nA similar study estimates that 48% of leukaemia patients in the Drug-6-MP group do not relapse for at least 15 weeks. Is the confidence interval in (7) consistent with this finding?\nIn a similar Drug-6-MP group of 10 leukaemia patients, estimate the probability that at least 2 patients relapse within 8 weeks.\nCalculate a 95% confidence interval for expected relapse time.\nCalculate a 95% confidence interval for median relapse time.\nIn R, fit an exponential failure time distribution to the relapse time data for the Drug-6-MP patients. Assess the evidence that a Weibull distribution is more appropriate than the exponential distribution as a model of the relapse time.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nWe have\n\n\\[\\hat{\\lambda}=\\frac{\\text{Numer of non-censored observations}}{\\text{total time in study}}=9/359=0.025\\]\n\nThe median \\(t_M\\) is found by solving \\(F(t_M)=1/2\\). This gives our estimator as \\[\\hat{t}_M=ln(2)/\\hat\\lambda=27.65\\]\nThe upper quartile \\(U\\) is found by solving \\(F(U)=3/4\\) and the lower quartile \\(L\\) is found by solving \\(F(L)=1/4\\). We get \\(U=ln(4)/\\lambda\\) and \\(L=ln(4/3)/\\lambda\\). This gives the estimate of the IQR as \\[ln(3)/\\lambda=43.82\\]\nWe have\n\n\\[\\hat{S}(6)=\\exp(-6\\hat{\\lambda})=0.86\\]\n\nWe have\n\n\\[\\hat{f}(6)=\\hat\\lambda \\exp(-6\\hat\\lambda)=0.02.\\]\n\nWe have\n\n\\[\\hat{h}(6)=\\hat \\lambda=0.03.\\]\n\nThe confidence interval is given by \\(\\hat\\lambda\\pm1.96\\frac{\\hat \\lambda}{\\sqrt{\\sum \\delta_i}}=(\\) 0.009, 0.041)\n\n\nThere are two ways to approach this. The simplest way is to estimate \\(\\lambda\\) using the new data. Solving \\(S(15)=0.48\\) gives \\(\\hat \\lambda=0.049\\) which is not within the 95% CI calculated in (7). An alternative (but much more complicated!) approach is to\n\n\nuse \\(\\hat \\lambda =0.025\\) from the original data set to get an estimate of \\(S(15)\\)\nobtain a 95% confidence interval for \\(S(15)\\)\ncheck to see whether 0.48 is in this 95% confidence interval.\n\nTo implement this second approach, we proceed as follows. The survivor function at \\(t=15\\) is \\(S(15)=\\exp(-15 \\lambda)\\). Using the invariance property of the MLE we know that the MLE of \\(S(15)\\) is \\(\\widehat{S(15)}=\\exp(-15 \\hat\\lambda)\\) = 0.687.\nTo get the variance of \\(\\widehat{S(15)}\\) we use the method in Section 3.1.3. with \\(g(\\lambda)=\\exp(-15 \\lambda)\\). \\[\\begin{align}\ng(\\lambda) &= \\exp(-15 \\lambda)\\\\\ng'(\\lambda)^2 &=225\\exp(-30 \\lambda)\n\\end{align}\\] Therefore \\[\\begin{align}\nVar(\\widehat{S(15)})&=225\\exp(-30 \\hat\\lambda)Var(\\hat\\lambda)\\\\\n&=225\\exp(-30 \\hat\\lambda)\\frac{\\hat \\lambda^2}{\\sum \\delta_i}\n\\end{align}\\] Assuming normality of \\(\\widehat{S(15)}\\) the 95% CI is therefore \\[\n\\widehat{S(15)} \\pm 1.96 \\times 15\\exp(-15 \\hat\\lambda)\\frac{\\hat \\lambda}{\\sqrt{\\sum \\delta_i}}\n\\]\nUsing \\(\\widehat{S(15)}=0.687\\) the 95% CI for \\(S(15)\\) is \\((0.518,0.855)\\). This is therefore not consistent with the 0.48 estimate for \\(S(15)\\) from the other study.\n\nLet \\(X\\) represent the number of patients that relapse within 8 weeks. Then \\(X \\sim \\text{bin}(10, p)\\) where \\(p=F(8)=1-\\exp(-8\\lambda).\\) Therefore \\(\\hat p = 1-exp(-8\\hat\\lambda)=0.182\\). We require \\(P(X \\geq 2)=1-P(X\\leq 1)\\) which you can get from R using 1-pbinom(1, size=10, prob=0.182). The probability is \\(0.567.\\)\nThe estimate of the expected relapse time \\(\\hat \\mu\\) is given by \\(\\hat \\mu=1/\\hat\\lambda=39.89\\) (using the invariance property of the MLE). Using the expression on slide 25 of the Chapters 2 and 3 slides, the 95% for \\(\\hat \\mu\\) is \\(\\hat \\mu \\pm 1.96 \\frac{\\hat \\mu}{\\sqrt{\\sum\\delta_i}}\\) which is (13.8,65.9).\nUsing the answer to (2) we know that the estimator of the median is \\(\\widehat{t_M}=ln2/\\hat \\lambda=27.649\\). Using equation 53 in the notes with \\(\\alpha=0.5\\) we get that Var\\((\\widehat{t_M})=(ln2)^2Var(\\frac{1}{\\hat \\lambda})=(ln2)^2Var(\\hat \\mu)=(ln2)^2\\frac{\\hat\\mu^2}{\\sum\\delta_i}\\)\n\nAgain assuming normality of \\(\\widehat{t_M}\\), a 95% CI is therefore \\(\\widehat{t_M}\\pm 1.96 \\times ln2 \\times \\frac{\\hat\\mu}{\\sqrt{\\sum\\delta_i}}=(21.8,58)\\).\n\nTo fit distributions for the relapse time in R we first specify the data\n\n\nleukaemia &lt;- data.frame(time = c(6, 6, 6, 6, 7, 9, 10,\n                                10, 11, 13, 16, 17, 19,\n                                20, 22, 23, 25, 32, 32,\n                                34, 35),\n                       censor = c(0, 1, 1, 1, 1, 0, 0, 1,\n                                  0, 1, 1, 0, 0, 0, 1, 1, 0, 0,\n                                  0, 0, 0)\n)\nleukaemia_sv &lt;- survival::Surv(leukaemia$time, leukaemia$censor)\n\nThen we fit the exponential model using\n\nleukexp &lt;- survival::survreg(leukaemia_sv ~ 1, dist = \"exponential\")\nsummary(leukexp)\n\n\nCall:\nsurvival::survreg(formula = leukaemia_sv ~ 1, dist = \"exponential\")\n            Value Std. Error    z      p\n(Intercept) 3.686      0.333 11.1 &lt;2e-16\n\nScale fixed at 1 \n\nExponential distribution\nLoglik(model)= -42.2   Loglik(intercept only)= -42.2\nNumber of Newton-Raphson Iterations: 4 \nn= 21 \n\n\nand we fit a Weibull using\n\nleukexp &lt;- survival::survreg(leukaemia_sv ~ 1, dist = \"weibull\")\nsummary(leukexp)\n\n\nCall:\nsurvival::survreg(formula = leukaemia_sv ~ 1, dist = \"weibull\")\n             Value Std. Error     z      p\n(Intercept)  3.519      0.273 12.87 &lt;2e-16\nLog(scale)  -0.303      0.278 -1.09   0.28\n\nScale= 0.739 \n\nWeibull distribution\nLoglik(model)= -41.7   Loglik(intercept only)= -41.7\nNumber of Newton-Raphson Iterations: 5 \nn= 21 \n\n\nThe null hypothesis here is that the scale parameter is 1 (log(scale)=0). The alternative hypothesis is that it is not equal to 1. The p-value of 0.28 means that there is little evidence to reject the null. There is little evidence that the more general weibull distribution is needed here.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Parametric one-sample models</span>"
    ]
  },
  {
    "objectID": "twosample.html",
    "href": "twosample.html",
    "title": "4  Two Sample Comparisons",
    "section": "",
    "text": "4.1 Introduction\nIn our work so far, we have only considered data from a single sample. However, a common problem is the comparison of two (or more) survival distributions, e.g. we might wish to find out which of two treatments is the better or whether the pattern of survivals/deaths for Males is different from that for Females.\nOne simple comparison would be to plot the Kaplan-Meier estimates for each group. The question is then whether there is a statistically significant difference between the two curves.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Two Sample Comparisons</span>"
    ]
  },
  {
    "objectID": "twosample.html#log-rank-test-non-parametric",
    "href": "twosample.html#log-rank-test-non-parametric",
    "title": "4  Two Sample Comparisons",
    "section": "4.2 Log Rank Test (Non-Parametric)",
    "text": "4.2 Log Rank Test (Non-Parametric)\n\n4.2.1 Example: Brain Tumour Survival Times\nTwelve brain tumour patients were randomized and received either radiation, or radiation and chemotherapy. One year after the start of the study the survival times in weeks are shown in Table 4.1.\n\n\n\nTable 4.1: Survival times of 12 patients undergoing either radiation (RT) or radiation and chemotherapy (RT+CT). A * denotes a censored observation.\n\n\n\n\n\nGroup 1 RT:\n10\n26\n28\n30\n41\n12*\n\n\n\n\nGroup 2 RT+CT:\n24\n30\n42\n15*\n40*\n42*\n\n\n\n\n\n\nWe can produce separate Kaplan-Meier plots for each group as follows\n\nlibrary(survival)\nlibrary(survminer)\nbraintu &lt;- data.frame(time = c(10, 26, 28, 30, 41, 12,\n                               24, 30, 42, 15, 40, 42),\n                      censor = c(1, 1, 1, 1, 1, 0,\n                                 1, 1, 1, 0, 0, 0),\n                      group = factor(rep(c(\"RT\", \"RT plus CT\"), each = 6)))\n\nbrain_sv &lt;- Surv(braintu$time, braintu$censor, type = \"right\")\nbrainSurv &lt;- survfit(brain_sv ~ group, data = braintu)\nggsurvplot(brainSurv, conf.int = TRUE)\n\n\n\n\n\n\n\nFigure 4.1: Kaplan-Meier Estimates of the survivor functions for two brain tumour treatments\n\n\n\n\n\nFigure 4.1 allows a visual comparison of the K-M estimates for each treatment but we want to formally test if there is a difference in the survival functions for the two types of treatment.\n\n\n4.2.2 Implementing the Log Rank Test\nIf \\(S_1(t)\\) and \\(S_2(t)\\) are the survival functions under the two conditions, we wish to test \\[\\begin{align}\nH_0: \\quad & S_1(t) = S_2(t) \\nonumber \\\\\nH_1: \\quad & S_1(t)\\neq S_2(t) \\quad \\textrm{for some } t\n\\end{align}\\] To perform the test, we order the times of death for the two groups combined i.e. \\[t_{(1)} &lt; t_{(2)} &lt; \\ldots\\] We then find the expected number of deaths in each group. Under \\(H_0\\) where there is no difference in groups, at the occurrence of the first death at time \\(t=10\\), there are 12 individuals at risk and one death. Six of those individuals are in group 1 and six are in group 2. Hence, under \\(H_0\\) we would expect \\(1 \\times \\frac{6}{12}\\) of the deaths at time \\(t_{(1)}\\) to be in group 1 and \\(1\\times \\frac{6}{12}\\) of them to be in group 2.\nThe next death occurs at \\(t_{(2)}=24\\) when there are four individuals in group 1 and five in group 2. Hence we would expect \\(1\\times \\frac{4}{9}\\) deaths in group 1 and \\(1 \\times \\frac{5}{9}\\) in group 2. We can continue to do this until all the deaths have occurred.\nHaving done this for every time \\(t_{(i)}\\) we can sum the number of expected deaths in each group to find \\(E_{i}\\), the expected total number of deaths in group \\(i\\). We can compare these with the total observed number of deaths \\(O_i\\) in the groups and find the log-rank statistic under \\(H_0\\). \\[\nLR = \\frac{(O_1 - E_1)^2}{E_1} + \\frac{(O_2 - E_2)^2}{E_2} \\sim \\chi^2_1\n\\]\n\n\n4.2.3 Log Rank Test applied to the Brain Tumour Data\nWe perform this test on our Brain Tumour data as shown in Table 4.2. In the table, \\(r\\), \\(d\\) and \\(e\\) represent the numbers at risk, actual number of deaths and expected number of deaths respectively. Here \\[LR = 2.46 &lt; \\chi^2_{1, 0.95}\\] i.e. there is no significant difference in survivor functions at the 5% level.\n\n\n\nTable 4.2: Calculations for the log rank test applied to the brain tumour data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(i\\)\n\\(t_i\\)\n\\(r_{1i}\\)\n\\(r_{2i}\\)\n\\(r_i\\)\n\\(d_{1i}\\)\n\\(d_{2i}\\)\n\\(d_i\\)\n\\(e_{1i}\\)\n\\(e_{2i}\\)\n\n\n1\n10\n6\n6\n12\n1\n0\n1\n1/2\n1/2\n\n\n2\n24\n4\n5\n9\n0\n1\n1\n4/9\n5/9\n\n\n3\n26\n4\n4\n8\n1\n0\n1\n1/2\n1/2\n\n\n4\n28\n3\n4\n7\n1\n0\n1\n3/7\n4/7\n\n\n5\n30\n2\n4\n6\n1\n1\n2\n2/3\n4/3\n\n\n6\n41\n1\n2\n3\n1\n0\n1\n1/3\n2/3\n\n\n7\n42\n0\n2\n2\n0\n1\n1\n0\n1\n\n\nTotals\n\n\n\n\n\\(O_1 = 5\\)\n\\(O_2 = 3\\)\n\n\\(E_1 = 2.87\\)\n\\(E_2 = 5.13\\)\n\n\n\n\n\n\nThe log rank test can be generalised to more than 2 groups. If we have \\(k\\) groups then the log rank statistic has a \\(\\chi^2_{k-1}\\) distribution under \\(H_0\\).\n\n\n4.2.4 Performing the Log Rank Test in R\nThe function for performing log rank tests is survdiff(). The procedure is similar to calculating a non-parametric (i.e. Kaplan-Meier) survival model. We start by creating a survival object using the Surv() function. We then regress the Surv object on the group variable. We do this for the brain tumour survival times:\n\nnames(braintu) # look for name of grouping variable\n\n[1] \"time\"   \"censor\" \"group\" \n\nsurvdiff(brain_sv ~ group, data = braintu)\n\nCall:\nsurvdiff(formula = brain_sv ~ group, data = braintu)\n\n                 N Observed Expected (O-E)^2/E (O-E)^2/V\ngroup=RT         6        5     2.87     1.575      2.88\ngroup=RT plus CT 6        3     5.13     0.882      2.88\n\n Chisq= 2.9  on 1 degrees of freedom, p= 0.09 \n\n\nNote that the numerical value of the test statistic is slightly different from our log rank statistic value of 2.46 since R handles ties in a more sophisticated way.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Two Sample Comparisons</span>"
    ]
  },
  {
    "objectID": "twosample.html#parametric-tests",
    "href": "twosample.html#parametric-tests",
    "title": "4  Two Sample Comparisons",
    "section": "4.3 Parametric Tests",
    "text": "4.3 Parametric Tests\nTo perform parametric tests, we generally need to use asymptotic properties of maximum likelihood estimates of parameters or likelihood ratios. The exponential lifetime model is given here as an illustration, but all other models could be handled similarly with numerical estimation of parameters and the asymptotic variances.\n\n4.3.1 MLE Test\nSuppose we have \\(n_1\\) observations of \\(T_1 \\sim Exp(\\lambda_1)\\) and \\(n_2\\) observations of \\(T_2 \\sim Exp(\\lambda_2)\\). Now the mles are given by \\[\\hat{\\lambda}_j = \\frac{\\sum_{i=1}^{n_j} \\delta_{ji}}{\\sum_{i=1}^{n_j} t_{ji}} = \\frac{\\Delta_j}{\\mathcal{T}_j} \\quad \\textrm{for } j = 1, 2;\\] where \\(\\Delta_j\\) is the number of deaths in group \\(j\\) and \\(\\mathcal{T}_j\\) is the total time on test in group \\(j\\) (i.e the sum of all observed failure times and censored times).\nUsing the results in Section 3.1.2 we know that \\[\\hat{\\lambda}_j \\approx N \\left( \\lambda_j, \\frac{\\lambda_j^2}{\\Delta_j} \\right) \\quad \\textrm{for } j = 1, 2.\\] Replacing \\(\\lambda_j\\) in the variance term with its estimate \\(\\hat{\\lambda}_j\\) we have \\[\\hat{\\lambda}_1 - \\hat{\\lambda}_2 \\approx N \\left( \\lambda_1 - \\lambda_2 , \\frac{\\hat{\\lambda}_1^2}{\\Delta_1} + \\frac{\\hat{\\lambda}_2^2}{\\Delta_2}\\right).\\] Hence, to test the hypothesis \\[\\begin{align}\nH_0: \\quad & S_1(t) = S_2(t) \\\\\nH_1: \\quad & S_1(t)\\neq S_2(t) \\quad \\textrm{for some } t\n\\end{align}\\] we first note that (in our parametric setting) it is equivalent to testing \\[\\begin{align}\nH_0: \\quad & \\lambda_1 =\\lambda_2 \\\\\nH_1: \\quad & \\lambda_1  \\neq \\lambda_2\n\\end{align}\\] and under \\(H_0\\) \\[\\begin{equation}\nW=\\frac{\\hat{\\lambda}_1-\\hat{\\lambda}_2}{\\sqrt{\\frac{\\hat{\\lambda}_1^2}{\\Delta_1}+\\frac{\\hat{\\lambda}_2^2}{\\Delta_2}}}\\approx N(0,1).\n\\end{equation}\\]\n\n\n4.3.2 MLE Test applied to the Brain Tumour Data\nWe have\n\n\\(n_1= 6, \\; \\Delta_1=5 \\; \\mathcal{T}_1=147, \\; \\hat{\\lambda}_1 = 5/147 = 0.034\\)\n\\(n_2=6, \\; \\Delta_2=3, \\; \\mathcal{T}_2=193, \\; \\hat{\\lambda}_2 = 3/193=0.0155\\)\n\nThese give \\(W=1.05\\) and comparing this with 1.96 (0.975 quantile of the \\(N(0,1)\\) distribution) we have no evidence at the 5% level that the survivor functions differ.\n\n\n4.3.3 Likelihood Ratio Test\nAn alternative parametric approach is to use the likelihood ratio test. We have the likelihood \\[L(\\lambda_1,\\lambda_2) = L(\\lambda_1)L(\\lambda_2)\\] and so to maximize \\(L(\\lambda_1,\\lambda_2)\\) we can maximise with respect to \\(\\lambda_1\\) and \\(\\lambda_2\\) separately. Equation @ref(eq:censexplike) gives \\[L_{max}(\\lambda_1,\\lambda_2) = L(\\hat{\\lambda}_1, \\hat{\\lambda}_2) = \\hat{\\lambda}_1^{\\Delta_1} e^{-\\hat{\\lambda}_1 \\mathcal{T}_1} \\hat{\\lambda}_2^{\\Delta_2} e^{-\\hat{\\lambda}_2 \\mathcal{T}_2}\\] where \\(\\hat{\\lambda}_i = \\frac{\\Delta_i}{\\mathcal{T}_i}\\) for \\(i = 1, 2\\). Whereas, if \\(H_0\\) is true, then the likelihood is \\[\\begin{align}\nL(\\lambda,\\lambda) &= \\lambda^{\\Delta_1 + \\Delta_2} e^{-\\lambda (\\mathcal{T}_1 +\\mathcal{T}_2)} \\\\\n\\Rightarrow l(\\lambda,\\lambda) &= (\\Delta_1+\\Delta_2)\\log \\lambda - \\lambda (\\mathcal{T}_1 +\\mathcal{T}_2) \\\\\n\\Rightarrow \\tilde{\\lambda} &= \\frac{\\Delta_1+\\Delta_2}{\\mathcal{T}_1 +\\mathcal{T}_2} \\\\\n\\Rightarrow L(\\tilde{\\lambda},\\tilde{\\lambda}) &=  \\tilde{\\lambda}^{\\Delta_1 + \\Delta_2} e^{-\\tilde{\\lambda} (\\mathcal{T}_1 +\\mathcal{T}_2)}.\n\\end{align}\\] where \\(\\tilde{\\lambda} = \\frac{\\Delta_1 + \\Delta_2}{\\mathcal{T}_1+ \\mathcal{T}_2}\\) is the MLE under \\(H_0\\). So using the generalised likelihood ratio test we have, under \\(H_0\\), \\[2 \\left\\{ l(\\hat{\\lambda}_1, \\hat{\\lambda}_2) - l(\\tilde{\\lambda},\\tilde{\\lambda}) \\right\\} \\approx \\chi^2_1\\] i.e. \\[\\begin{equation}\n2 \\left\\{ \\Delta_1 \\log \\frac{\\Delta_1}{\\mathcal{T}_1} + \\Delta_2 \\log \\frac{\\Delta_2}{\\mathcal{T}_2} - (\\Delta_1 + \\Delta_2) \\log \\frac{\\Delta_1 + \\Delta_2}{\\mathcal{T}_1 + \\mathcal{T}_2} \\right\\} \\approx \\chi^2_1\n\\end{equation}\\]\n\n\n4.3.4 Likelihood Ratio Test applied to the Brain Tumour Data\nCalculation gives the test statistic as 1.20 and since \\(1.20 &lt; 3.84\\) (\\(\\chi^2_{1, 0.95}=3.84\\)) we again have no evidence at the 5% level that the survivor functions differ.\n\n\n4.3.5 Performing the Likelihood Ratio Test in R\nWe can do the MLE test using the survreg() function by specifying the dist argument to be exponential.\n\nbrain_sv &lt;- Surv(braintu$time, braintu$censor)\nbr_regexp &lt;- survreg(brain_sv ~ group, data = braintu, dist = \"exponential\")\nsummary(br_regexp)\n\n\nCall:\nsurvreg(formula = brain_sv ~ group, data = braintu, dist = \"exponential\")\n                Value Std. Error    z     p\n(Intercept)     3.381      0.447 7.56 4e-14\ngroupRT plus CT 0.783      0.730 1.07  0.28\n\nScale fixed at 1 \n\nExponential distribution\nLoglik(model)= -37.4   Loglik(intercept only)= -38\n    Chisq= 1.2 on 1 degrees of freedom, p= 0.27 \nNumber of Newton-Raphson Iterations: 4 \nn= 12 \n\nbr_regexp$var # gives the covariance matrix of the model estimates\n\n                (Intercept) groupRT plus CT\n(Intercept)             0.2      -0.2000000\ngroupRT plus CT        -0.2       0.5333333\n\n\nNote that:\n\n\\(\\hat \\lambda_1 = 1/\\exp(3.381) = 0.034\\)\n\\(\\hat \\lambda_2 = 1/\\exp(3.381+0.783) = 0.015\\)\nPreviously, in Section 3.1.2, we found \\(\\text{se}(\\hat \\lambda_1) \\approx \\exp(-3.381)/\\sqrt{5}\\)\n\\(\\text{se}(\\hat \\lambda_2) \\approx (0.447^2+0.7302^2 + 2(-0.2))^{\\frac{1}{2}}/\\exp(3.381+0.783)\\)\nIt is simpler to note in the R output that the p-value for testing whether the parameter indicating the group is zero is 0.28 (this tests whether the groups differ in their survival curves)\nThis is close to the \\(p\\)-value of 0.29 for the MLE test statistic of 1.05 calculated in Section 4.3.2 given by 2*(1-pnorm(1.05)\nThe \\(\\chi^2_1\\) value of 1.2 (giving a p-value of 0.27) given above is the value of the likelihood ratio test statistic.\n\nIt can be shown that the MLE and likelihood ratio tests are asymptotically equivalent so for large sample sizes they should lead to similar conclusions. There is some research showing that the LRT is more appropriate for small samples although this is not a hard rule. In large samples, little power is gained by using parametric methods instead of the log-rank test.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Two Sample Comparisons</span>"
    ]
  },
  {
    "objectID": "twosample.html#tasks",
    "href": "twosample.html#tasks",
    "title": "4  Two Sample Comparisons",
    "section": "4.4 Tasks",
    "text": "4.4 Tasks\nConsider again the leukaemia data from the Chapter 2 Tasks. We have the time to relapse (in weeks), following remission, for a group of leukaemia patients treated with a particular drug “Drug-6-MP”: (* indicates a right censored value).\n\\[6^*, 6, 6, 6, 7, 9^*, 10^*, 10, 11^*, 13, 16,\\]\n\\[17^*, 19^*, 20^*, 22, 23, 25^*, 32^*, 34^*, 35^*\\]\nIn addition, there was a control group, with patients randomised to either the drug or control. Times to relapse for the control group were as follows: \\[\n1, 1, 2, 2, 3, 4, 4, 5, 5, 8, 8,\n\\] \\[\n8, 8, 11, 11, 12, 12, 15, 17, 22, 23\n\\]\n\nUse a log rank test to compare the two groups and report your conclusions. Do the calculations by hand!\nUse the following code to set up the data in R. Then do the log rank test in R to check your result. Note that the R output will give the same expected number of remissions in each group, but will report a slightly different test statistic value, as it applies an adjustment for tied event times. You don’t need to worry about this!\n\n\nleukaemia &lt;- data.frame(time = c(6, 6, 6, 6, 7, 9, 10,\n                                10, 11, 13, 16, 17, 19,\n                                20, 22, 23, 25, 32, 32,\n                                34, 35,\n                                1, 1, 2, 2, 3, 4, 4, 5, 5, 8, 8,\n                                8, 8, 11, 11, 12, 12, 15, 17, 22, 23),\n                       censor = c(0, 1, 1, 1, 1, 0, 0, 1,\n                                  0, 1, 1, 0, 0, 0, 1, 1, 0, 0,\n                                  0, 0, 0, rep(1, 21)),\n                       group = factor(rep(c(\"drug6MP\", \"control\"), each = 21))\n)\n\n\nDo a suitable plot in R to compare the two groups, and comment on how this relates to your result in question 1.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nPerforming the log-rank test by hand we get\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(i\\)\n\\(t_i\\)\n\\(r_{1i}\\)\n\\(r_{2i}\\)\n\\(r_i\\)\n\\(d_{1i}\\)\n\\(d_{2i}\\)\n\\(d_i\\)\n\\(e_{1i}\\)\n\\(e_{2i}\\)\n\n\n\n\n1\n1\n21\n21\n42\n0\n2\n2\n1.00\n1.00\n\n\n2\n2\n21\n19\n40\n0\n2\n2\n1.05\n0.95\n\n\n3\n3\n21\n17\n38\n0\n1\n1\n0.55\n0.45\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n17\n23\n6\n1\n7\n1\n1\n2\n1.71\n0.28\n\n\nTotals\n\n\n\n\n\\(O_1 = 9\\)\n\\(O_2 = 21\\)\n\n\\(E_1 = 19.25\\)\n\\(E_2 = 10.75\\)\n\n\n\nwhich gives us a \\(\\chi^2\\) statistic of 15.28 on 1 degree of freedom. Given that the 95th percentile of the \\(\\chi^2_1\\) distribution is 3.84, this test provides very strong evidence that the survival patterns are different with the treated group having better survival prospects.\n\nIn R we do\n\n\nlibrary(survival)\nleukS &lt;- Surv(leukaemia$time, leukaemia$censor)\nsurvdiff(leukS ~ group, data = leukaemia) \n\nCall:\nsurvdiff(formula = leukS ~ group, data = leukaemia)\n\n               N Observed Expected (O-E)^2/E (O-E)^2/V\ngroup=control 21       21     10.7      9.77      16.8\ngroup=drug6MP 21        9     19.3      5.46      16.8\n\n Chisq= 16.8  on 1 degrees of freedom, p= 4e-05 \n\n\nAs stated in the question, we get the same expected number of remissions in group, but a slightly different test statistic value.\n\nA suitable plot here would be Kaplan Meier estimates for the two groups:\n\n\nlibrary(survminer)\nleukR &lt;- survfit(leukS ~ group, data = leukaemia)\nggsurvplot(leukR, conf.int = TRUE)\n\n\n\n\n\n\n\n\nHere we can see the survivor functions are clearly different, and the confidence intervals do not overlap.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Two Sample Comparisons</span>"
    ]
  },
  {
    "objectID": "aft.html",
    "href": "aft.html",
    "title": "5  Accelerated Failure Time (AFT) Models",
    "section": "",
    "text": "5.1 Motivation\nSo far we have just looked at problems when all the failure times of all the individuals are assumed IID or at most there are a fixed number of distinct groups with a common failure time distribution for each group. However, each individual might have their own failure time distribution dependent upon a number of characteristics about either them or their treatment. For a more sensitive analysis, we might want to include in our failure-time model the possible effect of these explanatory variables. Examples of such variables might be:\nFor each individual, with explanatory variables \\(\\mathbf{x}\\) we may wish to allow their survival time to depend upon these variables. When we do so, we typically define \\(\\mathbf{x} = \\mathbf{0}\\) to correspond to a baseline and then measure \\(\\mathbf{x}\\) relative to this baseline.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Accelerated Failure Time (AFT) Models</span>"
    ]
  },
  {
    "objectID": "aft.html#motivation",
    "href": "aft.html#motivation",
    "title": "5  Accelerated Failure Time (AFT) Models",
    "section": "",
    "text": "Treatment given (typically binary or categorical if multiple treatments are considered)\nAge of individual (continuous)\nSex of individual (categorical)\nMeasured variables describing prior medical history",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Accelerated Failure Time (AFT) Models</span>"
    ]
  },
  {
    "objectID": "aft.html#a-simple-two-sample-example",
    "href": "aft.html#a-simple-two-sample-example",
    "title": "5  Accelerated Failure Time (AFT) Models",
    "section": "5.2 A Simple Two-Sample Example",
    "text": "5.2 A Simple Two-Sample Example\nConsider initially the example where we have \\(n\\) individuals belonging to one of two groups dependent upon whether they are controls (Group 0) or receive a treatment (Group 1). In this case the binary variable \\[\n\\mathbf{x_i} = \\left\\{ \\begin{array}{rl}\n                0 & \\textrm{if person $i$ is in the control group} \\\\\n                1 & \\textrm{if person $i$ is in the treatment group}\n\\end{array} \\right.\n\\] denotes which group the \\(i^{th}\\) individual belongs to. Suppose the failure times of the individuals in the control and treatment groups are labelled as \\(T_0\\) and \\(T_1\\) respectively. If \\(S_0(t)\\) and \\(S_1(t)\\) are the survivor functions then we could choose to model them as \\[\\begin{align}\n\\text{Group 0:}& \\quad S_0(t) = P(T_0 &gt; t)  \\\\\n\\text{Group 1:}& \\quad S_1(t) = P(T_1 &gt; t) = P(T_0 &gt; \\psi t) = S_0(\\psi t)\n\\end{align}\\] so that, in terms of random variables \\(T_1 = T_0/\\psi\\). Intuitively this means that lifetime is accelerated/decelerated in group 1 compared with group 0. Any individual with survival time \\(t\\) in group 0 would have survival time \\(t/\\psi\\) in group 1. If \\(\\psi &gt; 1\\) then individuals will fail more quickly in group 1 compared with group 0 while if \\(\\psi &lt; 1\\) then they will fail more slowly.\n\n\n\n\n\n\nNote\n\n\n\nWe refer to \\(S_0\\) as the “baseline” survivor function, and the distribution of \\(T_0\\) as the baseline survival distribution. AFT models involve specifying how explanatory variables change this baseline distribution.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Accelerated Failure Time (AFT) Models</span>"
    ]
  },
  {
    "objectID": "aft.html#sec-AFTgeneral",
    "href": "aft.html#sec-AFTgeneral",
    "title": "5  Accelerated Failure Time (AFT) Models",
    "section": "5.3 A General AFT Model Framework for including Explanatory Variables",
    "text": "5.3 A General AFT Model Framework for including Explanatory Variables\nSuppose instead that we have a vector of explanatory variables (\\(\\mathbf{x}\\)) for each individual. We can extend this idea by modelling the survivor function of an individual with variables \\(\\mathbf{x}\\) as \\[\\begin{equation}\nS(t;\\mathbf{x}) = S_0(t \\psi(\\mathbf{x}))\n\\end{equation}\\] where \\(S_0(t)\\) corresponds to the survivor function of the chosen baseline \\(\\mathbf{x} = \\mathbf{0}\\) for any positive function \\(\\psi(\\mathbf{x})\\) (with \\(\\psi(\\mathbf{0}) = 1\\) to satisfy \\(S(t;\\mathbf{x}=\\mathbf{0}) = S_0(t)\\)). With this parameterisation, we have the density and hazard functions (as functions of \\(t\\) and \\(\\mathbf{x}\\)) given by \\[\\begin{align}\nf(t;\\mathbf{x}) &= f_0(t \\psi(\\mathbf{x})) \\psi(\\mathbf{x})  \\\\\nh(t;\\mathbf{x}) &= h_0(t \\psi(\\mathbf{x})) \\psi(\\mathbf{x})\n\\end{align}\\]\nIn terms of random variables this is equivalent to defining \\[\nT = T_0 / \\psi(\\mathbf{x}),\n\\] where \\(T_0\\) has survivor function \\(S_0(\\cdot)\\). Since we require \\(\\psi(\\mathbf{x}) \\geq 0\\) and \\(\\psi(\\mathbf{0}) = 1\\) a natural choice is \\[\n\\psi(\\mathbf{x}) = e^{-\\boldsymbol{\\beta}'\\mathbf{x}}.\n\\] For this choice of \\(\\psi(\\mathbf{x})\\), we can write \\[\n\\log T = \\boldsymbol{\\beta}'\\mathbf{x} + \\log T_0.\n\\]\n\n\n\n\n\n\nNote\n\n\n\n\nWe can think of this of two equivalent ways to specify an AFT model: \\[\nS(t;\\mathbf{x}) = S_0(t \\exp(-\\boldsymbol{\\beta}'\\mathbf{x})) \\quad\\Leftrightarrow\\quad \\log T = \\boldsymbol{\\beta}'\\mathbf{x} + \\log T_0,\n\\] where \\(T\\) and \\(T_0\\) are two random survival times with survivor functions \\(S\\) and \\(S_0\\) respectively. Note that \\(T_0\\) is the survival time of an individual with covariates \\(\\mathbf{x}=\\mathbf{0}\\), and \\(T\\) is the survival time of an individual with some other value for the covariate vector \\(\\mathbf{x}\\).\nIf we denote \\(\\log T_0\\) by \\(\\varepsilon\\), noting that \\(\\varepsilon\\) is a random variable, and we write \\[\n\\log T = \\boldsymbol{\\beta}'\\mathbf{x} + \\varepsilon,\n\\] then we can see that we have specified a log-linear regression model that relates a log survival time \\(\\log T\\) to some covariates \\(\\mathbf{x}\\). If you have studied generalised linear models (GLMs), then note that this is not necessarily a GLM, as we may choose distributions for \\(\\varepsilon\\) that are not in the exponential family, and there may also be a need to incorporate censored observations.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Accelerated Failure Time (AFT) Models</span>"
    ]
  },
  {
    "objectID": "aft.html#choice-of-distribution-in-aft-models",
    "href": "aft.html#choice-of-distribution-in-aft-models",
    "title": "5  Accelerated Failure Time (AFT) Models",
    "section": "5.4 Choice of distribution in AFT models",
    "text": "5.4 Choice of distribution in AFT models\nTo specify an AFT model, we need to make a choice of distribution: the survivor function \\(S_0\\), or equivalently, the distribution of \\(\\log T_0\\). In theory, the only constraint is that we must have \\(T_0 &gt;0\\), otherwise, any distribution can be used. However, when fitting an AFT model in R (using the survival::survreg() function), there are some restrictions, and the recommended choices are weibull (or exponential as a special case of the weibull), log normal or log logistic (i.e. \\(\\log T_0\\) has a normal/logistic distribution.)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Accelerated Failure Time (AFT) Models</span>"
    ]
  },
  {
    "objectID": "aft.html#the-exponential-aft-model",
    "href": "aft.html#the-exponential-aft-model",
    "title": "5  Accelerated Failure Time (AFT) Models",
    "section": "5.5 The exponential AFT model",
    "text": "5.5 The exponential AFT model\nThe simplest AFT model is to assume an exponential distribution for the baseline distribution of \\(T_0\\). We then assume that each patient’s survival time has an exponential distribution, with a rate parameter that depends on the patient’s explanatory variables.\nSuppose that we have \\(n\\) individuals where for each we have observed\n\\[\\begin{align}\nt_i &= \\min(\\textrm{Failure time}, \\textrm{Time of right censoring}) \\\\\n\\delta_i &= \\left\\{ \\begin{array}{rl}\n                1 & \\textrm{if $i$ is observed to fail} \\\\\n                0 & \\textrm{if $i$ is censored}\n               \\end{array} \\right. \\\\\n\\mathbf{x_i} &= \\textrm{$p$-dimensional vector of explanatory variables}\n\\end{align}\\] For an exponential AFT model, following the specification set out in Section 5.3, with the choice \\[\n\\psi(\\mathbf{x}) = e^{-\\boldsymbol{\\beta}'\\mathbf{x}},\n\\] we have \\[\\begin{align}\nS_0(t)= e^{-\\lambda t} & \\quad \\Rightarrow \\quad S(t;\\mathbf{x}) = e^{-\\lambda t e^{-\\boldsymbol{\\beta}'\\mathbf{x}}}  \\\\\nh_0(t)=\\lambda & \\quad \\Rightarrow \\quad h(t;\\mathbf{x}) = \\lambda e^{-\\boldsymbol{\\beta}' \\mathbf{x}}  \\\\\nf_0(t)=\\lambda e^{-\\lambda t} & \\quad \\Rightarrow \\quad f(t;\\mathbf{x})=\\lambda e^{-\\boldsymbol{\\beta}'\\mathbf{x}}e^{-\\lambda te^{-\\boldsymbol{\\beta}' \\mathbf{x}}}\n\\end{align}\\] Note that this is equivalent to assuming that with explanatory variables \\(\\mathbf{x}\\) the failure time \\[T \\sim Exp(\\lambda e^{-\\boldsymbol{\\beta}' \\mathbf{x}}).\\]\n\n5.5.1 Parameter estimation\nWe estimate the \\(p+1\\) parameters \\(\\lambda, \\beta_1, \\beta_2, \\ldots, \\beta_p\\) by maximum likelihood. We write the likelihood as a product of terms representing the observed failures and a product of terms representing the censored observations. \\[\\begin{align}\nL(\\lambda,\\boldsymbol{\\beta}) &= \\prod_{i=1}^n f(t_i;\\mathbf{x_i})^{\\delta_i} S(t_i;\\mathbf{x_i})^{1-\\delta_i} \\\\\n&= \\prod_{i=1}^n \\left[ \\lambda e^{-\\boldsymbol{\\beta}' \\mathbf{x_i}} e^{-\\lambda t_i e^{-\\boldsymbol{\\beta}' \\mathbf{x_i} }} \\right]^{\\delta_i} \\left[  e^{-\\lambda t_i e^{-\\boldsymbol{\\beta}' \\mathbf{x_i}} } \\right]^{1- \\delta_i}  \\\\\n&= \\prod_{i=1}^n \\left[ \\lambda e^{-\\boldsymbol{\\beta}' \\mathbf{x_i}} \\right]^{\\delta_i} e^{ -\\lambda t_i e^{-\\boldsymbol{\\beta}' \\mathbf{x_i}}}.\n\\end{align}\\] Taking logs gives \\[\\begin{align}\nl(\\lambda,\\boldsymbol{\\beta}) &= \\log \\lambda \\sum_{i=1}^n \\delta_i  - \\sum_{i=1}^n \\delta_i \\boldsymbol{\\beta}' \\mathbf{x_i} - \\lambda \\sum_{i=1}^n  t_i e^{-\\boldsymbol{\\beta}' \\mathbf{x_i}}.\n\\end{align}\\] Differentiating gives \\[\\begin{align}\n\\frac{\\partial l}{\\partial \\lambda} &= \\frac{\\Delta}{\\lambda} - \\sum t_i e^{-\\boldsymbol{\\beta}' \\mathbf{x_i}} \\quad \\textrm{where } \\Delta = \\sum_{i=1}^n \\delta_i \\\\\n\\frac{\\partial l}{\\partial \\beta_j} &= -\\sum_{i=1}^n \\delta_i x_{ij} + \\lambda \\sum_{i=1}^n x_{ij} t_i e^{-\\boldsymbol{\\beta}' \\mathbf{x_i}} \\quad \\textrm{for } j = 1, \\ldots, p.\n\\end{align}\\]\nThe maximum likelihood estimates are the solutions of \\[\\begin{align}\n\\frac{\\Delta}{\\hat{\\lambda}} - \\sum t_i e^{-\\hat{\\boldsymbol{\\beta}}' \\mathbf{x_i}} &=0,\\\\\n-\\sum_{i=1}^n \\delta_i x_{ij} + \\hat{\\lambda} \\sum_{i=1}^n x_{ij} t_i e^{-\\hat{\\boldsymbol{\\beta}}' \\mathbf{x_i}}&=0.\n\\end{align}\\]\nWe cannot solve these equations analytically, so numerical methods have to be used (R uses the Newton-Raphson method).\nFor estimates of variance and standard errors we can use the asymptotic properties of the mle and note that \\[\\begin{align}\n\\frac{\\partial^2 l}{\\partial \\lambda^2} &= -\\frac{\\Delta}{\\lambda^2} \\\\\n\\frac{\\partial^2 l}{\\partial \\lambda \\partial \\beta_j} &=\\sum_{i=1}^n x_{ij} t_i e^{-\\boldsymbol{\\beta}' \\mathbf{x_i}}  &\\quad \\textrm{for } j = 1, \\ldots, p \\\\\n\\frac{\\partial^2 l}{\\partial \\beta_j \\partial \\beta_k} &=-\\lambda \\sum_{i=1}^n x_{ij} x_{ik} t_i e^{-\\boldsymbol{\\beta}' \\mathbf{x_i}}  &\\quad \\textrm{for } j = 1, \\ldots, p; \\quad k = 1, \\ldots, p\n\\end{align}\\]\n\n\n5.5.2 Model fitting in R\nAs an example, we will use the leuk data from the MASS package (this doesn’t need installing). The help file (see ?MASS::leuk) explains:\n\nSurvival times are given for 33 patients who died from acute myelogenous leukaemia. Also measured was the patient’s white blood cell count at the time of diagnosis. The patients were also factored into 2 groups according to the presence or absence of a morphologic characteristic of white blood cells. Patients termed AG positive were identified by the presence of Auer rods and/or significant granulation of the leukaemic cells in the bone marrow at the time of diagnosis.\n\n\nhead(MASS::leuk)\n\n    wbc      ag time\n1  2300 present   65\n2   750 present  156\n3  4300 present  100\n4  2600 present  134\n5  6000 present   16\n6 10500 present  108\n\n\nThe aim is to fit an exponential AFT model with log of white blood cell count, and ag as explanatory variables.\nThere is no censoring in this data. We first set up a survival object:\n\nlibrary(survival)\nleuk_sv &lt;- Surv(MASS::leuk$time)\n\n\n\n\n\n\n\nNote\n\n\n\nFor fitting in R, we define \\[\n\\beta_0:=- \\log \\lambda,\n\\] and then write \\[T \\sim Exp(\\theta(\\mathbf{x})),\\] where \\[\\theta(\\mathbf{x}) = \\exp \\left( -(\\beta_0 + \\beta_1 x_1 + \\ldots + \\beta_p x_p) \\right).\\]\n\n\nIn the example, we have \\(p=2\\) given the two explanatory variables, and we define\n\n\\(x_1\\): the log of the white blood cell count (log(wbc))\n\\(x_2\\): an indicator variable, taking the value 1 if the ag variable is present and 0 otherwise.\n\nWe have three parameters to estimate: an ‘intercept’ \\(\\beta_0\\) and the coefficients of \\(x_1\\) and \\(x_2\\), denoted by \\(\\beta_1\\) and \\(\\beta_2\\) respectively.\n\n\n\n\n\n\nNote\n\n\n\nA positive coefficient \\(\\beta_i\\) means that increasing \\(x_i\\) is associated with longer survival times.\n\n\nWe fit this model in R as follows:\n\nleuk_regexp &lt;- survreg(leuk_sv ~ log(wbc) + ag,\n                       dist = \"exponential\",\n                       data = MASS::leuk)\nsummary(leuk_regexp)\n\n\nCall:\nsurvreg(formula = leuk_sv ~ log(wbc) + ag, data = MASS::leuk, \n    dist = \"exponential\")\n             Value Std. Error     z       p\n(Intercept)  5.815      1.263  4.60 4.1e-06\nlog(wbc)    -0.304      0.124 -2.45  0.0144\nagpresent    1.018      0.364  2.80  0.0051\n\nScale fixed at 1 \n\nExponential distribution\nLoglik(model)= -146.5   Loglik(intercept only)= -155.5\n    Chisq= 17.82 on 2 degrees of freedom, p= 0.00014 \nNumber of Newton-Raphson Iterations: 5 \nn= 33 \n\n\nThe summary() command in the R output provides us with estimates of \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\) and their standard errors. Here we have\n\\[\n\\hat{\\beta}_0 = 5.815, \\quad \\hat{\\beta}_1 = -0.304,\\quad \\hat{\\beta}_2 = 1.018,\n\\] with estimated standard errors 1.263, 0.124 and 0.364 respectively.\n\n\n5.5.3 Hypothesis testing\nSuppose we wish to investigate whether particular explanatory variables have an effect on the survival distribution. We can consider testing hypotheses of the form \\[\nH_0:\\beta_i = 0\n\\]\nWe can perform partial z-tests: we compute \\[\nZ_{obs}:= \\frac{\\hat{\\beta}_i}{\\mbox{estimated standard error }(\\hat{\\beta}_i)},\n\\] with \\(Z_{obs}\\sim N(0, 1)\\) if \\(H_0\\) is true.\nThe tests are conditional on all the remaining covariates being included in the model and hence are termed partial z-tests.\n\n\n\n\n\n\nExample\n\n\n\nTo the test the significance of the ag variable (the “presence of Auer rods and/or significant granulation of the leukaemic cells in the bone marrow at the time of diagnosis”), we have\n\\[\nH_0: \\beta_2 = 0, \\quad Z_{obs} = \\frac{1.018}{0.364}=2.80,\n\\] and a \\(p\\)-value of \\[\n2\\times P(Z\\ge 2.80),\\quad \\mbox{ where } Z\\sim N(0,1),\n\\] which is 0.0051: strong evidence against \\(H_0\\): there is strong evidence that an ag test result of present is associated with longer survival times.\n\n\n\n\n5.5.4 Predicting survival times\nSuppose we want to predict survival for patient with a particular set of covariates \\(\\mathbf{x}^*\\). For this patient’s survival time \\(T\\), we would have \\[\nT\\sim Exp(\\theta(\\mathbf{x}^*)),\n\\] and we can estimate \\(\\theta(\\mathbf{x}^*)\\) by \\[\n\\hat{\\theta}(\\mathbf{x}^*):=\\exp \\left( -(\\hat{\\beta}_0 + \\hat{\\beta}_1 x^*_1 + \\ldots + \\hat{\\beta}_p x^*_p) \\right).\n\\] We can then estimate the mean survival \\(E(T)\\) by \\[\n\\hat{E}(T):=\\frac{1}{\\hat{\\theta}(\\mathbf{x}^*)}\n\\] and any percentile from the distribution of \\(T\\) by inverting the estimated cumulative distribution function. For example, to estimate the 95th percentile \\(t_{(0.95)}\\) of the distribution of \\(T\\), we note \\[\n0.05 = \\exp(-t_{(0.95)}\\theta(\\mathbf{x}^*)),\n\\] and so an estimate would be \\[\n\\hat{t}_{(0.95)} = \\frac{-\\log 0.05}{\\hat{\\theta}(\\mathbf{x}^*)}\n\\]\n\n\n\n\n\n\nExample\n\n\n\nWe estimate the mean survival time for a patient with a wbc of 10000 and who has an ag test result of present. We have \\[\\mathbf{x^*} = (x_1^*,\\,x_2^*)^T=(\\log 10000,\\, 1)^T\\] and \\[\\hat{\\lambda}(\\mathbf{x^*}) = \\exp(- 5.815 + 0.304\\log(10000) - 1.018)= 0.01772,\\] hence the estimated mean survival is \\(1/0.01772\\simeq 56\\) days.\n\n\n\n\n\n\n\n\nNote\n\n\n\nApproximate standard errors for these estimates can obtained using the general approximation \\[\nvar(g(\\hat{\\phi}))\\simeq \\left.\\{g'(\\phi)\\}^2\\right|_{\\phi=\\hat{\\phi}}var(\\hat{\\phi}),\n\\] with \\[\n\\hat{\\phi} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x^*_1 + \\ldots + \\hat{\\beta}_p x^*_p,\n\\] and, for example for \\(E(T)\\), \\[\ng(\\hat{\\phi}) = \\exp(\\hat{\\phi}).\n\\] Standard errors can be obtained from R: you will not be required to compute these by hand!\n\n\n\n\n5.5.5 Obtaining predictions using R\nPredictions can be obtained using R, similar to how predictions are in obtained in R for linear models.\nWe first set up a new data frame with the desired explanatory variables for prediction. Column names must match those used for model fitting. Consider again estimating the mean survival time for a patient with a wbc of 10000 and who has an ag test result of present.\n\nxstar &lt;- data.frame(wbc = 10000, ag = \"present\")\n\nWe then use the predict() function:\n\npredict(leuk_regexp, newdata = xstar, se.fit = TRUE)\n\n$fit\n       1 \n56.22816 \n\n$se.fit\n       1 \n13.65322 \n\n\nWe can also estimate percentiles from the distribution of the survival time, and obtain standard errors for the estimates. For example, to obtain the 5th and 95th percentiles:\n\npredict(leuk_regexp, newdata = xstar,\n        type = \"quantile\", p = c(0.05, 0.95),\n        se.fit = TRUE)\n\n$fit\n[1]   2.884127 168.444508\n\n$se.fit\n         1          1 \n 0.7003186 40.9013901",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Accelerated Failure Time (AFT) Models</span>"
    ]
  },
  {
    "objectID": "aft.html#the-weibull-aft-model",
    "href": "aft.html#the-weibull-aft-model",
    "title": "5  Accelerated Failure Time (AFT) Models",
    "section": "5.6 The Weibull AFT Model",
    "text": "5.6 The Weibull AFT Model\nWe briefly discuss the Weibull AFT model. Recall our Weibull parameterisation in Section 1.5.2: \\[\\begin{align}\n  f(t) &= \\lambda \\gamma (\\lambda t) ^ {\\gamma-1} \\exp\\left[ -(\\lambda t) ^ \\gamma \\right] \\\\\n  S(t) &= \\exp\\left[ -(\\lambda t) ^ \\gamma \\right] \\\\\n  h(t) &= \\lambda \\gamma (\\lambda t) ^ {\\gamma-1}\n\\end{align}\\]\nIf we model \\(T\\) as an AFT Weibull model with baseline \\(T_0 \\sim Weibull(\\lambda_0, \\gamma)\\) then\n\\[\nS(t; \\mathbf{x}) = \\exp\\left(- \\left(\\lambda_0 t \\exp(-\\boldsymbol{\\beta}' \\mathbf{x})\\right)^\\gamma\\right).\n\\] We again define \\(\\beta_0 = -\\log \\lambda\\), so that \\[\nS(t; \\mathbf{x})= \\exp\\left(- \\left(t  \\exp(-(\\beta_0 + \\beta_1 x_1 + \\ldots + \\beta_p x_p)) \\right)^\\gamma\\right)\n\\] This is equivalent to \\(T \\sim Weibull(\\lambda(\\mathbf{x}), \\gamma)\\) where \\[\\lambda(\\mathbf{x}) = \\exp\\left(-(\\beta_0 + \\beta_1 x_1 + \\ldots + \\beta_p x_p) \\right)\\] Note that \\(\\gamma\\) does not depend on \\(\\mathbf{x}\\).\nWe fit this model in R as follows (though weibull is the default argument for dist, so we could leave out dist = \"weibull\")\n\nleuk_regweib &lt;- survreg(leuk_sv ~ log(wbc) + ag,\n                       dist = \"weibull\",\n                       data = MASS::leuk)\nsummary(leuk_regweib)\n\n\nCall:\nsurvreg(formula = leuk_sv ~ log(wbc) + ag, data = MASS::leuk, \n    dist = \"weibull\")\n              Value Std. Error     z       p\n(Intercept)  5.8524     1.3227  4.42 9.7e-06\nlog(wbc)    -0.3103     0.1313 -2.36  0.0181\nagpresent    1.0206     0.3781  2.70  0.0069\nLog(scale)   0.0399     0.1392  0.29  0.7745\n\nScale= 1.04 \n\nWeibull distribution\nLoglik(model)= -146.5   Loglik(intercept only)= -153.6\n    Chisq= 14.18 on 2 degrees of freedom, p= 0.00084 \nNumber of Newton-Raphson Iterations: 6 \nn= 33 \n\n\nWe can obtain estimates of \\(\\beta_0, \\beta_1, \\beta_2\\) as before. Predictions for this model can be obtained using the predict() function as before.\nThe \\(\\gamma\\) parameter is referred to as Scale (and Log(scale) for \\(\\log gamma\\)). We see that \\(\\log \\hat{\\gamma} = 0.0399\\), and the partial \\(z\\)-test shows \\(\\log \\gamma\\) is not significantly different from 0. As \\(\\gamma=1\\) corresponds to the exponential distribution, we would conclude there is no evidence in favour of the Weibull distribution over the simpler exponential distribution.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Accelerated Failure Time (AFT) Models</span>"
    ]
  },
  {
    "objectID": "aft.html#model-checking",
    "href": "aft.html#model-checking",
    "title": "5  Accelerated Failure Time (AFT) Models",
    "section": "5.7 Model checking",
    "text": "5.7 Model checking\nWe should discuss how to check the assumptions/goodness of fit for an AFT model, but we are not going to! To cover the topic properly here would add too much content to the module; there will be some discussion of model-checking (for proportional hazards models) in the next chapter. Recommended reading on this topic is Chapter 7 of\n\nCollett, D. (2023). Modelling Survival Data in Medical Research (4th ed.). Chapman and Hall/CRC.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Accelerated Failure Time (AFT) Models</span>"
    ]
  },
  {
    "objectID": "aft.html#tasks",
    "href": "aft.html#tasks",
    "title": "5  Accelerated Failure Time (AFT) Models",
    "section": "5.8 Tasks",
    "text": "5.8 Tasks\n\nConsider the Weibull AFT model fitted to the leukaemia data\n\n\nlibrary(survival)\nleuk_sv &lt;- Surv(MASS::leuk$time)\nleuk_regweib &lt;- survreg(leuk_sv ~ log(wbc) + ag,\n                       dist = \"weibull\",\n                       data = MASS::leuk)\nsummary(leuk_regweib)\n\n\nCall:\nsurvreg(formula = leuk_sv ~ log(wbc) + ag, data = MASS::leuk, \n    dist = \"weibull\")\n              Value Std. Error     z       p\n(Intercept)  5.8524     1.3227  4.42 9.7e-06\nlog(wbc)    -0.3103     0.1313 -2.36  0.0181\nagpresent    1.0206     0.3781  2.70  0.0069\nLog(scale)   0.0399     0.1392  0.29  0.7745\n\nScale= 1.04 \n\nWeibull distribution\nLoglik(model)= -146.5   Loglik(intercept only)= -153.6\n    Chisq= 14.18 on 2 degrees of freedom, p= 0.00084 \nNumber of Newton-Raphson Iterations: 6 \nn= 33 \n\n\n\nAt what time does the survival probability for a leukaemia patient with a white blood cell count of 10000 and ag recorded as absent first drop below 0.25 for the Weibull model?\nBy using the relationship \\(h(t;x)=h_0(t\\psi(\\mathbf{x}))\\psi(\\mathbf{x})\\) or otherwise, show(using the usual notation) that the hazard at time \\(t\\) for a leukaemia patient with a log white blood cell count of \\(x_1\\) and ag recorded as present is given by \\(\\gamma \\exp(-(\\beta_0 + \\beta_1 x +\\beta_2))(t\\exp(-(\\beta_0 + \\beta_1 x +\\beta_2)))^{\\gamma-1}\\) for the Weibull model.\n\n\nFor an AFT model the survivor function for an individual with covariate vector \\(\\mathbf{x}\\) satisfies \\[S(t;\\mathbf{x}) = S_0(te^{-\\boldsymbol{\\beta}' \\mathbf{x}}),\\] where \\(S_0(t)\\) is some baseline survivor function. Show that the corresponding hazard function satisfies\n\\[ h(t;\\mathbf{x}) = e^{-\\boldsymbol{\\beta}' \\mathbf{x}} h_0 (t e^{-\\boldsymbol{\\beta}' \\mathbf{x}})\\] where \\(h_0(t)\\) is the baseline hazard function for \\(S_0(t)\\). Hint: write the hazard function solely in terms of the survivor function.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nFor the Weibull AFT model we have \\[S(t; \\mathbf{x}) = \\exp\\left(- \\left(t  \\exp(-(\\beta_0 + \\beta_1 x_1+\\beta_2x_2)) \\right)^\\gamma\\right),\\] with \\(x_1\\) the log white blood cell count, and \\(x_2=1\\) if ag is present and 0 otherwise. Hence we require\n\n\n\\[\n\\exp\\left(- \\left(t  \\exp(-(\\beta_0 + \\beta_1 x)) \\right)^\\gamma\\right)=0.25\n\\] So we have \\[\\begin{align*}\n\\left(t\\exp(-(\\beta_0 + \\beta_1 x)) \\right)^\\gamma &= \\log 4\\\\\nt\\exp(-(\\beta_0 + \\beta_1 x)) &= (\\log 4)^{1/\\gamma}\\\\\nt&=(\\log 4)^{1/\\gamma}\\exp(\\beta_0 + \\beta_1 x)\\\\\n\\end{align*}\\] Using the estimates from the R output: \\(\\hat{\\beta}_0=5.5824\\), \\(\\hat{\\beta}_1=-0.3103\\) and \\(\\hat{\\gamma}=1.04\\), and with \\(x=\\log 10000\\) we get that the first time that the survival probability for a leukaemia patient with a white blood cell count of 100 first drops below 0.25 is 21.46 weeks.\n\n\nUsing \\(h(t;x)=h_0(t\\psi(x))\\psi(x)\\) with \\(\\psi(x)=\\exp(-\\beta_1x)\\) and \\(\\lambda=\\exp(-\\beta_0)\\) we have \\[\\begin{align*}\nh(t;x)&=\\lambda\\gamma(\\lambda t \\exp(-\\beta_1 x-\\beta_2)^{\\gamma-1}\\exp(-\\beta_1 x-\\beta_2)\\\\\n&=\\gamma\\exp(-(\\beta_0+\\beta_1 x+\\beta_2))(t \\exp(-(\\beta_0+\\beta_1 x+\\beta_2))^{\\gamma-1}\n\\end{align*}\\]\n\nWe have \\(S(t) = S_0(t e^{-\\boldsymbol{\\beta}'  \\mathbf{x}})\\) and so \\[\\begin{align*}\nh(t; \\mathbf{x}) &= - \\frac{S'(t; \\mathbf{x})}{S(t; \\mathbf{x})} \\\\\n&= - \\frac{e^{-\\boldsymbol{\\beta}'  \\mathbf{x}} S'_0(t e^{-\\boldsymbol{\\beta}'  \\mathbf{x}})}{S_0(t e^{-\\boldsymbol{\\beta}'  \\mathbf{x}})} \\\\\n&= e^{-\\boldsymbol{\\beta}'  \\mathbf{x}} \\left\\{ - \\frac{S'_0(t e^{-\\boldsymbol{\\beta}'  \\mathbf{x}})}{S_0(t e^{-\\boldsymbol{\\beta}'  \\mathbf{x}})} \\right\\} \\\\\n&= e^{-\\boldsymbol{\\beta}'  \\mathbf{x}} h_0 (t e^{-\\boldsymbol{\\beta}'  \\mathbf{x}})\n\\end{align*}\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Accelerated Failure Time (AFT) Models</span>"
    ]
  },
  {
    "objectID": "proportionalhazards.html",
    "href": "proportionalhazards.html",
    "title": "6  Proportional Hazards Models",
    "section": "",
    "text": "6.1 Which AFT models exhibit the proportional hazards property?\nAn alternative approach to modelling the dependence of the failure time distribution on explanatory variables \\(\\mathbf{x}\\) is through the hazard function with \\[\nh(t; \\mathbf{x}) = \\psi(\\mathbf{x}; \\boldsymbol{\\beta}) h_0(t),\n\\] where \\(h_0(t)\\) corresponds to the hazard for an individual under the baseline conditions where \\(\\mathbf{x} = \\mathbf{0}\\) and we require \\(\\psi(\\mathbf{x}=\\mathbf{0}; \\boldsymbol{\\beta}) = 1\\). Typically we choose \\[\n\\psi(\\mathbf{x}; \\boldsymbol{\\beta}) = e^{\\boldsymbol{\\beta}' \\mathbf{x}}\n\\] so that \\[\nh(t; \\mathbf{x}) = e^{\\boldsymbol{\\beta}' \\mathbf{x}} h_0(t).\n\\] This is a semi-parametric model proposed by Sir David Cox 1. This work has become so widely used that his original paper is one of the most cited mathematical papers in history.\nProportional-hazards is called semi-parametric since the baseline hazard \\(h_0(t)\\) does not need to be specified. The dependence of the failure time \\(T\\) on explanatory variables \\(\\mathbf{x}\\) is precisely modelled; but actual distribution of failure is not parametrically specified. It is particularly useful in medical situations where\nOne should also note that for two patients with covariates \\(\\mathbf{x_1}\\) and \\(\\mathbf{x_2}\\) we have \\[\\begin{align}\n\\frac{h(t; \\mathbf{x_1})}{h(t; \\mathbf{x_2})} &= \\frac{e^{\\boldsymbol{\\beta}' \\mathbf{x_1}}}{e^{\\boldsymbol{\\beta}' \\mathbf{x_2}}} \\\\\n&= e^{\\boldsymbol{\\beta}'(\\mathbf{x_1}-\\mathbf{x_2})}\n\\end{align}\\] which is independent of time. Hence hazard functions for any two patients are proportional over time as the linear component of the model does not vary with time.\nSuppose \\(T \\sim Exp(\\lambda)\\) then for an AFT with an exponential survival time the hazard function is \\[h(t;\\mathbf{x}) = \\lambda e^{-\\boldsymbol{\\beta}' \\mathbf{x}}.\\] It follows that the ratio of any two hazards for two patients with covariates \\(\\mathbf{x_1}\\) and \\(\\mathbf{x_2}\\) is of the form above: \\[\n\\frac{h(t; \\mathbf{x_1})}{h(t; \\mathbf{x_2})} = \\frac{\\lambda e^{-\\boldsymbol{\\beta}' \\mathbf{x_1}}}{\\lambda e^{-\\boldsymbol{\\beta}' \\mathbf{x_2}}}\n= e^{-\\boldsymbol{\\beta}'(\\mathbf{x_1}-\\mathbf{x_2})}\n\\]\nand therefore has the proportional hazards property. (The change of sign from \\(\\boldsymbol{\\beta}\\) to \\(-\\boldsymbol{\\beta}\\) doesn’t matter: the important point is that the ratio is constant over time).\nSimilarly if \\(T \\sim Weibull(\\lambda, \\gamma)\\) then for an AFT with a Weibull survival time, the hazard is \\[h(t;\\mathbf{x}) =\\lambda^\\gamma \\gamma t^{\\gamma-1}exp(-\\gamma\\mathbf{\\beta}' \\mathbf{x}).\\] Hence the ratio of any two hazards for two patients with covariates \\(\\mathbf{x_1}\\) and \\(\\mathbf{x_2}\\) is \\[\n\\frac{h(t; \\mathbf{x_1})}{h(t; \\mathbf{x_2})} = \\frac{\\lambda^\\gamma \\gamma t^{\\gamma-1}\\exp(-\\gamma\\mathbf{\\beta}' \\mathbf{x_1})}{\\lambda^\\gamma \\gamma t^{\\gamma-1}\\exp(-\\gamma\\mathbf{\\beta}' \\mathbf{x_2})}\n= e^{-\\gamma\\boldsymbol{\\beta}'(\\mathbf{x_1}-\\mathbf{x_2})},\n\\]\nwhich is again independent of time and therefore has the proportional hazards property. Note that because the exponential is a special case of the Weibull we only really needed to demonstrate this property for the Weibull distribution. The Weibull distribution (and hence Exponential as a special case) is the only distribution for which the corresponding AFT model has the proportional hazards property.\nFor an example of an AFT model that does not have the proportional hazards property, consider an AFT model with a log normal baseline distribution: \\(\\log T_0 \\sim N(0,1)\\) (to make the example simpler, we have specified the mean and variance of \\(\\log T_0\\).) The baseline hazard for a log \\(N(0,1)\\) distribution is \\[\nh_0(t) = \\frac{f(t)}{S(t)} = \\frac{\\frac{1}{t\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}(\\log t)^2\\right)}{1-\\Phi(\\log(t))},\n\\] and so for a patient with covariates \\(\\mathbf{x}\\), we have \\[\nh(t;\\mathbf{x}) =  \\frac{\\frac{1}{t\\psi(\\mathbf{x})\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}(\\log (t\\psi(\\mathbf{x})))^2\\right)}{1-\\Phi(\\log(t\\psi(\\mathbf{x})))}\\psi(\\mathbf{x})\n\\] Now consider two patients: patient one with covariates \\(\\mathbf{x}_1\\) such that \\(\\psi(\\mathbf{x}_1)=1\\) and patient two with covariates \\(\\mathbf{x}_2\\) such that \\(\\psi(\\mathbf{x}_2)=2\\). We can plot the two hazard functions and their ratio, and see that the ratio is not constant.\n# Plot hazard function for patient 1\ncurve(dlnorm(x)/(1-plnorm(x)), from = 0, to = 10, ylim = c(0, 2),\n      xlab = \"t\", ylab = \"hazard/hazard ratio\")\n\n# Plot hazard function for patient 2\ncurve(dlnorm(x*2)/(1-plnorm(x*2)) * 2, from = 0, to = 10, add = TRUE,\n      col = \"red\")\n\n# Plot the hazard ratio\ncurve(dlnorm(x)/(1-plnorm(x)) / \n        (dlnorm(x*2)/(1-plnorm(x*2)) * 2),\n      from = 0, to = 10, add = TRUE, col = \"blue\",\n      lty = 2)\nlegend(\"topright\", legend = c(\"Patient 1 hazard\", \"Patient 2 hazard\", \"Hazard ratio\"),\n       lty = c(1, 1, 2),\n       col = c(\"black\", \"red\", \"blue\"))",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Proportional Hazards Models</span>"
    ]
  },
  {
    "objectID": "proportionalhazards.html#parameter-estimation",
    "href": "proportionalhazards.html#parameter-estimation",
    "title": "6  Proportional Hazards Models",
    "section": "6.2 Parameter Estimation",
    "text": "6.2 Parameter Estimation\nConsider our typical situation where we have \\(n\\) individuals and for each we have observed \\[\\begin{align}\nt_i &= \\min(\\textrm{Failure time}, \\textrm{Time of right censoring}) \\\\\n\\delta_i &= \\left\\{ \\begin{array}{rl}\n                1 & \\textrm{if $i$ is observed to fail} \\\\\n                0 & \\textrm{if $i$ is censored}\n               \\end{array} \\right. \\\\\n\\mathbf{x_i} &= \\textrm{$p$ dimensional vector of explanatory variables}\n\\end{align}\\] If we wish to model this with a proportional hazards model then \\[\\begin{align}\nh(t; x) &=   e^{\\boldsymbol{\\beta}' \\mathbf{x}} h_0(t) \\\\\nS(t; x) &=   \\exp \\left\\{ -\\int_0^t h(t)dt \\right\\} \\\\\n&= \\exp \\left\\{ -\\int_0^t e^{\\boldsymbol{\\beta}' \\mathbf{x}} h_0(t)dt \\right\\} \\\\\n&= \\exp \\left\\{ -e^{\\boldsymbol{\\beta}' \\mathbf{x}} \\int_0^t  h_0(t)dt \\right\\} \\\\\n&= \\left[ S_0(t) \\right]^{e^{\\boldsymbol{\\beta}' \\mathbf{x}}} \\\\\nf(t; \\mathbf{x}) &= h(t;\\mathbf{x}) S(t;\\mathbf{x}) = e^{\\boldsymbol{\\beta}' \\mathbf{x}} h_0(t) S(t) .\n\\end{align}\\] We can do our usual approach to writing down the likelihood of the censored and observed data \\[\\begin{align}\n\\textrm{Likelihood} &= \\prod_{i=1}^n f(t_i;\\mathbf{x_i})^{\\delta_i} S(c_i;\\mathbf{x_i})^{1-\\delta_i} \\\\\n&= \\prod_{i=1}^n \\left[\\frac{h(t_i;\\mathbf{x_i})}{S(t_i;\\mathbf{x_i})}\\right]^{\\delta_i} S(c_i;\\mathbf{x_i})^{1-\\delta_i}.\n\\end{align}\\] However, this involves the unspecified baseline hazard \\(h_0(t)\\) and so to proceed further we would need to specify its parametric form. However we don’t want to do that and so we will proceed by working with the partial likelihood.\n\n6.2.1 The Partial Likelihood approach\nIn the proportional hazards model, the baseline hazard \\(h_0(t)\\) is unspecified: the model does not attempt to describe when failure events are likely to occur. Instead, the model describes how some individuals have a higher risk of failure relative to others, through a relative increase in their hazard. Hence the information in the data that is relevant to the model is not the actual failure times, but who had the failure event whenever a failure event occurred. In particular, what was it about that particular individual (what were their covariates) that might have caused them to have the failure event, rather than someone else.\nWe work with the order that people fail i.e. who out of the people at risk of failing actually does so at each time. We define the random variable\n\n\\(\\phi_j\\): the index of the individual who fails at the \\(j^{th}\\) failure time, conditional on that failure time being \\(t_{(j)}\\). For \\(n\\) individuals, no censoring, and \\(n\\) distinct failure times, we would have \\(\\phi_j\\in \\{1,\\ldots,n\\}\\)\n\nWe first order the failure times \\[t_{(1)}&lt;t_{(2)}&lt; \\ldots &lt; t_{(n)}.\\] This creates a corresponding ordering on the rest of the variables so that\n\nthe \\(j^{th}\\) individual who fails (at time \\(t_{(j)}\\)) is labelled \\(i_{(j)}\\).\nThis individual has censoring indicator value \\(\\delta_{(j)}\\) and explanatory covariates \\(\\mathbf{x_{(j)}}\\).\n\nPartial Likelihood without Censoring\nSuppose that we have no censoring in the data and that there are no ties in failure times i.e. only one individual dies at each death time. Let us order the observed failure times and denote them by \\(t_{(1)}&lt;t_{(2)}&lt; \\ldots &lt; t_{(n)}\\). Also let us create an order amongst the individuals so that the individual who dies at failure time \\(t_{(j)}\\) is \\(i_{(j)}\\) i.e. \\(i_{(j)} = k\\) if and only if \\(t_k = t_{(j)}\\). Finally let define a risk set \\[\\mathcal{R}(t) = \\{ \\textrm{The set of individuals alive and in the trial just before time }t \\}.\\] Let \\(t^\\dagger = \\{ t_{(1)}, \\ldots, t_{(n)}\\}\\). We want to work with the likelihood or probability of the observed order \\((i_{(1)}, \\ldots, i_{(n)})\\) in which the individuals fail conditional on those failure times i.e. \\[P(\\phi_1 = i_{(1)}, \\phi_2 = i_{(2)}, \\ldots,  \\phi_n = i_{(n)}|t^\\dagger)\\] Using repeated conditioning we can split this probability as \\[\\begin{align}\n  &P(\\phi_1 = i_{(1)}, \\phi_2 = i_{(2)}, \\ldots,  \\phi_n = i_{(n)} | t^\\dagger) \\\\\n  =&P(\\phi_1 = i_{(1)}| t^\\dagger) \\prod_{j=2}^{n}P(\\phi_j = i_{(j)} | \\phi_1 = i_{(1)}, \\ldots,  \\phi_{j-1} = i_{(j-1)},t^\\dagger)\n\\end{align}\\] and we can deal with each of these probabilities separately. Now \\[P(\\phi_j = i | \\phi_1 = i_{(1)}, \\phi_2 = i_{(2)}, \\ldots,  \\phi_{j-1} = i_{(j-1)}, t^\\dagger))\\] is the probability that individual \\(i\\) fails at time \\(t_{(j)}\\) given that we know one individual does fail at that time from those individuals at risk (i.e. in the risk set \\(\\mathcal{R}(t_{(j)}))\\). This can be thought of as picking an individual at random from the risk set where the probability you pick any individual \\(k\\) is proportional to that individuals hazard \\(h(t_{(j)}; \\mathbf{x_k})\\). As such it can be written as \\[\\begin{align}\n\\frac{h(t_{(j)}; \\mathbf{x_i})}{\\sum_{k \\in \\mathcal{R}(t_{(j)})} h(t_{(j)}; \\mathbf{x_k})} &= \\frac{e^{\\boldsymbol{\\beta}' \\mathbf{x_i}} h_0(t_{(j)})}{\\sum_{k \\in \\mathcal{R}(t_{(j)})} e^{\\boldsymbol{\\beta}' \\mathbf{x_k}} h_0(t_{(j)})} \\\\\n&= \\frac{e^{\\boldsymbol{\\beta}' \\mathbf{x_i}}}{\\sum_{k \\in \\mathcal{R}(t_{(j)})} e^{\\boldsymbol{\\beta}' \\mathbf{x_k}}}\n\\end{align}\\] after cancelling the baseline hazards. Putting all these terms together, we can write down the probability of the observed order \\((i_{(1)}, \\ldots, i_{(n)})\\) as \\[P(\\phi_1 = i_{(1)}, \\phi_2 = i_{(2)}, \\ldots,  \\phi_n = i_{(n)}|t_{(1)}, \\ldots, t_{(n)}) = \\prod_{j=1}^n \\frac{e^{\\boldsymbol{\\beta}' \\mathbf{x_{(j)}}}}{\\sum_{k \\in \\mathcal{R}(t_{(j)})} e^{\\boldsymbol{\\beta}' \\mathbf{x_k}}}\\] where \\(\\mathbf{x_{(j)}}\\) is the vector of explanatory variables for individual \\(i_{(j)}\\) i.e. the \\(j^{th}\\) individual to fail.\nPartial Likelihood with Censoring\nWe can extend the above idea to also include censored observation by adjusting the risk set \\(\\mathcal{R}(t)\\) accordingly to take account of those individuals who have been censored by time \\(t\\). We also only need to take account of the actual failure times i.e. when \\(\\delta_i = 1\\) for each term in the product. As such we can write down a partial likelihood for the observed data and parameter \\(\\boldsymbol{\\beta}\\) as \\[L(\\boldsymbol{\\beta}) = \\prod_{j=1}^n \\left( \\frac{e^{\\boldsymbol{\\beta}' \\mathbf{x_{(j)}}}}{\\sum_{k \\in \\mathcal{R}(t_{(j)})} e^{\\boldsymbol{\\beta}' \\mathbf{x_k}}} \\right)^{\\delta_{(j)}}\\] Note that individuals for whom the survival time is censored do not contribute to the numerator but they do enter the summation over the risk set at death times of subject less than the censored time.\nNotes on the Partial Likelihood Approach\n\nIf there are no censored observations this is a conditional likelihood, conditional on the observed failure times \\(t_{(1)},t_{(2)},\\ldots,t_{(n)}\\).\nWith censored observations this is instead known as a partial likelihood. The use of such a partial likelihood was justified by Cox (1975)2. He showed that the usual likelihood methods apply in this case. So we maximize the (partial) likelihood to estimate \\(\\boldsymbol{\\beta}\\) by \\(\\hat{\\boldsymbol{\\beta}}\\) and asymptotically \\[\\hat{\\boldsymbol{\\beta}} \\sim N \\left( \\boldsymbol{\\beta}, \\left[ -\\frac{\\partial^2 l}{\\partial \\beta_i \\partial \\beta_j} \\right]^{-1}_{\\hat{\\boldsymbol{\\beta}}} \\right)\\] where \\(l\\) is the log likelihood.\nTies (Peto’s adjustment) If we have data with ties in the failure times i.e. \\[\\begin{align}\nt_{(1)}&lt;t_{(2)}&lt;\\ldots&lt;t_{(k)} & \\quad \\textrm{the $k$ distinct survival times} \\\\\nd_{(1)}, d_{(2)},\\ldots, d_{(k)} & \\quad \\textrm{numbers of deaths at these times} \\\\\n\\mathcal{D}(t_{(1)}), \\mathcal{D}(t_{(2)}),\\ldots, \\mathcal{D}(t_{(k)}) & \\quad \\textrm{death set at $t_{(j)}$}\n\\end{align}\\] we can allow each of \\(d_{(j)}\\) deaths at \\(t_{(j)}\\) to contribute a factor (as before) to partial likelihood, each with the same risk set \\(\\mathcal{R}(t_{(j)})\\) so that \\[L(\\boldsymbol{\\beta}) = \\prod_{j=1}^n \\left( \\frac{e^{ \\sum_{k \\in \\mathcal{D}(t_{(j)})} \\boldsymbol{\\beta}' \\mathbf{x_{k}}}}{\\sum_{k \\in \\mathcal{R}(t_{(j)})} e^{\\boldsymbol{\\beta}' \\mathbf{x_k}}} \\right)^{\\delta_{(j)}}\\] This is satisfactory provided \\(d_{(j)}/n_{(j)}\\) is small, where \\(n_{(j)}\\) is the number of individuals at risk at \\(t_{(j)}\\).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Proportional Hazards Models</span>"
    ]
  },
  {
    "objectID": "proportionalhazards.html#RCoxPh",
    "href": "proportionalhazards.html#RCoxPh",
    "title": "6  Proportional Hazards Models",
    "section": "6.3 Fitting a proportional hazards model in R",
    "text": "6.3 Fitting a proportional hazards model in R\nData is available for patients being treated for atrial fibrillation (irregular heart rhythm). The variables in the dataset (available on Blackboard) are as follows\n\ndrug: a binary covariate representing a new drug treatment of interest with levels A (a placebo representing the baseline) and B (the new drug being tested)\nage: a continuous covariate representing the age in years at the start of treatment (45 years have been subtracted from the ages)\nhvol: a continuous covariate representing the heart volume in cm\\(^3\\) (700 has been subtracted from all the measurements)\ndiet: a three-level categorical covariate representing the type of diet with levels “vegan” (baseline), “veggie” and “meat”\ndigit: a binary covariate representing whether the patient is also being treated with digitalis (a drug derived from the common foxglove plant) with levels “yes” and “no” (baseline)\nsex: a binary covariate representing the sex of the patient with levels “male” and “female” (baseline)\n\nThe purpose of the proposed new drug treatment is to maintain normal heart rhythm and the interest is in the time to relapse. We fit a proportional hazards models in R using the coxph function as follows:\n\nload(\"datasets/arfib.RData\")\nlibrary(survival)\narfib_sv &lt;- Surv(afib$time, afib$cens, type = \"right\")\nafibph &lt;- coxph(arfib_sv ~ drug + age + hvol + \n                           diet + digit + sex,\n                data = afib)\nsummary(afibph)\n\nCall:\ncoxph(formula = arfib_sv ~ drug + age + hvol + diet + digit + \n    sex, data = afib)\n\n  n= 40, number of events= 34 \n\n               coef exp(coef) se(coef)      z Pr(&gt;|z|)    \ndrugB      -2.09912   0.12256  0.60237 -3.485 0.000493 ***\nage        -0.02650   0.97385  0.01994 -1.329 0.183764    \nhvol        0.08176   1.08520  0.01446  5.652 1.58e-08 ***\ndietmeat    1.27795   3.58928  0.58380  2.189 0.028594 *  \ndietveggie -0.40800   0.66498  0.58999 -0.692 0.489232    \ndigityes   -0.70111   0.49603  0.46255 -1.516 0.129583    \nsexmale     0.81621   2.26192  0.51241  1.593 0.111181    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n           exp(coef) exp(-coef) lower .95 upper .95\ndrugB         0.1226     8.1590   0.03764    0.3991\nage           0.9738     1.0269   0.93653    1.0127\nhvol          1.0852     0.9215   1.05486    1.1164\ndietmeat      3.5893     0.2786   1.14308   11.2703\ndietveggie    0.6650     1.5038   0.20922    2.1135\ndigityes      0.4960     2.0160   0.20035    1.2281\nsexmale       2.2619     0.4421   0.82855    6.1750\n\nConcordance= 0.94  (se = 0.017 )\nLikelihood ratio test= 71.99  on 7 df,   p=6e-13\nWald test            = 33.8  on 7 df,   p=2e-05\nScore (logrank) test = 55.75  on 7 df,   p=1e-09\n\n\nTo describe this model mathematically we will need seven covariates as follows:\n\n\\(x_1\\) is a binary indicator representing drug treatment (\\(x_1\\) is 1 for drug “B” and is 0 for drug “A”)\n\\(x_2\\) represents age\n\\(x_3\\) represents heart volume\n\\(x_4\\) is a binary indicator representing “veggie” (\\(x_4\\) is 1 if they are vegetarian, 0 if they are not)\n\\(x_5\\) is a binary indicator representing “meat” (\\(x_5\\) is 1 if they eat meat, 0 if they do not)\n\\(x_6\\) is a binary indicator representing whether the patient is also being treated with digitalis (\\(x_6\\) is 1 if they are, 0 if they are not)\n\\(x_7\\) is a binary indicator representing their sex (\\(x_7\\) is 1 if they are male, 0 if female)\n\nThe model we are fitting for an individual with covariates \\(\\mathbf{x} = (x_1, \\ldots, x_7)'\\) is \\[h(t;x) = h_0(t) e^{\\beta_1 x_1 + \\beta_2 x_2+ \\ldots + \\beta_7 x_7}\\]\nThe model specification in the R code fits only main effects (no interactions). Note that the baseline level of any categorical covariates are not included in the output. The baseline level of categorical covariates is always coded as \\(x_i=0\\). Since the baseline hazard \\(h_0(t)\\) represents the hazard for a patient with all covariates values set to zero it follows that the effect of these baseline levels are all captured by the baseline hazard. The output first shows the significance of the covariates (via p-values) and then the confidence intervals for both \\(\\beta_i\\) (the model parameters) and \\(exp(\\beta_i)\\). Recall that we can interpret the \\(exp(\\beta_i)\\) terms as specific hazard ratios which are the primary quantities of interest in a proportional hazards analysis.\nThere are two main questions of interest:\n\nHow can we quantify the effects of treatment and other covariates on time to relapse?\nHow could the analysis consider treatment-covariate interactions?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Proportional Hazards Models</span>"
    ]
  },
  {
    "objectID": "proportionalhazards.html#what-the-r-output-tells-us",
    "href": "proportionalhazards.html#what-the-r-output-tells-us",
    "title": "6  Proportional Hazards Models",
    "section": "6.4 What the R output tells us",
    "text": "6.4 What the R output tells us\nConsider the drug treatment. \\(x_1=0\\) for treatment A (baseline treatment), and \\(x_1=1\\) for treatment B so that \\[\\begin{align}\nh(t;x_1=0, x_2, x_3 \\ldots x_7) &= h_0(t) e^{\\beta_2 x_2+ \\ldots + \\beta_7 x_7} & \\quad \\textrm{for treatment A} \\\\\nh(t;x_1=1, x_2, x_3 \\ldots x_7) &= h_0(t) e^{\\beta_1 + \\beta_2 x_2+ \\ldots + \\beta_7 x_7} & \\quad \\textrm{for treatment B}\n\\end{align}\\] Dividing the treatment B hazard by the treatment A hazard gives \\(e^{\\beta_1}\\). Thus \\(e^{\\beta_1}\\) is interpreted as the hazard ratio of treatment B to treatment A adjusting for all the other covariates in the model.\nContinuous & binary covariates we can assess whether they significantly affect the hazard by checking the p-values in the usual way. If the parameter is \\(\\beta_i\\) then the null and alternative hypotheses for the test are \\[\\begin{align}\nH_0: \\quad & \\beta_i=0 \\nonumber \\\\\nH_1: \\quad & \\beta_i \\neq 0\n\\end{align}\\]\nCategorical covariates with \\(k\\)-levels (\\(k \\geq 3\\)) To assess significance we calculate \\(\\chi^2_i = (\\hat{\\beta_i}/ se(\\hat{\\beta_i}))^2\\) for each of the \\(k-1\\) dummy variables (ignoring the baseline). We then compare \\(\\sum_{i=1}^{k-1} \\chi^2_i\\) with the quantiles of the \\(\\chi^2_{k-1}\\) distribution. When assessing the significance of a \\(k-\\)level factor. It is wrong to consider the significance of each level of the factor in isolation (a common exam mistake). You need to make an aggregated assessment of the overall effect of the covariate. If the \\(k-1\\) parameters being tested are \\(\\beta_1, \\beta_2, \\ldots \\beta_{k-1}\\) then the null and alternative hypotheses for the test are \\[\\begin{align}\nH_0: \\quad & \\beta_1=\\beta_2= \\ldots =\\beta_{k-1}=0 \\\\\nH_1: \\quad & \\beta_1, \\beta_2, \\ldots \\beta_{k-1} \\: \\text{are not all zero}\n\\end{align}\\]\nIn the atrial fibrilation data, the p-values for age, digitalisation and sex are all less than 0.05 so in each case there isn’t significant evidence at the 5% level to reject the null hypothesis that the parameter is 0 (versus an alternative that it is not 0). For the remaining covariates:\n\nTreatment: the p-value is 0.0005 so we have highly significant evidence that treatment has an effect on failure time. \\(h(t; x_1=1)/h(t; x_1=0) = \\exp(\\beta_1) &lt; 1\\) (since \\(\\beta_1 &lt; 0\\)) so treatment B decreases the hazard relative to treatment A. A 95% confidence interval for the hazard ratio is (0.04, 0.40) so that the hazard on treatment B is estimated to be between 4% and 40% of hazard on treatment A.\nHeart volume: the p-value is very small (&lt; 0.00001) so we have extremely significant evidence that treatment has an effect on failure time. \\(h(t; x_3=v+1)/h(t; x_3=v) = \\exp(\\beta_3) &gt; 1\\) so increasing heart volume increases the hazard. A 95% confidence interval for the hazard ratio is (1.05, 1.12) so every 1 cm\\(^3\\) increase in heart volume increases the hazard by between 5% and 12%\nDiet: \\((1.28/0.58)^2+(0.41/0.59)^2=5.3\\) and the 95th percentile of the \\(\\chi^2_2\\) distribution is 5.99 (2 degrees of freedom since \\(k=3\\)). Since 5.3 &lt; 5.99 there isn’t significant evidence, at the 5% level of test, that diet affects hazards.\n\nOur data suggest that neither age, digitalisation nor sex appear to affect hazard at the 5% level of significance. This is not the same as saying that they have no effect. For example, the 95% confidence interval for \\(e^{\\beta_7}\\) is \\((0.83, 6.18)\\) so that the hazard for men could be a fourth fifths that for women or more than six times as much so there could be a large difference in hazards between M & F. These data exclude neither possibility at the 95% confidence level. Ideally we would like more data to reduce the width of this interval.\nHazard ratios We have already seen how to relate the hazard to the parameters of the proportional hazards model. This enables us to calculate hazard ratios comparing two individuals. For example the hazard ratio comparing\n\n25 year old meat-eater on drug treatment A\n\nwith a\n\n75 year old vegan on drug treatment B\n\nis given by \\[\\begin{align*}\n  \\frac{h(t; x_1=0, x_2 = -20, x_4 = 1)}{h(t; x_1=1, x_2 = 30, x_4 = 0)}&=\\frac{h_0(t)exp(0 -0.03 \\times -20 + 1.28)}{h_0(t)exp(-2.10 -0.03 \\times 30 + 0)}  \\\\\n   &=\\frac{exp(1.88)}{exp(-3.00)} \\\\\n   &=exp(4.88)\\\\\n   &\\approx 132\n\\end{align*}\\]\nSo the 25 year old meat-eater on drug treatment A has a hazard which is 132 times greater than the 75 year old vegan on drug treatment B.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Proportional Hazards Models</span>"
    ]
  },
  {
    "objectID": "proportionalhazards.html#including-interaction-terms",
    "href": "proportionalhazards.html#including-interaction-terms",
    "title": "6  Proportional Hazards Models",
    "section": "6.5 Including interaction terms",
    "text": "6.5 Including interaction terms\nSuppose that we wanted to investigate the interaction between the two binary variables treatment and sex. Then we would handle this by creating a new variable e.g. \\[\nx_7 = x_1 \\times x_6,\n\\] the product of the treatment and sex variables. Since \\(x_1=0\\) for treatment A and \\(x_1=1\\) for treatment B, if follows that \\(x_7=0\\) for all subjects receiving A and \\(x_7=x_6\\) for those receiving treatment B. The hazards for the interaction model, by treatment, are \\[\\begin{align}\nh(t;x) &= h_0(t) e^{\\beta_2 x_2 + \\beta_3 x_3 +  \\beta_4 x_4 + \\beta_5 x_5  + \\beta_6 x_6} & \\quad \\textrm{for treatment A} \\\\\nh(t;x) &= h_0(t) e^{\\beta_1 + \\beta_2 x_2 +  \\beta_3 x_3 +  \\beta_4 x_4 + \\beta_5 x_5  + (\\beta_6 + \\beta_7) x_6 } & \\quad \\textrm{for treatment B}\n\\end{align}\\] where \\(\\beta_7\\) reflects the interaction effect. Interactions involving a \\(k\\)-level factor are handled by converting the factor into \\(k-1\\) dummy binary variables. There are\n\n\\(k-1\\) degrees of freedom for an interaction between a \\(k\\)-level factor and a continuous covariate\n\\((k-1)(j-1)\\) degrees of freedom for an interaction between a \\(k\\)-level and a \\(j\\)-level factor.\n\nThe separate parts of the \\(\\chi^2\\) statistic are added to assess significance.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Proportional Hazards Models</span>"
    ]
  },
  {
    "objectID": "proportionalhazards.html#model-checking",
    "href": "proportionalhazards.html#model-checking",
    "title": "6  Proportional Hazards Models",
    "section": "6.6 Model Checking",
    "text": "6.6 Model Checking\nWhenever fitting a proportional hazards model, it is important to check the proportional hazards assumption i.e. that for covariates \\(\\mathbf{x}\\) relative to the baseline where \\(\\mathbf{x} = 0\\), \\[h(t; x) =   e^{\\boldsymbol{\\beta}' \\mathbf{x}} h_0(t).\\] Typically this is done in two ways:\n\nLog-log plots\nResidual plots\n\nIf these diagnostic tests fails and it is concluded that a proportional hazards model is not appropriate then there are two options. One is in the special case where proportionality only breaks down for one particular factor in which case a stratified proportional hazards model might be considered. The other option is to consider a parametric AFT model which does not have the proportional hazards property i.e. not Weibull or Exponential.\n\n6.6.1 Log-Log Plots for Factor Variables\nSuppose that we have two treatments and we wish to test the proportional hazards assumption between these two treatments. We have defined an indicator variable \\(x_1\\) so that \\[x_1 = \\left\\{ \\begin{array}{rl}\n                0 & \\textrm{if treatment A} \\\\\n                1 & \\textrm{if treatment B}\n               \\end{array} \\right. \\\\\n\\] Then modelling via proportional hazards \\[h(t; x) =   e^{\\boldsymbol{\\beta}' \\mathbf{x}} h_0(t).\\] so for treatment A we have hazard \\(h(t; 0) =  h_0(t)\\) while for treatment B we have \\(h(t; 1) =   e^{\\boldsymbol{\\beta}} h_0(t)\\). Now, noting that \\(S(t) = \\exp \\left\\{ -\\int_0^t h(t) dt \\right\\}\\) we find \\[\\begin{align}\n- \\log S_A(t) &= \\int_0^t h(t) \\, dt \\\\\n- \\log S_B(t) &= e^\\beta \\int_0^t h(t) \\, dt \\\\\n\\Rightarrow \\log [ - \\log S_B(t)  ] &= \\beta +  \\log [ - \\log S_A(t)  ]\n\\end{align}\\] so, if we plot an estimate of \\(\\log [ - \\log S_j(t) ]\\) against \\(t\\) for both treatments we should get parallel curves a distance \\(\\beta\\) apart. If the curves cross then a proportional hazards model is not appropriate.\n\n\n6.6.2 Log-Log Implementation in \\(R\\)\nTo produce separate log-log survival plots for different levels of a factor variable you can use the strata function within the model specification of the coxph function. The proportional hazards model is then fitted within each level of the factor. For example, for the lymphoma data where there are two levels of the stage variable we write\n\nload(\"datasets/lymphoma.Rdata\")\nlym_sv &lt;- Surv(lymphoma$time, lymphoma$censor, type = \"right\")\nlym_ph &lt;- coxph(lym_sv ~ strata(lymphoma$stage))\n\nsurvminer::ggsurvplot(survfit(lym_ph), fun = \"cloglog\",\n                      data =  lymphoma,\n                      xlim = c(3,300),\n                      xlab = \"Time (weeks)\",\n                      legend.labs = c(\"Stage 3\", \"Stage 4\"))\n\n\n\n\n\n\n\nFigure 6.1: Log-log survival curves by lymphoma stage\n\n\n\n\n\nThe evidence for proportional hazards in Figure 6.1 is inconclusive. There is some crossing but it isn’t extensive. This is a small dataset from which it is hard to draw meaningful conclusions.\n\n\n6.6.3 Residual Plots\nVarious types of residuals can be defined for survival regression models. One choice we will discuss briefly are the Schoenfeld Residuals. Schoenfeld residuals are defined only for non-censored observations and there is a separate set for each of the covariates. They are the difference between the value of the covariate of interest for the subject experiencing the event (say death) and the expectation over all members of the risk set of the covariate: \\[\nr_{ik} = x_{ik} - \\sum_{j \\in \\mathcal{R}(t_{i})} x_{jk} \\hat{p}_j,\n\\] where \\(r_{ik}\\) is the Schoenfeld residual for individual \\(i\\) for the \\(k^{th}\\) covariate, \\(x_{jk}\\) are the values for that covariate for the individuals \\(j\\) in the risk set for \\(\\mathcal{R}(t_{i})\\) of individuals at time \\(t_i\\), the time of death for the \\(i^{th}\\) individual and \\(\\hat{p}_j\\) is the estimated probability that the \\(j^{th}\\) person dies by time \\(t_i\\). These should be independent of time so a plot of these versus time should show no dependence. Deviation from independence will indicate inadequacy of the proportional hazards model.\nSchoenfeld residuals can be obtained as follows (using the atrial fibrillation example afibph):\n\nschoenfeld_resid &lt;- resid(afibph, type = \"schoenfeld\")\n\nThis produces an array with one column per covariate. The function cox.zph() can be used to show plots of residuals against time, and implement hypothesis test of no association with time, but we will not study the use of this function in this module.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Proportional Hazards Models</span>"
    ]
  },
  {
    "objectID": "proportionalhazards.html#tasks",
    "href": "proportionalhazards.html#tasks",
    "title": "6  Proportional Hazards Models",
    "section": "6.7 Tasks",
    "text": "6.7 Tasks\n\nA cohort study to investigate diet was carried out on a group of men. Before entering the study each individual was asked for their smoking and drinking habits; and whether they were vegetarian or not. Their weights were also recorded (kg) and centred around 75 kg (i.e. 85 would be coded as 10 and 70 as -5). The outcome of interest was time to death. A Cox proportional hazards model was fitted to the data and some modified output is shown below.\n\n\n\n\nVariable\n\n\nCoefficient\nStandard Error\n\n\n\n\nSmoking Habits\n\nSmoker\nbaseline\n—\n\n\n\n\nNon-smoker\n-0.25\n0.03\n\n\nDrinking Status\n\nRegular Drinker\nbaseline\n—\n\n\n\n\nOccasional Drinker\n-0.15\n0.34\n\n\n\n\nNever Drink\n-0.33\n0.08\n\n\nWeight (centred on 75 kgs as baseline)\n\n\n0.0004\n0.0001\n\n\nVegetarian\n\nYes\nbaseline\n—\n\n\n\n\nNo\n-0.32\n0.31\n\n\n\n\nWrite down the model fitted.\nDescribe in detail the effects of these variables on survival.\nUsing the model, calculate the estimate of the hazard ratio comparing\n\n\nA smoker weighing 83kg who denotes themselves as regular drinker and who is a vegetarian;\nA non-smoker weighing 71kg who denotes themselves as a non-drinker and who is not a vegetarian.\n\n\nA cohort of live born babies was followed up for one year to explore the effect of birth weight and sex on survival. Some modified results from fitting a proportional hazards model in R are shown below. The model relates survival time to birth weight (grouped into four categories), sex of the baby, and the age of the mother.\n\n\n\n\n\n\n\n\n\n\n\nVariable\n\n\nCoefficient\nStandard Error\n\n\n\n\nSex of Baby\n\nMale\nbaseline\n—\n\n\n\n\nFemale\n-0.39\n0.07\n\n\nBirthweight\n\n\\(\\geq 4000\\) g\n0.12\n0.08\n\n\n\n\n3500–3999g\nbaseline\n—\n\n\n\n\n3000–3499g\n0.27\n0.15\n\n\n\n\n\\(&lt;3000\\)g\n1.61\n0.23\n\n\nAge of mother\n\n\\(&gt; 40\\) yrs\n0.65\n0.33\n\n\n\n\n\\(\\leq 40\\) yrs\nbaseline\n—\n\n\n\n\nSpecify the form of the hazard model used for this analysis, carefully defining each predictor variable.\nDescribe in detail the effects of these variables on infant survival.\nHow would you assess whether the proportional hazards model was appropriate for these data.\nUsing the model, calulate the estimate of the hazard ratio comparing\n\n\nA female baby with birth weight 3000–3499g and a 35 yr old mother,\nA male baby with birth weight 3500–3999g and a 41 yr old mother.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n1a) This is a Cox-proportional hazards model where the model fitted is \\[\nh(t, \\mathbf{x}) = h_0(t) \\exp \\{\\beta^T \\mathbf{x}\\}\n\\] where \\(\\mathbf{x} = (x_1, x_2, x_3, x_4, x_5)^T\\) with \\[\\begin{align*}\nx_1 &= 1_{\\textrm{[Non-smoker]}} \\\\\nx_2 &= 1_{\\textrm{[Occasional Drinker]}} \\\\\nx_3 &= 1_{\\textrm{[Never Drink]}} \\\\\nx_4 &= \\textrm{Weight} - 75 \\\\\nx_5 &= 1_{\\textrm{[Non-vegetarian]}} \\\\\n\\end{align*}\\] and \\(\\beta_i\\) are given in the output.\n1b)\n\nSmoking: Compare \\(-0.25/0.03 = -8.3\\) with \\(N(0,1)\\) and so conclude that there is overwhelming evidence that non-smokers have a lower hazard i.e. increased survival.\nDrinking: This is a three level factor and so need to use \\(\\chi^2\\) comparison. We find \\((-0.15/0.34)^2 + (-0.33/0.08)^2 = 17.2\\) and compare with \\(\\chi^2_2\\). Again overwhelming evidence that drinking affects survival. The heavier the drinker then the higher the hazard i.e. reduced survival\nWeight: Compare \\(0.0004/0.0001 = 4\\) with \\(N(0,1)\\) and so conclude that there is extremely strong evidence that weight influences survival. The heavier then the higher the hazard i.e. reduced survival.\nDiet: Compare \\(-0.32/0.31 = -1.03\\) with \\(N(0,1)\\) and so conclude that there is no significant evidence that being vegetarian affects survival.\n\n1c) Using the model, we find for the initial individual \\[h_1(t) = h_0(t) \\exp\\{(0.0004 \\times 8)\\}\\] while for the second individual we have \\[\nh_2(t) = h_0(t) \\exp\\{-0.25 + (0.0004 \\times -4) + (-0.33) + (-0.32) \\}\n\\] so hazard ratio is \\[\n\\frac{h_1(t)}{h_2(t)} = \\exp\\{0.25 + 0.0004 \\times 12 + 0.33  + 0.32\\} = 2.47\n\\]\n2a) Model fitted is \\[\nh(t, \\mathbf{x}) = h_0(t) \\exp \\{\\beta^T \\mathbf{x}\\}\n\\] where \\(\\mathbf{x} = (x_1, x_2, x_3, x_4, x_5)^T\\) with \\[\\begin{align*}\nx_1 &= 1_{[\\textrm{Baby is Female}]} \\\\\nx_2 &= 1_{[\\textrm{Weight $\\geq4000$g}]} \\\\\nx_3 &= 1_{[\\textrm{Weight 3000-3499g}]} \\\\\nx_4 &= 1_{[\\textrm{Weight $&lt;3000$g}]} \\\\\nx_5 &= 1_{[\\textrm{Mother $&gt; 40$ yrs}]}\n\\end{align*}\\] and \\(\\beta_i\\) are given in the table.\n2b)\n\nSex: Compare \\(-0.39/0.07 = -5.57\\) with \\(N(0,1)\\) and so conclude that there is extremely strong evidence that female babies have a lower hazard i.e. increased survival.\nWeight: This is a four level factor and so need to use \\(\\chi^2\\) comparison. We find \\((0.12/0.08)^2+(0.27/0.15)^2+(1.61/0.23)^2 = 54.5\\) and compare with \\(\\chi^2_3\\). There is therefore also extremely strong evidence that birthweight affects hazard. The data indicate that 3500-3999g has greatest survival while underweight babies \\(&lt;3000\\)g have significantly reduced survival.\nAge: Compare \\(0.65/0.33 = 1.97\\) with \\(N(0,1)\\) and so conclude that there is reasonably strong evidence that older mothers have an increased hazard i.e. reduced survival.\n\n2c) Would create Log-log plots of the sex, birthweights and ages and check seem reasonably parallel.\n2d) Using the model, we find for female baby \\[h_F(t) = h_0(t) \\exp\\{-0.39+0.27\\}\\] while for male baby we have \\[\nh_M(t) = h_0(t) \\exp\\{0.65\\}\n\\] so hazard ratio is \\[\n\\frac{h_F(t)}{h_M(t)} = \\exp\\{-0.39+0.27-0.65\\} = 0.46\n\\]",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Proportional Hazards Models</span>"
    ]
  },
  {
    "objectID": "proportionalhazards.html#footnotes",
    "href": "proportionalhazards.html#footnotes",
    "title": "6  Proportional Hazards Models",
    "section": "",
    "text": "Cox, D.R. (1972), Regression Models and Life-Tables. Journal of the Royal Statistical Society: Series B (Methodological), 34: 187-202. https://doi.org/10.1111/j.2517-6161.1972.tb00899.x↩︎\nCox, D. R. (1975). Partial Likelihood. Biometrika, 62(2), 269–276. https://doi.org/10.2307/2335362↩︎",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Proportional Hazards Models</span>"
    ]
  }
]