[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MAS370/61003 Sampling Theory and Design of Experiments",
    "section": "",
    "text": "Instructions\nStudents on MAS61003 should study all three parts of these notes:\nStudents on MAS370 should study Parts I and II only.",
    "crumbs": [
      "Instructions"
    ]
  },
  {
    "objectID": "index.html#acknowledgement",
    "href": "index.html#acknowledgement",
    "title": "MAS370/61003 Sampling Theory and Design of Experiments",
    "section": "Acknowledgement",
    "text": "Acknowledgement\nSome of the content in this module has been taught by various former colleagues over the years (going back to the former Department of Probability and Statistics at Sheffield), all of whom will have developed the notes: Vic Barnett, Richard Martin, Alex Donev, David Grey and Kevin Walters.",
    "crumbs": [
      "Instructions"
    ]
  },
  {
    "objectID": "Introduction to experimental design.html",
    "href": "Introduction to experimental design.html",
    "title": "1  Introduction to Experimental Design",
    "section": "",
    "text": "1.1 Modelling and Inference\nIn previous courses, you will have met the concept of statistical inference: using statistical methods to learn about the characteristics of some population or process given suitable data. Often, we will want to make inferences about relationships between variables. You will have seen how to do this using statistical models.\nSuppose we wish to investigate the relationship between car engine size and fuel economy. Data are available from the USA’s Department of Energy, and are plotted in Figure 1.1.\nFigure 1.1: Estimated fuel economy for 1182 cars. Data obtained from the USA’s Department of Energy 2009 Fuel Economy Guide\nWe can fit various models to these data such as \\[Y_i=\\beta_0 + \\beta_1 x_i + \\varepsilon_i\\] or \\[Y_i=\\beta_0 + \\beta_1 x_i + \\beta_2x_i^2 +  \\varepsilon_i\\] with \\(Y_i\\) defined to be the \\(i\\)th car’s estimated fuel economy, and \\(x_i\\) the corresponding engine size. We can find the most appropriate model using suitable statistical procedures, and provide a quantitative description of the relationship between fuel economy and engine size.",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Experimental Design</span>"
    ]
  },
  {
    "objectID": "Introduction to experimental design.html#design-aims",
    "href": "Introduction to experimental design.html#design-aims",
    "title": "1  Introduction to Experimental Design",
    "section": "1.2 Design aims",
    "text": "1.2 Design aims\nIn almost all your statistics courses so far, you will only have considered the analysis of existing datasets. This module is concerned with the design of datasets. Suppose a car manufacturer wishes to optimise the fuel economy of a particular model of car, by varying particular factors such as engine size, weight, aerodynamics, choice of transmission and so on. The manufacturer can build a prototype with a specified engine size, weight etc. and then test it for fuel efficiency. But building a prototype for every possible combination of these factors is not practical. Instead, the manufacturer could build a limited number of prototypes, and then from the resulting data construct a statistical model to predict the fuel efficiency under any combination of factors. But how many prototypes should the manufacturer build, and what should the choices of design feature be for each prototype?\n\n\n\n\n\n\nExample\n\n\n\n\nExample 1.1 A routine process in new industrial machinery is taking twice as long as in other machinery. Factors which may have been the cause were identified and a screening experiment is to be carried out. How to design an appropriate experiment to identify the relevant factors?\n\n\n\n\nLevel\n\n\n\nVariable\n\\(-1\\)\n\\(1\\)\n\n\nwater supply\ntown reservoir\nwell\n\n\nraw material\non site\nother\n\n\ntemperature\nlow\nhigh\n\n\nrecycle\nyes\nno\n\n\ncaustic soda\nfast\nslow\n\n\nfilter cloth\nnew\nold\n\n\nholding up time\nlow\nhigh",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Experimental Design</span>"
    ]
  },
  {
    "objectID": "Introduction to experimental design.html#design-notation",
    "href": "Introduction to experimental design.html#design-notation",
    "title": "1  Introduction to Experimental Design",
    "section": "1.3 Design notation",
    "text": "1.3 Design notation\nWe consider an experiment in which we are investigating the effect of a number of explanatory variables or covariates \\(x_1, x_2, \\ldots x_m\\) on a response variable \\(y\\). The element of design is that we can choose the values of \\(x_1, x_2, \\ldots x_m\\) from a set known as the design region, which can be thought of as choosing a point in a \\(m\\)-dimensional space. With these design points we could then construct an experiment and measure the response \\(y\\). The explanatory variable may be quantitative (e.g. the drug dose in milligrams) or qualitative (e.g. the presence/absence of a treatment). A variable may also be qualitative but with more than two levels (eg ‘diet’ consisting of vegan, vegetarian meat eater etc). We shall generally think of the response variable as quantitative and real-valued, although this will not be true of all experiments.\nThe collection of points chosen in the design region at which to take observations is known as the design of the experiment. Note that more than one observation may be taken at the same point; this is known as replication, and may be an important feature of a design.",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Experimental Design</span>"
    ]
  },
  {
    "objectID": "Introduction to experimental design.html#sec-issues",
    "href": "Introduction to experimental design.html#sec-issues",
    "title": "1  Introduction to Experimental Design",
    "section": "1.4 Some issues to consider in the design of experiments",
    "text": "1.4 Some issues to consider in the design of experiments\n\nWhat is the purpose of the investigation?\nA good design for one purpose is not guaranteed to be a good design for another. Two broad objectives that we will consider are\n\nInference: establishing whether there is a relationship between the response variable and the explanatory variables, and describing the nature of any such relationship.\nPrediction: estimating the value of the response variable at untried values of explanatory variables.\n\nHow will we achieve these objectives?\nBy using appropriate statistical models. You will have already seen in other modules the role of statistical models in both inference and prediction.\nWhat class of statistical models will we consider?\nIn this module, we restrict our attention to linear models. This is sufficient to cover a very wide range of applications. In practice, this requires the response variable to be continuous (or it must at least be sensible to treat the response variable as continuous).\nWhat are we looking for in a good design?\nThe key concept is efficiency. Assuming there is a cost to collecting data, we want to meet the objectives of the investigation using as little data as possible.\n\nIn the next section, we briefly recap the theory of linear models, considering both parameter estimation for inference and prediction. Throughout, we consider how to design efficiently.",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Experimental Design</span>"
    ]
  },
  {
    "objectID": "Review of linear models.html",
    "href": "Review of linear models.html",
    "title": "2  A review of linear models",
    "section": "",
    "text": "2.1 The general linear model\nIn this course, we shall mostly be concerned with the general linear model which takes the form \\[\\begin{equation}\nY = \\mathbf{f}(\\mathbf{x})^T\\boldsymbol{\\beta}+\\varepsilon=\\sum_{i=1}^p f_i(\\mathbf{x})\\beta_i + \\varepsilon,\n\\end{equation}\\] where \\(\\mathbf{f}(\\mathbf{x})^T = (f_1(\\mathbf{x}) ~~ f_2(\\mathbf{x})~~\\ldots ~~f_p(\\mathbf{x})\\) is a row vector of known functions of \\(\\mathbf{x}\\). \\(\\mathbf{x}= (x_1~~x_2~~\\ldots ~~x_m)^T\\) is a vector of known explanatory variable values and \\(\\varepsilon\\) is a random variable with mean zero. So the assumption is that the expected value of \\(Y\\) \\[E(Y) = \\mathbf{f}(\\mathbf{x})^T\\boldsymbol{\\beta}\\] is a linear combination of the components of \\(\\mathbf{f}(\\mathbf{x})\\), with the coefficients being the unknown parameters.\nNote that in both of these examples we have labelled the components of the parameter vector \\(\\boldsymbol{\\beta}\\) not in the standard way (\\(\\beta_1, \\beta_2, \\ldots\\)), but in a way which reflects more naturally the characteristics of the model: we shall see plenty more examples of this later.",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A review of linear models</span>"
    ]
  },
  {
    "objectID": "Review of linear models.html#the-general-linear-model",
    "href": "Review of linear models.html#the-general-linear-model",
    "title": "2  A review of linear models",
    "section": "",
    "text": "Example\n\n\n\n\nExample 2.1 The simple linear regression model may be written \\[Y = \\beta_0 + \\beta_1 x + \\varepsilon\\] so that in this case \\(m=1\\), \\(p=2\\), \\(\\mathbf{x}=x\\) and \\(\\mathbf{f}(\\mathbf{x})^T=(1~~x)\\).\nThe quadratic regression model may be written \\[Y = \\beta_0 + \\beta_1 x + \\beta_{11}x^2 + \\varepsilon\\] so that in this case \\(m=1\\), \\(p=3\\), \\(\\mathbf{x}=x\\) and \\(\\mathbf{f}(\\mathbf{x})^T = (1~~x~~x^2)\\).",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A review of linear models</span>"
    ]
  },
  {
    "objectID": "Review of linear models.html#matrix-notation",
    "href": "Review of linear models.html#matrix-notation",
    "title": "2  A review of linear models",
    "section": "2.2 Matrix notation",
    "text": "2.2 Matrix notation\nSuppose that our design consists of \\(n\\) points \\(\\mathbf{x_1}, \\mathbf{x_2},\\ldots , \\mathbf{x_n}\\) (some of which will be the same if replication occurs) and the corresponding observations are \\(Y_1, Y_2, \\ldots , Y_n\\). Then we have \\[Y_j = \\mathbf{f}(\\mathbf{x_j})^T\\boldsymbol{\\beta}+\\varepsilon_j \\;\\;\\;\\ \\text{for} \\;\\;\\;\\ j=1,2,\\ldots ,n\\] say, and these \\(n\\) equations may be collected together into the single matrix equation \\[\\mathbf{Y}= \\mathbf{X}\\boldsymbol{\\beta}+\\boldsymbol{\\varepsilon}\\] where \\(\\mathbf{Y}\\) and \\(\\boldsymbol{\\varepsilon}\\) are column vectors containing \\(Y_1,Y_2, \\ldots , Y_n\\) and \\(\\varepsilon_1, \\varepsilon_2, \\ldots ,\n\\varepsilon_n\\) respectively, and \\(\\mathbf{X}\\) is the \\(n\\times p\\) matrix whose rows are \\[\\mathbf{f}(\\mathbf{x_1})^T, \\mathbf{f}(\\mathbf{x_2})^T, \\ldots , \\mathbf{f}(\\mathbf{x_n})^T\\] respectively. This may also be written \\[E(\\mathbf{Y})= \\mathbf{X}\\boldsymbol{\\beta}.\\]\nThe matrix \\(\\mathbf{X}\\) is known as the design matrix of the experiment. It should be noted, however, that it depends not only on the design as previously defined, but also on the model chosen, via the function \\(\\mathbf{f}\\).\n\n\n\n\n\n\nExample\n\n\n\n\nExample 2.2 Suppose there is a single explanatory variable \\(x\\) and it is decided to take one observation at each of the five points \\(x=0,1,2,3\\) and \\(4\\). Then for the simple linear regression model the design matrix will be \\[\\mathbf{X}=\\left(\\begin{array}{cc} 1 & 0 \\\\ 1 & 1 \\\\ 1 & 2 \\\\ 1 & 3 \\\\ 1 & 4 \\end{array} \\right)\\] whereas for the quadratic regression model it will be \\[\\mathbf{X}=\\left(\\begin{array}{ccc} 1 & 0 & 0 \\\\ 1 & 1 & 1 \\\\ 1 & 2 & 4 \\\\ 1 & 3 & 9 \\\\ 1 & 4 & 16 \\end{array}\\right).\\]",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A review of linear models</span>"
    ]
  },
  {
    "objectID": "Review of linear models.html#least-squares-estimation",
    "href": "Review of linear models.html#least-squares-estimation",
    "title": "2  A review of linear models",
    "section": "2.3 Least squares estimation",
    "text": "2.3 Least squares estimation\nYou will have seen how to estimate \\(\\boldsymbol{\\beta}\\) by minimising the sum of squares: we get an expression for the sum of squares, differentiate with respect to \\(\\boldsymbol{\\beta}\\) and equate to zero. The aim of estimation is to find a value of \\(\\boldsymbol{\\beta}\\) such that, in some sense, the observed \\(\\mathbf{Y}\\) is close to its expected value \\(\\mathbf{X}\\boldsymbol{\\beta}\\).\nIn order to be able to identify \\(\\boldsymbol{\\beta}\\) from \\(\\mathbf{X}\\boldsymbol{\\beta}\\), it is desirable that the linear transformation \\(\\boldsymbol{\\beta}\\rightarrow \\mathbf{X}\\boldsymbol{\\beta}\\) be one-to-one, so that every possible value of \\(\\boldsymbol{\\beta}\\) gives a different \\(\\mathbf{X}\\boldsymbol{\\beta}\\). For this to be the case the matrix \\(\\mathbf {X}\\) must be of full rank \\(p\\) which, since \\(\\mathbf{X}\\) is an \\(n\\times p\\) matrix, requires \\(n\\geq p\\) (at least as many design points as parameters). If it is not the case that \\(\\mathbf{X}\\) is of full rank, the model is said to be over-parametrised.\nWhen \\(\\mathbf{X}\\) is of full rank \\(p\\), \\(\\mathbf{X}^T\\mathbf{X}\\) is a positive definite symmetric \\(p\\times p\\) matrix and is invertible, so the usual least-squares estimator applies: \\[\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T\\mathbf{Y}.\\]",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A review of linear models</span>"
    ]
  },
  {
    "objectID": "Review of linear models.html#statistical-properties-of-the-least-squares-estimator",
    "href": "Review of linear models.html#statistical-properties-of-the-least-squares-estimator",
    "title": "2  A review of linear models",
    "section": "2.4 Statistical properties of the least squares estimator",
    "text": "2.4 Statistical properties of the least squares estimator\nWe first note that the least squares estimator is unbiased: \\[E(\\hat{\\boldsymbol{\\beta}}) = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta}=\\boldsymbol{\\beta}.\\] Since \\(\\boldsymbol{\\beta}\\) is a function of \\(\\mathbf{Y}\\) which is a function of \\(\\boldsymbol{\\varepsilon}\\) we need to consider the distribution of the errors \\(\\boldsymbol{\\varepsilon}\\).\nThe usual assumption is that they are normally distributed and independent of each other with some common variance \\(\\sigma ^2\\), typically unknown. Under this assumption, there is further statistical justification for the estimation approach above: for example, it may be shown that \\(\\hat{\\boldsymbol{\\beta}}\\) is the maximum likelihood estimator of \\(\\boldsymbol{\\beta}\\).\nIf we adopt this assumption, then it follows that the components of \\(\\mathbf{Y}\\) are multivariate normally distributed with variance-covariance matrix given by \\[\\text{cov}(\\mathbf{Y})= \\text{cov}(\\boldsymbol{\\varepsilon})=\\sigma ^2\\mathbf{I}\\] where \\(\\mathbf{I}\\) is the \\(n\\times n\\) identity matrix, and it follows that \\[\\text{cov}(\\hat{\\boldsymbol{\\beta}}) = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\sigma^2\\mathbf{I}\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\] which simplifies1 to \\[\\text{cov}(\\hat{\\boldsymbol{\\beta}}) =\\sigma^2 (\\mathbf{X}^T\\mathbf{X})^{-1}\\] We can now state that \\[\\hat{\\boldsymbol{\\beta}} \\sim N(\\boldsymbol{\\beta}, \\sigma^2  (\\mathbf{X}^T\\mathbf{X})^{-1}).\\] The matrix \\(\\mathbf{X}^T\\mathbf{X}\\) is called the information matrix and is very relevant to the choice of design because, as seen above, it determines the accuracy of estimation of the parameters.\n\n\n\n\n\n\nExample\n\n\n\n\nExample 2.3 In the simple linear regression model, \\(E(Y)=\\beta_0+\\beta_1x\\), suppose one observation is taken at each of the points \\(x_1, x_2, \\ldots , x_n\\). Then \\[\\begin{align*}\n\\mathbf{X}^T\\mathbf{X} &= \\left(\\begin{array}{cc} n & \\sum_{j=1}^n x_j \\\\ \\sum_{j=1}^n x_j & \\sum_{j=1}^n x_j^2 \\end{array} \\right) \\\\\n&=\\left(\\begin{array}{cc} n & n\\bar{x} \\\\ n\\bar{x} & s_{xx}+n\\bar{x}^2 \\end{array} \\right)\\\\\n(\\mathbf{X}^T \\mathbf{X})^{-1} &=\\frac{1}{ns_{xx}}\\left( \\begin{array}{cc} s_{xx}+n\\bar{x}^2 & -n\\bar{x} \\\\\n-n\\bar{x} & n \\end{array} \\right)\n\\end{align*}\\] where \\(s_{xx}=\\sum_{j=1}^{n}(x_j-\\bar{x})^2=\\sum_{j=1}^{n}x_j^2-n\\bar{x}^2\\)\n\n\n\nDesign problem: How to choose design points \\(\\mathbf{X}_1,\\ldots,\\mathbf{X}_n\\) to give the most precise gradient estimate? See R example 2.3 on Blackboard - how do different designs affect gradient estimate in simple linear regression? - Is bigger always better?",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A review of linear models</span>"
    ]
  },
  {
    "objectID": "Review of linear models.html#fitted-values-residuals-and-the-residual-sum-of-squares",
    "href": "Review of linear models.html#fitted-values-residuals-and-the-residual-sum-of-squares",
    "title": "2  A review of linear models",
    "section": "2.5 Fitted values, residuals, and the residual sum of squares",
    "text": "2.5 Fitted values, residuals, and the residual sum of squares\nHaving found the least squares estimator \\(\\hat{\\boldsymbol{\\beta}}\\), we may decompose \\(\\mathbf{Y}\\) as \\[\\begin{align}\n\\mathbf{Y}&= \\mathbf{X}\\hat{\\boldsymbol{\\beta}}+(\\mathbf{Y}-\\mathbf{X}\\hat{\\boldsymbol{\\beta}}) \\\\\n&=\\hat{\\mathbf{Y}}+\\hat{\\boldsymbol{\\varepsilon}}\n\\end{align}\\] say, where \\(\\hat{\\mathbf{Y}}=\\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\) is the vector of fitted values and \\(\\hat{\\boldsymbol{\\varepsilon}}=\\mathbf{Y}-\\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\) is the vector of residuals. We may also write the above decomposition as \\[\\mathbf{Y}=\\mathbf{P}\\mathbf{Y}+(\\mathbf{I}-\\mathbf{P})\\mathbf{Y}\\] where \\(\\mathbf{P}=\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\) and \\(\\mathbf{I}\\) is now the \\(n\\times n\\) identity matrix. It follows (after some algebra) that \\[\\mathbf{Y}^T\\mathbf{Y}=\\hat{\\mathbf{Y}}^T\\hat{\\mathbf{Y}}+\\hat{\\boldsymbol{\\varepsilon}}^T\\hat{\\boldsymbol{\\varepsilon}}\\] which is a decomposition of the sum of squares of the data into the sum of squares of the fitted values and the sum of squares of the residuals. These are called, respectively, the sum of squares due to the model and the residual (or error) sum of squares.\nIf the usual distributional assumptions are made, then it follows from the nice properties of multivariate normal distributions that \\(\\hat{\\boldsymbol{\\beta}}\\) (or consequently \\(\\hat{\\mathbf{Y}}\\)) and \\(\\hat{\\boldsymbol{\\varepsilon}}\\) are independent of each other in the statistical sense, and also that \\(\\sigma^{-2}\\hat{\\boldsymbol{\\varepsilon}}^T\\hat{\\boldsymbol{\\varepsilon}}\\) has the \\(\\chi ^2\\) distribution with \\(n-p\\) degrees of freedom. \\(n-p\\) is the number of degrees of freedom for error. In particular, \\[\\hat{\\sigma }^2=\\frac{\\hat{\\boldsymbol{\\varepsilon}}^T \\hat{\\boldsymbol{\\varepsilon}}}{(n-p)}\\] is an unbiased estimator of \\(\\sigma ^2\\), and, among other things, measures the goodness of fit of the model: the smaller its value, the better the fit.\n\n\n\n\n\n\nThese questions will help you revise some matrix algebra, and are intended to help you to understand these lecture notes, but you won’t get exam questions like these - don’t worry if you find the questions hard!\n\n\n\n\nExercise 2.1  \n\nCheck that if \\(\\mathbf{X}\\) is of full rank \\(p\\) then \\(\\mathbf{X}^T\\mathbf{X}\\) is a positive definite symmetric matrix.\nCheck that \\(\\mathbf{P}^2=\\mathbf{P}\\), and use the representation \\[\\begin{align*}\n\\hat{\\mathbf{Y}} &= \\mathbf{P}\\mathbf{Y} \\;\\;\\; \\text{and} \\\\\n\\hat{\\boldsymbol{\\varepsilon}} &= (\\mathbf{I} -\\mathbf{P})\\mathbf{Y}\n\\end{align*}\\] to show that \\[\\begin{equation}\n\\hat{\\mathbf{Y}}^T\\hat{\\mathbf{Y}}+\\hat{\\boldsymbol{\\varepsilon}}^T\\hat{\\boldsymbol{\\varepsilon}}=\\mathbf{Y}^T\\mathbf{Y}.\n\\end{equation}\\]\nConsider what happens to the least squares estimator \\(\\hat{\\boldsymbol{\\beta} }\\) when \\(n=p\\), in both cases when \\(\\mathbf{X}\\) is of full rank and when it is not (\\(\\mathbf{X}\\) is an \\(n\\times p\\) matrix).\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nWe have that \\(\\mathbf{X}^T\\mathbf{X}\\) is symmetric since \\[(\\mathbf{X}^T\\mathbf{X} )^T = \\mathbf{X} ^T(\\mathbf{X}^T)^T = \\mathbf{X} ^T\\mathbf{X} .\\] To say that it is positive definite means that \\[\\mathbf{a}^T\\mathbf{X}^T\\mathbf{X} \\mathbf{a}\\] is positive for all non-zero vectors \\(\\mathbf{a}\\).This is true since it may be written \\[(\\mathbf{X} \\mathbf{a} )^T\\mathbf{X} \\mathbf{a}\\] which is the sum of squares of the components of the vector \\(\\mathbf{X} \\mathbf{a}\\), and this is positive unless \\(\\mathbf{X} \\mathbf{a}\\) is the zero vector, which in turn means that \\(\\mathbf{a}\\) is the zero vector, since \\(\\mathbf{X}\\) is of full rank.\nWe have \\[\\mathbf{P}^2 = \\mathbf{X} (\\mathbf{X}^T\\mathbf{X} )^{-1}\\mathbf{X}^T\\mathbf{X} (\\mathbf{X}^T\\mathbf{X} )^{-1} \\mathbf{X}^T = \\mathbf{X} (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X} ^T = \\mathbf{P}.\\] Note also that \\(\\mathbf{P}\\) is symmetric, since \\(\\mathbf{X}^T\\mathbf{X}\\) being symmetric implies that \\((\\mathbf{X}^T\\mathbf{X} )^{-1}\\) is also symmetric, and then \\[\\mathbf{P} ^T = (\\mathbf{X} ^T)^T (\\mathbf{X} ^T \\mathbf{X})^{-1} \\mathbf{X} ^T = \\mathbf{P} .\\] So \\[\\hat{\\mathbf{Y} }^T\\hat{\\mathbf{Y} } = \\mathbf{Y} ^T\\mathbf{P} ^T\\mathbf{P} \\mathbf{Y} = \\mathbf{Y} ^T\\mathbf{P} ^2\\mathbf{Y} =\n\\mathbf{Y} ^T\\mathbf{P} \\mathbf{Y}\\] and \\[\\hat{\\boldsymbol{\\varepsilon} } ^T\\hat{\\boldsymbol{\\varepsilon} } = \\mathbf{Y} ^T(\\mathbf{I} -\\mathbf{P} )^2\\mathbf{Y} =\n\\mathbf{Y} ^T(\\mathbf{I} - 2\\mathbf{P} + \\mathbf{P} ^2)\\mathbf{Y} = \\mathbf{Y} ^T(\\mathbf{I} - \\mathbf{P} )\\mathbf{Y}.\\] Adding these together gives \\[ \\mathbf{Y} ^T[\\mathbf{P} + (\\mathbf{I} - \\mathbf{P} )]\\mathbf{Y}  = \\mathbf{Y} ^T\\mathbf{Y} . \\]\nIf \\(n=p\\) and \\(\\mathbf{X}\\) is of full rank, both \\(\\mathbf{X}\\) and \\(\\mathbf{X}^T\\) are square and non-singular, and so invertible. Then \\[(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T =\\mathbf{X}^{-1}(\\mathbf{X}^{T})^{-1}\\mathbf{X}^T = \\mathbf{X}^{-1},\\] and so \\[ \\hat{\\boldsymbol{\\beta} } =  \\mathbf{X} ^{-1} \\mathbf{Y}\\] is an exact fit of the model to the data; the residuals are identically zero and there is no possibility of estimating \\(\\sigma^2\\). If \\(\\mathbf{X}\\) is not of full rank, there will be a least squares estimator of \\(\\mathbf{X} \\boldsymbol{\\beta}\\) satisfying \\[ \\mathbf{X} ^T\\hat{(\\mathbf{X} \\boldsymbol{\\beta} )} = \\mathbf{X} ^T\\mathbf{Y} \\] but this will not be sufficient to determine \\(\\hat{\\boldsymbol{\\beta} }\\) uniquely. The model must be reformulated with fewer parameters, \\(p^{\\prime }\\) say, such that the new design matrix is of full rank \\(p^{\\prime }\\) and then \\(n&gt;p^{\\prime }\\), so that the regular conditions prevail.",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A review of linear models</span>"
    ]
  },
  {
    "objectID": "Review of linear models.html#polynomial-models",
    "href": "Review of linear models.html#polynomial-models",
    "title": "2  A review of linear models",
    "section": "2.6 Polynomial models",
    "text": "2.6 Polynomial models\nIf all of \\(x_1, x_2, \\ldots , x_m\\) are quantitative, and the response variable is expected to depend in a smooth way on their values, then the design region can be regarded as a subset of \\(m\\)-dimensional space, whose boundaries are determined in practice by the limits to which the variables can be set. For example if each of \\(x_1, x_2, \\ldots , x_m\\) are constrained to lie in the real interval \\([-,1]\\) then the design region is \\([-1-,1]^m\\). Because any smooth function can be approximated within a bounded region by a polynomial, it is common and natural to try to fit polynomial regression models, of which the simple linear and quadratic regression models mentioned in Example 2.1 are the simplest examples.\nWith more than one explanatory variable (\\(m&gt;1\\)) and with higher order polynomials in contention, the number of parameters increases rapidly; for example with \\(m=4\\) the full quadratic model may be written \\[\\begin{align}\nEY &= \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_3x_3 + \\beta_4x_4 + \\beta_{12}x_1x_2 + \\beta_{13}x_1x_3 + \\beta_{14}x_1x_4 \\\\\n&+ \\beta_{23}x_2x_3 + \\beta_{24}x_2x_4 + \\beta_{34}x_3x_4 + \\beta_{11}x_1^2 + \\beta_{22}x_2^2 + \\beta_{33}x_3^2 + \\beta_{44}x_4^2 .\n\\end{align}\\] Note how in this example the \\(\\beta\\) parameters are labelled with subscripts reflecting the explanatory variables to which they are relevant, and in what way: this is a common convention.\nAs is often the case, there may be a large number of explanatory variables in contention, and if a model such as the above is to be taken seriously, a correspondingly large number of points must be chosen in the design region, if only to ensure that the design matrix is of full rank. In practice, one aim of the analysis of such data is to demonstrate that some of these parameters do not improve significantly the fit of the model and so can be removed: this applies especially to the higher order ones.\n\n\n\n\n\n\nExample\n\n\n\n\nExample 2.4 Suppose \\(m=2\\) and the model is multiple linear regression \\[E(Y)=\\beta_0+\\beta_1x_1+\\beta_2x_2.\\] If three points \\((x_{11},x_{21}), (x_{12},x_{22})\\) and \\((x_{13},x_{23})\\) are chosen at which to take observations, then the design matrix is \\[\\left( \\begin{array}{ccc} 1 & x_{11} & x_{21} \\\\ 1 & x_{12} &  x_{22} \\\\ 1 & x_{13} & x_{23} \\end{array} \\right)\\] and this will be of full rank (i.e. non-singular, since it is square) if and only if there exists no linear relationship of the form \\[\\lambda_0 + \\lambda_1 x_{1j} + \\lambda_2 x_{2j}=0 \\quad for \\quad j=1,2,3 \\quad (\\lambda_i \\;\\text{not all zero})\\] between its columns, or, in other words, the three points must be chosen to be not collinear in order to be able to estimate in the model.",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A review of linear models</span>"
    ]
  },
  {
    "objectID": "Review of linear models.html#qualvars",
    "href": "Review of linear models.html#qualvars",
    "title": "2  A review of linear models",
    "section": "2.7 Models with qualitative variables",
    "text": "2.7 Models with qualitative variables\nQualitative explanatory variables are usually called factor variables or categorical variables. They enter the design matrix via indicator variables to specify the level of the factor. For example, suppose the explanatory variables are pressure and temperature and the effect of using one of two possible catalysts. A model which could be assumed is \\[E(Y) = \\beta_0 + \\alpha z + \\beta_1x_1 + \\beta_2x_2,\\] where \\(z\\) is an indicator variable, taking the values, say, \\(0\\) for the first catalyst and \\(1\\) for the second catalyst. This model would be appropriate if the experimenter expects the relationship between the response and the factors temperature and pressure to be the same for both catalysts, except for a possible difference in level: the regression surfaces of \\(y\\) against temperature (\\(x_1\\)) and pressure (\\(x_2\\)) for the two catalysts will be parallel, with the parameter \\(\\alpha\\) measuring the distance between them.\n\n\n\n\n\n\nNotation\n\n\n\nFrom previous courses on linear models, you may be more used to representing categorical variables using additional subscripts on \\(Y\\), rather than with dummy variables. We will be using the dummy variable notation in this module. If you want an example that compares the two methods of notation, some notes are available here (sections 10.1 to 10.3).\n\n\nIf three different catalysts are used, a possible model is \\[E(Y) = \\beta_0 + \\alpha _1z_1 + \\alpha _2z_2 + \\alpha _3z_3 + \\beta_1x_1 + \\beta_2x_2\\] where \\(z_i=1\\) if catalyst \\(i\\) is used and \\(z_i=0\\) otherwise. However, provided each run of the experiment uses one catalyst, this model is over-parametrised, because \\(z_1+z_2+z_3=1\\), and so the second, third and fourth columns of the design matrix add up to the first column, and \\(X\\) is not of full rank. Another way of saying this is that \\(\\beta_0\\) is confounded with \\(\\alpha _1, \\alpha _2\\) and \\(\\alpha _3\\), in the sense that they cannot independently be estimated. Such an experiment cannot measure the absolute effects of the catalysts, but only the differences between their effects.\nThis over-parametrisation can be overcome by reducing the number of parameters by one, in various ways: for example, by removing \\(\\beta_0\\) from the model, or by imposing some linear constraint on the parameters \\(\\alpha _1, \\alpha _2\\) and \\(\\alpha _3\\) so that one of them can be expressed in terms of the other two and eliminated from the model. If, for example, we impose the constraint \\[\\alpha _1+\\alpha _2+\\alpha _3=0\\] then we may write \\(\\alpha _3=-\\alpha _1-\\alpha _2\\) and so eliminate \\(\\alpha _3\\) as a parameter of the model. Then \\(\\boldsymbol{\\beta}= (\\beta_0~~\\alpha _1~~\\alpha _2~~\\beta_1~~\\beta_2)^T\\) and for any design point \\(\\mathbf{x}\\) at which catalyst \\(3\\) is used, \\(\\mathbf{f}(\\mathbf{x})^T = (1~~-1~~-1~~x_1~~x_2 )\\).",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A review of linear models</span>"
    ]
  },
  {
    "objectID": "Review of linear models.html#orthogonality",
    "href": "Review of linear models.html#orthogonality",
    "title": "2  A review of linear models",
    "section": "2.8 Orthogonality",
    "text": "2.8 Orthogonality\n\n\n\n\n\n\nImportant\n\n\n\nOrthogonality is an important concept in experimental design, as orthogonal designs have desirable statistical properties.\n\n\nSuppose that in a linear model \\[\\boldsymbol{\\beta}= \\left( \\begin{array}{c} \\boldsymbol{\\gamma}\\\\ \\boldsymbol{\\delta}\\end{array}\\right) \\quad \\text{and} \\quad \\mathbf{X}=(\\mathbf {V} \\quad \\mathbf{W})\\] where, say, \\(\\boldsymbol{\\gamma}\\) is a \\(q\\)-vector, \\(\\boldsymbol{\\delta}\\) is a \\((p-q)\\)-vector, \\(\\mathbf{V}\\) is a \\(n\\times q\\) matrix and \\(\\mathbf{W}\\) is a \\(n\\times (p-q)\\) matrix; suppose further that every column of \\(\\mathbf{V}\\) is orthogonal (perpendicular) to every column of \\(\\mathbf{W}\\), which may be written succinctly as \\[\\mathbf{V}^T\\mathbf{W}=\\mathbf{0}.\\] Then the two groups of parameters comprising the components of \\(\\boldsymbol{\\gamma}\\) and \\(\\boldsymbol{\\delta}\\) are said to be orthogonal to each other.\nA particular consequence of orthogonality is that, under the usual normal assumptions, \\[\\text{cov}(\\hat{\\boldsymbol{\\beta}})=\\sigma ^2\\left(\\begin{array}{cc} (\\mathbf{V}^T\\mathbf{V})^{-1} & \\mathbf{0} \\\\ \\mathbf{0} & (\\mathbf{W}^T\\mathbf{W})^{-1} \\end{array} \\right )\\] which since, in a multivariate normal distribution, zero correlation implies independence, shows that the estimators of \\(\\boldsymbol{\\gamma}\\) and \\(\\boldsymbol{\\delta}\\) are independent in the statistical sense. Another obvious consequence of the above is that if, for whatever reason, the parameters of \\(\\boldsymbol{\\delta}\\) are excluded from the model, then it does not affect the estimator of \\(\\boldsymbol{\\gamma}\\) or its sampling distribution.\nIt is clear that more than two groups of parameters may be mutually orthogonal. An extreme example of this is when all of the parameters are mutually orthogonal. In this case \\(\\mathbf{X}^T\\mathbf{X}\\) is a diagonal matrix and its inverse is also diagonal, found by taking reciprocals of its diagonal elements. So the estimators of the components of \\(\\boldsymbol{\\beta}\\) are mutually independent and their variances are easily obtained.\n\n\n\n\n\n\nExample\n\n\n\n\nExample 2.5 The model \\[E(Y)=\\beta_0+\\alpha z+\\beta_1x+\\beta_{11}x^2\\] is proposed for which one observation is taken at each of the values \\(x=-1,0,1\\) of the quantitative variable \\(x\\) for each of the values \\(z=-1,1\\) of the dummy variable \\(z\\). Investigate in this design which combinations of the four parameters of this model are mutually orthogonal.\nOrdering the parameters and the design points in an obvious way, we get \\[\\mathbf{X}=\\left( \\begin{array}{cccc} 1 & -1 & -1 & 1 \\\\ 1 & -1 & 0 & 0 \\\\\n1 & -1 & 1 & 1 \\\\ 1 & 1 & -1 & 1 \\\\ 1 & 1 & 0 & 0 \\\\ 1 & 1 & 1 & 1 \\end{array} \\right)\\] from which it follows taking sums of squares and products of columns that \\[\\mathbf{X}^T\\mathbf{X}=\\left(\\begin{array}{cccc} 6 & 0 & 0 & 4 \\\\ 0 & 6 & 0 & 0 \\\\ 0 & 0 & 4 & 0 \\\\ 4 & 0 & 0 & 4 \\end{array} \\right).\\] From this we can see that \\(\\{\\beta_0, \\beta_{11}\\} ,\\{\\alpha\\}\\) and \\(\\{\\beta_1\\}\\) are mutually orthogonal groups, but that \\(\\beta_0\\) and \\(\\beta_{11}\\) are not orthogonal to each other.\n\n\n\nNote that we coded the indicator variable \\(z\\) as \\(-1\\) and \\(1\\), which is a choice often conducive to orthogonality, as we shall see later. R example 2.5 shows the effect of the choice of \\(z\\) constraints on the orthogonality of the design matrix.",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A review of linear models</span>"
    ]
  },
  {
    "objectID": "Review of linear models.html#prediction",
    "href": "Review of linear models.html#prediction",
    "title": "2  A review of linear models",
    "section": "2.9 Prediction",
    "text": "2.9 Prediction\nOne of the objectives we discussed in Section 1.4 was that of predicting the mean response at some point \\(\\mathbf{x_0}\\) in the design region which may or may not have been used in the design of the experiment. If we label this mean response as \\(y(\\mathbf{x_0})\\), then we have \\[y(\\mathbf{x_0})=\\mathbf{f}(\\mathbf{x_0})^T\\boldsymbol{\\beta}\\] The predicted value of the response at \\(\\mathbf{x_0}\\) is naturally estimated by \\[\\hat{y}(\\mathbf{x_0})=\\mathbf{f}(\\mathbf{x_0})^T\\hat{\\boldsymbol{\\beta}}\\] which is unbiased and has variance given by \\[\\begin{align}\n\\text{var}(\\hat{y}(\\mathbf{x_0})) &= \\mathbf{f}(\\mathbf{x_0})^T\\text{cov}(\\hat{\\boldsymbol{\\beta}})\\mathbf{f}(\\mathbf{x_0}) \\\\\n&=\\sigma^2 \\mathbf{f}(\\mathbf{x_0})^T(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{f}(\\mathbf{x_0}).\n\\end{align}\\] So the accuracy of prediction depends on the design matrix and also the point \\(\\mathbf{x_0}\\) chosen. R example 2.6 shows how prediction variance changes with choice of prediction point.\n\n\n\n\n\n\nExample\n\n\n\n\nExample 2.6 In the simple linear regression model we have \\[(\\mathbf{X}^T\\mathbf{X})^{-1}=\\frac{1}{ns_{xx}} \\left( \\begin{array}{cc} \\sum x^2 & -\\sum x \\\\ -\\sum x & n \\end{array} \\right)\\] and so \\[\\begin{align}\n\\text{var}(\\hat{y}(x_0)) &= \\text{var}(\\hat{\\beta_0}+\\hat{\\beta_1}x_0)\\\\\n&=\\text{var} \\left(\\begin{array}{cc} 1 & x_0 \\end{array} \\right)\\left( \\begin{array}{cc} \\hat{\\beta_0} & \\hat{\\beta_1} \\end{array} \\right)^T\\\\\n&=\\frac{\\sigma^2}{ns_{xx}} \\left( \\begin{array}{cc} 1 & x_0 \\end{array} \\right) \\left( \\begin{array}{cc} \\sum x^2 & -\\sum x \\\\ -\\sum x & n \\end{array} \\right) \\left( \\begin{array}{c} 1 \\\\ x_0 \\end{array} \\right)\\\\\n&= \\sigma ^2 \\left( \\frac{1}{n}+\\frac{(x_0-\\overline{x})^2}{s_{xx}} \\right)\n\\end{align}\\] where \\(\\overline{x}=\\sum x/n\\) and \\(s_{xx} = \\sum x^2 - n\\overline{x}^2\\).\n\n\n\nNote that if a model fits well within a certain region in which the design points have been chosen, there is a danger in predicting outside this region, because there is no evidence that the model continues to fit well here, and there might be good scientific reason to believe that it does not. For example, a polynomial model which happens to fit well within a limited range might become wildly unrealistic outside it – say, predicting negative values in a situation where these are impossible.\nFor future purposes it is convenient here to introduce the standardised information matrix \\[\\mathbf{M}=n^{-1} \\mathbf{X}^T\\mathbf{X}\\] and the standardised variance of prediction at a point \\(\\mathbf{x}\\) \\[d(\\mathbf{x})=n\\mathbf{f}(\\mathbf{x})^T(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{f}(\\mathbf{x})=\\mathbf{f}(\\mathbf{x})^T\\mathbf{M}^{-1}\\mathbf{f}(\\mathbf{x})\\] where the inclusion of the factor \\(n^{-1}\\) or \\(n\\) compensates for the fact that increasing the sample size inevitably increases the accuracy of estimation, and the exclusion of the factor \\(\\sigma ^2\\) from \\(d(\\mathbf{x})\\) makes this function a dimensionless and more intrinsic feature of the design. For example, if a whole design is replicated then the standardised information matrix and variance of prediction are the same for the larger experiment as for the smaller one.",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A review of linear models</span>"
    ]
  },
  {
    "objectID": "Review of linear models.html#sec-confregions",
    "href": "Review of linear models.html#sec-confregions",
    "title": "2  A review of linear models",
    "section": "2.10 Confidence regions",
    "text": "2.10 Confidence regions\nIn the general linear model, the usual distributional assumptions have the following consequences:\n\n\\(\\sigma^{-2}(\\hat{\\boldsymbol{\\beta}}-\\boldsymbol{\\beta})^T\\mathbf{X}^T\\mathbf{X}(\\hat{\\boldsymbol{\\beta}}-\\boldsymbol{\\beta})\\) has \\(\\chi^2_p\\) distribution;\n\\(p^{-1}\\hat{\\sigma}^{-2}(\\hat{\\boldsymbol{\\beta}}-\\boldsymbol{\\beta})^T\\mathbf{X}^T\\mathbf{X}(\\hat{\\boldsymbol{\\beta}}-\\boldsymbol{\\beta})\\) has an \\(F_{p,n-p}\\) distribution, provided \\(n&gt;p\\).\n\nIt follows that, regardless of whether \\(\\sigma ^2\\) is known or unknown, a \\(100(1-\\alpha)\\%\\) confidence region for \\(\\boldsymbol{\\beta}\\) will take the form \\[(\\boldsymbol{\\beta}-\\hat{\\boldsymbol{\\beta}})^T\\mathbf{X}^T\\mathbf{X}(\\boldsymbol{\\beta}-\\hat{\\boldsymbol{\\beta}}) \\leq p\\hat{\\sigma}^2F_{p, n-p, 1-\\alpha}\\] where \\(F_{p, n-p, 1-\\alpha}\\) is the \\(100(1-\\alpha)\\) percentile of the \\(F_{n,p}\\) distribution, obtained from R with the command\n\nqf(1-alpha, p, n - p)\n\nThis region will be the interior of an ellipsoid centred on \\(\\hat{\\boldsymbol{\\beta}}\\). The size of this ellipsoid will depend obviously on \\(\\sigma ^2\\) or \\(\\hat{\\sigma }^2\\) and the confidence level chosen. Its size, shape and orientation are also determined by the covariance matrix \\((\\mathbf{X}^T\\mathbf{X})^{-1}\\). The axes of the ellipsoid point in the directions given by the eigenvectors of \\((\\mathbf{X}^T\\mathbf{X})^{-1}\\), and the length along each axis is proportional to the square root of the corresponding eigenvalue.\n\n\n\n\n\n\nExample\n\n\n\n\nExample 2.7 Simple linear regression with observations taken at \\(x=-1,0 \\: \\text{and}\\: 1\\), and \\(\\sigma^2\\) is unknown. In this case \\[\\mathbf{X}^T\\mathbf{X}= \\left( \\begin{array}{cc} 3 & 0 \\\\ 0 & 2 \\end{array}\\right)\\] and so the confidence region for \\(\\beta_0\\) and \\(\\beta_1\\) is the interior of an ellipse \\[3(\\beta_0-\\hat{\\beta_0})^2 + 2(\\beta_1-\\hat{\\beta_1})^2 \\leq 2\\hat{\\sigma}^2F_{2,1,1-\\alpha }\\] where \\(100(1-\\alpha )\\%\\) is the confidence level. In this case, \\((\\mathbf{X}^T\\mathbf{X})^{-1}\\) is diagonal, and has eigenvectors \\((1,0)^T\\) and \\((0,1)^T\\) and so the two parameters are orthogonal and the ellipse has its principal axes parallel to the co-ordinate axes.",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A review of linear models</span>"
    ]
  },
  {
    "objectID": "Review of linear models.html#footnotes",
    "href": "Review of linear models.html#footnotes",
    "title": "2  A review of linear models",
    "section": "",
    "text": "If you’ve forgotten the definition/properties of variance-covariance matrices, some notes are available here: https://oakleyj.github.io/MAS61004LM/randomVectors.html#covariance-matrix↩︎",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A review of linear models</span>"
    ]
  },
  {
    "objectID": "Optimality.html",
    "href": "Optimality.html",
    "title": "3  Optimality",
    "section": "",
    "text": "3.1 Optimality criteria\nIf there is not a simple one-dimensional function of the design which is to be maximised or minimised, there are various different criteria which are candidates for consideration. We consider four here, although there are others.",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Optimality</span>"
    ]
  },
  {
    "objectID": "Optimality.html#optimality-criteria",
    "href": "Optimality.html#optimality-criteria",
    "title": "3  Optimality",
    "section": "",
    "text": "3.1.1 D-optimality\nA design is called D-optimal if it maximises the determinant of \\(\\mathbf{X}^T\\mathbf{X}\\), denoted by \\(|\\mathbf{X}^T\\mathbf{X}|\\), or equivalently the determinant of \\(\\mathbf{M}\\), denoted by \\(|\\mathbf{M}|\\). Thus, the determinant is being used as a measure of the ’’size” of a matrix. This makes sense when the matrix is positive definite symmetric, as here, because the determinant is the product of the eigenvalues, which are necessarily all real and positive. Another way of looking at this criterion is that minimises the “measure of content” (area, volume, …) of the confidence ellipsoids for \\(\\boldsymbol{\\beta}\\) mentioned in Section 2.10, and this measure of content is a reflection of the accuracy of estimation.\n\n\n\n\n\n\nExample\n\n\n\n\nExample 3.3 In simple linear regression, we noted that \\[\\begin{align}\n    \\mathbf{X}^T\\mathbf{X}&=\\left( \\begin{array}{cc} n & \\sum x \\\\\n    \\sum x & \\sum x^2 \\end{array} \\right) \\\\\n    \\text{and so } |\\mathbf{X}^T\\mathbf{X}| &= n\\sum x^2-(\\sum x)^2 \\\\\n    &= ns_{xx}\n    \\end{align}\\] so that for given sample size it is D-optimal to choose \\(s_{xx}\\) as large as possible. If the range of feasible \\(x\\) is a finite interval \\([x_{min},         x_{max}]\\) say, then this amounts to taking half the observations at \\(x=x_{min}\\) and half at \\(x=x_{max}\\), or as nearly as possible so. Although such a design is D-optimal, it has the same disadvantage as that of Example 3.1 in that it yields no evidence that the model fits well in the middle of the range of values of \\(x\\).*\n\n\n\n\n\n3.1.2 G-optimality\nLet \\(\\Omega\\) denote the design region: the space of possible positions of \\(k\\) design points. A design \\(\\mathbf{x_1}^*, \\mathbf{x_2}^*, \\ldots                 ,\\mathbf{x_k}^ \\in \\Omega\\) is called G-optimal if it minimises the maximum standardised variance of prediction over the whole design region. Equivalently \\[\\mathbf{x_1}^*,\\mathbf{x_2}^*,\\ldots,\\mathbf{x_k}^*=\\mathop{\\mathrm{argmin}}_{\\mathbf{x_1},\\mathbf{x_2},\\ldots,\\mathbf{x_k}\\in\\Omega}\n    \\left\\{ \\max_{\\mathbf{x_r}\\in\\Omega} d(\\mathbf{x_r})\\right\\}\\] where \\(d(\\mathbf{x})\\) is the standardised variance of prediction at a point \\(\\mathbf{x}\\). So in principle for each potential design we evaluate the worst possible variance of prediction within the design region, and then choose the design for which this is least.\n\n\n\n\n\n\nExample\n\n\n\n\nExample 3.4 In the simple linear regression model, following on from Example 3.3, we get \\[d(x)=1+\\frac{n(x-\\overline{x})^2}{s_{xx}}\\] which is maximised when \\(x\\) is as far as possible from \\(\\overline{x}\\), namely whichever endpoint \\(x_{min}\\) or \\(x_{max}\\) is further away from \\(\\overline{x}\\). In particular, the maximised value of \\((x-\\overline{x})^2\\) will always be at least \\(\\frac{1}{4}(x_{max}-x_{min})^2\\). It can be seen that by taking half the observations at \\(x=x_{min}\\) and half at \\(x=x_{max}\\) we get \\(\\overline{x}=\\frac{1}{2}(x_{min}+x_{max})\\) and then we simultaneously minimise the maximum value of \\((x-\\overline{x})^2\\) and maximise the value of \\(s_{xx}\\), thereby minimising the maximum value of \\(d(x)\\) in the interval. Hence in this case the G-optimality criterion leads to the same design as the D-optimality criterion.\n\n\n\n\n\n3.1.3 V-optimality.\nA design is called V-optimal if it minimises a weighted average \\[\\int_{\\Omega } d(\\mathbf{x}) w(\\mathbf{x}) d\\mathbf{x}\\] of the standardised variance of prediction over the design region. Here \\(w\\) is some non-negative weighting function, which could be scaled to integrate to \\(1\\), although this is not necessary.\n\n\n\n\n\n\nExample\n\n\n\n\nExample 3.5 Simple linear regression with \\(x\\in [-1,1]\\) and uniform weighting: we require to minimise \\[\\begin{align}\n\\int_{-1}^1 \\left(1+\\frac{n(x-\\overline{x})^2}{s_{xx}}\\right) dx &=2+\\frac{n}{s_{xx}} \\left[\\frac{(x-\\overline{x})^3}{3}\\right]_{-1}^1 \\\\\n&= 2 + \\frac{n}{3s_{xx}} \\{ (1-\\overline{x})^3- (-1-\\overline{x})^3 \\} \\\\\n&= 2 + \\frac{n}{3s_{xx}} \\{ 2 + 6\\overline{x}^2\\}.\n\\end{align}\\] Once again, we see that if there are an even number of observations, taking half of the observations at each of the end-points we simultaneously minimise \\(\\overline{x}^2\\) (as zero) and maximise \\(s_{xx}\\), and therefore minimise the above expression. So, in this case, D-optimality, G-optimality and V-optimality all yield the same design.\n\n\n\n\n\n3.1.4 A-optimality\nA design is called A-optimal if it minimises the trace (sum of the diagonal elements) of \\((\\mathbf{x}^T\\mathbf{x})^{-1}\\), denoted by \\(\\text{tr}(\\mathbf{x}^T\\mathbf{x})^{-1}\\). Equivalently, it minimises the sum of the variances of the parameter estimators. This concept is only really appropriate when the parameters are all measured in the same dimensions, since otherwise the weighting given to any parameter will depend upon the units in which it is measured. For example, in the simple linear regression model, \\(\\beta_0\\) has the dimensions of \\(y\\), the response variable, whereas \\(\\beta_1\\) has the dimensions of \\(yx^{-1}\\), where \\(x\\) is the explanatory variable, and so there is no good reason to add the variances of \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\) together.\n\n\n\n\n\n\nQuestion is 1 designed to help you practise the sort of algebra you need on this module. Questions 2 and 3 are good exam revision exercises. Questions 4 and 5 are a little more challenging!\n\n\n\n\nExercise 3.1  \n\nCheck the details of the calculations in Example 2.6. (Start with the left hand side of each expression, and check you get the same result for the right hand side.)\nTo estimate in the quadratic regression model \\[ E(Y) = \\beta_0 + \\beta_1x + \\beta_{11}x^2 \\] for \\(x\\in [-1,1]\\) it is proposed to take observations at the three points \\(x=1\\), \\(x=-1\\) and \\(x=a\\) for some \\(a\\in [-1,1]\\). Show that the D-optimal choice of \\(a\\) is \\(a=0\\). Note that if \\(\\mathbf{X}\\) is a square matrix, then \\(|\\mathbf{X}^T\\mathbf{X}|=|\\mathbf{X}^T||\\mathbf{X}|=|\\mathbf{X}|^2\\). Show also that for this design with \\(a=0\\), \\[\\max _{x\\in [-1,1]} d(x) = 3\\] and the maximum occurs at each of the three points of the design \\(x=\\pm 1\\) and \\(x=0\\). [The significance of this result will become clear later.]\nWhat is the V-optimal design in Example 3.5 if the weighting function is \\(w(x)=x^2\\) with an even number of observations? Hint: follow the opimisation approach in the example: don’t try to differentiate anything!\nStill using V-optimality, give a weighting function in Example 3.5 that does not lead to the same design as in Example 3.5 with 2 observations.\nSuppose we choose the uniform weight function in Example 3.5 and there are an odd number \\(n\\) of observations. Consider two designs\n\nDesign A: \\(\\left\\{ \\begin{array}{ll}\\frac{n-1}{2} \\text{ observations at } &  x=-1 \\\\\n\\frac{n+1}{2} \\text{ observations at } &  x=1 \\end{array}\\right.\\)\nDesign B: \\(\\left\\{ \\begin{array}{ll}\\frac{n-1}{2} \\text{ observations at } &  x=-1 \\\\\n\\frac{n-1}{2} \\text{ observations at } &  x=1 \\\\\n1 \\quad \\text{ observations at } &  x=0 \\end{array}\\right.\\)\n\nIf Designs A and B are the only available designs, which is \\(V\\)-optimal?",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Optimality</span>"
    ]
  },
  {
    "objectID": "Qualitative-single factor.html",
    "href": "Qualitative-single factor.html",
    "title": "4  Designs for qualitative explanatory variables: one factor",
    "section": "",
    "text": "4.1 Randomisation\nAn important aspect of the design of experiments, in cases where the aim is to investigate the effect of treatments on subjects say, is to choose the combinations of treatments which we wish to apply, and then allocate them to subjects in a completely random way. This helps to avoid bias, possibly subconscious, in the allocation, but there is also a more technical justification for randomisation.",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Designs for qualitative explanatory variables: one factor</span>"
    ]
  },
  {
    "objectID": "Qualitative-single factor.html#completely-randomised-designs-crd",
    "href": "Qualitative-single factor.html#completely-randomised-designs-crd",
    "title": "4  Designs for qualitative explanatory variables: one factor",
    "section": "4.2 Completely Randomised Designs (CRD)",
    "text": "4.2 Completely Randomised Designs (CRD)\nA completely randomised design (CRD) is one where every possible allocation of subjects to treatments is equally likely to be chosen. The model may be written \\[Y_{ij} = \\mu _i + \\epsilon _{ij}~~~~(i=1,2,\\ldots ,m; j=1,2,\\ldots n_i)\\] where \\(n_i\\) is the size of the \\(i\\)-th group, \\(Y_{ij}\\) is the \\(j\\)-th observation in the \\(i\\)-th group, \\(\\mu _i\\) is the unknown mean for the \\(i\\)-th treatment, and \\(\\epsilon _{ij}\\) is random error.\nUnder the usual assumptions of the general linear model, the analysis is easy, because the parameters are orthogonal and we get \\[\\begin{align}\n\\hat{\\mu}_i=\\overline{Y}_{i.}\\\\\n\\text{var}(\\hat{\\mu})_i=\\frac{\\sigma ^2}{n_i}\n\\end{align}\\] where \\(\\overline{Y}_{i.}\\) is the mean of the \\(i\\)-th group. The residual sum of squares is \\[S^2 = \\sum _{i=1}^m \\sum _{j=1}^{n_i}(Y_{ij}-\\overline{Y}_{i.})^2\\] with \\(n-m\\) degrees of freedom, where \\(n\\) is the total number of observations.\nFrom the design point of view, the main question here is how best to choose the sizes of the groups. Obviously, as ever, the larger the experiment, the more accurate the estimators will be, and so it is of more interest to look at the problem of choice of \\(n_1, n_2, \\ldots ,n_m\\) subject to a fixed total number of observations \\(n\\) say.\nIf, for example, we adopt the \\(D\\)-optimality criterion, then, noting that in this case \\(\\mathbf{X}^T\\mathbf{X}\\) is the diagonal matrix with \\(n_1, n_2, \\ldots , n_m\\) down the diagonal, its determinant is \\(n_1n_2\\ldots n_m\\) and this is maximised subject to \\(n_1+n_2+\\ldots\n+n_m=n\\) by taking \\(n_1, n_2, \\ldots , n_m\\) as nearly as possible equal to each other.\nIf there is a placebo, an alternative might be to aim to minimise the variance of the estimated treatment difference between each of the genuine treatments and the placebo. Suppose that each of the genuine treatments is administered to \\(t\\) patients, and the placebo to the remaining \\(n-(m-1)t\\) patients; then this variance may be written \\[v(t) = \\sigma ^2\\left( \\frac{1}{t} + \\frac{1}{n-(m-1)t} \\right).\\] We minimise this by treating \\(t\\) as a continuous variable and using calculus. Leaving out the constant multiplicative factor \\(\\sigma\n^2\\), \\[\\begin{align}\n\\frac{d}{dt} v(t)&=-\\frac{1}{t^2} + \\frac{m-1}{(n-(m-1)t)^2}.\\\\\n\\frac{d^2}{dt^2}v(t)&=\\frac{2}{t^3}+\\frac{2(m-1)^2}{(n-(m-1)t)^3}&gt;0.\n\\end{align}\\] Hence a minimum occurs when \\((m-1)t^2 = (n-(m-1)t)^2\\), and this is an equation which may be solved for \\(t\\). The solution will not necessarily be an integer, and so in practice it may have to be rounded up or down.\n\n\n\n\n\n\nExample\n\n\n\n\nExample 4.1 Let \\(n=24\\) and \\(m=3\\). In this case the equation is\n\\[2t^2 = (24-2t)^2\\] leading to \\[t = \\frac{24}{2+\\sqrt{2}} \\simeq 7.03\\] In practice we would take \\(t=7\\), and administer each of the two genuine treatments to seven patients, and the placebo to the remaining ten.",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Designs for qualitative explanatory variables: one factor</span>"
    ]
  },
  {
    "objectID": "Qualitative-single factor.html#blocks",
    "href": "Qualitative-single factor.html#blocks",
    "title": "4  Designs for qualitative explanatory variables: one factor",
    "section": "4.3 Blocks",
    "text": "4.3 Blocks\nSometimes we can increase the accuracy of an experiment by making use of knowledge of uncontrollable sources of variation by incorporating them in the model as blocking variables. A typical example here is an agricultural experiment, where a number of plants are to be grown, perhaps in order to estimate the effects of several different fertilisers, but the plot on which the plants are to be grown is known to be inhomogeneous with respect to its natural fertility. If a model were fitted which ignored this inhomogeneity, estimates would not be very accurate because the residuals would have to carry the associated variability, and some of them might be expected to be large in magnitude. Also, the assumption of independent observations might be questionable, since those from plants grown close together might be expected to be positively correlated.\nA way of dealing with this is to divide the plot into relatively homogeneous sub-plots known as blocks and to incorporate into the model block effects, which are additive parameters, one for each block, and which can be dealt with in the formulation of the model using dummy variables, similarly to the catalyst effects discussed in Section @ref(qualvars). The chief difference here is that the block effects may not be of interest in their own right, but are introduced merely to produce a better-fitting model, and increase the accuracy of estimation of parameters which are of interest.\n\n\n\n\n\n\nExample\n\n\n\n\nExample 4.2 If there are just two blocks, introducing two block parameters \\(B_1\\) and \\(B_2\\) would over-parametrise the model, and so imposing a constraint, say \\(B_1+B_2=0\\), or equivalently \\(B_2=-B_1\\), enables us to eliminate \\(B_2\\), giving a single extra parameter \\(B_1\\). Correspondingly, the design matrix will have an extra column consisting of a \\(1\\) for every observation from the first block and a \\(-1\\) for every observation from the second block.\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\nExample 4.3 In an experiment with two blocks, suppose that each observation in one block is twinned with an observation with the same treatment combination in the other block. If the block effect is represented by a parameter \\(B_1\\) which is included in the model as an additive effect \\(+B_1\\) in the first block and \\(-B_1\\) in the second block, explain why this parameter is orthogonal to all of the other parameters in the model.",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Designs for qualitative explanatory variables: one factor</span>"
    ]
  },
  {
    "objectID": "Qualitative-single factor.html#RBD",
    "href": "Qualitative-single factor.html#RBD",
    "title": "4  Designs for qualitative explanatory variables: one factor",
    "section": "4.4 Randomised Block Designs (RBD)",
    "text": "4.4 Randomised Block Designs (RBD)\nIn an experiment to compare the effects of different drugs, for example, we might wish to use blocks which consist of patients of the same sex and fairly homogeneous with respect to age, so that variation due to these factors can be separated from variation due to the effects of the drugs. If all of the blocks contain the same number of units, and each treatment is used the same number of times within each block, with randomisation taking place independently within each block, we have a randomised block design (RBD).\nThe simplest case is where each treatment is administered once within each block. If \\(b\\) denotes the number of blocks and \\(t\\) denotes the number of treatments, the model which we might consider is the additive one \\[Y_{ij} = \\mu + \\alpha _i + \\tau _j + \\epsilon_{ij}~~~~(i=1,2,\\ldots ,b;~~j=1,2,\\ldots ,t)\\] where \\(Y_{ij}\\) is the observation in block \\(i\\) for treatment \\(j\\), \\(\\mu\\) is the overall mean, \\(\\alpha _i\\) is the effect of block \\(i\\), and \\(\\tau _j\\) is the effect of treatment \\(j\\). This model is over-parametrised as it stands; to overcome this, and to give \\(\\mu\\) a genuine meaning, we need to impose the constraints \\[\\sum _{i=1}^b \\alpha _i = \\sum _{j=1}^t \\tau _j = 0\\] so that, in applying the machinery of the general linear model, we need to express the parameters \\(\\alpha _b\\) and \\(\\tau _t\\) in terms of the others and adjust the design matrix accordingly.\n\n\n\n\n\n\nExample\n\n\n\n\nExample 4.4 Let \\(b=2\\) and \\(t=3\\). Then the model may be expressed as \\[\nE\\left( \\begin{array}{c} Y_{11} \\\\ Y_{12} \\\\ Y_{13} \\\\ Y_{21} \\\\\nY_{22} \\\\ Y_{23} \\end{array} \\right) = \\left( \\begin{array}{cccc} 1 & 1 & 1 & 0 \\\\ 1 & 1 & 0 & 1 \\\\ 1 & 1 & -1 & -1 \\\\ 1 & -1 & 1 & 0 \\\\\n1 & -1 & 0 & 1 \\\\ 1 & -1 & -1 & -1 \\end{array} \\right) \\left(\\begin{array}{c} \\mu \\\\ \\alpha _1 \\\\ \\tau _1 \\\\ \\tau _2 \\end{array}\\right)\n\\] and in this case \\[\\mathbf{X}^T\\mathbf{X}=\\left(\\begin{array}{cccc} 6 & 0 & 0 & 0 \\\\ 0 & 6 & 0 & 0 \\\\ 0 & 0 & 4 & 2 \\\\ 0 & 0 & 2 & 4 \\end{array} \\right)\n\\text{ and }(\\mathbf{X}^T\\mathbf{X})^{-1}=\\left(\\begin{array}{rrrr}\\frac{1}{6} & 0 & 0 & 0 \\\\ 0 & \\frac{1}{6} & 0 & 0 \\\\ 0 & 0 & \\frac{1}{3} &-\\frac{1}{6} \\\\ 0 & 0 & -\\frac{1}{6} & \\frac{1}{3} \\end{array} \\right).\\] Note that the three groups of parameters \\(\\{ \\mu \\} , \\{ \\alpha _i\\}\\) and \\(\\{ \\tau _j\\}\\) are mutually orthogonal.\n\n\n\nThe symmetry of the RBD makes it relatively easy to analyse. If \\(\\overline{Y}_{..}\\) denotes the overall mean of the observations, \\(\\overline{Y}_{i.}\\) denotes the mean of block \\(i\\) and \\(\\overline{Y}_{.j}\\) denotes the mean of treatment \\(j\\), then we have \\[\\begin{align}\n\\hat{\\mu}&=\\overline{Y}_{..} \\\\\n\\text{var}(\\hat{\\mu})&=\\frac{\\sigma ^2}{bt}\\\\\n\\hat{\\alpha}_i&=\\overline{Y}_{i.} - \\overline{Y}_{..}\\\\\n\\text{var}(\\hat{\\alpha}_i)&=\\frac{\\sigma ^2}{t}\\left( 1 - \\frac{1}{b} \\right)\\\\\n\\text{cov}(\\hat{\\alpha}_i, \\hat{\\alpha }_{i^{\\prime }})&=-\\frac{\\sigma ^2}{bt}\\text{ for }i\\neq i^{\\prime}\\\\\n\\hat{\\tau }_j&= \\overline{Y}_{.j} - \\overline{Y}_{..}\\\\\n\\text{var}(\\hat{\\tau }_j)=&\\frac{\\sigma^2}{b}\\left( 1 - \\frac{1}{t} \\right)\\\\\n\\text{cov}(\\hat{\\tau}_j, \\hat{\\tau }_{j^{\\prime }})&=-\\frac{\\sigma^2}{bt}\\text{ for }j\\neq j^{\\prime}\n\\end{align}\\] and the three groups of parameters are mutually orthogonal and therefore independent. The residual sum of squares is \\[\\sum _{i=1}^b \\sum _{j=1}^t (Y_{ij} - \\overline{Y}_{i.} -\\overline{Y}_{.j} + \\overline{Y}_{..})^2\\] and the number of degrees of freedom for error is \\[bt - 1 - (b-1) - (t-1) = (b-1)(t-1) .\\]\nSuppose now we are able to take \\(r&gt;1\\) observations per ’’cell” (block/treatment combination). If we stick to the additive model above, the analysis is similar except that the observations take the form \\(Y_{ijk}\\) (block \\(i\\), treatment \\(j\\), replicate \\(k\\)), the estimators have some added suffices, their variances and covariances are divided by a factor \\(r\\), the residual sum of squares becomes \\[\\sum _{i=1}^b \\sum _{j=1}^t \\sum _{k=1}^r (Y_{ijk} - \\overline{Y}_{i..} - \\overline{Y}_{.j.} + \\overline{Y}_{...})^2\\] and the number of degrees of freedom is increased by \\(bt(r-1)\\).\nHowever, with replication it is possible to investigate interactions between blocks and treatments, by introducing extra parameters to the model. Roughly speaking, the interaction between block \\(i\\) and treatment \\(j\\), denoted by \\((\\alpha \\tau )_{ij}\\), measures the extent to which the observations in this cell depart from what might be expected on the additive model. We may write the model with interactions as \\[Y_{ijk} = \\mu + \\alpha _i + \\tau _j + (\\alpha \\tau )_{ij} + \\epsilon _{ijk} .\\] Once again, this model is over-parametrised as it stands, and we need to impose the constraints \\[\\begin{align}\n\\sum _{i=1}^b (\\alpha \\tau )_{ij} &= 0\\text{ for each }j \\\\\n\\text{and } \\sum_{j=1}^t (\\alpha \\tau)_{ij} &= 0\\text{ for each }i\n\\end{align}\\] so that, in applying the machinery of the general linear model, any interaction \\((\\alpha \\tau )_{ij}\\) with either \\(i=b\\) or \\(j=t\\) must be expressed in terms of the others.\n\n\n\n\n\n\nExample\n\n\n\n\nExample 4.5 Let \\(b=2, t=3\\) and \\(r=2\\). Then the model may be expressed as \\[E\\left( \\begin{array}{c} Y_{111} \\\\ Y_{112} \\\\ Y_{121} \\\\ Y_{122}  \\\\ Y_{131} \\\\ Y_{132} \\\\ Y_{211} \\\\ Y_{212} \\\\ Y_{221} \\\\ Y_{222} \\\\\nY _{231} \\\\ Y_{232} \\end{array} \\right) = \\left(\n\\begin{array}{cccccc}\n1 & 1 & 1 & 0 & 1 & 0 \\\\\n1 & 1 & 1 & 0 & 1 & 0 \\\\\n1 & 1 & 0 & 1 & 0 & 1 \\\\\n1 & 1 & 0 & 1 & 0 & 1 \\\\\n1 & 1 & -1 & -1 & -1 & -1 \\\\\n1 & 1 & -1 & -1 & -1 & -1 \\\\\n1 & -1 & 1 & 0 & -1 & 0 \\\\\n1 & -1 & 1 & 0 & -1 & 0 \\\\\n1 & -1 & 0 & 1 & 0 & -1 \\\\\n1 & -1 & 0 & 1 & 0 & -1 \\\\\n1 & -1 & -1 & -1 & 1 & 1 \\\\\n1 & -1 & -1 & -1 & 1 & 1\\end{array} \\right)\n\\left(\\begin{array}{c} \\mu \\\\ \\alpha_1 \\\\ \\tau_1 \\\\ \\tau_2 \\\\ (\\alpha\\tau)_{11} \\\\ (\\alpha\\tau)_{12} \\end{array} \\right).\\] For example, the two last entries in the bottom row of the design matrix follow from the fact that, because of the constraints, \\[(\\alpha \\tau )_{23} = -(\\alpha \\tau )_{13} = (\\alpha \\tau )_{11} + (\\alpha \\tau )_{12} .\\]\n\n\n\nIn general, the inclusion of interactions in the model adds \\((b-1)(t-1)\\) extra parameters and so the number of degrees of freedom for error is \\[btr - 1 - (b-1) - (t-1) - (b-1)(t-1) = bt(r-1).\\] The interactions are orthogonal to the other parameters and so the estimators of the latter remain as they were before; the interactions themselves are estimated by \\[\\widehat{(\\alpha\\tau)}_{ij} = \\overline{Y}_{ij.} -\\overline{Y}_{i..} - \\overline{Y}_{.j.} + \\overline{Y}_{...}\\] and the residual sum of squares is \\[\\sum _{i=1}^b \\sum _{j=1}^t \\sum _{k=1}^r (Y_{ijk}-\\overline{Y}_{ij.})^2\\]\n\n\n\n\n\n\nQuestion 1 is a good exam revision exercise (you will have an exam formula sheet: you won’t need to memorise specific formulae). Question 2 is more challenging!\n\n\n\n\nExercise 4.1  \n\nDerive the variance of an estimated treatment difference:\n\n\nin a CRD with \\(t\\) treatments and each group of size \\(b\\);\nin an RBD (one observation per cell) with the usual notation.\n\nIn the first case, such a treatment difference would be denoted by \\(\\mu_i-\\mu_{i^{\\prime}}\\) say, whereas in the second it would be denoted by \\(\\tau_j-\\tau_{j^{\\prime}}\\). If a RBD is used where the block effects are negligible, has any advantage been gained?\n\nShow that, for given positive integers \\(n&gt;m\\), the product \\(n_1n_2\\ldots n_m\\) of positive integers subject to the constraint \\(n_1+n_2+\\ldots +n_m=n\\) is maximised by taking \\(n_1, n_2, \\ldots,n_m\\) as nearly as possible equal to each other, using the following approach. If two of the factors differ by more than two, say \\(n_1\\geq n_2+2\\), then the product could be increased by replacing \\(n_1\\) and \\(n_2\\) by \\(n_1-1\\) and \\(n_2+1\\); hence such a choice cannot be optimal. Using a similar approach, show that a D-optimal CRD is also A-optimal in that it minimises the sum \\[ \\frac{1}{n_1} + \\frac{1}{n_2} + \\ldots + \\frac{1}{n_m} \\] subject to the same constraint.",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Designs for qualitative explanatory variables: one factor</span>"
    ]
  },
  {
    "objectID": "Qualitative-multiple factors.html",
    "href": "Qualitative-multiple factors.html",
    "title": "5  Designs for qualitative explanatory variables: multiple factors",
    "section": "",
    "text": "5.1 Latin square designs\nThe concept of a Latin square design arises in the following context. There are three different qualitative explanatory variables (factors), each operating at the same number of levels, which we shall denote by \\(k\\). For whatever reason, it is not possible to conduct an experiment in which all possible combinations of the factor levels appear, which would involve at least \\(k^3\\) observations. However, it is possible to arrange \\(k^2\\) units in a \\(k\\times k\\) array, where rows and columns represent levels of two of the factors, which may be blocking factors. If then the third factor is assigned in such a way that each level of this factor appears exactly once in each row and each column, we have a Latin square design.\nThe model to be fitted to a Latin square design may be written \\[Y_{ij} = \\mu + \\alpha _i + \\beta_j + \\tau _{\\kappa (i,j)} + \\epsilon _{ij}\\] where \\(Y_{ij}\\) is the observation in row \\(i\\) and column \\(j\\), the \\(\\{ \\alpha _i\\}\\) and \\(\\{ \\beta_j\\}\\) are the parameters for row and column effects respectively, subject to the usual constraints, and the \\(\\{ \\tau _{\\kappa }\\}\\) are parameters for treatment effects, subject to a similar constraint (\\(\\sum _{\\kappa }\\tau _{\\kappa }=0\\)). \\(\\kappa (i,j)\\) means the treatment which is applied in row \\(i\\) and column \\(j\\), which obviously depends upon the particular Latin square design chosen.\nOne advantage of Latin square designs is ease of analysis. For example, the treatment parameters are orthogonal to all of the other parameters, and their estimators are given by \\[\\hat{\\tau }_{\\kappa } = \\overline{Y}_{(\\kappa )} -\\overline{Y}_{..}\\] where \\(\\overline{Y}_{(\\kappa )}\\) denotes the mean of all observations corresponding to treatment \\(\\kappa\\).\nThe total number of parameters in the model, bearing in mind the constraints, is \\(1+3(k-1)=3k-2\\) and so the number of degrees of freedom for error is \\(k^2-(3k-2)=(k-1)(k-2)\\). The residual sum of squares may be written \\[\\sum _{i=1}^k \\sum _{j=1}^k (Y_{ij}-\\overline{Y}_{i.}-\\overline{Y}_{.j}-\\overline{Y}_{(\\kappa(i,j))}+2\\overline{Y}_{..})^2.\\] In a Latin square design, the analysis of variance, into whose details we shall not go, is relatively simple because of all the orthogonality: each of the three factors has \\(k-1\\) degrees of freedom and their sums of squares are mutually independent, each taking a similar form, for example for treatments \\[k\\sum _{\\kappa } (\\overline{Y}_{(\\kappa )}-\\overline{Y}_{..})^2.\\]",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Designs for qualitative explanatory variables: multiple factors</span>"
    ]
  },
  {
    "objectID": "Qualitative-multiple factors.html#latin-square-designs",
    "href": "Qualitative-multiple factors.html#latin-square-designs",
    "title": "5  Designs for qualitative explanatory variables: multiple factors",
    "section": "",
    "text": "Example\n\n\n\n\nExample 5.1 A wear-testing machine consists of four brass plates, on each of which is fastened an abrading surface. Four weighted bushes, into which test samples of material are fixed, rest on the abrading surfaces and are moved mechanically. The loss in weight over a given number of cycles is used as a measure of the resistance to wear. It is suspected that there are variations in the results for different positions on the machine and for different runs even under apparently identical conditions. Four materials (A, B, C and D) are to be compared in a Latin square arrangement so that differences between positions and runs may be eliminated from the comparisons of material wear. A possible design is the following.\n\n\n\n\nPosition\nin\nmachine\n\n\n\nRun\n1\n2\n3\n4\n\n\n1\nA\nB\nD\nC\n\n\n2\nD\nC\nA\nB\n\n\n3\nC\nD\nB\nA\n\n\n4\nB\nA\nC\nD\n\n\n\nwhere the choice of material is indicated by the appropriate letter.\n\n\n\n\n\n\n\n\n\n\n\n\nQ1 is to help you relate an experimental design to a design matrix: it’s important you can do this! Q2 tests your understanding of a key definition (orthogonality) in this module. Q3 will help you to check your understanding of the notation. All good exam revision!\n\n\n\n\nExercise 5.1  \n\nWrite down the \\(16\\) by \\(10\\) design matrix in Example 5.1.\nHence show that \\(\\tau_A\\) is orthogonal to all the non-treatment parameters but not the other treatment parameters.\nShow that \\(\\text{var}(\\hat{\\tau}_A)= \\frac{3 \\sigma^2}{16}\\) where \\(\\text{var}\\left(Y_{ij}\\right)=\\sigma^2\\).\n\n\n\n\n\n5.1.1 Orthogonal Latin squares\nIf we add to an experiment with Latin square design a further treatment factor, also with \\(k\\) levels, by superimposing a new Latin square on the existing one, such as the one represented by the Greek letters in the following diagram:\n\n\n\n\nA\\(\\alpha\\)\nB\\(\\beta\\)\nD\\(\\gamma\\)\nC\\(\\delta\\)\n\n\nD\\(\\beta\\)\nC\\(\\alpha\\)\nA\\(\\delta\\)\nB\\(\\gamma\\)\n\n\nC\\(\\gamma\\)\nD\\(\\delta\\)\nB\\(\\alpha\\)\nA\\(\\beta\\)\n\n\nB\\(\\delta\\)\nA\\(\\gamma\\)\nC\\(\\beta\\)\nD\\(\\alpha\\)\n\n\n\n\nthen it may happen, as it does above, that each combination of levels of the two treatment factors appears once in the diagram. In this case, we have a Graeco-Latin square, and the two Latin square designs, represented by the Latin letters and the Greek letters, are called orthogonal Latin squares. Not surprisingly, this nomenclature reflects the fact that in such a design the parameters for the new factor are orthogonal to all the others.\n\n\n\n\n\n\nThese are both suitable exam revision questions, and will help you to revise the notation, link between experimental design and design matrix, and definition of orthogonality.\n\n\n\n\nExercise 5.2  \n\nWhat dimensions would the design matrix have in the orthogonal Latin square above?\nLet \\(\\tau_A\\) be the effect of treatment \\(A\\) in the first treatment factor and \\(\\tau_{\\alpha}\\) be the effect of treatment \\(\\alpha\\) in the second treatment factor. Show that \\(\\tau_A\\) is orthogonal to \\(\\tau_{\\alpha}\\) in the orthogonal Latin square above.\n\n\n\n\nThe question whether orthogonal Latin squares exist, and, if so, how many, for a given value of \\(k\\), is of considerable combinatorial interest in its own right, and gets input from the algebraic theory of finite fields. A finite field is a finite set including \\(0\\) and \\(1\\) and in which addition, multiplication, subtraction and division by non-zero elements are possible and satisfy the usual rules. The simplest example of a finite field is ’’integers modulo \\(p\\)“, where \\(p\\) is a prime number. This is a field with \\(p\\) elements, \\(\\{ 0,1,\\ldots ,p-1\\}\\) say. However, the theory states that a finite field with \\(k\\) elements exists if and only if \\(k\\) is of the form \\(p^r\\), where \\(p\\) is prime and \\(r\\) is any positive integer. This is relevant to the following theorem.\n\n\n\n\n\n\nTheorem\n\n\n\n\nTheorem 5.1 If \\(k\\) is of the form \\(p^r\\), where \\(p\\) is a prime number and \\(r\\) is a positive integer, then there exists a complete set of \\(k-1\\) mutually orthogonal \\(k\\times k\\) Latin squares.\n\n\n\nBy a complete set we mean that no further additions to the set are possible. We shall not prove this in general, but illustrate it by the construction of such a complete set in the case \\(k=5\\). Working in arithmetic modulo \\(5\\), suppose we label the rows and columns of any square, by the elements of the finite field: in this case \\(0,1,2,3,4\\). In the first square, in position \\((i,j)\\) we place the value of \\(i+j\\); in the second square, the value of \\(i+2j\\); in the third square, the value of \\(i+3j\\); and in the fourth square, the value of \\(i+4j\\). (In general, if we denote the elements of the field by \\(0,1,a,b,c\\) (as all finite fields must include 0 and 1), we actually want the values of \\(i+aj\\), \\(i+bj\\) and \\(i+cj\\) in the second, third and fourth squares). Then we get the following.\n\\(i+j\\) \\[\\begin{array}\n{|ccccc|} \\hline  0 & 1 & 2 & 3 & 4 \\\\ 1 & 2 &  3 & 4 & 0 \\\\ 2 & 3 & 4 & 0 & 1 \\\\ 3 & 4 & 0 & 1 & 2 \\\\ 4 & 0 & 1 & 2 & 3 \\\\ \\hline\n\\end{array}\\]\n\\(i+2j\\) \\[\\begin{array}{|ccccc|} \\hline 0 & 2 & 4 & 1 & 3 \\\\ 1 & 3 & 0 & 2 & 4 \\\\ 2 & 4 & 1 & 3 & 0 \\\\ 3 & 0 & 2 & 4 & 1 \\\\ 4 & 1 & 3 & 0 & 2 \\\\ \\hline \\end{array}\\]\n\\(i+3j\\) \\[\\begin{array}{|ccccc|} \\hline 0 & 3 & 1 & 4 & 2 \\\\ 1 & 4 & 2 & 0 & 3 \\\\ 2 & 0 & 3 & 1 & 4 \\\\ 3 & 1 & 4 & 2 & 0 \\\\ 4 & 2 & 0 & 3 & 1 \\\\ \\hline\n\\end{array}\\]\n\\(i+4j\\) \\[\\begin{array}{|ccccc|} \\hline 0 & 4 & 3 & 2 & 1 \\\\ 1 & 0 & 4 & 3 & 2 \\\\ 2 & 1 & 0 & 4 & 3 \\\\ 3 & 2 & 1 & 0 & 4 \\\\ 4 & 3 & 2 & 1 & 0 \\\\ \\hline \\end{array}\\] Clearly any two of these squares are orthogonal. This construction works because integers modulo \\(5\\) constitute a finite field.\nThe smallest value of \\(k\\) which is not of the form \\(p^r\\) is \\(k=6\\). It has been shown that there do not even exist two \\(6\\times 6\\) Latin squares which are orthogonal to each other. However, for \\(k=10\\) there do exist a complete set of orthogonal Latin squares, and so the condition that \\(k\\) takes the form \\(p^r\\) is not a necessary condition.",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Designs for qualitative explanatory variables: multiple factors</span>"
    ]
  },
  {
    "objectID": "Qualitative-multiple factors.html#BIBD",
    "href": "Qualitative-multiple factors.html#BIBD",
    "title": "5  Designs for qualitative explanatory variables: multiple factors",
    "section": "5.2 Balanced incomplete block designs (BIBD)",
    "text": "5.2 Balanced incomplete block designs (BIBD)\nIn some experiments there are too many treatments of interest to be accommodated in each block. So each block can only contain some of the treatments and not the rest. The most symmetrical and efficient way of dealing with such a situation is to ensure the following.\n\nAll blocks are of the same size.\nNo treatment appears more than once within a block.\nEach treatment appears the same number of times.\nEach pair of treatments appears in the same number of blocks.\n\nThen we have a balanced incomplete block design (BIBD).\n\n\n\n\n\n\nExample\n\n\n\n\nExample 5.2 Four different rubber compounds A, B, C and D are to be tested for their effects on the life of a car tyre. Large variations usually exist between one car and another, and so one car is to be used in the experiment. The position of the wheel on the car also affects the amount of wear. Therefore composite tyres are used consisting of three-part treads enabling three compounds to be tested in the same position. The following design permits the desired comparison.\n\n\n\nTyre 1\nA, B, C\n\n\nTyre 2\nA, B, D\n\n\nTyre 3\nA, C, D\n\n\nTyre 4\nB, C, D\n\n\n\nThis is a balanced incomplete block design (BIBD).\n\n\n\nThe following is standard notation in discussing BIBDs. \\[\\begin{align}\nt &= \\text{number of treatments} \\\\\nk &= \\text{number of units in a block} \\\\\nb &= \\text{number of blocks} \\\\\nr &= \\text{number of applications of each treatment} \\\\\n\\lambda &= \\text{number of times each pair of treatments appears together in a block}\n\\end{align}\\] Thus, in example @ref(exm:BIBDex), \\(t=4, k=3, b=4, r=3\\) and \\(\\lambda=2\\).\nThere are certain relationships between these quantities. Apart from the obvious \\(t&gt;k\\), which necessitates using incomplete blocks in the first place, we have the following equations. \\[bk = rt = \\text{total number of observations}\\] \\[r(k-1) = \\lambda (t-1) = \\text{total number of block neighbours of a given treatment}\\] These two equations mean that, for given values of \\(t\\) and \\(k\\), the ratios \\(b:r:\\lambda\\) are determined. The question remains whether, for given \\(b, r\\) and \\(\\lambda\\) in the correct ratios, a BIBD exists with these properties.\n\n\n\n\n\n\nExample\n\n\n\n\nExample 5.3 Find the smallest possible BIBD for \\(t=8\\) and \\(k=4\\).\nIn this case the equations are \\(4b = 8r\\) and \\(3r = 7\\lambda\\), and so \\(b:r:\\lambda = 14:7:3\\). These numbers have no common factor, and so if we can find a BIBD with \\(b=14, r=7\\) and \\(\\lambda =3\\), it must be the smallest possible. By trial and error, one possibility is the following, where columns represent blocks.\n\n\n\nA\nA\nA\nA\nA\nA\nA\nB\nB\nB\nB\nC\nC\nE\n\n\nB\nB\nB\nC\nC\nD\nD\nC\nC\nD\nD\nD\nD\nF\n\n\nC\nE\nG\nE\nF\nE\nF\nE\nF\nE\nF\nE\nG\nG\n\n\nD\nF\nH\nG\nH\nH\nG\nH\nG\nG\nH\nF\nH\nH\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 1 is a good revision exercise, fairly typical of exam-style questions. Questions 2 and 3 are more challenging.\n\n\n\n\nExercise 5.3  \n\nFind a BIBD which is as small as possible in the case \\(t=7\\) and \\(k=3\\).\nIf an attempt were made to construct orthogonal \\(6\\times 6\\) Latin squares using a similar method to that used for \\(5\\times 5\\) ones, at what point, and in what way, would the procedure go wrong?\nIn the scenario of Example 5.2, if more than one car could be used, suggest an appropriate design.\n\n\n\n\n\n5.2.1 Construction of BIBD’s\nWe have looked at the definition and some properties of balanced incomplete block designs, but the construction of examples has been somewhat ad hoc. Here we review some more systematic methods of constructing them.\n\nUnreduced design\n\nFor any given \\(t\\) and \\(k\\), one design which always exists is found by taking each possible combination of \\(k\\) treatments out of \\(t\\) in a different block. This is known as the unreduced design, and clearly satisfies the conditions of a BIBD. In this case, \\[b = {t \\choose k} = \\frac{t!}{k!(t-k)!}\\] and so, either by direct consideration or using the equations mentioned in Section @ref(BIBD), \\[r = {t-1 \\choose k-1}\\text{ and }\\lambda = {t-2 \\choose k-2}.\\] Unreduced designs tend to be rather large: indeed, as large as possible for given \\(t\\) and \\(k\\), unless complete replication takes place. For example, if \\(t=8\\) and \\(k=4\\) then the number of blocks is \\(70\\) and the number of observations is \\(280\\).\nExample 5.3 is a case of an unreduced design in which \\(t=4\\) and \\(k=3\\).\n\nDesigns based on Latin squares\n\nThe easiest way of creating a BIBD from a Latin square is simply to delete, say, the final row, and to treat the columns that remain as blocks. If the Latin square was \\(t\\times t\\), this gives an unreduced design in which \\(k=t-1\\). Again, example @ref(exm:BIBDex) is of this kind.\nA rather more subtle use of Latin squares is when we have a complete set of \\(k-1\\) mutually orthogonal \\(k\\times k\\) Latin squares, all superimposed. If the number of treatments is \\(t=k^2\\), we label the cells, each with a different treatment label. We then create blocks of size \\(k\\), as follows. The first \\(k\\) blocks contain the treatments in each of the rows of the square; the next \\(k\\) blocks contain the treatments in each of the columns of the square; the next \\(k\\) blocks contain the treatments corresponding to each of the Latin labels; the next \\(k\\) blocks contain the treatments corresponding the the Greek labels; and so on. We end up with a BIBD in which \\[\\begin{align}\n    t & =  k^2 \\\\\n    b & =  k(k+1) \\\\\n    r & =  k+1 \\\\\n    \\lambda & = 1.\n    \\end{align}\\]\n\n\n\n\n\n\nExample\n\n\n\n\nExample 5.4 Consider \\(t=9, k=3\\).\n\n\n\n1A\\(\\alpha\\)\n2B\\(\\beta\\)\n3C\\(\\gamma\\)\n\n\n4B\\(\\gamma\\)\n5C\\(\\alpha\\)\n6A\\(\\beta\\)\n\n\n7C\\(\\beta\\)\n8A\\(\\gamma\\)\n9B\\(\\alpha\\)\n\n\n\nHere the treatments have been labelled with the numbers \\(1,2, \\ldots,9\\). The method now produces the following design, where rows on the right hand side of the table represent blocks in the BIBD.\n\n\n\nRows\n1\n1\n2\n3\n\n\n\n2\n4\n5\n6\n\n\n\n3\n7\n8\n9\n\n\nColumns\n1\n1\n4\n7\n\n\n\n2\n2\n5\n8\n\n\n\n3\n3\n6\n9\n\n\nLatin\nA\n1\n6\n8\n\n\n\nB\n2\n4\n9\n\n\n\nC\n3\n5\n7\n\n\nGreek\n\\(\\alpha\\)\n1\n5\n9\n\n\n\n\\(\\beta\\)\n2\n6\n7\n\n\n\n\\(\\gamma\\)\n3\n4\n8\n\n\n\nFor example, suppose we wish to compare the fuel economy of 9 different cars when driven around a city\n\n\n\n1\nKia Rio\n\n\n2\nSmart Fortwo\n\n\n3\nToyota Prius\n\n\n4\nVauxhall Corsa\n\n\n5\nSkoda Fabia\n\n\n6\nVolkswagen Polo\n\n\n7\nSEAT Ibiza\n\n\n8\nLexus CT\n\n\n9\nRenault Clio\n\n\n\nWe have 12 drivers we can use in the experiment. It is believed that driving style affects fuel economy. Any single driver does not have time to test all 9 cars. We can use the above BIBD with block=driver and treatment=car.\n\nDriver 1 tests the Kia, Smart and Toyota\nDriver 2 tests the Vauxhall, Skoda and Volkswagen\n\\(\\vdots\\)\n\nDriver 12 tests the Toyota, Vauxhall and Lexus.\n\nA possible model is \\[E(Y_{ij})=\\beta_i+\\tau_j.\\] \\(Y_{ij}\\): observation in block (driver) \\(i\\), treatment (car) \\(j\\) (not column j), and \\(\\sum _j\\tau_j=0\\). Note that we could instead have \\(E(Y_{ij})=\\mu +\\beta_i+\\tau_j,\\) with the extra constraint \\(\\sum _i\\beta_i=0\\). The model in matrix notation is \\(\\mathbf{Y}=\\mathbf{X}\\boldsymbol{\\beta}+\\boldsymbol{\\varepsilon}\\) with observation vector \\[\\mathbf{Y}^T=(Y_{11}, Y_{12}, Y_{13}, Y_{24}, Y_{25},Y_{26},\\ldots,Y_{12,3},Y_{12,4},Y_{12,8}).\\] The parameter vector is \\(\\boldsymbol{\\beta}^T=(\\beta_1,\\beta_2,\\ldots,\\beta_{12},\\tau_1,\\tau_2,\\ldots,\\tau_8)\\) with the constraint implying \\(\\tau_9=-\\tau_1-\\tau_2-\\ldots-\\tau_8\\). The design matrix \\(\\mathbf{X}\\) is\n\\[\\left(\\begin{array}{cccccccccccccccccccc}\n    1&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0\\\\\n    1&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0\\\\\n    \\vdots&\\vdots&\\vdots&\\vdots&\\vdots&\\vdots&\\vdots&\\vdots&\\vdots &\\vdots &\\vdots &\\vdots &\\vdots &\\vdots &\\vdots &\\vdots &\\vdots &\\vdots &\\vdots &\\vdots\\\\\n    0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&1&0&0&0&0\\\\\n    0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&1\\\\\n    \\end{array}\\right)\\]\n\n\n\nAn extension to the above method is to introduce \\(k+1\\) additional treatments, add each to one of the sub-collections of blocks already constructed, and then add an extra block containing all of these treatments. So the new number of treatments is \\(t^*=k^2+k+1\\) and the new block size is \\(k^*=k+1\\). This method is best illustrated by applying it to the above example.\n\n\n\n\n\n\nExample\n\n\n\n\nExample 5.5  \n\n\n\nRows\n1\n1\n2\n3\n10\n\n\n\n2\n4\n5\n6\n10\n\n\n\n3\n7\n8\n9\n10\n\n\nColumns\n1\n1\n4\n7\n11\n\n\n\n2\n2\n5\n8\n11\n\n\n\n3\n3\n6\n9\n11\n\n\nLatin\nA\n1\n6\n8\n12\n\n\n\nB\n2\n4\n9\n12\n\n\n\nC\n3\n5\n7\n12\n\n\nGreek\n\\(\\alpha\\)\n1\n5\n9\n13\n\n\n\n\\(\\beta\\)\n2\n6\n7\n13\n\n\n\n\\(\\gamma\\)\n3\n4\n8\n13\n\n\nNew block\n\n10\n11\n12\n13\n\n\n\n\n\n\n\nComplementary design\n\nGiven a BIBD, we can construct another one by replacing each block by one containing all the treatments which did not appear in that block. It is clear that this is also a BIBD.\n\n\n\n\n\n\nExample\n\n\n\n\nExample 5.6 The complementary design of the one constructed in example @ref(exm:LSdesign) is\n\n\n\n4\n5\n6\n7\n8\n9\n\n\n1\n2\n3\n7\n8\n9\n\n\n1\n2\n3\n4\n5\n6\n\n\n2\n3\n5\n6\n8\n9\n\n\n1\n3\n4\n6\n7\n9\n\n\n1\n2\n4\n5\n7\n8\n\n\n2\n3\n4\n5\n7\n9\n\n\n1\n3\n5\n6\n7\n8\n\n\n1\n2\n4\n6\n8\n9\n\n\n2\n3\n4\n6\n7\n8\n\n\n1\n3\n4\n5\n8\n9\n\n\n1\n2\n5\n6\n7\n9\n\n\n\n\n\n\nNote also that the design given in example @ref(exm:BIBDex2) is self-complementary in the sense that the complement of each block already appears in the same design as another block.\n\n\n5.2.2 Efficiency of the BIBD\nThe simplest model to fit to a BIBD is \\[Y_{ij} = \\beta_i + \\tau _j + \\epsilon _{ij}\\] where \\(Y_{ij}\\) is the observation in block \\(i\\) for treatment \\(j\\), and the treatment effects are subject to the usual constraint \\(\\sum_j\\tau _j=0\\). In this formulation, we have excluded a constant parameter \\(\\mu\\) and so the block effects \\(\\beta_i\\) are subject to no such constraint.\nIt can be shown that \\[\\begin{align}\n\\text{var}(\\hat{\\tau }_j) &= \\frac{k(t-1)}{\\lambda t^2 }\\sigma ^2.\n\\end{align}\\]\nWe now compare the BIBD with the RBD. In particular, for a given number of treatments and total number of observations, is it better to use a BIBD or a RBD? If the total number of observations is \\(rt\\), then in a RBD there will be \\(r\\) blocks and so (see Section @ref(RBD)) \\[\\text{var}(\\hat{\\tau }_j) = \\frac{\\sigma ^2}{r}\\left(1-\\frac{1}{t} \\right)\\] whereas in a BIBD, using the preceding result, \\[\\text{var}(\\hat{\\tau }_j) = \\frac{k(t-1)}{\\lambda t^2}\\sigma ^2 =\\frac{k(t-1)^2}{rt^2(k-1)}\\sigma ^2.\\] The ratio of these (putting BIBD in the numerator) may be written as the efficiency factor \\[E := \\frac{1-t^{-1}}{1-k^{-1}}\\] $E&gt;1 $since \\(t&gt;k\\). It would appear, therefore, that there is a loss of efficiency due to the use of incomplete blocks. However, the whole point of blocking is to attempt to reduce the effect of uncontrollable variation which contributes to the value of \\(\\sigma^2\\), and, since the BIBD uses smaller blocks, the reduction in \\(\\sigma ^2\\) might more than offset the loss of efficiency indicated above, where it was assumed that \\(\\sigma ^2\\) was the same for both designs.\n\n\n\n\n\n\nExample\n\n\n\n\nExample 5.7 Suppose \\(t=8\\) and \\(k=4\\). In example @ref(exm:BIBDex2) we found the following BIBD with \\(14\\) blocks each of size \\(4\\).\n\n\n\nA\nA\nA\nA\nA\nA\nA\nB\nB\nB\nB\nC\nC\nE\n\n\nB\nB\nB\nC\nC\nD\nD\nC\nC\nD\nD\nD\nD\nF\n\n\nC\nE\nG\nE\nF\nE\nF\nE\nF\nE\nF\nE\nG\nG\n\n\nD\nF\nH\nG\nH\nH\nG\nH\nG\nG\nH\nF\nH\nH\n\n\n\nThe comparable RBD will have \\(7\\) blocks each of size \\(8\\). The efficiency factor is \\[E = \\frac{7/8}{3/4} = \\frac{7}{6}\\] which might be seen as more than compensated for by the fact that the BIBD uses blocks which are only half the size of those in the RBD.",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Designs for qualitative explanatory variables: multiple factors</span>"
    ]
  },
  {
    "objectID": "Factorial designs.html",
    "href": "Factorial designs.html",
    "title": "6  Factorial designs",
    "section": "",
    "text": "6.1 Complete factorial designs (CFD)\nA complete factorial design (CFD) includes all possible combinations of the explanatory variables. If no replications are included, the total number of observations in a CFD is \\[n = \\prod _{i=1}^m l_i\\] where \\(m\\) is the number of explanatory variables and \\(l_i\\) is the number of levels of the \\(i\\)-th of these explanatory variables. In the above example, \\(m=3\\), \\(l_1=3\\) and \\(l_2=l_3=2\\) and so \\(n=3\\times 2\\times 2 = 12\\). Originally, such designs were proposed for discrete explanatory variables, hence the name ’’factorial”. However, in general, the explanatory variables may be quantitative (temperature and pressure) as well as qualitative (catalyst).\nIf \\(m\\) is large, even if the number of levels of each factor is small, the design may require an impractically large number of observations. Also, there may be a need to use blocks. For the moment, we shall only consider designs where each of the factors occurs at two levels and no blocking is required. The models which can be fitted using such designs will be investigated.\nIf a factor occurs at two levels, it is convenient to denote these levels by \\(-1\\) (or \\(-\\)), the low level, and \\(+1\\) (or \\(+\\)), the high level. Sometimes the factors \\(x_1, x_2, \\ldots\\) are denoted instead by \\(a,b,c,\\ldots\\) and the response corresponding to each combination is given subscripts for each of the factors which occurs at the high level, except that the response corresponding to all the factors being at the low level is given the subscript \\((1)\\). The following tables illustrate the design for \\(m=2\\) and \\(m=3\\).\nNote that in these tables, the rows are in standard order which means that the first column alternates between \\(-1\\) and \\(1\\), the second column has alternating pairs of \\(-1\\)’s and \\(1\\)’s, the third column has alternating groups of four, and so on. This practice ensures that every combination of levels of the factors occurs exactly once, but it should be noted that, in applying such a design, randomisation should take place in assigning factor combinations to experimental units.\nThe type of linear model which we can fit to a CFD is the polynomial model \\[E(Y)=\\beta_0+\\sum_{i=1}^m\\beta_ix_i+\\sum_{i=1}^{m-1}\\sum_{j=i+1}^m\\beta_{ij}x_ix_j+\\sum_{i=1}^{m-2}\\sum_{j=i+1}^{m-1}\\sum_{k=j+1}^m\\beta_{ijk}x_ix_jx_k+\\ldots\\] where, because we have chosen \\(\\pm 1\\) as the values taken by the \\(x_i\\)’s, there is no need to include any power of \\(x_i\\) greater than one, since \\(x_i^2\\) is identically equal to \\(1\\). Another good reason for this choice is that it makes the parameters relatively easy to interpret; for instance, in the first-order model \\[E(Y) = \\beta_0 + \\sum _{i=1}^m \\beta_ix_i\\] \\(\\beta_0\\) can be interpreted as the overall mean and, for \\(i\\geq 1\\), \\(2\\beta_i\\) can be interpreted as the difference in effect between high and low levels of factor \\(i\\).\nIn the more general model, the \\(\\beta_i\\)’s are called the main effects and the higher order parameters \\(\\beta_{ij}, \\beta_{ijk},\\ldots\\) are called the interactions. The fullest possible model is the one which includes all orders of interaction up to \\(m\\), so that the last term in the equation above is \\[\\ldots + \\beta_{12\\ldots m}x_1x_2\\ldots x_m\\] and the total number of parameters is \\[1 + {m \\choose 1} + {m \\choose 2} + \\ldots + 1 = (1+1)^m = 2^m\\] which is the same as the total number of observations. The model is said to be saturated: it fits exactly, the residuals are zero and there are no degrees of freedom for error. In practice, an experimenter will rarely expect it necessary to include any but the lowest order interactions in the model, and so the number of parameters will be much smaller, and the usual analysis of variance techniques may be applied.\nAnother property of a CFD is that, however many interactions are included in the model, the matrix \\(\\mathbf{X}^T\\mathbf{X}\\) turns out to be a multiple of the identity matrix, and so all the parameters are mutually orthogonal and it is very easy to write down their estimators.\nWe have already remarked that a CFD may be used even when the explanatory variables are quantitative, by choosing two values of each and, by suitable scaling, calling them \\(+1\\) and \\(-1\\). We can think of the design points as the corners of a cuboid in \\(m\\) dimensions. However, in this case the only model we can fit is of the kind we have just considered, where there do not exist, for example, quadratic terms in \\(x_i^2\\), because \\(x_i^2\\) is identically equal to \\(1\\) at all of the design points and so the coefficient \\(\\beta_{ii}\\), say, would be confounded with the constant term. If we wanted to fit such a model, we would have to add some other design points.",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Factorial designs</span>"
    ]
  },
  {
    "objectID": "Factorial designs.html#complete-factorial-designs-cfd",
    "href": "Factorial designs.html#complete-factorial-designs-cfd",
    "title": "6  Factorial designs",
    "section": "",
    "text": "\\(x_1 (a)\\)  \n  \\(x_2 (b)\\)  \n  Response  \n\n\n\n\n\\(-1\\)\n\\(-1\\)\n\\(Y_{(1)}\\)\n\n\n\\(1\\)\n\\(-1\\)\n\\(Y_a\\)\n\n\n\\(-1\\)\n\\(1\\)\n\\(Y_b\\)\n\n\n\\(1\\)\n\\(1\\)\n\\(Y_{ab}\\)\n\n\n\nCFD for \\(m=2\\).\n\n\n\n  \\(x_1 (a)\\)  \n  \\(x_2 (b)\\)  \n  \\(x_3 (c)\\)  \n  Response  \n\n\n\n\n\\(-1\\)\n\\(-1\\)\n\\(-1\\)\n\\(Y_{(1)}\\)\n\n\n\\(1\\)\n\\(-1\\)\n\\(-1\\)\n\\(Y_a\\)\n\n\n\\(-1\\)\n\\(1\\)\n\\(-1\\)\n\\(Y_b\\)\n\n\n\\(1\\)\n\\(1\\)\n\\(-1\\)\n\\(Y_{ab}\\)\n\n\n\\(-1\\)\n\\(-1\\)\n\\(1\\)\n\\(Y_c\\)\n\n\n\\(1\\)\n\\(-1\\)\n\\(1\\)\n\\(Y_{ac}\\)\n\n\n\\(-1\\)\n\\(1\\)\n\\(1\\)\n\\(Y_{bc}\\)\n\n\n\\(1\\)\n\\(1\\)\n\\(1\\)\n\\(Y_{abc}\\)\n\n\n\nCFD for \\(m=3\\).\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\nExample 6.1 Let \\(m=3\\) and consider the model with two-factor interactions \\[E(Y) = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_3x_3 + \\beta_{12}x_1x_2 + \\beta_{13}x_1x_3 + \\beta_{23}x_2x_3 .\\] Then the design matrix is \\[\\mathbf{X}= \\left( \\begin{array}{ccccccc} 1 & -1 & -1 & -1 & 1 & 1 & 1 \\\\\n1 & 1 & -1 & -1 & -1 & -1 & 1 \\\\ 1 & -1 & 1 & -1 & -1 & 1 & -1 \\\\\n1 & 1 & 1 & -1 & 1 & -1 & -1 \\\\ 1 & -1 & -1 & 1 & 1 & -1 & -1 \\\\\n1 & 1 & -1 & 1 & -1 & 1 & -1 \\\\ 1 & -1 & 1 & 1 & -1 & -1 & 1 \\\\\n1 & 1 & 1 & 1 & 1 & 1 & 1 \\end{array} \\right) .\\] Note how here the second, third and fourth columns are lifted from the diagram of the design which we gave earlier, and the last three columns are found by multiplying together in pairs the preceding three columns. It is easy to see now that \\(\\mathbf{X}^T\\mathbf{X}=8\\mathbf{I}_7\\) where \\(\\mathbf{I}_7\\) is the \\(7\\times 7\\) identity matrix, and so \\[\\hat{\\boldsymbol{\\beta}}=\\frac{1}{8}\\mathbf{X}^T\\mathbf{Y}\\] or, more fully, \\[\\begin{align}\n\\hat{\\beta_0} &= \\frac{1}{8}\\left(Y_{(1)}+Y_a+Y_b+Y_{ab}+Y_c+Y_{ac}+Y_{bc}+Y_{abc} \\right) \\\\\n\\hat{\\beta_1} &= \\frac{1}{8}\\left(-Y_{(1)}+Y_a-Y_b+Y_{ab}-Y_c+Y_{ac}-Y_{bc}+Y_{abc} \\right) \\\\\n\\hat{\\beta_2} &= \\frac{1}{8}\\left(-Y_{(1)}-Y_a+Y_b+Y_{ab}-Y_c-Y_{ac}+Y_{bc}+Y_{abc} \\right) \\\\\n\\hat{\\beta_3} &= \\frac{1}{8}\\left(-Y_{(1)}-Y_a-Y_b-Y_{ab}+Y_c+Y_{ac}+Y_{bc}+Y_{abc} \\right) \\\\\n\\hat{\\beta_{12}} &= \\frac{1}{8}\\left(Y_{(1)}-Y_a-Y_b+Y_{ab}+Y_c-Y_{ac}-Y_{bc}+Y_{abc} \\right) \\\\\n\\hat{\\beta_{13}} &= \\frac{1}{8}\\left(Y_{(1)}-Y_a+Y_b-Y_{ab}-Y_c+Y_{ac}-Y_{bc}+Y_{abc} \\right) \\\\\n\\text{and  }\\hat{\\beta_{23}} &= \\frac{1}{8}\\left(Y_{(1)}+Y_a-Y_b-Y_{ab}-Y_c-Y_{ac}+Y_{bc}+Y_{abc} \\right).\n\\end{align}\\] It is instructive to observe the pattern of plus and minus signs in these expressions. For instance, in the expression for the estimator of \\(\\beta_{13}\\), the interaction between \\(x_1\\) and \\(x_3\\), the part inside the brackets is the difference between the sum of the observations where \\(x_1\\) and \\(x_3\\) are both acting at the same level (high or low) and the sum of the observations where they are acting at opposite levels.",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Factorial designs</span>"
    ]
  },
  {
    "objectID": "Factorial designs.html#blocking-factorial-designs",
    "href": "Factorial designs.html#blocking-factorial-designs",
    "title": "6  Factorial designs",
    "section": "6.2 Blocking Factorial Designs",
    "text": "6.2 Blocking Factorial Designs\nIt may happen that blocking in a CFD is desirable because of the size of the experiment. This can be done provided it is not expected that the higher-order interactions will be significant, and so they can be excluded from the model, and degrees of freedom may be used to introduce blocking parameters instead.\nThere is a particularly efficient way of doing this when the number of blocks is a power of \\(2\\) and all of the blocks are the same size, necessarily also a power of \\(2\\). Consider first the case of two blocks. If we take\n\nBlock I: all observations for which \\(x_1x_2\\ldots x_m= 1\\)\nBlock II: all observations for which \\(x_1x_2\\ldots x_m=-1\\)\n\nthen we are effectively confounding blocks with the highest-order interaction which we have already assumed to be not significant. If we choose \\(\\pm 1\\) as the two values of the dummy variable for blocks, then the model is identical to the one containing that interaction, except that the parameter must now be interpreted differently.\nIn this model, \\(x_1x_2\\ldots x_m\\) is called the block generator. The choice of such a generator is not unique, but in this case it is the obvious choice since it enables all the main effects and interactions below the highest still to be included in the model, and is least likely to be harmful to the estimation of parameters.\n\n\n\n\n\n\nExample\n\n\n\n\nExample 6.2 In the case \\(m=3\\), if we take \\(x_1x_2x_3\\) as the block generator then we get two blocks which may be represented as follows.\nBlock I\n\\[\\begin{array}{|ccc|} \\hline + & - & - \\\\ - & +   & - \\\\ - & - & + \\\\ + & + & + \\\\ \\hline \\end{array}\\] Block II\n\\[\\begin{array}{|ccc|} \\hline - & - & - \\\\ + & + & - \\\\ + & - & + \\\\ - & + & + \\\\ \\hline \\end{array}\\]\nSuppose we then choose to fit the first-order model without interactions \\[E(Y)=\\beta_0 + \\alpha z + \\beta_1x_1 + \\beta_2x_2 + \\beta_3x_3\\] where \\(z\\) is the dummy variable for blocks. Then we get the following design matrix \\[\\mathbf{X}=\\left( \\begin{array}{ccccc} 1 & 1 & 1 & -1 & -1 \\\\\n1 & 1 & -1 & 1 & -1 \\\\ 1 & 1 & -1 & -1 & 1 \\\\ 1 & 1 & 1 & 1 & 1 \\\\ 1\n& -1 & -1 & -1 & -1 \\\\ 1 & -1 & 1 & 1 & -1 \\\\ 1 & -1 & 1 & -1 & 1 \\\\\n1 & -1 & -1 & 1 & 1 \\end{array} \\right)\\] where the first four rows represent Block I and the last four rows represent Block II. It is easy to see that \\(\\mathbf{X}^T\\mathbf{X}\\) is a multiple of the identity matrix, and so the parameters are all orthogonal, and the estimators of those of interest are the same as they would be without blocking.\n\n\n\nNow consider the case of four blocks. We now have to choose two block generators so that blocks are determined by which of the four possible pairs of values these generators take. If we denote these block generators by \\(z_1\\) and \\(z_2\\), we might take the following.\n\nBlock I: all observations for which \\(z_1=z_2=1\\)\nBlock II: all observations for which \\(z_1=1,z_2=-1\\)\nBlock III: all observations for which \\(z_1=-1,z_2=1\\)\nBlock IV: all observations for which \\(z_1=z_2=-1\\)\n\nThe choice of two block generators is more subtle than the choice of one, the reason being that, in the full model with all interactions, not only is each of the block generators confounded with an interaction, but so is their product \\(z_1z_2\\) which, bearing in mind the algebra of these numbers (\\(x_i^2\\equiv 1\\)), may be a low-order interaction or even a main effect, which would be a bad choice. For example, in the case \\(m=5\\), it would be bad to choose \\(z_1=x_1x_2x_3x_4x_5\\) and \\(z_2=x_2x_3x_4x_5\\) because then \\[z_1z_2 = (x_1x_2x_3x_4x_5)(x_2x_3x_4x_5)=x_1\\] and so the main effect of factor 1 is confounded with blocks.\n\n\n\n\n\n\nExample\n\n\n\n\nExample 6.3 When \\(m=3\\), the best we can do is to choose \\(z_1=x_1x_2\\) and \\(z_2=x_1x_3\\) so that their product is \\(x_2x_3\\), and all of the two-factor interactions are confounded with blocks. Then the model we can fit contains only main effects of the factors, say \\[E(Y) = \\beta_0 + \\alpha _1z_1 + \\alpha _2z_2 + \\alpha _{12}z_1z_2 + \\beta_1x_1 + \\beta_2x_2 + \\beta_3x_3\\] where \\(z_1=x_1x_2\\) and \\(z_2=x_1x_3\\). There is one degree of freedom left for error.\n\n\n\n\n\n\n\n\n\nThese are both good revision questions, typical of exam questions.\n\n\n\n\nExercise 6.1  \n\nConstruct the design matrix of a CFD for \\(m=4\\) factors, in which the model fitted contains the main effects and only the interaction between factors \\(1\\) and \\(2\\). What are the parameter estimators in this model? You may find it easier to describe them rather than writing them out in full.\nSuggest a good choice of two block generators in a CFD with \\(m=6\\) factors and four blocks, and discuss what models can be fitted to this design.",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Factorial designs</span>"
    ]
  },
  {
    "objectID": "Factorial designs.html#fractional-factorial-designs-ffd",
    "href": "Factorial designs.html#fractional-factorial-designs-ffd",
    "title": "6  Factorial designs",
    "section": "6.3 Fractional factorial designs (FFD)",
    "text": "6.3 Fractional factorial designs (FFD)\nIn cases where the experimenter would like to use a CFD but this would entail taking an impractically large number of observations, the best that can be done is to select a fraction of the possible factor combinations, giving a fractional factorial design (FFD). This must be done in a way which maintains as much efficiency as possible.\nClassical FFD’s will be discussed. They consist of \\(n=2^{m-t}\\) observations, where \\(m\\) is the number of factors and \\(2^{-t}\\) is the proportion of the possible factor combinations used.\nThe way in which the factor combinations are chosen resembles the way in which blocks were constructed in Section 6.2. Essentially, we choose just one of the \\(2^t\\) blocks which would have been constructed using \\(t\\) different block generators. Thus, we choose all possible combinations of the factor levels satisfying \\(t\\) equations known as design generators.\n\n\n\n\n\n\nExample\n\n\n\n\nExample 6.4 Construct a FFD for \\(m=3\\) and \\(t=1\\) using the design generator \\(x_1x_2x_3=1\\).\n\n\n\n\\(x_1\\)\n\\(x_2\\)\n\\(x_3\\)\n\n\n\n\n\\(1\\)\n\\(-1\\)\n\\(-1\\)\n\n\n\\(-1\\)\n\\(1\\)\n\\(-1\\)\n\n\n\\(-1\\)\n\\(-1\\)\n\\(1\\)\n\n\n\\(1\\)\n\\(1\\)\n\\(1\\)\n\n\n\n\n\n\nIf a FFD is used, it is obvious that not all of the interactions can be included in the model, because the CFD was already saturated when all of the interactions were included. The pattern by which the main effects and interactions are confounded with each other in a FFD is called the alias structure. This may be found by taking the design generators and finding all the possible equations that can be derived from them by combining them together or multiplying both sides by the same quantity, remembering that in the algebra of these symbols (\\(x_i^2\\equiv 1\\)) there is simplification because any square may be deleted.\n\n\n\n\n\n\nExample\n\n\n\n\nExample 6.5 Find the alias structure when \\(m=5\\), \\(t=2\\) and the design generators are \\(x_1x_2x_3=1\\) and \\(x_3x_4x_5=1\\). Discuss what models is it possible to fit using this design.\nThe design generators give \\[1 = x_1x_2x_3 = x_3x_4x_5 = x_1x_2x_4x_5\\] where the last term is found by multiplying the two design generators together. We can then multiply these equations by symbols until we have exhausted all the possibilities, as follows.\n\\[\\begin{align}\nx_1&= x_2x_3 = x_1x_3x_4x_5 = x_2x_4x_5\\\\\nx_2&= x_1x_3 = x_2x_3x_4x_5 = x_1x_4x_5\\\\\nx_3&= x_1x_2 = x_4x_5 = x_1x_2x_3x_4x_5\\\\\nx_4&= x_1x_2x_3x_4 = x_3x_5 = x_1x_2x_5\\\\\nx_5&= x_1x_2x_3x_5 = x_3x_4 = x_1x_2x_4\\\\\nx_1x_4&= x_2x_3x_4 = x_1x_3x_5 = x_2x_5\\\\\nx_1x_5&= x_2x_3x_5 = x_1x_3x_4 = x_2x_4.\n\\end{align}\\]\nThis is the alias structure. To fit a model to this design without confounding, we can only include at most one variable from each of these groups. Once we have included the main effects, this only leaves the possibility of two of the two-factor interactions such as \\(x_1x_4\\) and \\(x_1x_5\\), although including both of these would saturate the model. Note that it rarely makes sense to include a higher-order interaction when at least some of the corresponding lower-order ones are missing.\n\n\n\nTo construct a FFD for given \\(m\\) and \\(t\\) and design generators, the following is an efficient method. Construct a CFD with \\(m-t\\) of the factors which are not constrained to be related to each other by the design generators. Then express each of the remaining factors in terms of the previous ones using the design generators, and add a column to the design for each of these, by multiplying together the appropriate preceding columns.\n\n\n\n\n\n\nExample\n\n\n\n\nExample 6.6 Construct the design defined in Example 6.5\n\n\n\n\\(x_1\\)\n\\(x_2\\)\n\\(x_4\\)\n\\(x_3=x_1x_2\\)\n\\(x_5=x_3x_4\\)\n\n\n\n\n\\(-1\\)\n\\(-1\\)\n\\(-1\\)\n\\(1\\)\n\\(-1\\)\n\n\n\\(1\\)\n\\(-1\\)\n\\(-1\\)\n\\(-1\\)\n\\(1\\)\n\n\n\\(-1\\)\n\\(1\\)\n\\(-1\\)\n\\(-1\\)\n\\(1\\)\n\n\n\\(1\\)\n\\(1\\)\n\\(-1\\)\n\\(1\\)\n\\(-1\\)\n\n\n\\(-1\\)\n\\(-1\\)\n\\(1\\)\n\\(1\\)\n\\(1\\)\n\n\n\\(1\\)\n\\(-1\\)\n\\(1\\)\n\\(-1\\)\n\\(-1\\)\n\n\n\\(-1\\)\n\\(1\\)\n\\(1\\)\n\\(-1\\)\n\\(-1\\)\n\n\n\\(1\\)\n\\(1\\)\n\\(1\\)\n\\(1\\)\n\\(1\\)\n\n\n\nIn this construction, we have used \\(x_1, x_2\\) and \\(x_4\\) in the initial CFD, represented by the first three columns, since the obvious first choice of \\(x_1, x_2\\) and \\(x_3\\) does not work, as these three factors are constrained by the design generator \\(x_1x_2x_3=1\\), and so their combinations of values do not range over all eight possibilities. There is no such problem with \\(x_1, x_2\\) and \\(x_4\\).\n\n\n\nThe resolution \\(R\\) of a FFD is defined as the order of the lowest order interaction which is confounded with \\(\\beta_0\\), and is traditionally denoted by a Roman numeral. Thus, in Example 6.3.2, \\(R=\\)III, since the lowest order interaction confounded with \\(\\beta_0\\) is \\(\\beta_{123}\\) or \\(\\beta_{345}\\). Resolution which is as high as possible is obviously desirable. Also, a FFD with given values of \\(m\\) and \\(t\\) is often denoted by \\(2^{m-t}_R\\), so that the design of Example 6.5 may be described as a \\(2^{5-2}_{\\text{III}}\\) design.",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Factorial designs</span>"
    ]
  },
  {
    "objectID": "Factorial designs.html#screening-experiments",
    "href": "Factorial designs.html#screening-experiments",
    "title": "6  Factorial designs",
    "section": "6.4 Screening experiments",
    "text": "6.4 Screening experiments\nIn many industrial applications, the number of variables that may affect the response variable of interest is very large. It is therefore important at an early stage of an investigation to study the relative importance of the factors. In such cases screening experiments are carried out. The aim is to reduce the number of factors included in models for future experiments. Low resolution fractional factorial designs may be used for this purpose.\nFor practical reasons it may only be feasible to fit the first-order model \\[E(Y) = \\beta_0 + \\sum _{i=1}^m \\beta_ix_i\\] and for this, the smallest possible FFD is \\(2^{m-t}\\), where \\(t\\) is the largest integer satisfying \\[2^{m-t}\\geq m+1.\\] If there is equality in the above, the design is saturated and it is not possible to perform significance tests, but it is still possible to calculate the parameter estimates and compare their magnitudes to get some idea which factors are most influential.\n\n\n\n\n\n\nExample\n\n\n\n\nExample 6.7 The filtration stage in a newly constructed industrial plant had been taking almost twice as long as in other plants. To address the problem, the factors which may have been the cause were identified and a screening experiment was carried out. The factors are listed below.\n\n\n\n\n\nLevel\n\n\n\nVariable\n\n\\(-1\\)\n\\(1\\)\n\n\n1\nwater supply\ntown reservoir\nwell\n\n\n2\nraw material\non site\nother\n\n\n3\ntemperature\nlow\nhigh\n\n\n4\nrecycle\nyes\nno\n\n\n5\ncaustic soda\nfast\nslow\n\n\n6\nfilter cloth\nnew\nold\n\n\n7\nholding up time\nlow\nhigh\n\n\n\nA CFD would require \\(2^7=128\\) observations. However, the first-order model has eight parameters, and, if we are not worried about saturating the design but more worried about the size of the screening experiment, a \\(2^{7-4}\\) FFD may be used. This is what was done, using a CFD involving the factors \\(x_1, x_2\\) and \\(x_3\\) and then the generators \\(x_4=x_1x_2, x_5=x_1x_3, x_6=x_2x_3\\) and \\(x_7=x_1x_2x_3\\). The results, including the data, are given below.\n\n\n\n\\(x_1\\)\n\\(x_2\\)\n\\(x_3\\)\n\\(x_4=x_1x_2\\)\n\\(x_5=x_1x_3\\)\n\\(x_6=x_2x_3\\)\n\\(x_7=x_1x_2x_3\\)\n\\(y\\)\n\n\n\n\n\\(-1\\)\n\\(-1\\)\n\\(-1\\)\n\\(1\\)\n\\(1\\)\n\\(1\\)\n\\(-1\\)\n68.4\n\n\n\\(1\\)\n\\(-1\\)\n\\(-1\\)\n\\(-1\\)\n\\(-1\\)\n\\(1\\)\n\\(1\\)\n77.7\n\n\n\\(-1\\)\n\\(1\\)\n\\(-1\\)\n\\(-1\\)\n\\(1\\)\n\\(-1\\)\n\\(1\\)\n66.4\n\n\n\\(1\\)\n\\(1\\)\n\\(-1\\)\n\\(1\\)\n\\(-1\\)\n\\(-1\\)\n\\(-1\\)\n81.0\n\n\n\\(-1\\)\n\\(-1\\)\n\\(1\\)\n\\(1\\)\n\\(-1\\)\n\\(-1\\)\n\\(1\\)\n78.6\n\n\n\\(1\\)\n\\(-1\\)\n\\(1\\)\n\\(-1\\)\n\\(1\\)\n\\(-1\\)\n\\(-1\\)\n41.2\n\n\n\\(-1\\)\n\\(1\\)\n\\(1\\)\n\\(-1\\)\n\\(-1\\)\n\\(1\\)\n\\(-1\\)\n68.7\n\n\n\\(1\\)\n\\(1\\)\n\\(1\\)\n\\(1\\)\n\\(1\\)\n\\(1\\)\n\\(1\\)\n38.7\n\n\n\nThe regression equation fitted by a standard package (and fitting exactly, because the design is saturated) was \\[y = 65.1-5.44x_1-1.39x_2-8.29x_3+1.59x_4-11.4x_5-1.71x_6+0.262x_7.\\] The magnitudes of the coefficients suggest that \\(x_1,  x_3\\) and \\(x_5\\) are the influential factors. Note that it is legitimate to compare these magnitudes, because the explanatory variables are all on the same scale \\(\\{ -1,1\\}\\) and the estimates of the regression coefficients all have the same standard error, even though we cannot estimate it because of the saturation.",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Factorial designs</span>"
    ]
  },
  {
    "objectID": "Factorial designs.html#plackett-and-burman-designs",
    "href": "Factorial designs.html#plackett-and-burman-designs",
    "title": "6  Factorial designs",
    "section": "6.5 Plackett and Burman designs",
    "text": "6.5 Plackett and Burman designs\nPlackett and Burman (1946) proposed a method of constructing a FFD in which the total number of observations is not necessarily a power of two, but nevertheless the columns of the design diagram are orthogonal to each other. They discovered designs for values of \\(n\\) which are multiples of 4 up to 100, excluding 92, for which a design was later discovered.\nThe method is to find a sequence of \\(\\frac{1}{2}n\\) plus signs and \\(\\frac{1}{2}n-1\\) minus signs, make this the first row of the diagram, form all the cyclical permutations of this sequence to give the next \\(n-2\\) rows, and finish the diagram off with a row of \\(-1\\)’s. If the initial sequence is chosen right, this design will have the desired orthogonality property; the difficulty is to find a sequence which does the trick. This sequence is called the generator.\n\n\n\n\n\n\nExample\n\n\n\n\nExample 6.8 For \\(n=12\\), the generator takes the form \\[\\begin{array}{ccccccccccc} + & + & - & + & + & + & - & - & - & + & - \\\\ \\end{array}\\] and so the design may be represented by the following table. \\[\\begin{array}{ccccccccccc}\n+ & + & - & + & + & + & - & - & - & + & - \\\\\n- & + & + & - & + & + & + & - & - & - & + \\\\\n+ & - & + & + & - & + & + & + & - & - & - \\\\\n- & + & - & + & + & - & + & + & + & - & - \\\\\n- & - & + & - & + & + & - & + & + & + & - \\\\\n- & - & - & + & - & + & + & - & + & + & + \\\\\n+ & - & - & - & + & - & + & + & - & + & + \\\\\n+ & + & - & - & - & + & - & + & + & - & + \\\\\n+ & + & + & - & - & - & + & - & + & + & - \\\\\n- & + & + & + & - & - & - & + & - & + & + \\\\\n+ & - & + & + & + & - & - & - & + & - & + \\\\\n- & - & - & - & - & - & - & - & - & - & -\n\\end{array}\\] It may be checked that any two columns in this diagram are orthogonal to each other.\n\n\n\nA Plackett and Burman design can be used in a screening experiment with anything up to \\(n-1\\) factors, and is particularly useful if the number of factors is not close to a power of two, and the screening experiment is to be as small and economical as possible. For instance, the design in the preceding example can test 11 factors with 12 observations, whereas the nearest classical FFD would require 16 observations.\nPlackett and Burman designs are less useful in full experiments because the parameters may not be so easy to interpret.\n\n\n\n\n\n\nQuestion 1 is a good revision question. Questions 2 and 3 are more challenging.\n\n\n\n\nExercise 6.2  \n\nConstruct a \\(2^{6-2}\\) FFD using the two block generators of Exercise 6.1 Question 2, now as design generators. What is the resolution of this design?\nIn the screening experiment of Example 6.7, suppose that the first factor, water supply, has four levels rather than two. Suggest a way in which the experiment might be modified to take account of this.\nIn the construction of designs along the lines of Plackett and Burman, in which any column contains equal numbers of plus and minus signs and any two columns are orthogonal, why must the total number of observations be a multiple of 4?",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Factorial designs</span>"
    ]
  },
  {
    "objectID": "Factorial designs.html#composite-designs",
    "href": "Factorial designs.html#composite-designs",
    "title": "6  Factorial designs",
    "section": "6.6 Composite designs",
    "text": "6.6 Composite designs\nWe have already mentioned that CFD’s cannot be used if at least some of the factors are quantitative and it is desired to fit a quadratic regression model in such factors, so that more design points must be added. This may be done in a lot of ways, but there are certain ways which are fairly standard.\nIn a central composite design (CCD), the factors are varied at three equally spaced levels, scaled so as to take the values \\(-1, 0\\) and \\(1\\). The addition of a third level is sufficient to be able to fit the second order polynomial model \\[E(Y) = \\beta_0 + \\sum _{i=1}^m \\beta_ix_i + \\sum _{i=1}^{m-1}\\sum_{j=i+1}^m \\beta_{ij}x_ix_j + \\sum _{i=1}^m \\beta_{ii}x_i^2\\] provided the design points are chosen appropriately.\nA CCD consists of three types of points:\n\nthe points of a CFD or a FFD;\n’’star” points, namely points all of whose co-ordinates are zero except one, which is \\(\\pm \\alpha\\), where \\(\\alpha &gt;0\\), so that the point is a distance \\(\\alpha\\) from the origin;\ncentre points, all of whose co-ordinates are zero.\n\nIf for the star points we choose \\(\\alpha =1\\), we get the points in the middle of the faces of the cuboid (some of) whose corners appear in the CFD or FFD. Another common choice is \\(\\alpha =\\sqrt{m}\\) which means that the star points are the same distance from the origin as the points of the CFD or FFD, and the design region must be regarded as a spheroid. In this case, we get a rotatable composite design (RCD). This choice also means that the variables are observed at five levels (\\(\\pm 1, 0, \\pm \\alpha\\)), a consequence of which is that it is possible to test for significance of higher order models than quadratic. If we are able to add a few extra points, replication of centre points adds precision and does not destroy any of the symmetry of the design.\nIn general, if all three types of point are used in the design, and if \\(n_c\\) denotes the number of centre points used, then the total number of observations may be written \\[n = 2^{m-t} + 2m + n_c .\\]\n\n\n\n\n\n\nExample\n\n\n\n\nExample 6.9 Construct a CCD for two factors and 10 observations, where it is not expected that a higher order model than quadratic will be needed.\nIn this case, \\(m=2\\) and we may choose \\(\\alpha =1\\). We may use a \\(2^2\\) CFD which takes up four observations, the star points will take up another four observations and so we may use two centre points. The design is described by the diagram below.\n\n\n\n   \\(x_1\\)   \n   \\(x_2\\)   \n\n\n\n\n\\(-1\\)\n\\(-1\\)\n\n\n1\n\\(-1\\)\n\n\n\\(-1\\)\n1\n\n\n1\n1\n\n\n1\n0\n\n\n\\(-1\\)\n0\n\n\n0\n1\n\n\n0\n\\(-1\\)\n\n\n0\n0\n\n\n0\n0\n\n\n\nThe model that can be fitted is the full quadratic model \\[E(Y) = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_{12}x_1x_2 + \\beta_{11}x_1^2 + \\beta_{22}x_2^2\\] and there are four degrees of freedom for error.",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Factorial designs</span>"
    ]
  },
  {
    "objectID": "Designs for mixture experiments.html",
    "href": "Designs for mixture experiments.html",
    "title": "7  Designs for mixture experiments",
    "section": "",
    "text": "Suppose that the explanatory variables are the proportions of \\(q\\) different constituents in a mixture, so that they satisfy \\[0\\leq x_i\\leq 1~~(i=1,2,\\ldots ,q)~~\\text{and}~~\\sum_{i=1}^qx_i=1.\\] If an attempt were made to fit a linear regression model, the equation above amounts to a linear relationship between the columns of the design matrix, and so it would not be of full rank. The neatest way of overcoming this is to omit the constant term, giving what is called the first Scheffé polynomial model \\[E(Y)=\\sum_{i=1}^q \\beta_ix_i.\\] Similarly, for a second order polynomial model, noting that \\[x_i^2 = x_i(1-\\sum _{j\\neq i}x_j) = x_i -\\sum_{j\\neq i}x_ix_j\\] we see that, in order to avoid over-parametrisation in this model, we need, for example, to omit all of the squared terms, giving the second order Scheffé polynomial model \\[E(Y)=\\sum_{i=1}^q \\beta_ix_i + \\sum_{i=1}^{q-1}\\sum_{j=i+1}^q \\beta_{ij}x_ix_j.\\] Commonly used experimental designs are the simplex lattice design (SLD) and the simplex centroid design (SCD). Sometimes these will coincide.\nThe simplex lattice design, which may be used to fit a polynomial of degree \\(d\\), includes all possible mixtures in which the proportions of each constituent are numbers from the set \\[\\left\\{0, \\frac{1}{d}, \\frac{2}{d}, \\ldots ,\\frac{d-1}{d} , 1 \\right\\}.\\]\n\n\n\n\n\n\nExample\n\n\n\n\nExample 7.1 Construct a SLD for a mixture with three constituents when a second order model is to be fitted.\n\n\n\n   \\(x_1\\)   \n   \\(x_2\\)   \n   \\(x_3\\)   \n\n\n\n\n1\n0\n0\n\n\n0\n1\n0\n\n\n0\n0\n1\n\n\n\\(\\frac{1}{2}\\)\n\\(\\frac{1}{2}\\)\n0\n\n\n\\(\\frac{1}{2}\\)\n0\n\\(\\frac{1}{2}\\)\n\n\n0\n\\(\\frac{1}{2}\\)\n\\(\\frac{1}{2}\\)\n\n\n\nNote that the design is saturated for this model, since there are six observations and six parameters (\\(\\beta_1,\\beta_2,\\beta_3,\\beta_{12},\\beta_{13}\\) and \\(\\beta_{23}\\)).\n\n\n\nSimplex centroid designs include all possible mixtures where some, but not necessarily all, of the constituents are present, and those which are present are in equal proportions.\n\n\n\n\n\n\nExample\n\n\n\n\nExample 7.2 Construct a SCD for a mixture with three constituents.\n\n\n\n   \\(x_1\\)   \n   \\(x_2\\)   \n   \\(x_3\\)   \n\n\n\n\n1\n0\n0\n\n\n0\n1\n0\n\n\n0\n0\n1\n\n\n\\(\\frac{1}{2}\\)\n\\(\\frac{1}{2}\\)\n0\n\n\n\\(\\frac{1}{2}\\)\n0\n\\(\\frac{1}{2}\\)\n\n\n0\n\\(\\frac{1}{2}\\)\n\\(\\frac{1}{2}\\)\n\n\n\\(\\frac{1}{3}\\)\n\\(\\frac{1}{3}\\)\n\\(\\frac{1}{3}\\)\n\n\n\nNote that, because four different values of each variable appear in this design (\\(0, \\frac{1}{3}, \\frac{1}{2}\\) and 1), the third order polynomial model \\[E(Y)=\\beta_1x_1 + \\beta_2x_2 + \\beta_3x_3 + \\beta_{12}x_{12} + \\beta_{13}x_1x_3 + \\beta_{23}x_2x_3 + \\beta_{123}x_1x_2x_3\\] may be fitted, and in this case, once again, the design is saturated, since there are seven observations and seven parameters.\n\n\n\n\n\n\n\n\n\nQuestion 1 is a good revision exercise. Question 2 is more challenging.\n\n\n\n\nExercise 7.1  \n\nWrite out a rotatable composite design for \\(m=3\\) factors which uses a \\(2^3\\) CFD, star points and two centre points. Which models can be fitted to this design?\nThe simplex lattice design is saturated for the mixture model with \\(d=2\\) and \\(q=3\\), as seen in Example 7.1. Investigate what happens when \\(d=3\\) and \\(q=4\\).",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Designs for mixture experiments</span>"
    ]
  },
  {
    "objectID": "Continuous and exact designs.html",
    "href": "Continuous and exact designs.html",
    "title": "8  Continuous and exact designs",
    "section": "",
    "text": "8.1 The General Equivalence Theorem\nIn other words, for continuous designs, \\(D\\)-optimality and \\(G\\)-optimality are equivalent, and an optimal design by either of these criteria can be identified by the fact that the maximum variance of prediction is equal to the number of parameters in the model. The proof of the GET is given in a pdf on Blackboard. It is not examinable.",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Continuous and exact designs</span>"
    ]
  },
  {
    "objectID": "Continuous and exact designs.html#the-general-equivalence-theorem",
    "href": "Continuous and exact designs.html#the-general-equivalence-theorem",
    "title": "8  Continuous and exact designs",
    "section": "",
    "text": "The General Equivalence Theorem (GET)\n\n\n\n\nTheorem 8.1 The following three statements are equivalent.\n\nThe design \\(\\xi^*\\) maximises \\(|\\mathbf{M}(\\xi )|\\).\nThe design \\(\\xi^*\\) minimises \\(\\max_{\\mathbf{x}\\in \\Omega}d(\\mathbf{x},\\xi)\\).\n\\(\\max_{\\mathbf{x}\\in \\Omega }d(\\mathbf{x}, \\xi^*)=p\\).\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\nExample 8.1 In Question 2 of Exercise 3.1 we showed for the quadratic regression model that the design which gives equal weights to the points \\(-1, 0\\) and 1 satisfies \\[\\max _{x \\in [-1,1] }d(x) = 3\\] which is the number of parameters in the model. We also showed that this design is \\(D\\)-optimal over a restricted range of choices, and that the maximum in the above is achieved at each of the points of support of the design. The proof of the GET now tells us that this latter is no accident, and, more importantly, that the design is both \\(D\\)-optimal and \\(G\\)-optimal over all possible choices of design for this model.\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\nExample 8.2 Still with the quadratic regression model, consider the design \\[\\xi = \\left( \\begin{array}{cccc}-1&-\\frac{1}{3}&\\frac{1}{3}&1\\\\ \\frac{1}{4}&\\frac{1}{4}&\\frac{1}{4}&\\frac{1}{4}\\end{array}\\right).\\] In this case, with one observation at each of the four points, \\[\\mathbf{x}= \\left( \\begin{array}{ccc}1&-1&1\\\\1&-\\frac{1}{3}&\\frac{1}{9}\\\\1&\\frac{1}{3}&\\frac{1}{9}\\\\1&1&1\\end{array}\\right)\\] and therefore \\[\\mathbf{x}^T\\mathbf{x}=\\left(\\begin{array}{ccc}4&0&\\frac{20}{9}\\\\0&\\frac{20}{9}&0\\\\ \\frac{20}{9}&0&\\frac{164}{81}\\end{array}\\right).\\] The standardised variance of prediction is \\[\\begin{align}\nd(x)&=4\\mathbf{f}(\\mathbf{x})^T(\\mathbf{x}^T\\mathbf{x})^{-1}\\mathbf{f}(\\mathbf{x})\\\\\n&= 2.562 - 3.811x^2 + 5.062x^4\\\\\n&= 5.062[(x^2-0.376)^2-0.376^2]+2.562\n\\end{align}\\] which attains its maximum value 3.814 at \\(x=\\pm 1\\). This is greater than 3, and so the design is not \\(D\\)-optimal or \\(G\\)-optimal for this model.\n\n\n\n\n\n\n\n\n\nThis is quite hard! Don’t worry if you can’t do it, but the solution will help you understand the GET a little better.\n\n\n\n\nExercise 8.1 Show that a CFD or FFD satisfies the conditions of the GET for a model with any number of product terms (but no squared terms) included, if the factors are quantitative variables, each taking values in the interval \\([-1,1]\\). Hint: consider the standardised variance of prediction, consider what sort of terms it can have, and how these might be maximised.",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Continuous and exact designs</span>"
    ]
  },
  {
    "objectID": "Tailor-made designs.html",
    "href": "Tailor-made designs.html",
    "title": "9  Tailor-made designs",
    "section": "",
    "text": "9.1 Adding a point\nHere, using \\(D\\)-optimality as a guiding principle, we look at the problem of either adding a new point to an existing design to accommodate an extra observation, or deleting a point from an existing design. The following result from matrix theory is of independent interest elsewhere, and particularly useful here.\nWe shall not prove this lemma, but merely explain its application. Suppose we have an existing design with design matrix \\(\\mathbf{x}\\) for a particular model, and we wish to add an extra point \\(\\mathbf{x}\\) to the design to yield an extra observation. Then, bearing in mind the definition of the information matrix, the information matrix of the new design may be written \\[\\mathbf{G}^*=\\mathbf{X}^T\\mathbf{X}+\\mathbf{f}(\\mathbf{x})\\mathbf{f}(\\mathbf{x})^T=\\mathbf{G}+\\mathbf{f}(\\mathbf{x})\\mathbf{f}(\\mathbf{x})^T\\] where \\(\\mathbf{G}\\) is the information matrix of the original design. If this result isn’t obvious to you then check it in the simple linear regression case with \\(n\\) observation in the original design matrix. If we now apply the lemma to determinants, with \\(n=1\\), \\(\\mathbf{A}=\\mathbf{G}\\), \\(\\mathbf{B}=\\mathbf{f}(\\mathbf{x})\\) and \\(\\mathbf{C}=\\mathbf{f}(\\mathbf{x})^T\\), then, noting that the second determinant on the right hand side is just that of a scalar, namely itself, we get \\[|\\mathbf{G}^*|=|\\mathbf{G}|\\left(1+\\mathbf{f}(\\mathbf{x})^T\\mathbf{G}^{-1}\\mathbf{f}(\\mathbf{x}) \\right).\\] It follows from the simple multiplicative factor on the right hand side that it is \\(D\\)-optimal to choose the point \\(\\mathbf{x}\\) for which this is as large as possible. But \\(\\mathbf{f}(\\mathbf{x})^T\\mathbf{G}^{-1}\\mathbf{f}(\\mathbf{x})\\) is just, apart from a multiplicative factor, the variance of prediction at the point \\(\\mathbf{x}\\). It follows that if a new point is to be added to the design, it is \\(D\\)-optimal to choose one with the largest variance of prediction (for the existing design). This is intuitive: such a point is in a part of the design region where we know least about the fit of the model.",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Tailor-made designs</span>"
    ]
  },
  {
    "objectID": "Tailor-made designs.html#adding-a-point",
    "href": "Tailor-made designs.html#adding-a-point",
    "title": "9  Tailor-made designs",
    "section": "",
    "text": "Lemma\n\n\n\n\nLemma 9.1 If \\(\\mathbf{A}\\) is a non-singular \\(p\\times p\\) matrix, \\(\\mathbf{B}\\) is a \\(p\\times n\\) matrix, \\(\\mathbf{C}\\) is a \\(n\\times p\\) matrix and \\(\\mathbf{I}\\) is the \\(n\\times n\\) identity matrix, then \\[|\\mathbf{A}+\\mathbf{B}\\mathbf{C}|=|\\mathbf{A}||\\mathbf{I}+\\mathbf{C}\\mathbf{A}^{-1}\\mathbf{B}|.\\]",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Tailor-made designs</span>"
    ]
  },
  {
    "objectID": "Tailor-made designs.html#deleting-a-point",
    "href": "Tailor-made designs.html#deleting-a-point",
    "title": "9  Tailor-made designs",
    "section": "9.2 Deleting a point",
    "text": "9.2 Deleting a point\nWe can work out a similar result for deleting an existing point \\(\\mathbf{x}\\) from a design. The plus signs may be replaced by minus signs in the lemma, and in the modification of the information matrix, and so we can say that the new information matrix has determinant \\[|\\mathbf{G}^*|=|\\mathbf{G}|\\left(1-\\mathbf{f}(\\mathbf{x})^T\\mathbf{G}^{-1} \\mathbf{f}(\\mathbf{x})\\right)\\] and so it is \\(D\\)-optimal to delete a point of the existing design at which the variance of prediction is least.",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Tailor-made designs</span>"
    ]
  },
  {
    "objectID": "Tailor-made designs.html#adding-two-points",
    "href": "Tailor-made designs.html#adding-two-points",
    "title": "9  Tailor-made designs",
    "section": "9.3 Adding two points",
    "text": "9.3 Adding two points\nIf we wish to modify a design by changing more than one point, the situation is not so clear: doing it optimally stepwise one point at a time is not obviously going to produce a design which is optimal among all choices of more than one design point, although often it will.\n\n\n\n\n\n\nExample\n\n\n\n\nExample 9.1 Quadratic regression on one explanatory variable. Suppose we have taken one observation at \\(x=-1\\), one observation at \\(x=0\\) and two observations at \\(x=1\\). Then we have \\[\\mathbf{x}= \\left( \\begin{array}{ccc} 1 & -1 & 1 \\\\ 1 & 0 & 0 \\\\ 1 & 1& 1 \\\\ 1 & 1 & 1 \\end{array} \\right) ~~\\text{and}~~ \\mathbf{G}= \\left(\\begin{array}{ccc} 4 & 1 & 3 \\\\ 1 & 3 & 1 \\\\ 3 & 1 & 3 \\end{array}\\right).\\] Then \\[\\mathbf{G}^{-1}=\\frac{1}{8}\\left(\\begin{array}{ccc}8&0&-8\\\\0&3&-1\\\\-8&-1&11\\end{array}\\right)\\] and \\[\\begin{align}\n\\mathbf{f}(\\mathbf{x})^T\\mathbf{G}^{-1}\\mathbf{f}(\\mathbf{x})&=\\frac{1}{8} \\left(\\begin{array}{ccc} 1 & x & x^2 \\end{array} \\right) \\left(\n\\begin{array}  {ccc} 8 & 0 & -8 \\\\ 0 & 3 & -1 \\\\ -8 & -1 & 11\n\\end{array} \\right) \\left( \\begin{array}{c} 1 \\\\ x \\\\ x^2\n\\end{array} \\right) \\\\\n&=\\frac{1}{8}\\left( 8 - 13x^2 - 2x^3 + 11 x^4 \\right)\n\\end{align}\\] This quartic (fourth order polynomial) has a local maximum value of 1 at \\(x=0\\), as may be checked by calculus. It also takes the value 1 at \\(x=-1\\), the left-hand endpoint of the interval, and it takes a smaller value at the other endpoint, \\(x=1\\). We conclude from this that, if an extra point is to be added to the design, \\(x=0\\) and \\(x=-1\\) are equally \\(D\\)-optimal. To check this sketch the curve. So according to the \\(D\\) optimality criterion, we can add either one of these two points . The following task asks you to work out the next \\(D\\)-optimal point to add, assuming we have added \\(x=-1\\).\n\n\n\n\n\n\n\n\n\nThis is a good revision exercise. Hint: it follows a fairly similar pattern to the example above.\n\n\n\n\nExercise 9.1 Assume that \\(x=-1\\) is added to the design in Example 9.1. Is it true that it is now \\(D\\)-optimal to add the point \\(x=0\\) and hence restore the symmetry of the design?\n\n\n\nIf the intention is merely to investigate the effect of a two-stage modification of the design, the following lemma comes in useful. Again, we state the lemma without proof.\n\n\n\n\n\n\nLemma\n\n\n\n\nLemma 9.2 Under the same conditions as in Lemma 9.1, \\[(\\mathbf{A}+\\mathbf{B}\\mathbf{C})^{-1}=\\mathbf{A}^{-1}-\\mathbf{A}^{-1}\\mathbf{B}(\\mathbf{I}+\\mathbf{C}\\mathbf{A}^{-1}\\mathbf{B})^{-1} \\mathbf{C}\\mathbf{A}^{-1}\\] provided the inverses on both sides exist.\n\n\n\nThe application of this is when, say, we first add a new point \\(\\mathbf{x}\\) to the design, changing the information matrix from \\(\\mathbf{G}\\) to \\(\\mathbf{G}_1\\), say, and then add another new point \\(\\mathbf{y}\\), changing it to \\(\\mathbf{G}_2\\), say. We then have \\[\\begin{align}\n|\\mathbf{G}_2|&=|\\mathbf{G}_1|\\left(1+\\mathbf{f}(\\mathbf{y})^T\\mathbf{G}_1^{-1}\\mathbf{f}(\\mathbf{y})\\right) \\\\\n&=|\\mathbf{G}_1|\\left(1+\\mathbf{f}(\\mathbf{y})^T(\\mathbf{G}+\\mathbf{f}(\\mathbf{x})\\mathbf{f}(\\mathbf{x})^T)^{-1}\\mathbf{f}(\\mathbf{y}) \\right) \\\\\n&=|\\mathbf{G}_1|\\left( 1 + \\mathbf{f}(\\mathbf{y})^T\\mathbf{G}^{-1}\\mathbf{f}(\\mathbf{y})-\\mathbf{f}(\\mathbf{y})^T\\mathbf{G}^{-1}\\mathbf{f}(\\mathbf{x})(1 + \\mathbf{f}(\\mathbf{x})^T\\mathbf{G}^{-1}\\mathbf{f}(\\mathbf{x}))^{-1}\\mathbf{f}(\\mathbf{x})^T\\mathbf{G}^{-1}\\mathbf{f}(\\mathbf{y})\\right)\n\\end{align}\\] using Lemma 9.2 with the same interpretations of the matrices as in the application of Lemma 9.1. This expression now simplifies because the right hand side consists of scalar quantities, and also because we may substitute \\(|\\mathbf{G}_1| = |\\mathbf{G}|(1 + \\mathbf{f}(\\mathbf{x})^T\\mathbf{G}^{-1}\\mathbf{f}(\\mathbf{x}))\\). This gives \\[|\\mathbf{G}_2| = |\\mathbf{G}|\\left\\{ \\left( 1 + \\mathbf{f}(\\mathbf{x})^T\\mathbf{G}^{-1}\\mathbf{f}(\\mathbf{x}\n) \\right) \\left( 1 + \\mathbf{f}(\\mathbf{y})^T\\mathbf{G}^{-1}\\mathbf{f}(\\mathbf{y}) \\right) -\n\\left( \\mathbf{f}(\\mathbf{x})^T\\mathbf{G}^{-1}\\mathbf{f}(\\mathbf{y}) \\right) ^2 \\right\\}.\\] Using this we can assess the effect on the information matrix of adding two new points \\(x\\) and \\(y\\).",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Tailor-made designs</span>"
    ]
  },
  {
    "objectID": "Tailor-made designs.html#changing-a-point",
    "href": "Tailor-made designs.html#changing-a-point",
    "title": "9  Tailor-made designs",
    "section": "9.4 Changing a point",
    "text": "9.4 Changing a point\nSimilarly if we delete an existing point \\(\\mathbf{x}\\) from a design and then add a new point \\(\\mathbf{y}\\) we get \\[|\\mathbf{G}_2| = |\\mathbf{G}|\\left\\{ \\left( 1 - \\mathbf{f}(\\mathbf{x})^T\\mathbf{G}^{-1}\\mathbf{f}(\\mathbf{x})  \\right) \\left( 1 + \\mathbf{f}(\\mathbf{y})^T\\mathbf{G}^{-1}\\mathbf{f}(\\mathbf{y}) \\right) + \\left( \\mathbf{f}(\\mathbf{x})^T\\mathbf{G}^{-1}\\mathbf{f}(\\mathbf{y}) \\right) ^2 \\right\\} .\\]\n\n\n\n\n\n\nExample\n\n\n\n\nExample 9.2 Simple linear regression with four observations at \\(x=-1, -1, 0\\) and \\(1\\). In this case, \\[\\mathbf{G}= \\left( \\begin{array}{cc} 4 & -1 \\\\ -1 & 3 \\end{array}\n\\right)~~\\text{and}~~\\mathbf{G}^{-1} = \\frac{1}{11}\\left(\\begin{array}{cc} 3 & 1 \\\\ 1& 4 \\end{array} \\right).\\] Suppose we replace one of the observations at \\(x=-1\\) with one at \\(x=1\\). Then the relevant quantities are \\[\\begin{align}\n\\mathbf{f}(-1)^T\\mathbf{G}^{-1}\\mathbf{f}(-1) &= \\frac{1}{11} \\left(\n\\begin{array}{cc} 1 & -1 \\end{array} \\right) \\left(\n\\begin{array}{cc} 3 & 1 \\\\ 1 & 4 \\end{array} \\right) \\left(\n\\begin{array}{c} 1 \\\\ -1 \\end{array} \\right) = \\frac{5}{11} \\\\\n\\mathbf{f}(1)^T\\mathbf{G}^{-1}\\mathbf{f}(1) &= \\frac{1}{11} \\left(\n\\begin{array}{cc} 1 & 1 \\end{array} \\right) \\left(\n\\begin{array}{cc} 3 & 1 \\\\ 1 & 4 \\end{array} \\right) \\left(\n\\begin{array}{c} 1 \\\\ 1 \\end{array} \\right) = \\frac{9}{11} \\\\\n\\text{and}~~\\mathbf{f}(-1)^T\\mathbf{G}^{-1}\\mathbf{f}(1) &= \\frac{1}{11} \\left(\n\\begin{array}{cc} 1 & -1 \\end{array} \\right) \\left(\n\\begin{array}{cc}  3 & 1 \\\\ 1 & 4 \\end{array} \\right) \\left(\n\\begin{array}{c} 1 \\\\ 1 \\end{array} \\right) = -\\frac{1}{11}\n\\end{align}\\] and so, substituting, \\[|\\mathbf{G}_2|=|\\mathbf{G}|\\left\\{\\left(1-\\frac{5}{11}\\right)\\left(1+\\frac{9}{11}\\right)+\\left(-\\frac{1}{11}\\right)^2\\right\\}=|\\mathbf{G}|.\\] In this case, the result is not surprising, since the design is merely being reflected about the origin, and so we would expect its essential properties not to change.\n\n\n\n\n\n\n\n\n\nThese are both good revision exercises.\n\n\n\n\nExercise 9.2  \n\nFind an optimal design with two observations for the model \\[\\begin{equation} E(Y)=\\beta_1x_1+\\beta_2x_2 \\end{equation}\\] using the points \\((-1,-1), (1,-1), (-1,1)\\) and \\((1,1)\\) as candidate points. Does it satisfy the conditions of the General Equivalence Theorem? If a third observation were to be added to the design, where would you choose to take it?\nIn the simple linear regression model \\[\\begin{equation} E(Y)=\\beta_0+\\beta_1x \\end{equation}\\] a design has three observations taken at \\(x=-1,0\\) and \\(1\\). Investigate the effect of replacing the observation at \\(x=0\\) by an observation at \\(x=a\\neq 0\\). Which value(s) of \\(a\\) give(s) the best results?",
    "crumbs": [
      "Part I: Experimental Design",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Tailor-made designs</span>"
    ]
  },
  {
    "objectID": "Introduction to sampling.html",
    "href": "Introduction to sampling.html",
    "title": "10  Introduction to Sampling",
    "section": "",
    "text": "10.1 A classic example of the problems with self-selecting sampling\nBefore we study ‘proper’ sampling methods, it is first worth discussing ‘self-selecting’ sampling. The technique is actually quite old, but has perhaps become more popular given the ease of conducting such surveys via the internet. The idea is to send an invitation to everyone in the study population, inviting them to take part in a survey. For example, readers of a website are invited to submit their views on an issue. There is obvious potential for bias, as those with stronger views may be more likely to respond, and the study population receiving the invitation may not be representative of the target population in any case. An old example of how this can go wrong is given on the website (Kellner 2018).\nIn some cases, it may be possible to adjust for bias if the appropriate covariates are recorded (e.g.in an opinion poll for voting intention, we might ask for how the respondent previously voted). Typically, though, self-selecting samples aren’t reliable for inferring target population characteristics. They can, however, be useful for tracking changes in characteristics within particular groups.",
    "crumbs": [
      "Part II: Sampling Theory",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Introduction to Sampling</span>"
    ]
  },
  {
    "objectID": "Introduction to sampling.html#a-classic-example-of-the-problems-with-self-selecting-sampling",
    "href": "Introduction to sampling.html#a-classic-example-of-the-problems-with-self-selecting-sampling",
    "title": "10  Introduction to Sampling",
    "section": "",
    "text": "“A biased sample is a biased sample, however large it is. One celebrated example of this was the US Presidential Election in 1936. A magazine, Literary Digest, sent out 10 million post cards asking people how they would vote, received almost 2.3 million back and said that Alfred Landon was leading Franklin Roosevelt by 57-43 per cent. The Digest did not gather information that would allow it to judge the quality of its sample and correct, or”weight”, groups that were under- or over-represented. A young pollster called George Gallup employed a much smaller sample (though, at 50,000, it was much larger than those normally used today), but because he ensured that it was representative, he correctly showed Roosevelt on course to win by a landslide. In the event, Roosevelt won 60% and Landon just 37%. The Literary Digest closed down soon afterwards.”",
    "crumbs": [
      "Part II: Sampling Theory",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Introduction to Sampling</span>"
    ]
  },
  {
    "objectID": "Introduction to sampling.html#notation",
    "href": "Introduction to sampling.html#notation",
    "title": "10  Introduction to Sampling",
    "section": "10.2 Notation",
    "text": "10.2 Notation\nConsider a finite population of \\(N\\) units.\n\nThe \\(N\\) population values are represented by \\(X_1, \\ldots ,X_N\\)\nThe \\(n\\) sampled values are represented by \\(x_1, \\ldots ,x_n\\)\nThe proportion of the population sampled (the Sampling fraction) is \\(f=n/N\\)\nThe population mean (usual quantity of interest) is \\(\\overline{X} = \\sum_{1}^{N}X_{i}/N\\)\nThe population variance is \\[\\begin{align*}\nS^{2} &= \\frac{1}{N-1} \\sum_{1}^{N} (X_{i}-\\overline{X})^{2}\\\\\n&=\\frac{1}{N-1} \\left(\\sum_{1}^{N} X_{i}^2- N\\overline{X}^{2}\\right)\n\\end{align*}\\]\n\n\n\n\n\n\n\nImportant\n\n\n\nNote that the notational convention for random and non-random variables here is the opposite to that more commonly used: here capitals (\\(X_1, \\ldots ,X_N\\)) denote fixed population values and lower case letters \\(x_1, \\ldots ,x_n\\) denoting the sampled values are the random variables.\n\n\n\n\n\n\nKellner, Peter. 2018. “British Polling Council: A Journalist’s Guide to Opinion Polls.” Available at https://publications.parliament.uk/pa/ld201719/ldselect/ldppdm/106/10616.htm (2014/03/03).",
    "crumbs": [
      "Part II: Sampling Theory",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Introduction to Sampling</span>"
    ]
  },
  {
    "objectID": "Simple random sampling.html",
    "href": "Simple random sampling.html",
    "title": "11  Simple Random Sampling",
    "section": "",
    "text": "11.1 Estimation of Population Variance \\(S^2\\)\nThe formula for var \\((\\overline{x})\\) is used:\nHowever the formula depends on \\(S^2\\), which is usually unknown, so it is estimated from the sample, by the sample variance \\(s^2\\), defined as \\[s^2 = \\frac{1}{n-1} \\sum_1^n (x_{i}-\\overline{x})^{2} = \\frac{1}{n-1} \\left( \\left\\{\\sum_1^n x^2_{i}\\right\\}-n\\overline{x}^{2}\\right)\\] giving \\[\\widehat{\\text{var}(\\overline{x})}=\\left(1-\\frac{n}{N}\\right)\\frac{s^2}{n}.\\] where \\(\\widehat{\\text{var}(\\overline{x})}\\) is the estimated variance of the sample mean. Under SRS \\[E(s^2) = S^2.\\] and so the above is an unbiased estimator of var\\((\\overline{x})\\). We prove this as follows: \\[\\begin{align}\nE(n-1)s^2 &= E\\left( \\sum_{i=1}^n (x_i - \\overline{X} + \\overline{X}- \\overline{x})^2\\right) \\\\\n&=E\\left(\\sum_1^n(x_{i}-\\overline{X})^2-n(\\overline{x} - \\overline{X})^{2}\\right)\\\\\n&=\\frac{n(N-1)}{N} S^2 - n \\left(1-\\frac{n}{N}\\right)\\frac{S^2}{n}\\\\\n&=(n-1)S^2.\n\\end{align}\\]",
    "crumbs": [
      "Part II: Sampling Theory",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Simple Random Sampling</span>"
    ]
  },
  {
    "objectID": "Simple random sampling.html#estimation-of-population-variance-s2",
    "href": "Simple random sampling.html#estimation-of-population-variance-s2",
    "title": "11  Simple Random Sampling",
    "section": "",
    "text": "to estimate the precision obtained in the survey;\nto compare the precision obtained with that given by other sampling schemes (for example the stratified SRS considered later);\nto estimate the sample size needed to achieve a specified precision.",
    "crumbs": [
      "Part II: Sampling Theory",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Simple Random Sampling</span>"
    ]
  },
  {
    "objectID": "Simple random sampling.html#confidence-intervals-for-overlinex",
    "href": "Simple random sampling.html#confidence-intervals-for-overlinex",
    "title": "11  Simple Random Sampling",
    "section": "11.2 Confidence Intervals for \\(\\overline{X}\\)",
    "text": "11.2 Confidence Intervals for \\(\\overline{X}\\)\nConfidence intervals for \\(\\overline{X}\\) are usually based on the assumption that under SRS \\[\\overline{x} \\sim N(\\overline{X},(1-f)S^2/n)\\] approximately. Using this approximation, we then have \\[\\frac{\\overline{x}-\\overline{X}}{\\sqrt{(1-f)S^2/n}}\\sim N(0,1)\\] and if \\(S^2\\) is unknown replace it by \\(s^2\\) and assume \\[\\frac{\\overline{x}-\\overline{X}}{\\sqrt{(1-f)s^2/n}} \\sim t_{n-1}\\] and obtain \\[\\overline{x}\\pm t_{n-1,0.025} \\sqrt{(1-f)s^2/n}\\] as the approximate 95% confidence interval for \\(\\overline{X}\\), where \\(t_{n-1,0.0.025}\\) is the 0.975 quantile of the \\(t_{n-1}\\) distribution. Two reasons why the normal approximation might not be ‘good’ are\n\nthe underlying population distribution is (heavily) skewed;\nthe sample is drawn from a finite population: the distribution of \\(\\bar{x}\\) may not be well approximated by a continuous distribution.\n\nThese issues are discussed briefly below, but both issues can be investigated further using simulation.\n\n11.2.1 The normality of \\(\\overline{x}\\) in Finite populations via the CLT\nFor infinite populations with finite variance the normal distribution assumption is true asymptotically as \\(n \\rightarrow \\infty\\) by the standard central limit theorem. There are various versions of the central limit theorem for finite populations in which normality of \\(\\bar{x}\\) can be derived (see the discussion in Sugden et al. for references), but as far as I know no useful practical guidance about necessary sample sizes exists.\n\n\n\n\n\n\nExample\n\n\n\n\nExample 11.1 Confidence Interval for \\(\\overline{X}\\). A SRS of size 1000 was taken from the UK population (when it was approx 66 million) and body weight (kg) was recorded. \\(x_1, x_2, \\ldots x_{1000}\\) are the 1000 sample weights which had a mean of 78.2 kg. Further calculation gave \\(\\sum_{i=1}^{1000}x_i^2=6,241,154\\) kg\\(^2\\). Find a 95% CI for \\(\\overline X\\). We have \\[s^2 = \\frac{1}{999}(6241154-1000(78.2)^2)=126.04\\] Ignoring the finite population correction, we have that a 95% CI for \\(\\overline{X}\\)is given by \\[\\begin{align}\n&\\overline{x}\\pm t_{n-1,0.025} \\sqrt{s^2/n}   \\\\\n&=78.2 \\pm 1.962 \\sqrt{126.04/1000} \\\\\n&=(77.5, 78.9) kg\n\\end{align}\\] 1.962 is the relevant \\(t_{999}\\) quantile (from R) but the degrees of freedom is so large that we could use the normal \\(z\\) quantile.",
    "crumbs": [
      "Part II: Sampling Theory",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Simple Random Sampling</span>"
    ]
  },
  {
    "objectID": "Simple random sampling.html#choice-of-sample-size-n",
    "href": "Simple random sampling.html#choice-of-sample-size-n",
    "title": "11  Simple Random Sampling",
    "section": "11.3 Choice of Sample Size \\(n\\)",
    "text": "11.3 Choice of Sample Size \\(n\\)\nThe larger \\(n\\), the more precise is \\(\\overline{x}\\) as an estimate of \\(\\overline{X}\\), but usually the more costly the sampling. A compromise between precision and cost can often be worked out by the following method.\nThe user specifies an acceptable width \\(d\\) for a confidence interval for \\(\\overline{X}\\), and the corresponding desired level of confidence. Since the width of a \\(100(1-\\alpha)\\%\\) confidence interval for \\(\\overline{X}\\) is approximately \\[\\mbox{width}= 2 z_{1-\\alpha/2} \\sqrt{ \\left( 1-\\frac{n}{N} \\right) \\frac{S^2}{n}}\\] \\(n\\) is found by solving \\(\\mbox{width}\\leq d\\). It is easily found that \\[n \\geq \\frac{N}{1+N(d/(2Sz_{1-\\alpha/2}))^2}.\\] To use this expression \\(S^2\\) would need to be estimated. An estimate might come from a pilot survey or from previous work with this or a related population.\n\n\n\n\n\n\nExample\n\n\n\n\nExample 11.2 Sample size needed for specific precision of \\(\\overline{X}\\). Another SRS of size \\(n\\) is to be taken from the current UK population (68,900,000). Body weight (kg) will again be recorded. A 95% CI for the population mean body weight will be formed. What is the smallest sample size (\\(n\\)) that would give a 95% CI with a width of less than 1 kg?\nWe can use the estimate of \\(S^2\\) from the previous sample. The population size is \\(N=68,900,000\\), the specified CI width is \\(d=1\\), the \\(z\\) quantile is \\(z_{1-\\alpha/2} = 1.96\\) and the population variance estimate is 126.04 kg\\(^2\\). Using \\[n \\geq \\frac{N}{1+N(d/(2Sz_{\\alpha/2}))^2}\\] gives \\(n \\geq 2016.6\\) so the smallest integer sample size is \\(n=2017\\).\n\n\n\n\n\n\n\n\n\nQuestion 1 and 2 are good revision questions routine; Question 3 is harder.\n\n\n\n\nExercise 11.1  \n\nA SRS of 30 households was drawn from a city area containing 14848 households. The numbers of people in each household sampled were: \\[\\begin{array}{rrrrrrrrrrrrrrr}\n5 & 6 & 3 & 3 & 2 & 3 & 3 & 3 & 4 & 4 & 3 & 2 & 7 & 4 & 3 \\\\\n5 & 4 & 4 & 3 & 3 & 4 & 3 & 3 & 1 & 2 & 4 & 3 & 4 & 2 & 4\n\\end{array}\\] Estimate the total number of people in the area and find approximately the probability that such an estimate is within 10% of the true value.\nTwo SRSs of sizes 200 and 450 were chosen one after the other (without replacement) from a population of 2400 students in a non-residential College. Each student was asked how far (in miles) from the College that he or she lived. The sample means and variances of the distances were:\n\\[\\begin{align}\n\\overline{x}_1 &= 5.14 \\quad \\overline{x}_2 = 4.90\\\\\ns_1^2&=3.87 \\quad s_2^2=4.02\n\\end{align}\\]\n\nCalculate an approximate 99% confidence interval for the mean distance from a student`s living place to the College.\n\nA SRS of size \\(2n\\) is chosen from a finite population of size \\(N\\) (\\(N&gt;2n\\)). The population mean and variance are \\(\\overline{X}\\) and \\(S^2\\) respectively. The sample is divided into two equal parts: the first \\(n\\) observations, and the second \\(n\\) observations. Only the two sample means are available (not the actual sample values) and are given by \\(\\overline{x}_1\\) and \\(\\overline{x}_2\\). Explain why \\(k\\left(\\overline{x}_1-\\overline{x}_2\\right)^2\\) is a sensible estimator of \\(S^2\\) where \\(k\\) is a constant. Hence derive a simple unbiased estimator of \\(S^2\\) based only on \\(\\overline{x}_1,\\overline{x}_2\\) and \\(n\\) (ie find \\(k\\)).",
    "crumbs": [
      "Part II: Sampling Theory",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Simple Random Sampling</span>"
    ]
  },
  {
    "objectID": "Simple random sampling.html#estimation-of-a-proportion-p",
    "href": "Simple random sampling.html#estimation-of-a-proportion-p",
    "title": "11  Simple Random Sampling",
    "section": "11.4 Estimation of a Proportion \\(P\\)",
    "text": "11.4 Estimation of a Proportion \\(P\\)\nSuppose we wish to estimate the proportion (\\(P\\)) of units in the population that have a particular characteristic (\\(C\\)). We can still use the results from SRS to estimate the proportion \\(P\\) if we specify \\[X_i=\\left\\{\\begin{array}{ll}1 & \\mbox{if unit $i$ has characteristic $C$,}\\\\\n0 & \\mbox{if unit $i$ does not have $C$.}\\end{array}\\right.\\] Let \\(p\\) represent the proportion of the sample that have characteristic \\(C\\). Then:\n\n\\(\\overline{x} = p\\);\n\\(E(p)=E(\\overline{x})= \\overline{X} = P\\) (using SRS results);\nvar\\((p) = (1-f)S^2/n\\) (Using SRS results) where \\[S^2=\\frac{1}{N-1} \\sum_1^N (\\chi_i-P)^2 = \\frac{1}{N-1}\\left(NP - 2NP^2 + NP^2 \\right) = \\frac{NPQ}{N-1}\\] and \\(Q=1-P.\\) Since \\(P\\) is unknown we replace it with the estimator \\(p\\).\n\\(s^2=npq/(n-1)\\) is an unbiased estimator of \\(S^2\\) (using SRS results). Thus an unbiased estimate of var \\(p\\) is \\[(1-f)\\frac{p(1-p)}{n-1},\\] (using \\(N/(N-1) \\approx 1\\) for large \\(N\\)).",
    "crumbs": [
      "Part II: Sampling Theory",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Simple Random Sampling</span>"
    ]
  },
  {
    "objectID": "Simple random sampling.html#sec-approx-interval-prop",
    "href": "Simple random sampling.html#sec-approx-interval-prop",
    "title": "11  Simple Random Sampling",
    "section": "11.5 Confidence Interval for \\(P\\)",
    "text": "11.5 Confidence Interval for \\(P\\)\nAppealing to the central limit theorem with \\(N/(N-1) \\approx 1\\) gives \\[p \\sim N\\left( P, \\frac{(1-f)}{n}PQ \\right)\\] so that \\[Pr \\left(\\left| \\frac{p-P}{\\sqrt{ \\frac{1-f}{n}PQ }}\\right| \\leq z_{1-\\alpha/2} \\right) \\simeq 1- \\alpha.\\] An approximate \\(100(1-\\alpha)\\%\\) confidence interval for \\(P\\) is therefore \\[p - z_{1-\\alpha/2} \\sqrt{ (1-f)PQ/n } \\leq P \\leq p + z_{1-\\alpha/2} \\sqrt{(1-f)PQ/n}.\\] Replacing the unknown \\(PQ\\) (approximate population variance for large \\(N\\)) by the sample variance estimate \\(npq/(n-1)\\), gives the approximate interval \\[p \\pm z_{1-\\alpha/2} \\sqrt{ (1-f)\\frac{pq}{(n-1)}}.\\] If any part of the interval is outside \\([0,1]\\) then it should be truncated so that it is inside the interval.\n\n\n\n\n\n\nExample\n\n\n\n\nExample 11.3 Approximate 95% CI for population proportion \\(P\\) for large \\(N\\). A survey is undertaken to assess what proportion of undergraduate (UG) students at Sheffield University plan to study for a postgraduate (PG) degree. A SRS of 200 of the 18,000 UGs is undertaken. Of those 200 sampled, 56 plan to go on to PG study. Calculate a 95% CI for the proportion of Sheffield UGs that plan to go on to PG study.\nWe have \\(N=18,000\\), \\(n=200\\) and \\(p=56/200=0.28\\) which gives the CI as \\[\\begin{align}\n& p \\pm z_{1-\\alpha/2} \\sqrt{ (1-f)\\frac{pq}{(n-1)}} \\\\\n&=0.28 \\pm 1.96\\sqrt{\\frac{(0.28)(0.72)}{199}}\\\\\n&=(0.22, 0.34)\n\\end{align}\\]",
    "crumbs": [
      "Part II: Sampling Theory",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Simple Random Sampling</span>"
    ]
  },
  {
    "objectID": "Simple random sampling.html#sample-size-for-p",
    "href": "Simple random sampling.html#sample-size-for-p",
    "title": "11  Simple Random Sampling",
    "section": "11.6 Sample Size for \\(P\\)",
    "text": "11.6 Sample Size for \\(P\\)\nSuppose we want to find how large \\(n\\) needs to be to estimate \\(P\\) with a \\(100(1-\\alpha)\\%\\) confidence interval of specified width? We could argue as in Section 11.5 but we don’t have a sample to use to estimate \\(p\\) and \\(P\\) is unknown. We take a conservative approach by taking the largest value possible for \\(PQ = P(1-P)\\) which is \\(1/4\\), obtained when \\(P=1/2\\). Assuming \\(P=1/2\\) gives the largest \\(n\\) needed to estimate \\(P\\) with the desired precision. How conservative this value of \\(n\\) is depends on how far the true value of \\(P\\) is from 1/2. If we had other information about \\(P\\) we would use that information to obtain a smaller value of \\(n\\).\n\n\n\n\n\n\nExample\n\n\n\n\nExample 11.4 Smallest sample size needed for the CI width for \\(P\\) to be less than a specified width. A SRS is to be undertaken to assess how many UK households do not own a car. There are approx 28,000,000 households in the UK. How large a sample is required to give a 95% confidence interval for \\(P\\) that has a width less than 0.05?\nAssume \\(f\\) is small so we ignore \\(1-f\\). We require \\[2z_{1-\\alpha/2}\\sqrt{\\frac{PQ}{n}}&lt; 0.05\\] Setting \\(P=1/2\\) and using \\(z_{1-\\alpha/2}=2\\) gives \\[\\frac{4}{\\sqrt{4n}}&lt; 0.05\\] which gives \\(n&gt;1600\\). We should check that \\(f\\) is small since we assumed that this was true.",
    "crumbs": [
      "Part II: Sampling Theory",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Simple Random Sampling</span>"
    ]
  },
  {
    "objectID": "Simple random sampling.html#dependent-proportions-for-polychotomous-variables",
    "href": "Simple random sampling.html#dependent-proportions-for-polychotomous-variables",
    "title": "11  Simple Random Sampling",
    "section": "11.7 Dependent proportions for polychotomous variables",
    "text": "11.7 Dependent proportions for polychotomous variables\nSuppose for each member of the population, the characteristic of interest is polychotomous, rather than binary. For example, in a UK election poll, a respondent may declare a voting intention for one of several parties: Conservative, Labour, Lib Dem etc. The previous methods can be used for point estimation: we can estimate the proportion of Labour voters by redefining the population characteristic simply as “Labour voter” or “not Labour voter”, and we can do likewise for Conservative voters etc. However, the analysis is a little more complex if we want to quantify uncertainty about which proportion is largest, as the proportions are not independent, and one proportion does not determine the others.\nSuppose the interest is in who is likely to win out of Conservative and Labour. One way to analyse this is as follows. For member \\(i\\) of the population, define \\[\\begin{align}\nX_i &= 1 \\mbox{ if Conservative voter, and 0 otherwise},\\\\\nY_i &= 1 \\mbox{ if Labour voter, and 0 otherwise},\\\\\nZ_i &= X_i - Y_i,\n\\end{align}\\] so the population proportions of Conservative and Labour voters are \\(\\overline{X}\\) and \\(\\overline{Y}\\) respectively, with \\(\\overline{Z}\\) the difference between these two proportions. Define the corresponding sample equivalents \\(x_i, y_i\\) and \\(z_i\\). By SRS results for the sample mean, we have \\(\\bar{z}\\) an unbiased estimator of \\(\\overline{Z}\\), with sample variance given by \\[var(\\bar{z})=(1-f)\\frac{S^2_Z}{n},\\] where \\[\\begin{align}\n(N-1)S^2_Z &= \\sum_{i=1}^N(Z_i-\\overline{Z})^2\\\\\n&= N(\\overline{X} + \\overline{Y} - (\\overline{X} - \\overline{Y})^2).\n\\end{align}\\] (noting that \\(Z_i^2 = X_i + Y_i\\)). Approximately (and ignoring the finite population correction), we have \\[\\bar{z} \\sim N\\left(\\overline{Z}, \\frac{S^2_Z}{n}  \\right).\\]\nSince \\(s_z^2\\) is an unbiased estimator of \\(S_Z^2\\) (we are still taking an SRS) the 95% CI for \\(\\overline{Z}=\\overline{X}-\\overline{Y}\\) is \\[\\overline{z}\\pm s\\sqrt{\\frac{S_Z^2}{n}}\\]\n\n\n\n\n\n\nExample\n\n\n\n\nWales has a population of 3.2 million. A survey of 400 people asked for their voting intentions at the next election. In the sample, 45% said they will vote Labour, 40% will vote Plaid Cymru and 15% will vote Other. Give a 95% CI for the population Labour lead over Plaid Cymru.\nIn this example let \\(X_i=1\\) if the \\(i^{th}\\) person will vote Labour (0 otherwise) and let \\(Y_i=1\\) if the \\(i^{th}\\) person will vote Plaid Cymru (0 otherwise). So we are interested in \\(\\overline{Z}=\\overline{X}-\\overline{Y}\\). We have \\(\\overline{x}=0.45\\) and \\(\\overline{y}=0.40\\) and we can calculate \\(s_z^2\\) as \\[s^2_z=\\frac{400}{399}(0.45+0.40-(0.45-0.40)^2)=0.85\\] So our 95% CI for \\(\\overline{Z}\\) is \\[0.05 \\pm 2 \\sqrt{\\frac{0.85}{400}}=(-0.04, 0.14)\\]\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\nExample 11.5 Suppose we want a 95% probability that our estimate of the Labour lead over Plaid Cymru will be no more than \\(\\varepsilon\\) away from the true population lead. This can be written \\(P(\\mid\\overline{z}-\\overline{Z}\\mid &lt; \\varepsilon)=0.95\\) which gives \\(2S_Z/\\sqrt{n}=\\varepsilon\\). Now, \\(S^2_Z\\) is maximised when \\(\\overline{X} = \\overline{Y} = 0.5\\), giving \\(S_Z \\simeq 1\\), which gives \\(n=(2 / \\varepsilon)^2\\).\n\n\n\n\n\n\n\n\n\nThis is a good revision exercise.\n\n\n\n\nExercise 11.2 A residential area has 5000 private houses. We wish to estimate the proportions of houses with:\n\nmore than three people living in them,\nmore than one car owned by the occupants of the house.\n\nThe estimators are required to have standard errors not exceeding 0.02 and 0.01 respectively. From other surveys it is thought that the proportions in (a) and (b) lie in the ranges \\((0.35,0.55)\\) and \\((0.10,0.20)\\) respectively. The two proportions are to be estimated from a single SRS. How large a sample is needed?",
    "crumbs": [
      "Part II: Sampling Theory",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Simple Random Sampling</span>"
    ]
  },
  {
    "objectID": "Simple random sampling.html#regression-estimator-in-srs",
    "href": "Simple random sampling.html#regression-estimator-in-srs",
    "title": "11  Simple Random Sampling",
    "section": "11.8 Regression Estimator in SRS",
    "text": "11.8 Regression Estimator in SRS\nWe now return to the problem of estimating a population mean, but consider a scenario where we can reduce the variance through the use of a related variable.\nFor example, suppose the South Yorkshire Mayor’s office wants to estimate the mean number of minutes spent walking/cycling per week by adults in Sheffield (\\(\\overline{X}\\)). Further, suppose that they know the mean number of cars per household in Sheffield (\\(\\overline{Y}\\)). Imagine drawing a SRS of Sheffield adults and measuring their weekly exercise level and the number of cars in their household. We could estimate the mean population level of exercise (\\(\\overline{X}\\)) by the sample mean \\(\\overline{x}\\), but what if the corresponding mean number of cars per houisehold of the sampled individuals (\\(\\overline{y}\\)) is very different from the known population mean \\(\\overline{Y}\\)? This result would suggest that our sample was not quite representative, and that we might improve \\(\\overline{x}\\) by adjusting it in the light of how much \\(\\overline{y}\\) differs from \\(\\overline{Y}\\).\nThe regression estimator is appropriate when \\[X_j \\approx \\overline{X} + \\beta(Y_j-\\overline{Y})\\] or in other words when \\(X\\) and \\(Y\\) are approximately linearly related, but not necessarily by a line through the origin.\nAssuming, as before, that \\(\\overline{Y}\\) is known, then we use the following as an estimator of \\(\\overline{X}\\) \\[\\overline{x}_{lr} = \\overline{x} - \\hat{\\beta}(\\overline{y} - \\overline{Y})\\] known as the linear regression estimator. For this we need to estimate \\(\\beta\\): it is natural to do so by \\(\\hat{\\beta} = s_{xy}/s^2_y\\), the usual regression estimate based on the sample \\(((x_1,y_1),\\ldots,(x_n,y_n))\\) but where we are regressing \\(X\\) on \\(Y\\).\nDefine:\nThe we have the following properties of \\(\\overline{x}_{lr}\\).\n\n\\(E(\\overline{x}_{lr}) \\simeq \\overline{X}\\).\n\\(\\text{var}(\\overline{x}_{lr}) \\simeq \\frac{1-f}{n}S_X^2(1-\\rho ^2)\\).\n\nFrom (i) and (ii) and the Normal assumption we find confidence intervals for \\(\\overline{X}\\) as \\[\\overline{x}_{lr}\\pm 1.96 \\sqrt{\\text{var}(\\overline{x}_{lr})}\\]\nProof.\nThe proof is based on assuming that \\(\\beta\\) is known to be the population value, which may be written \\(\\beta = S_{XY}/S_Y^2 = \\rho S_X/S_Y\\) (note that we’re regressing \\(X\\) on \\(Y\\) here!) \\[E(\\overline{x}_{lr}) = \\overline{X}-\\beta(\\overline{Y}-\\overline{Y}) = \\overline{X} .\\] Then, in obvious notation, \\[\\begin{align}\n\\mbox{var}(\\overline{x}_{lr}) &= \\mbox{var}(\\overline{x}-\\beta\\overline{y}) \\\\\n&= \\frac{1-f}{n}S^2_{X-\\beta Y}\\\\\n&=S_X^2(1-\\rho ^2).\n\\end{align}\\] This follows since \\[\\begin{align}\nS^2_{X-\\beta Y} & =  S_X^2 - 2 \\beta S_{XY} + \\beta ^2S_Y^2 \\\\\n&= S_X^2 - 2\\left( \\rho \\frac{S_X}{S_Y}\\right) \\rho S_XS_Y +\\left( \\rho \\frac{S_X}{S_Y}\\right) ^2 S_Y^2 \\\\\n&=S_X^2(1-\\rho ^2).\n\\end{align}\\]\nFrom the above, \\(\\text{var}(\\overline{x}_{lr}) \\leq \\mbox{var}(\\overline{x})\\), with equality iff \\(\\rho =0\\).\n\n\n\n\n\n\nExample\n\n\n\n\nExample 11.6 Consider the motivating example in which the South Yorkshire Mayor’s office wants to know how many minutes per week Sheffield adults spend walking or cycling (with the aim of increasing it through active travel initiatives). They take a SRS of 100 Sheffield adults and ask them how many minutes, on average, they spend walking/cycling per week and how many cars they have in their household. They believe an approximate linear relationship between these two variables is plausible. The data they record is shown in Figure @ref(fig:xlrplot).\nThe gradient estimate is -29.8, the mean number of cars per household in the sample was 1.05 and the mean number of minutes walking/cycling in the sample was 132. If the Mayor’s office knows that the mean number of cars per household in Sheffield is 1.45, calculate a 95% confidence interval for the mean number of minutes spent walking or cycling by adults in Sheffield using\n\njust the data on the number of minutes spent walking/cycling per week;\nthe data on both variables and the linear regression estimate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCall:\nlm(formula = x ~ y)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-163.192  -49.432    2.831   47.120  129.235 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  163.192      9.838  16.587  &lt; 2e-16 ***\ny            -29.785      6.618  -4.501 1.87e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 69.65 on 98 degrees of freedom\nMultiple R-squared:  0.1713,    Adjusted R-squared:  0.1628 \nF-statistic: 20.26 on 1 and 98 DF,  p-value: 1.867e-05",
    "crumbs": [
      "Part II: Sampling Theory",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Simple Random Sampling</span>"
    ]
  },
  {
    "objectID": "Stratified sampling.html",
    "href": "Stratified sampling.html",
    "title": "12  Stratified Sampling: Introduction",
    "section": "",
    "text": "12.1 Notation\nSuppose there are \\(l\\) strata, with sizes \\(N_i, i=1,\\dots,l\\), so that \\(\\sum_1^lN_i=N\\). Members of \\(i\\)th stratum denoted by \\(X_{i,j}, j=1,\\dots,N_i\\).\nThe population mean of the \\(i\\)th stratum is \\(\\overline{X}_i\\) and its variance is \\[S_i^2=\\frac{1}{N_i-1} \\sum_{j=1}^{N_i}(X_{i,j}-\\overline{X}_i)^2.\\] Then the population mean is \\[\\overline{X}=\\frac{1}{N}\\sum_{i=1}^l\\sum_{j=1}^{N_i}X_{i,j}=\\sum_{i=1}^l\\frac{N_i}{N}\\overline{X}_i,\\] is a weighted mean of the stratum means \\(\\overline{X}_i\\), and \\[\nS^2 =\\frac{1}{N-1} \\sum_{i=1}^l\n\\sum_{j=1}^{N_i}(X_{i,j}-\\overline{X})^2\\]\n\\[= \\frac{1}{N-1} \\left\\{\\sum_{i=1}^l (N_i-1)S_i^2+\\sum_{i=1}^lN_i (\\overline{X}_i - \\overline{X})^2 \\right\\} \\tag{12.1}\\] where inside the curly brackets we have a decomposition of the total sum of squares of the population values about their mean into a pooled within-stratum sum of squares and a between-strata sum of squares, identical algebraically to that found in the one-way analysis of variance. We will use this result in Section 12.3.\nSuppose we draw a SRS of size \\(n_i\\) from the \\(i\\)th stratum, for \\(i=1,\\dots,l\\). Let \\(n=\\sum_1^l n_i\\) denote the size of the combined sample, and \\(f_i=n_i/N_i\\) the sampling fraction for stratum \\(i\\), \\(i=1,\\dots,l\\). Let \\(x_{ij}, j=1,\\dots,n_i\\) denote the observations from the SRS from stratum \\(i\\), and \\(\\overline{x}_i, s_i^2\\) the corresponding sample mean and variance. Initially we take \\(n\\) and the \\(n_i\\) to be fixed.",
    "crumbs": [
      "Part II: Sampling Theory",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Stratified Sampling: Introduction</span>"
    ]
  },
  {
    "objectID": "Stratified sampling.html#estimation-of-overlinex",
    "href": "Stratified sampling.html#estimation-of-overlinex",
    "title": "12  Stratified Sampling: Introduction",
    "section": "12.2 Estimation of \\(\\overline{X}\\)",
    "text": "12.2 Estimation of \\(\\overline{X}\\)\nThe overall sample mean \\(\\overline{x}^\\prime = \\sum_i\\sum_j x_{ij}/n = \\sum_i n_i \\overline{x}_i/n\\) has expectation \\[E(\\overline{x}^\\prime )=\\frac{1}{n} \\sum_1^l n_i \\overline{X}_i\\] evidently equal to \\(\\overline{X}\\) if \\(n_i = nN_i/N\\), and so \\(\\overline{x}^\\prime\\) in general is biased for \\(\\overline{X}\\), except if \\(f_i = \\mbox{constant} = n/N\\), the so-called proportional allocation of sampling effort (the size of the SRS from each stratum is proportional to the stratum size).\nThis suggests a better unbiased estimate, the stratified estimate using proportional allocation: \\[\\overline{x}_{st} = \\frac{1}{N} \\sum_1^l N_i \\overline{x}_i.\\]\nIt can be proven that whatever the \\(n_i,i=1,\\dots,l\\), under SSRS \\(\\overline{x}_{st}\\) is the best linear unbiased estimator (BLUE) of \\(\\overline{X}\\), and it has variance \\[\n\\mbox{var}(\\overline{x}_{st}) = \\sum_1^l\\left(\\frac{N_i}{N}\\right)^2\\frac{1-f_i}{n_i} S_i^2.\n\\tag{12.2}\\] Usually the stratum population variances \\(S_i^2\\) are unknown, and so they are estimated by the corresponding stratum sample variances \\(s_i^2\\), which by SRS are unbiased. Confidence intervals for \\(\\overline{X}\\) may then be based on a normal approximation in the usual way.\n\n\n\n\n\n\nThis is quite a hard exercise! You won’t be asked prove results such as this in the exam.\n\n\n\n\nExercise 12.1 Prove Equation 12.2. You may quote results for SRS. (For completeness, the solution to this exercise will also show that \\(\\overline{x}_{st}\\) is BLUE).\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\nExample 12.1 Sheffield University wants to estimate the mean CO\\(_2\\) transport emissions of its staff. The university has 2000 employees. Three transport mode strata have been identified: car, bus/tram/train and walk/cycle. The strata sizes are 200(car), 500(bus/tram/train) and 1300 (walk/cycle). A stratified SRS of 100 is to be taken. Using proportional allocation, the sample sizes are 10, 25 and 65 respectively. The results of the sample are as follows:\n\n\n\n\nSample mean (tonnes)\nSample variance (tonnes\\(^2\\))\n\n\ncar\n0.82\n0.35\n\n\nbus/tram/train\n0.28\n0.26\n\n\nwalk/cycle\n0.13\n0.02\n\n\n\nCalculate a 95% CI for the mean CO\\(_2\\) transport emissions of its staff based on these results. \\[\\overline{x}_{st}=0.1(0.82)+0.25(0.28)+0.65(0.13)=0.24 ~\\text{tonnes of CO}_2.\\] Using proportional allocation we have \\(f_i=n/N=100/2000=0.05\\) which is small enough to ignore. This gives the estimator variance as: \\[\\text{var}(\\overline{x}_{st}) = (0.1)^2\\frac{0.35}{10}+(0.25)^2\\frac{0.26}{25}+(0.65)^2\\frac{0.02}{65}=0.0011\\] The 95% CI is then \\[\\overline{x}_{st} \\pm 2 \\sqrt{\\text{var}(\\overline{x}_{st})}=0.24 \\pm 2\\sqrt{0.0011}=(0.17, 0.31)\\]",
    "crumbs": [
      "Part II: Sampling Theory",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Stratified Sampling: Introduction</span>"
    ]
  },
  {
    "objectID": "Stratified sampling.html#sec-stratvsrs",
    "href": "Stratified sampling.html#sec-stratvsrs",
    "title": "12  Stratified Sampling: Introduction",
    "section": "12.3 Stratification vs SRS",
    "text": "12.3 Stratification vs SRS\nWhen is \\(\\overline{x}_{st}\\) better than \\(\\overline{x}\\) from SRS? Both are unbiased, and so we compare variances. Using Equation 11.1 and Equation 12.2, it can then be shown that \\(\\mbox{var}(\\overline{x})\\geq\\mbox{var}(\\overline{x}_{st})\\) requires \\[\n\\sum N_i(\\overline{X}_i - \\overline{X})^2 \\geq\\sum \\left(1-\\frac{N_i}{N}\\right) S_i^2.\n\\tag{12.3}\\]\nThe term on the left hand side is the between-strata sum of squares, and the one on the right hand side is a measure of the within-stratum variance. Thus \\(\\overline{x}_{st}\\) will have smaller variance than \\(\\overline{x}\\), i.e. SSRS will be preferable to SRS, when the strata are homogeneous (small within-stratum variance) but well-separated (large difference between stratum means).\n\n\n\n\n\n\nAgain, you won’t be asked to prove results like this in the exam, but this is a little more straightfoward, using the equations suggested.\n\n\n\n\nExercise 12.2 Prove Equation 12.3 using Equation 12.1, Equation 11.1 and Equation 12.2.",
    "crumbs": [
      "Part II: Sampling Theory",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Stratified Sampling: Introduction</span>"
    ]
  },
  {
    "objectID": "Stratified sampling.html#choice-of-sample-sizes-n-n_1dotsn_l",
    "href": "Stratified sampling.html#choice-of-sample-sizes-n-n_1dotsn_l",
    "title": "12  Stratified Sampling: Introduction",
    "section": "12.4 Choice of Sample Sizes \\(n, n_1,\\dots,n_l\\)",
    "text": "12.4 Choice of Sample Sizes \\(n, n_1,\\dots,n_l\\)\nAs before, the larger the samples within strata, the more precise the estimation. However there is usually a cost associated with sampling that inhibits use of very large samples. The best values will depend therefore on the detailed costs of sampling in relation to the value of precision of estimation. The following captures the main features of many cases, at least approximately.\nSuppose that the cost of the survey consists of a basic overhead cost, \\(c_0\\) say, and a cost associated with each unit measured. This unit measurement cost could differ between strata. We denote its value in stratum \\(i\\) by \\(c_i\\). Then the overall cost, \\(C\\) say, of the survey is of the form \\[C = c_0+\\sum_{i=1}^l c_in_i.\\]\nSuppose we wish to choose the \\(n_i\\) to minimise var \\(\\overline{x}_{st}\\) for a specified total cost \\(C\\). (We might equally wish to minimise cost for a specified variance: the argument in that case is similar.)\n\n12.4.1 Minimising Variance Within a Fixed Cost.\nWe want to choose the \\(n_i\\) to minimize \\[\\begin{align}\n\\mbox{var}(\\overline{x}_{st}) & =  \\sum_1^l\\left(\\frac{N_i}{N}\\right)^2\\left(1-\\frac{n_i}{N_i}\\right)\\frac{S_i^2}{n_i}\\\\\n& = \\sum_1^l \\left(\\frac{N_i}{N}\\right)^2\\frac{S_i^2}{n_i}-\\sum_1^l \\left(\\frac{N_i}{N}\\right)^2\\frac{S_i^2}{N_i}\n\\end{align}\\] subject to \\[c_0+\\sum_1^l c_in_i = C.\\] Using Lagrange multipliers, we find that the optimal size of sample from the \\(i\\)th stratum is \\[n_i \\propto \\frac{N_iS_i}{\\sqrt{c_i}}\\mbox{~~~~for~}i=1,\\dots,l.\\] As one might expect, we should take a larger sample in a given stratum if\n\nthe stratum is larger,\nthe stratum is more variable internally,\nsampling is cheaper in the stratum.\n\nPutting the formula for \\(n_i\\) in the cost constraint, we can deduce the constant of proportionality, and the optimal total number of observations for cost \\(C\\): \\[\\begin{align}\nn_i &=\\frac{(C-c_0)\\frac{N_iS_i}{\\sqrt{c_i}}}{\\sum_1^lN_iS_i\\sqrt{c_i}},\\\\\nn &= \\frac{(C-c_0)\\sum_1^l \\frac{N_iS_i}{\\sqrt{c_i}}}{\\sum_1^lN_iS_i\\sqrt{c_i}}.\n\\end{align}\\] (In practice we would round \\(n\\) and \\(n_i\\) to the nearest integers.) To use this result, we need to know the sizes \\(N_i\\) of the strata and the stratum variances \\(S_i^2\\). The latter are usually unknown and would have to be estimated from a pilot survey or from knowledge of another similar survey.\n\n\n12.4.2 Special Case: \\(c_i\\) constant.\nIn the case when measurement of a unit costs the same whichever stratum it comes from, so that the overall cost is \\(C=c_0+cn\\), then \\[\\begin{align}\nn & =  \\frac{C-c_0}{c} \\\\\n\\mbox{and}\\; n_i & =  \\frac{nN_iS_i}{\\sum_1^l N_iS_i}\\; \\mbox{for} i=1,\\dots,l.\n\\end{align}\\] This is called the Neyman allocation. For the Neyman allocation, we have \\[\\mbox{var~}\\overline{x}_{st} =  \\frac{1}{N^2} \\frac{ (\\sum N_iS_i)^2}{n} - \\frac{ \\sum N_iS_i^2 }{N^2}.\\]\n\n\n\n\n\n\nExample\n\n\n\n\nExample 12.2 A large village council wants to know the proportion of households that have good insulation in the village. They inspect properties in order to make this assessment. The inspection costs are different according to whether the property is in the centre of the village, on the outskirts or very remote.\n\n\n\n\ncost per property (£100s)\nnumber of properties (1000s)\n\n\nVillage center\n1\n8\n\n\nOutskirts\n4\n2\n\n\nRemote\n9\n1\n\n\n\nThe baseline cost (overheads etc) is £10,000 and the overall budget is £15,000. The Council has no information about the strata variances so assumes that \\(S_1=S_2=S_3\\). Suppose the Council wants to minimise the variance of the estimate of the proportion of households with good insulation. What sample sizes should they use in each strata?\n\n\n\n\n\n\n\n\n\nThis is to help you with the definitions. Note that the formula for the Neyman allocation is on the exam formula sheet.\n\n\n\n\nExercise 12.3 Show that if all the stratum variances are the same, the Neyman allocation is the same as proportional allocation.",
    "crumbs": [
      "Part II: Sampling Theory",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Stratified Sampling: Introduction</span>"
    ]
  },
  {
    "objectID": "Stratified sampling.html#stratification-after-sampling",
    "href": "Stratified sampling.html#stratification-after-sampling",
    "title": "12  Stratified Sampling: Introduction",
    "section": "12.5 Stratification After Sampling",
    "text": "12.5 Stratification After Sampling\nOften the stratum to which a unit belongs is unknown until after sampling (for example: age, educational level). If the \\(N_i\\) are known (e.g. from census) a possibility is to take an SRS and then classify units into strata and use as estimator of \\(\\overline{X}\\) \\[\\overline{x}_w = \\sum \\left( \\frac{N_i}{N}\\right)\\overline{x}_i\\] where \\(\\overline{x}_i\\) is the sample mean of the units that happened to be in stratum \\(i\\). This ‘corrects’ the sample so that it has the same profile as the population. The technical difference here compared to the standard stratified estimator is that the \\(n_i\\) are now random.\nAs an example, suppose we sample 100 people from the population to estimate average weight, but find sample contains only 17 men instead of the 50% in the population. The above suggests using \\(\\overline{x}_w = 0.5\\overline{x}_{women} +0.5 \\overline{x}_{men}\\) instead of the usual SRS estimator \\(\\overline{x}\\).\n\n\n\n\n\n\nThis is a good revision question.\n\n\n\n\nExercise 12.4 The following data show the stratification of all the farms in a county by farm size. Also shown is the average number of acres of cereal per farm in each stratum, and the corresponding stratum standard deviations.\n\n\n\nFarm size\nNumber of farms\nAverage cereal\nStd. deviation\n\n\n(acres)\n(\\(N_i\\))\nacreage (\\(\\overline{X}_i\\))\n(\\(S_i\\))\n\n\n0-40\n394\n5.4\n8.3\n\n\n41-80\n461\n16.3\n13.3\n\n\n81-120\n391\n24.3\n15.1\n\n\n121-160\n334\n34.5\n19.8\n\n\n161-200\n169\n42.1\n24.5\n\n\n201-240\n113\n50.1\n26.0\n\n\n241-\n148\n63.8\n35.2\n\n\n\nTotal=2010\n\\(\\overline{X} = 26.3\\)\n\n\n\n\nIt is proposed to use a sample of 100 farms to estimate total cereal acreage in the county. Find the sample sizes in each stratum for SSRS under:\n\nproportional allocation;\nNeyman allocation.\n\nCompare the precision of these methods with that of simple random sampling.",
    "crumbs": [
      "Part II: Sampling Theory",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Stratified Sampling: Introduction</span>"
    ]
  },
  {
    "objectID": "Cluster sampling.html",
    "href": "Cluster sampling.html",
    "title": "13  Cluster Sampling",
    "section": "",
    "text": "13.1 One-stage cluster sampling with equal cluster sizes\nAnalysis of cluster sampling is difficult unless we make the assumption that clusters are equal-sized. (If the clusters are not equal-sized, the total sample size will be random, even though the number of clusters sampled is fixed).\nSuppose there are \\(L\\) clusters, each of size \\(K\\): \\(LK=N\\). Take SRS of \\(l\\) of the clusters and totally enumerate members, giving \\(\\{x_{ij}\\}, i=1,\\ldots,l,\\;j=1,\\ldots,K\\). Consider \\[\\overline{x}_{cl} = \\frac{1}{lK} \\sum_{1}^{l} \\sum_{1}^{K} x_{ij}.\\]\nProperties:\nwhere \\(f=l/L=lK/LK\\) is the sampling fraction, and \\(\\overline{X}_i\\) is the mean of the \\(i\\)th cluster. We can estimate \\[S_{bc}^{2} = \\frac{1}{L-1} \\sum_1^L(\\overline{X}_i-\\overline{X})^2\\quad \\mbox{the between-cluster variance}\\] by \\[s_{bc}^2=\\frac{1}{l-1} \\sum _1^l(\\overline{x}_i-\\overline{x})^2\\] which by SRS results is unbiased for it.",
    "crumbs": [
      "Part II: Sampling Theory",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Cluster Sampling</span>"
    ]
  },
  {
    "objectID": "Cluster sampling.html#one-stage-cluster-sampling-with-equal-cluster-sizes",
    "href": "Cluster sampling.html#one-stage-cluster-sampling-with-equal-cluster-sizes",
    "title": "13  Cluster Sampling",
    "section": "",
    "text": "\\(E(\\overline{x}_{cl})=\\overline{X}\\).\n\\(\\mbox{var}(\\overline{x}_{cl}) = \\frac{1-f}{l} \\frac{1}{L-1}\\sum_1^L(\\overline{X}_i-\\overline{X})^2\\)\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\nExercise 13.1 Using the fact that \\(\\overline{x}_{cl}\\) is the mean of a SRS of size \\(l\\) drawn from the population \\(\\overline{X}_{1},\\dots, \\overline{X}_{L}\\), show that properties (1) and (2) are true. You may find the SRS properties in Chapter 11 useful here.",
    "crumbs": [
      "Part II: Sampling Theory",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Cluster Sampling</span>"
    ]
  },
  {
    "objectID": "Cluster sampling.html#comparison-with-srs",
    "href": "Cluster sampling.html#comparison-with-srs",
    "title": "13  Cluster Sampling",
    "section": "13.2 Comparison with SRS",
    "text": "13.2 Comparison with SRS\nConsider SRS with the same sampling fraction, so that the number sampled is \\(n = lK\\). Then the variance of the SRS estimator is \\[\\mbox{var~}\\overline{x} = \\left(1-\\frac{l}{L} \\right) \\frac{S^2}{lK}.\\] On the other hand the variance of the cluster estimator is \\[\\mbox{var}(\\overline{x}_{cl}) = \\left(1-\\frac{l}{L} \\right)\\frac{1}{l(L-1)} \\sum _1^L(\\overline{X}_i-\\overline{X})^2 = \\left(1-\\frac{l}{L}\\right) \\frac{S_{bc}^2}{l}.\\] We can express \\(S^2_{bc}\\) in terms of \\(S\\) via the following \\[\\begin{align}\n(N-1)S^2= (LK-1)S^2 & =\\sum_{i=1}^L \\sum_{j=1}^K (X_{ij}-\\overline{X})^2 \\\\\n&=\\sum_1^L \\sum_1^K (X_{ij}-\\overline{X}_i)^2 + \\sum_1^L K (\\overline{X}_i-\\overline{X})^{2}\\\\\n&=L(K-1)\\overline{S}_{wc}^2 + K(L-1)S_{bc}^2\n\\end{align}\\] where \\(\\overline{S}_{wc}^2\\) is the mean within-cluster variance. After some algebra, we can show \\[\\mbox{var}(\\overline{x}_{cl})-\\mbox{var}(\\overline{x})=\\left(\\frac{1}{lK}-\\frac{1}{LK}\\right)\\frac{L(K-1)}{L-1}(S^2 -\\overline{S}_{wc}^2)\\] and so \\(\\overline{x}\\) is more precise than \\(\\overline{x}_{cl}\\) when the average within-cluster variance \\(\\overline{S}_{wc}^2\\) is less than the population variance \\(S^2\\). Note that this is in contrast to the case for stratified random sampling, in which the stratified estimator is more precise than \\(\\overline{x}\\) when within-strata variance is smaller. The reason for the difference is that if clusters are similar, then the use of clusters - even large ones - may give a poorer estimate than SRS because little extra information is gained from measuring many observations.\n\n\n\n\n\n\nThis is a good revision question.\n\n\n\n\nExercise 13.2 A survey is to be taken to estimate the mean starting wage of individuals following completion of a new training course. Two pilot studies have been conducted. In the first study, a simple random sample of size 20 was used, and the following summary statistics were observed. \\[\n\\sum_{i=1}^{20} x_i=564.9, \\hspace{2cm} \\sum_{i=1}^{20} x_i^2=16131.2,\n\\] where each \\(x_i\\) is measured in 1000. In the second study, cluster sampling was used. Courses are run at different training centres around the country, with 10 students at each centre. Two training centres were selected as the clusters. The following summary statistics were observed. \\[\\begin{align}\n\\sum_{j=1}^{10} x_{1j}&=258.7,   & \\sum_{j=1}^{10} x_{1j}^2=6714.3,\\\\\n\\sum_{j=1}^{10} x_{2j}&=305.0,   & \\sum_{j=1}^{10} x_{2j}^2=9319.2,\n\\end{align}\\] where \\(x_{ij}\\) is observation \\(j\\) within cluster \\(i\\). Based on the pilot survey data, would you recommend the use of cluster sampling or simple random sampling for the new survey?",
    "crumbs": [
      "Part II: Sampling Theory",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Cluster Sampling</span>"
    ]
  },
  {
    "objectID": "Population size.html",
    "href": "Population size.html",
    "title": "14  Estimating a Population Size",
    "section": "",
    "text": "14.1 Capture-recapture sampling\nThis method for estimating a population size is as follows.\nA simple estimator of the population size \\(N\\), known as the Peterson estimator (or “Lincoln index”) is \\[\\widehat{N}_p = \\frac{nm}{r}.\\] This is based on the assumption that \\(n/N\\) should be similar to \\(r/m\\). If \\(N\\) is large relative to \\(m\\), we can suppose that \\[r\\sim Bin\\left(m, \\frac{n}{N}\\right)\\] Based on asymptotic properties of maximum likelihood estimators, we can derive an approximate variance for \\(\\widehat{N_p}\\) as \\[\\widehat{Var}(\\hat{N}_p)= \\frac{mn^2(m-r)}{r^3}\\] Clearly, this will cause difficulties if we observe \\(r=0\\). Additionally, \\(\\widehat{N}_p\\) is biased, with \\[E(\\widehat{N_p})\\simeq N\\left(1+\\frac{N-n}{mn}\\right).\\] An alternative is the Chapman estimator: \\[\\widehat{N_c} = \\frac{(n+1)(m+1)}{r+1} -1\\] which can be shown to be unbiased for \\(n+m&gt;N\\) and has estimated variance \\[\\widehat{Var}(\\widehat{N}_c)= \\frac{(n+1)(m+1)(n-r)(m-r)}{(r+1)^2(r+2)}.\\] There are two assumptions with this method.",
    "crumbs": [
      "Part II: Sampling Theory",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Estimating a Population Size</span>"
    ]
  },
  {
    "objectID": "Population size.html#capture-recapture-sampling",
    "href": "Population size.html#capture-recapture-sampling",
    "title": "14  Estimating a Population Size",
    "section": "",
    "text": "Draw (‘capture’) a random sample of size \\(n\\) (without replacement) from the population. Each member of the sample is tagged (e.g. a ring may be placed on the leg of a bird).\nRelease all members of the sample back into the population.\nDraw a second random sample of size \\(m\\) (without replacement) from the population. Observe the number of tagged individuals, denoted by \\(r\\) in the second sample.\n\n\n\nThe population size \\(N\\) does not change between the first and second samples.\nAll members of the population are equally likely to be captured; there are no ‘trap shy’ or ‘trap happy’ members.\n\n\n14.1.1 The case study as a capture-recapture experiment\nReturning to the case study, suppose there were just two lists: LA and NG. From the data in (Silverman 2016), we have\n\n94 victims were named in total on the LA list;\n567 victims were named in total on the NG list;\n18 victims were named on both lists.\n\nThe assumptions required almost certainly won’t hold, but for illustration, we could interpret this as a capture-recapture experiment:\n\n94 members of the population were ‘tagged’;\na second sample of 567 members of the population (of victims) was drawn;\n18 members of the second sample were ‘observed to be tagged’.\n\nWe then obtain \\[\\begin{align}\n\\widehat{N}_p &= 2961\\\\\n\\widehat{N}_c &= 2839, \\\\\n\\sqrt{\\widehat{Var}(\\widehat{N}_p)} &= 627\\\\\n\\sqrt{\\widehat{Var}(\\widehat{N}_c)} &= 558.\n\\end{align}\\] The estimates do at least exceed the number of known victims that only appeared on the other three lists, but otherwise aren’t very convincing. It’s not plausible that all members of the population are equally likely to be ‘captured’, and names may not appear on the two lists independently.",
    "crumbs": [
      "Part II: Sampling Theory",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Estimating a Population Size</span>"
    ]
  },
  {
    "objectID": "Population size.html#log-linear-models-for-capture-recapture-data",
    "href": "Population size.html#log-linear-models-for-capture-recapture-data",
    "title": "14  Estimating a Population Size",
    "section": "14.2 Log-linear models for capture-recapture data",
    "text": "14.2 Log-linear models for capture-recapture data\nAnother way to think about capture-recapture data is as a contingency table with a missing observation. Continuing the example with two lists only, we have\n\n\n\n\n\n\nLA\n\n\n\n\n\nno\nyes\n\n\nNG\nno\n?\n76\n\n\n\nyes\n549\n18\n\n\n\n\nso that the estimate of the population size would be \\(76+549+18\\) plus the estimate of the number of victims not appearing on either list. We will now consider how to model these data, in such a way that the missing count can be estimated. In particular, we are going to use log-linear models, a particular case of generalised linear model. This may appear unnecessarily complex, but there are benefits:\n\nit is relatively straightforward to extend the approach to multiple lists (i.e. 5-way contingency tables);\nwith more than two lists, we can incorporate dependencies between the lists;\nwe can fit the models in R and get standard errors for the parameter estimates.\n\nWe think of the counts in each cell as independent Poisson random variables, with expectations\n\n\n\n\n\n\nLA (\\(j\\))\n\n\n\n\n\nno\nyes\n\n\nNG (\\(i\\))\nno\n\\(\\mu_{11}\\)\n\\(\\mu_{12}\\)\n\n\n\nyes\n\\(\\mu_{21}\\)\n\\(\\mu_{22}\\)\n\n\n\n\nThe interpretation of this is a little difficult, in particular the distinction between observed and expected counts, but one can imagine that a repeat of the data collection process would produce different counts, with \\(\\mu_{ij}\\) representing a mean count, assuming no trends over time. The objective is therefore to estimate \\(\\mu_{11}\\). With only two lists, we will have to assume independence between NG and LA, which we express by saying that the two ratios \\(\\mu_{12}/\\mu_{11}\\) and \\(\\mu_{22}/\\mu_{21}\\) should be equal. This gives us a way to link \\(\\mu_{11}\\) to the three observed counts.\nWe parameterise \\(\\mu_{ij}\\) as follows: \\[\n\\log \\mu_{ij} = \\phi + \\alpha_i + \\beta_j,\n\\tag{14.1}\\] with \\(\\alpha_1=\\beta_1=0\\), so the total number of parameters is three (the most we could have with only three independent observations). We call this a log-linear model, because the log of the expectation is a linear function of the parameters.\n\n\n\n\n\n\nThis is to help you understand the model, but is not the sort of thing you’d be asked to show in an exam.\n\n\n\n\nExercise 14.1 Show that Equation 14.1 satisfies the independence requirement: \\(\\mu_{12}/\\mu_{11}=\\mu_{22}/\\mu_{21}\\)\n\n\n\nMaximum likelihood can be used to estimate \\(\\mu\\), \\(\\alpha_2\\) and \\(\\beta_2\\). The full-likelihood is \\[L=\\frac{e^{-\\mu_{12}}\\mu_{12}^{76}}{76!}\\times\\frac{e^{-\\mu_{21}}\\mu_{21}^{549}}{549!}\\times\\frac{e^{-\\mu_{22}}\\mu_{22}^{18}}{18!},\\] with \\(\\mu_{ij}=e^{\\phi + \\alpha_i + \\beta_j}\\). Although we could do the maximisation analytically, we will set the data up in R, and get R to do the maximisation for us. Setting up the data in R:\n\ntwo.list &lt;- data.frame(count = c(76, 549, 18),\n                       LA = as.factor(c(\"yes\", \"no\", \"yes\")),\n                       NG = as.factor(c(\"no\", \"yes\", \"yes\")))\ntwo.list\n\n  count  LA  NG\n1    76 yes  no\n2   549  no yes\n3    18 yes yes\n\nglm1 &lt;- glm(count ~ LA + NG, family = poisson(log), data = two.list)\nsummary(glm1)\n\n\nCall:\nglm(formula = count ~ LA + NG, family = poisson(log), data = two.list)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   7.7485     0.2656  29.175  &lt; 2e-16 ***\nLAyes        -3.4177     0.2395 -14.268  &lt; 2e-16 ***\nNGyes        -1.4404     0.2621  -5.495 3.91e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance:  7.8597e+02  on 2  degrees of freedom\nResidual deviance: -1.0969e-13  on 0  degrees of freedom\nAIC: 25.055\n\nNumber of Fisher Scoring iterations: 3\n\n\nThe maximum likelihood estimates are \\(\\hat{\\phi}=7.75\\), \\(\\hat{\\alpha}_2=-1.44\\) and \\(\\hat{\\beta}_2=-3.41\\). The fitted value in the missing cell is \\[e^{\\hat{\\phi}} = 2318,\\] and adding on the other cell counts give \\(\\hat{N}=2318 + 76 + 549 + 18 = 2961\\), the same as the Peterson estimator. To get an approximate 95% confidence interval, we take the estimated standard error of \\(\\hat{\\phi}\\) which is reported as 0.2656, and do \\[\\left(76 + 549 + 18 + e^{\\hat{\\phi} -2\\times 0.2656},  76 + 549 + 18 + e^{\\hat{\\phi} +2\\times 0.2656}\\right),\\] which gives (2005, 4585). (Technically, we’ve only considered an interval for the expectation of the cell count, not the actual count. But as we are assuming a Poisson distribution, the standard deviation is the square root of the mean, and so random variation about the mean is relatively small.)\n\n14.2.1 Interaction terms\nIt is unlikely that names would appear on the two lists independently. Some agencies may share names, and others may be reluctant to do so. For example, a non-governmental organisation may offer complete anonymity to potential victims. If this were the case, we wouldn’t expect the ratios \\(\\mu_{12}/\\mu_{11}\\) and \\(\\mu_{22}/\\mu_{21}\\) to be equal; \\(\\mu_{22}\\) may be relatively small, for example. Ideally, an appropriate model would be \\[\\log \\mu_{ij} = \\phi + \\alpha_i + \\beta_j+(\\alpha\\beta)_{ij},\\] with the constraints \\((\\alpha\\beta)_{i1}=(\\alpha\\beta)_{1j}=0\\), so that the interaction parameter is \\((\\alpha_\\beta)_{22}\\). Again, with only three observations, we can’t fit a model with four parameters, but such interaction terms can be included once we introduce more lists.",
    "crumbs": [
      "Part II: Sampling Theory",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Estimating a Population Size</span>"
    ]
  },
  {
    "objectID": "Population size.html#multiple-systems-estimation",
    "href": "Population size.html#multiple-systems-estimation",
    "title": "14  Estimating a Population Size",
    "section": "14.3 Multiple systems estimation",
    "text": "14.3 Multiple systems estimation\nWith more lists, we may have better ‘coverage’ of the population (different organisations may have different means of identifying victims), and we can now account for dependencies between the lists. For simplicity, we now consider three lists: LA, NG and PF. From the data in (Silverman 2016), we have\n\n\n\n\nPF = no\n\nLA\n\n\n\n\n\nno\nyes\n\n\nNG\nno\n?\n57\n\n\n\nyes\n483\n16\n\n\n\n\n\n\nPF = yes\n\nLA\n\n\n\n\n\nno\nyes\n\n\nNG\nno\n1082\n19\n\n\n\nyes\n66\n2\n\n\n\n\nWith more than two lists, we don’t have the simple equivalence with capture-recapture sampling, but the log-linear modelling framework can still be used. (Note that the more complex setting with more than two lists is referred to as multiple systems estimation). Again, we suppose each observed count is an independent Poisson random variable, with expected value \\(\\mu_{ijk}\\) given by \\[\\log \\mu_{ijk} = \\phi + \\alpha_i + \\beta_j + \\gamma_k + (\\alpha\\beta)_{ij} + (\\alpha\\gamma)_{ik}+(\\beta\\gamma)_{jk},\\] with \\(i,j,k\\) corresponding to the PF, NG and LA lists respectively, each taking values 1 or 2, corresponding to a ‘no’ or ‘yes’ respectively. We apply corner point constraints: all parameters are set to 0 except \\(\\phi,\\alpha_2,\\beta_2,\\gamma_2,(\\alpha\\beta)_{22},(\\alpha\\gamma)_{22},(\\beta\\gamma)_{22}\\). This gives seven free parameters, which is the most we can fit, as we have seven observations.\nSetting up the data in R (now with a missing value for \\(i=j=k=1\\) to make it easier to input the data into R using the rep function):\n\nthree.list &lt;- data.frame(count = c(NA, 57, 483, 16, 1082, 19, 66, 2),\n                         LA = as.factor(rep(c(\"no\", \"yes\"),2)),\n                         NG = as.factor(rep(c(\"no\", \"no\", \"yes\",\"yes\"), 2)),\n                         PF = as.factor(rep(c(\"no\", \"yes\"), each = 4)))\nthree.list\n\n  count  LA  NG  PF\n1    NA  no  no  no\n2    57 yes  no  no\n3   483  no yes  no\n4    16 yes yes  no\n5  1082  no  no yes\n6    19 yes  no yes\n7    66  no yes yes\n8     2 yes yes yes\n\n\nFitting the model:\n\nglm2 &lt;- glm(count ~ LA*NG + NG*PF + LA*PF, family = poisson(log), data = three.list)\nsummary(glm2)\n\n\nCall:\nglm(formula = count ~ LA * NG + NG * PF + LA * PF, family = poisson(log), \n    data = three.list)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  7.99610    0.80673   9.912  &lt; 2e-16 ***\nLAyes       -3.95305    0.79579  -4.967 6.78e-07 ***\nNGyes       -1.81608    0.80545  -2.255   0.0241 *  \nPFyes       -1.00953    0.80616  -1.252   0.2105    \nLAyes:NGyes  0.54562    0.75413   0.724   0.4694    \nNGyes:PFyes -0.98083    0.79541  -1.233   0.2175    \nLAyes:PFyes -0.08908    0.76139  -0.117   0.9069    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 3.3068e+03  on 6  degrees of freedom\nResidual deviance: 3.7748e-15  on 0  degrees of freedom\n  (1 observation deleted due to missingness)\nAIC: 54.782\n\nNumber of Fisher Scoring iterations: 3\n\n\nThe fitted value in the missing cell is \\[e^{\\hat{\\phi}} = 2969,\\] and adding on the other cell counts give \\(\\hat{N}=4694\\). To get an approximate 95% confidence interval, we take the estimated standard error of \\(\\hat{\\mu}\\) which is reported as 0.80673, and calculate \\[\\left(1725 + e^{\\hat{\\phi} -2\\times 0.80673},  1725 + e^{\\hat{\\phi} +2\\times 0.80673}\\right),\\] which gives (2361, 16631).\n\n\n\n\nSilverman, Bernard. 2016. “Modern Slavery: An Application of Multiple Systems Estimation.” Available at https://www.gov.uk/government/publications/modern-slavery-an-application-of-multiple-systems-estimation (2016/05/04).",
    "crumbs": [
      "Part II: Sampling Theory",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Estimating a Population Size</span>"
    ]
  },
  {
    "objectID": "Practicalities.html",
    "href": "Practicalities.html",
    "title": "15  Practicalities of Surveys: Problems at the Planning Stage",
    "section": "",
    "text": "15.1 Ethics and data protection\nA survey conducted by staff or students at a UK university for research purposes would need ethics approval first, and other organisations may have similar requirements. A brief guide to ethics approval at Sheffield is available here.\nSurveys may involve the collection of personal data. In the UK, there are legal requirements regarding the handling of personal data: the UK General Data Protection Regulation (UK GDPR) (similar to the European Union GDPR) and the Data Protection Act 2018. For an overview, see https://www.gov.uk/data-protection.",
    "crumbs": [
      "Part II: Sampling Theory",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Practicalities of Surveys: Problems at the Planning Stage</span>"
    ]
  },
  {
    "objectID": "Practicalities.html#questionnaire-design",
    "href": "Practicalities.html#questionnaire-design",
    "title": "15  Practicalities of Surveys: Problems at the Planning Stage",
    "section": "15.2 Questionnaire Design",
    "text": "15.2 Questionnaire Design\nConsiderations:\n\nIntroduction/instructions: should be clear and courteous.\nResist the temptation to include too many questions: too many can be demoralizing, off-putting and likely to increase refusal rates.\nQuestions: factual questions should be such that it is reasonable to expect an accurate answer. Example: How many hours have you spent on social media in the past week? And how much in the same week a year ago?\n\nQuestions about opinions have to recognise that the respondent may have many-sided, not necessarily logically coherent views, or may not have thought about the question at all. The wording or setting of the question can produce quite divergent results, making objective value debatable.\nExample: attitude to bus priority: effect of wording of alternative. Survey of 2157 people in London.\n\nAre you in favour of giving priority to buses in the rush hour? (69%) \n\nOr not? (31%)\n\nAre you in favour of giving priority to buses in the rush hour? (55%)\n\n\nOr should cars have just as much priority as buses? (B): 45%\nSo question effect makes a difference of \\(\\approx 14\\%\\) (s.e. 2%).\n\nAmbiguous questions. Some examples\n\n\nIs your work made more difficult because you are expecting a baby?\n\nDoes NO mean ‘I’m not expecting’ or ‘Even if I were expecting my work wouldn’t be made more difficult’ ?\n\nDo you like travelling on trains or buses?\n\nOnly one, or both?\n\nHow strongly do you agree (using a Likert scale) with the statement, “I do my best work every day”?\n\nDo you mean ‘Every day I work productively and achieve all my goals’ (for example), or, ‘Every day, the work I do is better than any work I have done before’?\n\nLeading Questions:\n\n\nDo you agree that it is unfair that \\(\\ldots\\)?\n\n\nEmbarrassing/sensitive Questions:\n\nWe discuss this in the next section",
    "crumbs": [
      "Part II: Sampling Theory",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Practicalities of Surveys: Problems at the Planning Stage</span>"
    ]
  },
  {
    "objectID": "Practicalities.html#randomized-response-technique",
    "href": "Practicalities.html#randomized-response-technique",
    "title": "15  Practicalities of Surveys: Problems at the Planning Stage",
    "section": "15.3 Randomized Response Technique",
    "text": "15.3 Randomized Response Technique\nThis is a technique for estimating the population proportion with a specified characteristic without needing to know the status of individual respondents. It is useful for sensitive (embarrassing, incriminating \\(\\ldots\\)) issues. It works by having two questions:\nQ1 The sensitive question of interest, with a Yes/No answer. e.g. I have used intravenous drugs at some point in my life.\nQ2 An innocuous question also with a Yes/No answer for which the population proportion of YES responses is known: e.g. Is your birthday in April?\nThe respondent is asked to choose Q1 with a known probability \\(P\\) and Q2 with probability \\(1-P\\), tossing a possibly biased coin to decide which. The respondent is assumed to answer truthfully whichever of the questions comes up, but no one else is told which of the questions it was.\nIf \\(p\\) is the proportion of YES answers observed in the sample, and \\(\\pi_1\\) and \\(\\pi_2\\) are the population proportions of YES answers to questions 1 and 2 respectively (so that \\(\\pi_1\\) is required and \\(\\pi_2\\) is known), then \\[Ep=P \\pi_1 + (1-P) \\pi_2,\\] whence an unbiased estimator of \\(\\pi_1\\) is \\[\\widehat{\\pi}_1 =  \\frac{p-(1-P)\\pi_2}{P}.\\\\[2ex]\\]\n\n\n\n\n\n\nThese questions are quite hard, but will help you understand how the randomised response method works.\n\n\n\n\nExercise 15.1  \n\nDerive the variance of \\(\\hat{\\pi}_1\\). Based on the variance and more general considerations, should \\(P\\) be as large as possible?\n\nHint: use the result that\n\\[\n\\frac{n(P\\pi_1 + (1-P)\\pi_2)(1 - P\\pi_1 - (1-P)\\pi_2)}{P^2}=n\\left(\\pi_1 + \\frac{\\pi_2}{P} - \\pi_2\\right)\\left(\\frac{1-\\pi_2}{P} - \\pi_1 + \\pi_2\\right),\n\\] and think about what value of \\(P\\) minimises the expression on the right hand side.\n\nSuppose Q2 is replaced by the complement of Q1 (I have never used intravenous drugs). Derive an unbiased estimator of \\(\\pi_1\\). What would happen if \\(P=\\frac{1}{2}\\)?\n\n\n\n\n\n\n\n\nMoser, A., and G. Kalton. 1971. Survey Methods in Social Investigation. Heinemann.",
    "crumbs": [
      "Part II: Sampling Theory",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Practicalities of Surveys: Problems at the Planning Stage</span>"
    ]
  },
  {
    "objectID": "Introduction-computer-experiments.html",
    "href": "Introduction-computer-experiments.html",
    "title": "16  Introduction",
    "section": "",
    "text": "Please note\n\n\n\nThis part of the notes is for students on MAS61003 only.\n\n\nWe consider the use of computer models for conducting experiments, and some statistical tools for analysing uncertainty in such experiments. By computer model, we simply mean a mathematical model of a physical system, which we represent by a function \\(y=f(x)\\), where we use a computer to evaluate the outputs \\(y\\) given the inputs \\(x\\). A “computer experiment” refers to evaluating the output at different choices of inputs \\(x\\). Some models may be simple equations that we can write down; others may be much more complex, for example, systems of differential equations that are solved numerically, so that there is no closed form expression for \\(f\\). (For illustrative purposes, we will consider examples where \\(f\\) is a simple function, but in general, you may think of a computer model as a ‘black box’: the model produces output \\(y\\) when given input \\(x\\), but the input-output relationship is not transparent).\nThe motivation for performing a computer experiment may be that a physical experiment would either be impossible or too costly. For example, to predict the effect of reducing CO\\(_2\\) emissions (by different amounts) on climate, we have to rely on computer model predictions; there is no physical experiment we could do instead. In engineering applications (e.g. designing an aircraft engine), it will be too expensive to build and test prototypes of every plausible configuration; computer modelling is a much cheaper alternative.\nComputer models may be deterministic: evaluating \\(f(x)\\) twice at the same value of \\(x\\) produces the same value of \\(y\\). However, even when there is no randomness, we may still have uncertainty! The following are some problems where statistical methods may be helpful.\n\nUncertainty analysis/uncertainty propogation In some situations, there may be a true value of the input that should be used for the application at hand, but we don’t know what the true value is. If we denote the true, uncertain, input by \\(X\\), how do we quantify uncertainty about \\(Y=f(X)\\)?\n(Probabilistic/global) sensitivity analysis In an extension to (i), suppose the uncertain input is a vector \\(\\mathbf{X}=(X_1,\\ldots,X_d)\\). How do elements of \\(\\mathbf{X}\\) contribute to uncertainty in \\(Y=f(\\mathbf{X})\\)? Suppose we could pay to reduce uncertainty about any element of \\(\\mathbf{X}\\). Which element(s) should we learn about?\nCalibration/inverse problems Suppose a model has two types of inputs: control inputs \\(x_{cont}\\) that we can vary freely, and calibration inputs \\(x_{calib}\\), for which there is an unknown, true value \\(X_{calib}\\). Now suppose we have observed physical data, corresponding to different known values of the control inputs. Denote this data by \\(z(x_{cont,1}),\\ldots,z(x_{cont,n})\\). Can we infer the value of \\(X_{calib}\\), such that the computer model outputs \\(f(x_{cont,i},X_{calib})\\) match the physical data \\(z(x_{cont,i})\\) for all \\(i\\)?\nComputationally expensive models Some models take a long time to run, even with supercomputer resources: obtaining a single model ‘run’ (evaluating \\(f(x)\\) for a single choice of \\(x\\)) may take hours, days, or even months. How can we tackle problems (i) to (iii) above with limited numbers of model runs?\nReality! A computer model is only a model! How uncertain should we be about what we would observe in the real world, given what we have seen from the model?\n\nThese problems are all the focus of much current research. In this module, we will consider the first two only.",
    "crumbs": [
      "Part III: Computer Experiments (MAS61003 only)",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "Uncertainty-analysis.html",
    "href": "Uncertainty-analysis.html",
    "title": "17  Uncertainty analysis",
    "section": "",
    "text": "17.1 Probabilistic uncertainty analysis\nThe examples in the previous section have already hinted at a probabilistic approach, but we will now consider this more formally. Given an uncertain \\(\\mathbf{x}\\), the aim is to quantify uncertainty about \\(Y=f(\\mathbf{x})\\) by deriving a probability distribution for \\(Y\\). This first requires a probability distribution for \\(\\mathbf{x}\\). If suitable data are available that inform us about \\(\\mathbf{x}\\), we may derive a (posterior) probability distribution for \\(\\mathbf{x}\\) using Bayesian inference. Without data, we may instead use expert judgement (so that we simply have a prior distribution for \\(\\mathbf{x}\\)).\nWe now have a transformation of a random variable \\(Y=f(\\mathbf{x})\\). In very simple cases, given a density function for \\(\\mathbf{x}\\), we may be able to derive the density function for \\(Y\\), but typically, this will not be possible. (At the very least, we would need a closed form expression for \\(f\\)). We can instead use a Monte Carlo technique.\nBecause each \\(Y_i\\) is a random draw from the distribution of \\(Y\\), we must have \\(E(Y_i)=E(Y)\\) and \\(P(Y_i\\le y)= P(Y \\le y)\\) (in addition to \\(Var(Y_i)=Var(Y)\\)), so the estimators above are unbiased: \\[\\begin{align}\nE\\{\\hat{E}(Y)\\}&=\\frac{1}{N}\\sum_{i=1}^NE(Y_i)\\\\&=\\frac{1}{N}\\sum_{i=1}^NE(Y)\\\\&=E(Y), \\\\\nE\\{\\hat{P}(Y\\le y )\\}&=\\frac{1}{N}\\sum_{i=1}^NE\\{I(Y_i\\le y)\\}\\\\&=\\frac{1}{N}\\sum_{i=1}^NP(Y_i\\le y)\\\\\n&=\\frac{1}{N}\\sum_{i=1}^NP(Y\\le y)\\\\&=P(Y\\le y).\n\\end{align}\\] The variances of the estimators decrease as we increase \\(N\\): \\[\\begin{align}\nVar\\{\\hat{E}(Y)\\}&=\\frac{1}{N^2}\\sum_{i=1}^NVar(Y_i)=\\frac{1}{N^2}\\sum_{i=1}^NVar(Y)=\\frac{Var(Y)}{N}, \\\\\nVar\\{\\hat{P}(Y\\le y )\\}&=\\frac{1}{N^2}\\sum_{i=1}^NVar\\{I(Y_i\\le y)\\}\\\\&=\\frac{1}{N^2}\\sum_{i=1}^NP(Y_i\\le y)\\{1-P(Y_i\\le y)\\}\\\\\n&=\\frac{1}{N^2}\\sum_{i=1}^NP(Y\\le y)\\{1-P(Y\\le y)\\}\\\\ &=\\frac{P(Y\\le y)\\{1-P(Y\\le y)\\}\n}{N}.\n\\end{align}\\] If \\(N\\) is large, we can construct approximate confidence intervals based on the central limit theorem. For example, an approximate 95% confidence interval for \\(E(Y)\\) would be \\[\\hat{E}(Y) \\pm 1.96 \\sqrt{\\frac{\\widehat{Var}(Y)}{N}},\\] where \\[\\widehat{Var}(Y)=\\frac{1}{N-1}\\sum_{i=1}^N\\{Y_i-\\hat{E}(Y)\\}^2.\\\\\\]\nset.seed(61003)\nN &lt;- 1000 \nx &lt;- rnorm(N)\ny &lt;- x^2\n# Estimate E(Y)\nmean(y)\n\n[1] 1.046482\n# Confidence interval for E(Y)\ns.e &lt;- (var(y) / N)^0.5\nc(mean(y) - 1.96 * s.e, mean(y) + 1.96 * s.e)\n\n[1] 0.9493342 1.1436297\n# Estimate  P(Y&lt;1)\nmean(y &lt; 1)\n\n[1] 0.677\n# Confidence interval for P(Y&lt;1)\ns.e &lt;- (mean(y &lt; 1) * mean(y &gt;= 1) / N)^0.5\nc(mean(y &lt; 1) - 1.96 * s.e, mean(y &lt; 1) + 1.96 * s.e)\n\n[1] 0.6480164 0.7059836\nIncreasing N will give better results: the confidence intervals will be narrower, and we expect the estimates to be closer to the true values.",
    "crumbs": [
      "Part III: Computer Experiments (MAS61003 only)",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Uncertainty analysis</span>"
    ]
  },
  {
    "objectID": "Uncertainty-analysis.html#probabilistic-uncertainty-analysis",
    "href": "Uncertainty-analysis.html#probabilistic-uncertainty-analysis",
    "title": "17  Uncertainty analysis",
    "section": "",
    "text": "Generate a random sample \\(\\mathbf{x}_1,\\ldots,\\mathbf{x}_N\\) from the distribution of \\(\\mathbf{x}\\).\nEvaluate \\(Y_1=f(\\mathbf{x}_1),\\ldots,Y_N=f(\\mathbf{x}_N)\\).\nUse the sample \\(Y_1,\\ldots,Y_N\\) to estimate summaries of the distribution of \\(Y\\). For example, estimate \\(E(Y)\\) and \\(P(Y\\le y)\\) by \\[\\begin{aligned}\n\\hat{E}(Y)&=\\frac{1}{N}\\sum_{i=1}^NY_i,\\\\\n\\hat{P}(Y\\le y )&=\\frac{1}{N}\\sum_{i=1}^NI(Y_i\\le y),\\\\\n\\end{aligned}\\] where \\(I()\\) is the indicator function.\n\n\n\n\n\n\n\n\nExample\n\n\n\n\nExample 17.3 Continuing Example 17.1, we show how this works in R. We estimate \\(E(Y)\\) and \\(P(Y&lt;1)\\) (which have true values of 1 and 0.683 to 3 d.p.)",
    "crumbs": [
      "Part III: Computer Experiments (MAS61003 only)",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Uncertainty analysis</span>"
    ]
  },
  {
    "objectID": "Uncertainty-analysis.html#probabilistic-uncertainty-analysis-in-health-economic-evaluation",
    "href": "Uncertainty-analysis.html#probabilistic-uncertainty-analysis-in-health-economic-evaluation",
    "title": "17  Uncertainty analysis",
    "section": "17.2 Probabilistic uncertainty analysis in health economic evaluation",
    "text": "17.2 Probabilistic uncertainty analysis in health economic evaluation\nThe above technique is used in many disciplines. One area where it is (almost) a requirement is in health economic evaluations conducted for the National Institute for Health and Care Excellence (NICE). Below are some extracts from their guidance:1 (although they have used the term “probabilistic sensitivity analysis” to mean probabilistic uncertainty analysis. We will use the term sensitivity analysis to mean something else).\nSome extracts for the guidance are as follows.\n\nA third source of uncertainty comes from parameter precision, once the most appropriate sources of information have been identified […] Assign distributions to characterise the uncertainty associated with [parameter/input] values. The distributions chosen for probabilistic sensitivity analysis should not be chosen arbitrarily but chosen to represent the available evidence on the parameter of interest, and their use should be justified\n\n\nWhen doing a probabilistic analysis, enough model simulations should be used to minimise the effect of Monte Carlo error. Reviewing the variance around probabilistic model outputs […] as the number of simulations increases can provide a way of assessing if the model has been run enough times or more runs are needed.\n\n\nUsing univariate and best- or worst-case sensitivity analysis [one-at-a-time experiments] is an important way of identifying parameters that may have a substantial effect on the cost-effectiveness results and of explaining the key drivers of the model. However, such analyses become increasingly unhelpful in representing the combined effects of multiple sources of uncertainty as the number of parameters increase. Using probabilistic sensitivity analysis can allow a more comprehensive characterisation of the parameter uncertainty associated with all input parameters.",
    "crumbs": [
      "Part III: Computer Experiments (MAS61003 only)",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Uncertainty analysis</span>"
    ]
  },
  {
    "objectID": "Uncertainty-analysis.html#case-study",
    "href": "Uncertainty-analysis.html#case-study",
    "title": "17  Uncertainty analysis",
    "section": "17.3 Case study",
    "text": "17.3 Case study\nWe consider a (modified version of a) cost-effectiveness model that was developed to support the National Institute for Health and Care Excellence physical activity guidance2. The model predicts the “incremental net benefit” of an intervention: exercise on prescription (e.g. from a general medical practitioner) to promote physical activity against a ‘do nothing’ scenario. The model assumes that the intervention affects health by reducing the risks of three diseases: coronary heart disease, stroke and diabetes. The health effects included in the model are those that relate to these three diseases, and the model counts costs that accrue as a result of the treatment of the three diseases, as well as those that relate to the intervention itself.\nThe incremental net benefit describes the value of the intervention per patient treated. Health benefits are converted into money, on an assumption that the NHS can afford to spend up to £20000 to achieve one extra year of life in perfect health for one patient. A positive incremental net benefit means the intervention is cost effective; a negative incremental net benefit means the money would be better spent on something else. Incremental net benefit is modelled for a cohort of patients with average age 50, at the time of the intervention.\nThe model has 24 uncertain inputs \\(X_1,\\ldots,X_{24}\\). We do not list them all here, but examples include the relative risk of a stroke for an active versus a sedentary population, and the probability of new exercise in the intervention group. Further details and R code are available on Blackboard.\nWe illustrate a Monte Carlo uncertainty analysis. 100,000 random inputs are drawn from the distribution of \\(\\mathbf{x}\\). The model is run at each input, to obtain a sample \\(y_1,\\ldots,y_{100000}\\) from the distribution of \\(Y\\). Note the interpretation of \\(Y\\) here: it is the mean net benefit per patient treated. It is a fixed, uncertain quantity, because it is a function of 24 input values that are unknown.\nWe find \\[\\begin{align}\n\\hat{E}(Y)&=\\frac{1}{100000}\\sum_{i=1}^{100000}y_i=248.5\\\\\n\\hat{P}(Y\\le 0 )&=\\frac{1}{100000}\\sum_{i=1}^{100000}I(y_i\\le 0)=0.225,\n\\end{align}\\] with 95% confidence intervals for \\(E(Y)\\) and \\(P(Y\\le 0)\\) given by (246.4, 250.6) and (0.223, 0.228) respectively, suggesting that \\(N=100000\\) is sufficiently large for estimating \\(E(Y)\\) and \\(P(Y\\le 0)\\) reliably. Hence, although we may estimate that the intervention is cost-effective, based on \\(E(Y)\\), we are fairly uncertain about this; our probability that the intervention is not cost-effective is about 20%.\nTo visualise uncertainty about \\(Y\\), we can plot a (kernel) density estimate using the sample \\(y_1,\\ldots,y_{100000}\\). We mark on the estimated 2.5th and 97.5th percentiles as vertical dashed lines.",
    "crumbs": [
      "Part III: Computer Experiments (MAS61003 only)",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Uncertainty analysis</span>"
    ]
  },
  {
    "objectID": "Uncertainty-analysis.html#footnotes",
    "href": "Uncertainty-analysis.html#footnotes",
    "title": "17  Uncertainty analysis",
    "section": "",
    "text": "National Institute for Health and Care Excellence (2002). NICE health technology evaluations: the manual. Available at https://www.nice.org.uk/process/pmg36. See section 4.7 Exploring Uncertainty↩︎\nNational Institute for Clinical Excellence (2006) Four commonly used methods to increase physical activity: PH2. Technical Report. National Institute for Clinical Excellence, London. (Available from http://www. nice.org.uk/PH2.) The version used in this case study is the model described in Strong, M., Oakley J. E. and Chilcott, J. (2012). Managing structural uncertainty in health economic decision models: a discrepancy approach. Journal of the Royal Statistical Society, Series C, 61(1), 25-45. Thanks to Mark Strong for providing the R code.↩︎",
    "crumbs": [
      "Part III: Computer Experiments (MAS61003 only)",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Uncertainty analysis</span>"
    ]
  },
  {
    "objectID": "Sensitivity-analysis.html",
    "href": "Sensitivity-analysis.html",
    "title": "18  Sensitivity analysis",
    "section": "",
    "text": "18.1 Functional ANOVA and variance decompositions\nFor independent inputs, we can decompose \\(Var(Y)\\) into main effect variances, two-way interaction variances, three-way interaction variances and so on. We first write \\[\nf(\\mathbf{x})=f_0+\\sum_{i=1}^df_i(x_i)+\\sum_{i&lt;j}f_{i,j}(x_i,x_j)+\\\\ \\sum_{i&lt;j&lt;k}f_{i,j,k}(x_i,x_j,x_k)+\\ldots+f_{1,\\ldots,d}(x_1,\\ldots,x_d).\n\\tag{18.2}\\] This is trivial, in that the last term is a function of all the inputs, so we could choose to set \\(f_{1,\\ldots,d}(x_1,\\ldots,x_d)=f(\\mathbf{x})\\) and set all the other terms equal to zero. We now define \\[\\begin{align}\nf_0&=E\\{f(\\mathbf{X})\\},\\\\\nf_i(x_i)&=E\\{f(\\mathbf{X})|X_i=x_i\\}-f_0,\\label{fi}\\\\\nf_{i,j}(x_i,x_j)&=E\\{f(\\mathbf{X})|X_i=x_i,X_j=x_j\\}-f_i(x_i)-f_j(x_j)-f_0\\\\\nf_{i,j,k}(x_i,x_j,x_k)&=E\\{f(\\mathbf{X})|X_i=x_i,X_j=x_j,X_k=x_k\\}-f_{i,j}(x_i,x_j)-f_{i,k}(x_i,x_k)\\nonumber\\\\\n&-f_{j,k}(x_j,x_k)-f_i(x_i)-f_j(x_j)-f_k(x_k)-f_0,\n\\end{align}\\] and so on. Inspecting, for example, the definition of \\(f_{i,j}(x_i,x_j)\\), we can think of this term as representing an additional contribution to \\(f(\\mathbf{x})\\), from the interaction between \\(x_i\\) and \\(x_j\\), on top of the contributions from the single input functions \\(f_i(x_i)\\) and \\(f_j(x_j)\\). Note also that, following this construction, \\(f_{1,\\ldots,d}(x_1,\\ldots, x_d)\\) is simply \\(f(\\mathbf{x})\\), minus all the other terms on the RHS of Equation 18.2, so again, it is trivial to see that the decomposition holds.\nNow, with \\(Y=f(\\mathbf{X})\\), write \\[\\begin{align}\nY=f(\\mathbf{X})&=f_0+\\sum_{i=1}^df_i(X_i)+\\sum_{i&lt;j}f_{i,j}(X_i,X_j)\\nonumber\\\\\n&+\\sum_{i&lt;j&lt;k}f_{i,j,k}(X_i,X_j,X_k)+\\ldots+f_{1,\\ldots,d}(X_1,\\ldots,X_d).\n\\end{align}\\] If inputs in \\(\\mathbf{X}\\) are independent, we find that all the terms above are independent, and \\[\nVar\\{Y\\}=\\sum_{i=1}^dVar\\{f_i(X_i)\\}+\\sum_{i&lt;j}Var\\{f_{i,j}(X_i,X_j)\\}\\\\\n+\\sum_{i&lt;j&lt;k}Var\\{f_{i,j,k}(X_i,X_j,X_k)\\}+\\ldots\\\\\\\n+Var\\{f_{1,\\ldots,d}(X_1,\\ldots,X_d)\\}.\n\\tag{18.3}\\] Noting the definition of \\(f_i(x_i)\\), the main effect variance for \\(X_i\\) is \\[Var\\{f_i(X_i)\\}=Var_{X_i}\\{E(Y|X_i)\\}\\}\\] We can define a two-way interaction variance for \\(X_i\\) and \\(X_j\\) as \\[Var\\{f_{i,j}(X_i,X_j)\\},\\] which gives the additional contribution to \\(Var(Y)\\) from \\(X_i\\) and \\(X_j\\), above their main effect contributions. We define a total effect variance for \\(X_i\\) as the sum of all terms in \\(Var(Y)\\) involving \\(X_i\\) and the total effect index for \\(X_i\\) as its total effect variance, divided by \\(Var(Y)\\). We",
    "crumbs": [
      "Part III: Computer Experiments (MAS61003 only)",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Sensitivity analysis</span>"
    ]
  },
  {
    "objectID": "Sensitivity-analysis.html#main-effect-and-total-effect-variances",
    "href": "Sensitivity-analysis.html#main-effect-and-total-effect-variances",
    "title": "18  Sensitivity analysis",
    "section": "18.2 Main effect and total effect variances",
    "text": "18.2 Main effect and total effect variances\nAn input can have a small main effect variance, but a large total effect variance. A small main effect variance doesn’t imply ‘unimportant’, but a small total effect variance does imply ‘unimportant’. We can calculate total effect variance \\(V_i^T\\) for input \\(X_i\\) via (joint) main effect variance of remaining inputs \\[Var(Y)=Var_{\\mathbf{X}_{-i}}\\{E(Y|\\mathbf{X}_{-i})\\} + V^T_i,\\] The effect of calculating \\(Var_{\\mathbf{X}_{-i}}\\{E(Y|\\mathbf{X}_{-i})\\}\\) is to sum all the terms in Equation 18.3 that do not include input \\(X_i\\). The total effect index for input \\(X_i\\) is therefore \\[1-\\frac{Var_{\\mathbf{X}_{-i}}\\{E(Y|\\mathbf{X}_{-i})\\}}{Var(Y)}.\\] We can therefore interpret the total effect index for \\(X_i\\) as the expected proportion of variance remaining if we were to learn the true value of all inputs except \\(X_i\\).\n\n\n\n\n\n\nImportant\n\n\n\nTotal effect indices are only meaningful for independent inputs; Equation 18.3 assumes independence between inputs. Main effect indices are valid (their interpretation holds) regardless of whether the inputs are independet or not.\n\n\nDependent inputs \\(\\mathbf{X}_u\\) should be treated as a single group, so that one would only calculate a total effect index for the whole group. We sum all terms in Equation 18.3 involving any elements of \\(\\mathbf{X}_u\\). This corresponds to calculating \\[1-\\frac{Var_{\\mathbf{X}_{-u}}\\{E(Y|\\mathbf{X}_{-u})\\}}{Var(Y)}.\\]\n\n\n\n\n\n\nExample\n\n\n\n\nExample 18.2 Continuing Example 18.1, consider \\[Y=1.1X_1 + X_2 +X_1X_2+X_3^2\\] with \\[X_1\\sim N(0,4),\\quad X_2\\sim U[-4,4],\\quad X_3\\sim N(0,3).\\] We can partition the variance as follows.\n\n\n\n\n\\(Var\\{f_1(X_1)\\}\\)\n4.8\n\n\n\\(Var\\{f_2(X_2)\\}\\)\n5.3\n\n\n\\(Var\\{f_3(X_3)\\}\\)\n18.0\n\n\n\\(Var\\{f_{1,2}(X_1,X_2)\\}\\)\n21.3\n\n\n\\(Var\\{f_{1,3}(X_1,X_3)\\}\\)\n0.0\n\n\n\\(Var\\{f_{2,3}(X_2,X_3)\\}\\)\n0.0\n\n\n\\(Var\\{f_{1,2,3}(X_1,X_2,X_3)\\}\\)\n0.0\n\n\n\\(Var(Y)\\)\n49.5\n\n\n\n\nThe total effect variance for \\(X_1\\) is \\(4.8+21.3\\). Note that we can obtain this via \\[49.4 - Var_{X_2,X_3}\\{E(Y|X_2,X_3)\\} = 49.4 - Var(X_2 + X_3^2) = 49.4-64/12 - 18 = 26.1\\] (note \\(E(X_3^4)=27\\)).",
    "crumbs": [
      "Part III: Computer Experiments (MAS61003 only)",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Sensitivity analysis</span>"
    ]
  },
  {
    "objectID": "Sensitivity-analysis.html#computation",
    "href": "Sensitivity-analysis.html#computation",
    "title": "18  Sensitivity analysis",
    "section": "18.3 Computation",
    "text": "18.3 Computation\n\n18.3.1 Using Monte Carlo\nPartition the input \\(\\mathbf{X}\\) into \\((\\mathbf{X}_u,\\mathbf{X}_{-u})\\). A Monte Carlo procedure for calculating \\(Var_{\\mathbf{X}_u}\\{E(Y|\\mathbf{X}_u)\\}\\) is as follows.\n\nSample a value \\(\\mathbf{X}_{u,j}\\) from the distribution of \\(\\mathbf{X}_u\\).\n\nSample a set of values \\(\\mathbf{X}_{-u,1},\\ldots,\\mathbf{X}_{-u,K}\\) from the distribution of \\(\\mathbf{X}_{-u}|\\mathbf{X}_u=\\mathbf{X}_{u,j}.\\)\nEstimate \\(E(Y|\\mathbf{X}_u=\\mathbf{X}_{u,j})\\) by \\[m_j=\\frac{1}{K}\\sum_{k=1}^K f(\\mathbf{X}_{u,j},\\mathbf{X}_{-u,k}).\\]\n\nRepeat for \\(j=1,\\ldots,J\\) and estimate \\(Var_{\\mathbf{X}_u}\\{E(Y|\\mathbf{X}_u)\\}\\) by \\[\\frac{1}{J-1}\\sum_{j=1}^J(m_j - \\bar{m})^2,\\] where \\(\\bar{m} = \\frac{1}{J}\\sum_{j=1}^Jm_j.\\)\n\nThis can be computationally expensive, in that large values of \\(J\\) and \\(K\\) may be needed to get accurate estimates. An R implementation for Example 18.2 is available on Blackboard.\n\n\n18.3.2 Using regression\nWe first consider estimating a main effect variance \\(Var_{X_i}\\{E(Y|X_i)\\}\\). We will shortly see how to do this, using regression, and a sample of inputs and outputs generated for a probabilistic uncertainty analysis. To illustrate this, we consider again Example 18.1. Suppose we have conducted a Monte Carlo uncertainty analysis:\n\nWe have generated a sample of \\(N\\) values from the distribution of \\(\\mathbf{X}\\). Denote the observed values of this sample by \\(\\mathbf{x}_1,\\ldots,\\mathbf{x}_N\\), with \\(\\mathbf{x}_i=(x_{1,i},x_{2,i},x_{3,i})\\).\nWe evaluate \\(y_i=f(\\mathbf{x}_i)\\) for \\(i=1,\\ldots,N\\) to get a sample from the distribution of \\(Y\\).\n\nNow suppose we want to calculate \\(Var_{X_3}\\{E(Y|X_3)\\}\\). We can think of \\(E(Y|X_3=x_3)\\) as a function of \\(x_3\\), which we will write as \\(g(x_3)\\). We could estimate \\(Var\\{g(X_3)\\}\\) using the sample variance of \\(g(x_{3,1}),\\ldots,g(x_{3,N})\\), if we could work out how to evaluate (or estimate) \\(g(x_{3,i})\\) for each \\(i\\). In this particular example, by looking at the function, we can see that \\(g(x_3)=E(Y|X_3=x_3)=x_3^2\\). Now let’s try plotting \\(y_i\\) against \\(x_{3,i}\\), together with the function \\(g(x_3^2)=x_3^2\\).\n\n\n\n\n\n\n\n\n\nCould we have used the data to estimate the function \\(g(x_3)\\)? Yes: this is a regression problem. The observed \\((y_i,x_{3,i})\\) pairs can be related via \\[y_i=g(x_{3,i})+\\varepsilon_i,\\] If we consider a random pair \\((Y,X_3)\\) pair with corresponding error term \\(\\varepsilon\\), we have \\[E(Y)=E\\{g(X_3)\\}+E(\\varepsilon),\\] and \\[E_{X_3}\\{g(X_3)\\}=E_{X_3}\\{E(Y|X_3)\\}=E(Y).\\] so that \\(E(\\varepsilon)=0\\). (Note that the variance of \\(\\varepsilon\\) is not constant for all \\(x_3\\), which ideally we would account for, though the aim here is merely to estimate \\(g\\); we are not trying to do any formal inference).\nWe can then estimate \\(g(x_{3,1}),\\ldots,g(x_{3,N})\\) by estimating the function \\(g\\), and calculating the fitted values, with the variance of the fitted values giving us an estimate of \\(Var_{X_3}\\{E(Y|X_3)\\}\\). We could use ordinary least squares regression to estimate \\(g\\), though there are other methods that can be use. (In the R code, we use a technique called generalised additive modelling (GAM), which is a more flexible regression technique. The details of GAM are outside the scope of this module).\nWe can calculate variance contributions for groups of inputs, e.g \\(X_1,X_2\\), by fitting regression models for \\(y\\) on \\(x_1\\) and \\(x_2\\), to estimate \\(E(Y|X_1=x_1,X_2=x_2)\\) as a function of \\(x_1\\) and \\(x_2\\). We can then use the sample of fitted values to estimate \\(Var_{X_1,X_2}\\{E(Y|X_1,X_2)\\}\\).\nR code implementations for Example 18.2 using generalised additive modelling are available on Blackboard.\n\n\n\n\n\n\nExample\n\n\n\n\nExample 18.3 For the Case Study model \\(X_{11}, X_{12}, X_{13}\\) and \\(X_{14}\\) represent proportions of the target population taking up exercise, and maintaining their exercise, in the control and intervention groups. Below are estimates of the total effect indices for various combinations of \\(X_{11}, X_{12}, X_{13}\\) and \\(X_{14}\\)\n\n\n\n\n\\(\\mathbf{X}_u\\)\n\\(Var_{\\mathbf{X}_u}\\{E(Y|\\mathbf{X}_u)\\}/Var(Y)\\)\n\n\n\\(X_{11}\\)\n0.10\n\n\n\\(X_{12}\\)\n0.15\n\n\n\\(X_{13}\\)\n0.01\n\n\n\\(X_{14}\\)\n0.10\n\n\n\\(X_{11},X_{12}\\)\n0.49\n\n\n\\(X_{13},X_{14}\\)\n0.26\n\n\n\\(X_{11},X_{12},X_{13},X_{14}\\)\n0.78\n\n\n\n\nSo, for example, the expected reduction in the variance of the net benefit from learning \\(X_{13}\\) only is 1%, and the expected reduction in variance from learning \\(X_{14}\\) only is 10%, but the expected reduction in variance from learning both \\(X_{13}\\) and \\(X_{14}\\) is about 25%. Learning all four inputs (\\(X_{11},X_{12},X_{13},X_{14}\\)) out of the 24 is expected to reduce the variance of \\(Y\\) by about 80%. There are other uncertain inputs, such as the relative risk of developing diabetes, for exercising versus sedentary adults, that do not contribute substantially to the uncertainty in the net benefit; their total effect indices are relatively small.",
    "crumbs": [
      "Part III: Computer Experiments (MAS61003 only)",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Sensitivity analysis</span>"
    ]
  },
  {
    "objectID": "Sensitivity-analysis.html#appendix-proof-of-the-variance-identity",
    "href": "Sensitivity-analysis.html#appendix-proof-of-the-variance-identity",
    "title": "18  Sensitivity analysis",
    "section": "18.4 Appendix: proof of the variance identity",
    "text": "18.4 Appendix: proof of the variance identity\nFor any two random variables \\(A\\) and \\(B\\) (with density functions \\(p_A(a)\\) and \\(p_B(b)\\) and sample spaces \\(\\mathcal{X}_A\\) and \\(\\mathcal{X}_B\\)), \\[Var(A)=Var_B\\{E(A|B)\\} + E_B\\{Var(A|B)\\}.\\label{varident2}\\] To prove this, we first prove the ‘tower property’ of expectations: \\[\\begin{align}\nE_B\\{E(A|B)\\}&=\\int_{\\mathcal{X}_B} E(A|B=b) p_B(b) db\\\\\n&=\\int_{\\mathcal{X}_B} \\int_{\\mathcal{X}_A} a p_{A|B}(a|b) p_B(b) da\\,db\\\\\n&=\\int_{\\mathcal{X}_A} a \\int_{\\mathcal{X}_B}  p_{A,B}(a,b)db\\, da\\\\\n&=\\int_{\\mathcal{X}_A} a   p_{A}(a) da\\\\\n&=E(A).\n\\end{align}\\] Now, using the tower property, \\[\\begin{align}\nVar(A)&= E(A^2) - E(A)^2\\nonumber\\\\\n&= E_B\\{E(A^2|B)\\} -E(A)^2,\\label{v1}\n\\end{align}\\] and \\[\\begin{align}\nVar_B\\{E(A|B)\\}&= E_B[\\{E(A|B)\\}^2)] - \\left[E_B\\{E(A|B)\\}\\right]^2\\nonumber\\\\\n&=E_B[\\{E(A|B)\\}^2] - E(A)^2,\\label{v2}\n\\end{align}\\] and \\[E_B\\{Var(A|B)\\}= E_B\\{E(A^2|B)\\}-E_B[\\{E(A|B)\\}^2]\\label{v3}\\] Adding these expressions for \\(E_B\\{Var(A|B)\\}\\) and \\(Var_B\\{E(A|B)\\}\\) gives \\(Var(A)\\), which proves the identity.",
    "crumbs": [
      "Part III: Computer Experiments (MAS61003 only)",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Sensitivity analysis</span>"
    ]
  }
]