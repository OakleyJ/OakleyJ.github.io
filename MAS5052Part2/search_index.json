[["index.html", "MAS5052 Part 2: Likelihood and Linear Models About these notes", " MAS5052 Part 2: Likelihood and Linear Models Jeremy Oakley 2022-07-06 About these notes These notes are for students on MAS5052 Basic Statistics, but are largely the same as parts of MAS223 Statistical Modelling and Inference. The notes on likelihood have a long history, to the extent that I am not sure who first wrote them! But a number of colleagues have lectured this material over the years, and no doubt each made additions and improvements: Tony O’Hagan, Miguel Juarez, Jonathan Jordan, Nic Freeman and Alison Poulston. "],["intro.html", "Chapter 1 Introducing likelihood methods 1.1 Recap: maximising functions 1.2 Maximum likelihood estimation: a first example", " Chapter 1 Introducing likelihood methods We now return to the problem of estimating parameters of statistical models (probability distributions). Previously, we simply stated the estimators we would use, for example, for \\(X\\sim Bin(n, \\theta)\\), we just stated that our estimator for \\(\\theta\\) would be \\(X/n\\), and then justified this choice. But for more complicated models, it may not be obvious what a good choice of estimator is. We now introduce the concept of likelihood, which is an important concept in statistical theory. Likelihood can be used to find estimators for statistical models, construct confidence intervals, perform hypothesis tests, and also plays a central role in Bayesian statistics. 1.1 Recap: maximising functions We first recap how to maximise a function, as we will shortly be maximising likelihood functions. Suppose that \\(I\\subseteq\\mathbb{R}\\) and that we have a differentiable function \\(f:I\\to\\mathbb{R}\\). We say that a point \\(x_0\\) maximises \\(f\\) if \\[f(x_0)\\geq f(x)\\;\\text{ for all }x\\in I.\\] That is, if \\(x_0\\) is the location of the global maximum value of \\(f(\\cdot)\\). Two key facts: A point \\(x_0\\in I\\) is a turning point of \\(f\\) if and only if \\[\\frac{df}{dx}\\bigg|_{x=x_0}=0.\\] A turning point \\(x_0\\) of \\(f\\) is a local maximum if \\[\\frac{d^2f}{dx^2}\\bigg|_{x=x_0}&lt;0. \\] If a differentiable function \\(f\\) has a single turning point, and this turning point is a local maximum, then it is automatically the global maximum. Example 1.1 (Maximisation of a function) Find the value of \\(\\theta\\) which maximises \\(f(\\theta)=\\theta^5(1-\\theta)\\) on the range \\(\\theta\\in[0,1]\\). Solution First, we look for turning points. We have \\[\\begin{align*} f&#39;(\\theta) &amp;=5\\theta^4(1-\\theta)+\\theta^5(-1)\\\\ &amp;=\\theta^4(5-6\\theta) \\end{align*}\\] So the turning points are at \\(\\theta=0\\) and \\(\\theta=\\frac56\\). To see which ones are local maxima, we calculate the second derivative: \\[\\begin{align*} f&#39;&#39;(\\theta) &amp;=4\\theta^3(5-6\\theta)+\\theta^4(-6)\\\\ &amp;=\\theta^3(20-30\\theta). \\end{align*}\\] So, \\(f&#39;&#39;(\\frac56)=(\\frac56)^3(20-25)&lt;0\\) and \\(\\theta=\\frac56\\) is a local maximum. Unfortunately, \\(f&#39;&#39;(0)=0\\), so we don’t know if \\(\\theta=0\\) is a local maximum, minimum or inflection. However, we can check that \\(f(0)=0\\), so it doesn’t matter which, we still have \\(f(0)&lt;f(\\frac56)\\). Hence, \\(\\theta=\\frac56\\) is the global maximiser. If more than one turning point appears, we have to be more careful (and, in this course, we will approach such cases through curve sketching or by using R). 1.2 Maximum likelihood estimation: a first example Let us first illustrate the idea with an example. Suppose that we think that it is sensible to use the Geometric distribution \\(Geom(\\theta)\\) to model the number of times we have to roll a biased die before we get a \\(6\\). What we don’t know, is which value of \\(\\theta\\) is best to use. Clearly, we must have \\(\\theta\\in(0,1)\\) because we use the Geometric distribution, but how should we choose exactly which value of \\(\\theta\\) to use? Since we now care about the value of the parameter(s) \\(\\theta\\), we will write the probability mass function as \\(p_X(x;\\theta)\\), to make the dependence on \\(\\theta\\) explicit. In this case, the Geometric distribution has probability mass function \\(p_X(x;\\theta)=\\theta^x(1-\\theta)\\). Suppose, for now, that we have a just single item of data; we roll the die \\(5\\) times until we first see a \\(6\\). We will worry about handling multiple data points later. If we let \\(X\\sim Geom(\\theta)\\), the probability of this event is \\[p_X(5;\\theta)=\\theta^5(1-\\theta).\\] This is a function of \\(\\theta\\). Now, here is the key idea: since we observed the value \\(5\\), it would make sense if we chose \\(\\theta\\) to make \\(p_X(5;\\theta)=Pr[X=5]\\) as large as possible. That is, we want to choose \\(\\theta\\) so as our model \\(Geom(\\theta)\\) is as likely as possible to reproduce the data that we actually observed. So, what it comes down to, is finding the value of \\(\\theta\\) which maximises the function \\[L(\\theta; 5)=p_X(5; \\theta)=\\theta^5(1-\\theta)\\] amongst the range of possible choices of \\(\\theta\\), in this case \\(\\Theta=[0,1)\\). We call \\(L(\\theta; 5)\\) the likelihood of the parameter value \\(\\theta\\), given the data \\(5\\). We write the (hopefully, unique) maximiser of \\(L(\\theta;5)\\) as \\(\\hat\\theta\\), and we call it the maximum likelihood estimator of \\(\\theta\\), given the (single) data point \\(5\\). We can find \\(\\hat\\theta\\) using Example 1.1: we know that \\(\\hat\\theta=\\frac{5}{6}\\). "],["introducing-the-likelihood-function.html", "Chapter 2 Introducing the likelihood function", " Chapter 2 Introducing the likelihood function We will often work with continuous rather than discrete random variables, so we would use the probability density function instead of the probability mass function. The theory in this case is very similar, so it is common to use the same notation in both cases. Let \\(X\\) be a random variable, whose distribution depends on some unknown parameter \\(\\theta\\). Let \\(x\\) be the observed value of \\(X\\). We now write \\(f(x; \\theta)\\) to represent either the probability density function \\(f_X(x)\\) if \\(X\\) is continuous, or the probability mass function \\(p_X(x)\\) if \\(X\\) is discrete, where \\(f_X(x)\\) (or \\(p_X(x)\\)) is evaluated at the observation \\(x\\). We denote the set of possible values of \\(\\theta\\) by \\(\\Theta\\), and define the likelihood function as follows. Definition 2.1 (likelihood function) The likelihood function of \\(X\\), given the data \\(x\\), is \\(L:\\Theta\\to\\mathbb{R}\\) defined by \\[L(\\theta; x)=f(x;\\theta)\\] We refer to the value of \\(L(\\theta;x)\\) as the likelihood of \\(\\theta\\) given the data \\(x\\). Think of the likelihood as the probability or density of the observed data, expressed as a function of \\(\\theta\\). The likelihood is thought of as function of the unknown parameter \\(\\theta\\). The observed data \\(x\\) is known, so any instance of \\(x\\) in the function \\(L(\\theta; x)\\) will be replaced by a number. Note that viewed as a function of \\(\\theta\\), the likelihood is not a probability density function. For example, it typically will not integrate over \\(\\Theta\\) to give \\(1\\). The (hopefully unique) \\(\\theta\\in\\Theta\\) which maximises \\(L(\\theta; x)\\) is known as the maximum likelihood estimator of \\(\\theta\\). We usually denote it by \\(\\widehat\\theta\\). Sometimes, we will refer to the likelihood function of a distribution; this is the likelihood function of a random variable with that distribution. The process of finding the maximum likelihood estimator \\(\\hat\\theta\\) is known as maximum likelihood estimation. Example 2.1 (Likelihood functions and maximum likelihood estimators) Let \\(X\\) be a random variable with \\(Exp(\\theta)\\) distribution, where the parameter \\(\\theta\\) is unknown. Find the and sketch the likelihood function of \\(X\\), given the data \\(x=3\\). Solution The likelihood function is \\[L(\\theta; 3)=f(3;\\theta)=\\theta e^{-3\\theta}\\] defined for all \\(\\theta\\in\\Theta=(0,\\infty)\\). We can plot this in R, for \\(\\theta\\in(0,4)\\) curve(x*exp(-3*x), from = 0, to = 4, xlab=expression(theta), ylab=expression(L(theta~&quot;;&quot;~3))) (Note that we use x as the \\(\\theta\\) variable here because R hard-codes its use of x as a graph variable in the curve() function.) Given this data, find the likelihood of \\(\\theta=\\frac{1}{10},\\frac12,1,2,5\\). Amongst these values of \\(\\theta\\), which has the highest likelihood? Solution The likelihoods are \\[\\begin{alignat*}{2} L(\\tfrac{1}{10};3)&amp;=\\tfrac{1}{10}e^{-\\frac{3}{10}}&amp;&amp;\\approx 0.07\\\\ L(\\tfrac12;3)&amp;=\\tfrac12e^{-\\frac32}&amp;&amp;\\approx 0.11\\\\ L(1;3)&amp;=1e^{-3}&amp;&amp;\\approx 0.05\\\\ L(2;3)&amp;=2e^{-6}&amp;&amp;\\approx 0.005\\\\ L(5;3)&amp;=5e^{-15}&amp;&amp;\\approx 1.5 \\times 10^{-6}\\\\ \\end{alignat*}\\] So, restricted to looking at these values, \\(\\theta=\\frac12\\) has the highest likelihood. Find the maximum likelihood estimator of \\(\\theta\\in(0,\\infty)\\), based on the (single) data point \\(x=3\\). Solution We need to find the value of \\(\\theta\\in\\Theta\\) which maximises \\(L(\\theta;3)\\). We differentiate, to look for turning points, obtaining \\[\\begin{align*} \\frac{dL}{d\\theta} &amp;=e^{-3\\theta}-3\\theta e^{-3\\theta}\\\\ &amp;=e^{-3\\theta}(1-3\\theta). \\end{align*}\\] Hence, there is only one turning point, at \\(\\theta=\\frac13\\). We differentiate again, obtaining \\[\\begin{align*} \\frac{d^2L}{d\\theta^2} &amp;=-3e^{-3\\theta}(1-3\\theta)+e^{-3\\theta}(-3)\\\\ &amp;=e^{-3\\theta}(-6+9\\theta) \\end{align*}\\] At \\(\\theta=\\frac13\\), we have \\(\\frac{d^2L}{d\\theta^2}=e^{-1}(-6+3)&lt;0\\), so the turning point at \\(\\theta=\\frac13\\) is a local maximum. Since it is the only turning point, it is also the global maximum. Hence, the maximum likelihood estimator of \\(\\theta\\) is \\(\\hat\\theta=\\frac13\\). The value \\(\\hat\\theta\\) which maximises \\(L(\\theta; x)\\) changes if we use a different value for \\(x\\). This is natural - the choice of parameters that we think is best, depends on the data that we have. "],["models-and-data.html", "Chapter 3 Models and data 3.1 Data: notation 3.2 Models and parameters 3.3 Likelihood functions for i.i.d data", " Chapter 3 Models and data In the previous Chapter we estimated a parameter value based on a single data point. We will usually have more than one observation, and in this section we discuss how to carry out maximum likelihood estimation using many data points. To simplify the presentation we will assume here that we are working with continuous random variables. For discrete random variables, we can simply replace the term “probability density function” with “probability mass function.” However, in a slight abuse of notation, we will use \\(f_X(x)\\) to represent either a probability density function or a probability mass function: for discrete \\(X\\) we will write \\(f_X(x) = Pr(X = x)\\). 3.1 Data: notation Typically we will have a set of \\(n\\) data points which we can think of as a vector, \\(\\mathbf{x}=(x_1,x_2,\\ldots,x_n)\\). We will think of the data as being a realisation of a random vector \\(\\mathbf{X}=(X_1,X_2,\\ldots,X_n)\\). Note the use of capital letters for the random variables and lower case letters for the values they take. The random vector \\(\\mathbf{X}\\) will have a joint probability density function \\[ f_{\\mathbf{X}}(\\mathbf{x})=f_{X_1,X_2,\\cdots,X_n}(x_1,x_2,\\ldots,x_n). \\] This function will be unknown (it will depend on unknown parameters), the aim of the inference being to obtain information about it. We will assume that our data points \\(x_1,x_2,\\ldots,x_n\\) come from independent, identically distributed experiments. With this in mind, we call them i.i.d. samples. Because of this, we also assume that the random variables \\(X_1,X_2,\\ldots,X_n\\) are independent and identically distributed. In this case, the joint probability function \\(f_{\\mathbf{X}}(\\mathbf{x})\\) will be a product of terms for each experiment: \\[\\begin{equation} f_{\\mathbf{X}}(\\mathbf{x})=\\prod_{i=1}^n f_X(x_i), \\end{equation}\\] where \\(f_X(x)\\) is the common probability density function of the random variables \\(X_1,X_2,\\ldots,X_n\\). 3.2 Models and parameters We assume that we already know that the joint probability density function of \\(\\mathbf{X}\\) takes a particular form, usually involving some standard distribution. We refer to this as our model.However, the parameters of this standard distribution are unknown, and our aim in analysing the data will be to obtain good choices of values for these parameters, based on the data we have. Example 3.1 (Models, parameters and data: aerosols.) The ‘particle size distribution’ of an aerosol is the distribution of the diameter of aerosol particles within a typical region of air. The term is also used for particles within a powder, or suspended in a fluid. In many situations, the particle size distribution is modelled using the log-normal distribution. If a random variable \\(Y\\) has log-normal distribution, this simply means that \\(\\log Y\\) is normally distributed. The p.d.f. of the log-normal distribution is \\[ f_Y(y)= \\begin{array}{ll} \\frac{1}{y\\sigma\\sqrt{2\\pi}}\\exp\\left(-\\frac{(\\log y-\\mu)^2}{2\\sigma^2}\\right) &amp; \\text{if }y\\in(0,\\infty)\\\\ 0 &amp; \\text{otherwise}. \\end{array} \\] It is typically reasonable to assume that the diameters of particles are independent. Assuming this model, find the joint probability density function of the diameters observed in a sample of \\(n\\) particles, and state the parameters of the model. Solution The parameters of this distribution, and hence also the parameters of our model, are \\(\\mu\\in\\mathbb{R}\\) and \\(\\sigma\\in(0,\\infty)\\). Since the diameters of particles are assumed to be independent, the joint probability density function of \\(\\mathbf{Y}=(Y_1,Y_2,\\ldots,Y_n)\\), where \\(Y_i\\) is the diameter of the \\(i^{th}\\) particle, is \\[\\begin{align*} f_{\\mathbf{Y}}(y_1,\\ldots,y_n) &amp;=\\prod\\limits_{i=1}^n f_{Y_i}(y_i)\\\\ &amp;=\\begin{cases} \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\frac{1}{y_1y_2\\ldots y_n}\\exp\\left(-\\sum\\limits_{i=1}^n\\frac{(\\log y_i-\\mu)^2}{2\\sigma^2}\\right) &amp; \\text{if }y_i&gt;0\\text{ for all }i\\\\ 0 &amp; \\text{otherwise}. \\end{cases} \\end{align*}\\] Note that, if one (or more) of the \\(y_i\\) is less than or equal to zero then \\(f_{Y_i}(y_i)=0\\), which means that also \\(f_{\\mathbf{Y}}(y_1,\\ldots,y_n)=0\\). It may seem odd to declare that \\(f\\) is unknown, and then assume that in fact \\(f\\) takes a particular form with only unknown parameters. There are statistical methods aimed at handling completely unknown \\(f\\), but they are outside of the scope of this course. In many situations it is sensible to assume a carefully chosen model with unknown parameters. Our choice of model may well be wrong. But we hope that it is approximately correct, there are various ‘goodness of fit’ tests to help us check. We denote the parameters of our model by \\(\\boldsymbol{\\theta}\\) and we represent \\(\\boldsymbol{\\theta}=(\\theta_1,\\theta_2,\\ldots)\\) as a vector. We write \\(\\Theta\\) for the set of possible parameter values. Sometimes some of the unknown parameters are so-called nuisance parameters: their values are unknown, and we have to take account of this in our analysis, but they are not what we are really interested in. Given a model, and a particular set of parameter values \\(\\boldsymbol{\\theta}\\), we write the probability density function of \\(\\mathbf{X}\\) as \\(f_{\\mathbf{X}}(\\mathbf{x};\\boldsymbol{\\theta})\\), to make sure that we don’t forget the importance of the parameters. 3.3 Likelihood functions for i.i.d data How can we apply the ideas of maximum likelihood estimation in our new setting? The key is to note that Definition 2.1 already makes sense in the multivariate case, with our vector \\(\\mathbf{X}\\) in place of \\(X\\) and a vector of data \\(\\mathbf{x}\\) in place of \\(x\\). We also allow more than just one unknown parameter. So, to summarise, let \\(\\mathbf{X}=(X_1,\\ldots,X_k)\\) be a random vector, with a known distribution that has one or more unknown parameters \\(\\boldsymbol\\theta=(\\theta_1,\\theta_2,\\ldots,\\theta_j)\\). Write \\(\\Theta\\) for the set of all possible choices \\(\\boldsymbol\\theta\\) of parameter(s). Let \\(\\mathbf{x}\\) be our vector of data, which we think of as a sample of \\(\\mathbf{X}\\). The likelihood function of \\(\\mathbf{X}\\), given the data \\(\\mathbf{x}\\), is the function \\(L:\\Theta\\to\\mathbb{R}\\) defined by \\[L(\\boldsymbol\\theta;\\mathbf{x})=f_{\\mathbf{X}}(\\mathbf{x};\\boldsymbol\\theta).\\] The likelihood function is therefore the joint density of the observed data, expressed as a function of the model parameters. The (hopefully, unique) value \\(\\boldsymbol\\theta\\in\\Theta\\) which maximises \\(L(\\boldsymbol\\theta;\\mathbf{x})\\) is known as the maximum likelihood estimator of \\(\\boldsymbol\\theta\\), written \\(\\widehat{\\boldsymbol\\theta}\\). It is worth noting that, when an explicit numerical value is found for \\(\\hat\\theta\\), some people refer to the value found for \\(\\hat\\theta\\) as a maximum likelihood estimate (and they would only refer to the general formula for \\(\\hat\\theta\\), in terms of \\(\\mathbf{x}\\), as the maximum likelihood estimator). You can choose whichever convention you prefer. We are mainly interested in the case where our data are i.i.d. samples, and we assume the \\(X_1,X_2,\\ldots,X_n\\) are independent and identically distributed. In this case, from , the likelihood function of our model is \\[\\begin{equation} L(\\boldsymbol\\theta;\\mathbf{x})=f_\\mathbf{X}(\\mathbf{x};\\boldsymbol\\theta)=\\prod\\limits_{i=1}^n f_X(x_i;\\boldsymbol\\theta). \\end{equation}\\] where \\(f_X\\) is the common p.d.f. of the \\(X_i\\). Example 3.2 (Maximum likelihood estimation with i.i.d. data.) Let \\(X\\sim Bern(\\theta)\\), where \\(\\theta\\) is an unknown parameter. Suppose that we have \\(3\\) independent samples of \\(X\\), which are \\[\\mathbf{x}=\\{0,1,1\\}.\\] Find the likelihood function of \\(\\theta\\), given this data. Solution The probability mass function of a single \\(Bern(\\theta)\\) random variable is \\[f_X(x;\\theta)= \\begin{cases} \\theta &amp; \\text{if }x=1\\\\ 1-\\theta &amp; \\text{if }x=0\\\\ 0 &amp; \\text{otherwise} \\end{cases} \\] Since our three samples are independent, we model \\(\\mathbf{x}\\) as a sample from the joint distribution \\(\\mathbf{X}=(X_1,X_2,X_3)\\), where \\[\\begin{align*} f_\\mathbf{X}(\\mathbf{x};\\theta)=\\prod\\limits_{i=1}^3 f_{X_i}(x_i;\\theta) \\end{align*}\\] and \\(f_{X_i}\\) is the probability mass function of a single \\(Bern(\\theta)\\) random variable. Since \\(f_{X_i}\\) has several cases, it would be unhelpful to try and expand out this formula before we put in values for the \\(x_i\\). Our likelihood function is therefore \\[\\begin{align*} L(\\theta;\\mathbf{x})&amp;=f_{X_1}(0;\\theta)\\,f_{X_2}(1;\\theta)\\,f_{X_3}(1;\\theta)\\\\ &amp;=(1-\\theta)\\theta\\theta\\\\ &amp;=\\theta^2-\\theta^3. \\end{align*}\\] The range of values that the parameter \\(\\theta\\) can take is \\(\\Theta=[0,1]\\). Find the maximum likelihood estimator of \\(\\theta\\), given the data \\(\\mathbf{x}\\). Solution We seek to maximize \\(L(\\theta;\\mathbf{x})\\) for \\(\\theta\\in[0,1]\\). Differentiating once, \\[\\frac{dL}{d\\theta}=2\\theta-3\\theta^2=\\theta(2-3\\theta)\\] so the turning points are at \\(\\theta=0\\) and \\(\\theta=\\frac23\\). Differentiating again, \\[\\frac{d^2L}{d\\theta^2}=2-6\\theta\\] which gives \\(\\frac{d^2L}{d\\theta^2}\\big|_{\\theta=0}=2\\) and \\(\\frac{d^2L}{d\\theta^2}\\big|_{\\theta=2/3}=2-4=-2\\). Hence, \\(\\theta=0\\) is a local minimum and \\(\\theta=\\frac23\\) is a local maximum, so \\(\\theta=\\frac23\\) maximises \\(L(\\theta;\\mathbf{x})\\) over \\(\\theta\\in[0,1]\\). The maximum likelihood estimator of \\(\\theta\\) is therefore \\[\\hat\\theta=\\frac23.\\] This is, hopefully, reassuring. The number of \\(1\\)s in our sample of \\(3\\) was \\(2\\), so (using independence) \\(\\theta=\\frac23\\) seems like a good guess. Example 3.3 (Maximum likelihood estimation (radioactive decay).) Atoms of radioactive elements decay as time passes, meaning that any such atom will, at some point in time, suddenly break apart. This process is known as radioactive decay. The time taken for a single atom of, say, carbon-15 to decay is usually modelled as an exponential random variable, with unknown parameter \\(\\lambda\\in(0,\\infty)\\). The parameter \\(\\lambda\\) is known as the ‘decay rate.’ The times at which atoms decay are known to be independent. Using this model, find the likelihood function for the time to decay of a sample of \\(n\\) carbon-15 atoms. Solution The decay time \\(X_i\\) of the \\(i^{th}\\) atom is exponential with parameter \\(\\lambda\\in(0,\\infty)\\), and therefore has p.d.f. \\[f_{X_i}(x_i;\\lambda)= \\begin{cases} \\lambda e^{-\\lambda x_i} &amp; \\text{if }x_i&gt;0\\\\ 0 &amp; \\text{otherwise}. \\end{cases} \\] Since each atom decays independently, the joint distribution of \\(\\mathbf{X}=(X_i)_{i=1}^n\\) is \\[\\begin{align*} f_{\\mathbf{X}}(\\mathbf{x};\\lambda) =\\prod\\limits_{i=1}^n f_{X_i}(x_i;\\lambda) &amp;= \\begin{cases} \\prod\\limits_{i=1}^n \\lambda e^{-\\lambda x_i} &amp; \\text{if }x_i&gt;0\\text{ for all }i\\\\ 0 &amp; \\text{otherwise.} \\end{cases}\\\\ &amp;= \\begin{cases} \\lambda^n \\exp\\left(-\\lambda \\sum_{i=1}^nx_i\\right) &amp; \\text{if }x_i&gt;0\\text{ for all }i\\\\ 0 &amp; \\text{otherwise.} \\end{cases} \\end{align*}\\] Therefore, the likelihood function is \\[L(\\lambda;\\mathbf{x})= \\begin{cases} \\lambda^n \\exp\\left(-\\lambda \\sum_{i=1}^nx_i\\right) &amp; \\text{if }x_i&gt;0\\text{ for all }i\\\\ 0 &amp; \\text{otherwise.} \\end{cases} \\] The range of possible values of the parameter \\(\\lambda\\) is \\(\\Theta=(0,\\infty)\\). Suppose that we have sampled the decay times of \\(15\\) carbon-15 atoms (in seconds, accurate to two decimal places), and found them to be \\[\\begin{align*} \\mathbf{x}&amp;=\\{ 0.50,\\, 2.19,\\, 0.88,\\, 4.06,\\, 9.75,\\, 2.62,\\, 0.13,\\, 2.70,\\, 0.03,\\, 0.28,\\, 4.15,\\, 9.52,\\, 2.67,\\, 3.79,\\, 4.31 \\}, \\end{align*}\\] with \\[ \\sum\\limits_{i=1}^{15} x_i= 47.58. \\] Find the maximum likelihood estimator of \\(\\lambda\\), based on these data. Solution Given this data, for which \\(\\sum\\limits_{i=1}^{15} x_i= 47.58\\), our likelihood function is \\[L(\\lambda;\\mathbf{x})=\\lambda^{15}e^{-47.58\\lambda}.\\] Differentiating, we have \\[\\begin{align*} \\frac{dL}{d\\lambda}&amp;=15\\lambda^{14}e^{-47.58\\lambda}-47.58\\lambda^{15}e^{-47.58\\lambda}\\\\ &amp;=\\lambda^{14}(15-47.58\\lambda)e^{-47.58\\lambda} \\end{align*}\\] which is zero only when \\(\\lambda=0\\) or \\(\\lambda=15/47.58\\approx 0.32\\). Since \\(\\lambda=0\\) is outside of the range \\(\\Theta=(0,\\infty)\\) of possible parameter values, the only turning point of interest is \\(\\lambda=15/47.58\\). Differentiating again (with the details left to you), we end up with \\[\\begin{align*} \\frac{d^2L}{d\\lambda^2} &amp;=(210\\lambda^{13}- 1427.4\\lambda^{14}+2263.86\\lambda^{15})e^{-47.58 \\lambda}\\\\ &amp;=\\lambda^{13}\\left(210-1427.4\\lambda+2263.86\\lambda^2\\right)e^{-47.58\\lambda} \\end{align*}\\] Evaluating at our turning point gives \\[\\frac{d^2L}{d\\lambda^2}\\Big|_{\\lambda=15/47.58}=\\left(\\frac{15}{47.58}\\right)^{13}\\left(-14.9996\\right)e^{-15}&lt;0.\\] So, our turning point is a local maximum. Since there are no other turning points (within the allowable range) our turning point is the global maximum. Hence, the maximum likelihood estimator of \\(\\lambda\\), given our data \\(\\mathbf{x}\\), is \\[\\hat\\lambda=\\frac{15}{47.58}\\approx 0.32.\\] In reality, physicists are able to collect vastly more data than \\(n=15\\), but even with \\(15\\) data points we are not far away from the true value of \\(\\lambda\\), which is \\(\\lambda\\approx 0.283033\\). Of course, by ‘true’ value here we mean the value that has been discovered experimentally, with the help of statistical inference. "],["maximisation-techniques.html", "Chapter 4 Maximisation techniques 4.1 Log-likelihood 4.2 Discrete parameters 4.3 Multi-parameter problems 4.4 Using a computer 4.5 A warning example", " Chapter 4 Maximisation techniques Maximum likelihood estimation comes down to a maximisation problem. Whether this is easy or difficult depends on (a) the statistical model we use in the form \\(f_{\\mathbf{X}}(\\mathbf{x};\\boldsymbol\\theta)\\) and (b) the parameter vector \\(\\boldsymbol{\\theta}\\). One-parameter problems are clearly easier to handle and in many cases multi-parameter problems require the use of numerical maximisation techniques. 4.1 Log-likelihood When maximising \\(L(\\boldsymbol{\\theta};\\mathbf{x})\\) it is usually easier to work with the logarithm of the likelihood instead of the likelihood itself. In this course we always work with natural logarithms. These work well when dealing with the many standard distributions whose p.d.f.s include an exponential term. Definition 4.1 (Log likelihood) Given a likelihood function \\(L(\\boldsymbol\\theta;\\mathbf{x})\\), the log-likelihood function is \\[ \\ell (\\boldsymbol{\\theta};\\mathbf{x})=\\log L(\\boldsymbol{\\theta};\\mathbf{x}). \\] You may be used to reading \\(\\log x\\) as “log base 10 of \\(x\\).” In statistics, the convention is to read \\(\\log x\\) as “log base \\(e\\) of \\(x\\).” In R, log(x) will use base \\(e\\). Maximising \\(\\ell(\\boldsymbol{\\theta};\\mathbf{x})\\) over \\(\\boldsymbol{\\theta}\\in\\Theta\\) produces the same estimator \\(\\widehat{\\boldsymbol\\theta}\\) as maximising \\(L(\\boldsymbol\\theta;\\mathbf{x})\\), because the function \\(\\log(\\cdot)\\) is strictly increasing. However, maximising \\(\\ell\\) is usually easier! Using the log-likelihood is by far the most important maximisation technique. Part of the reason is that \\(\\log(ab)=\\log(a)+\\log(b)\\), so in the case of i.i.d. data points, we have \\[\\ell(\\boldsymbol\\theta;\\mathbf{x})=\\log(L(\\boldsymbol\\theta;\\mathbf{x}))=\\log\\left(\\prod\\limits_{i=1}^nf(x_i;\\boldsymbol\\theta)\\right)=\\sum\\limits_{i=1}^n\\log(f(x_i;\\boldsymbol\\theta)).\\] Using \\(\\ell\\) instead of \\(L\\) changes the \\(\\prod\\) into a \\(\\sum\\), and it is usually easier to work with a sum than a product. Example 4.1 (Maximum likelihood estimation through log-likelihood: mutations in DNA.) When organisms reproduce, the DNA (or RNA) of the offspring is a combination of the DNA of its (one, or two) parents. Additionally, the DNA of the offspring contains a small number of locations in which it differs from its parent(s). These locations are called ‘mutations.’ The number of mutations per unit length of DNA is typically modelled using a Poisson distribution1, with an unknown parameter \\(\\theta\\in(0,\\infty)\\). The numbers of mutations found in disjoint sections of DNA are independent. Using this model, find the likelihood function for the number of mutations present in a sample of \\(n\\) (disjoint) strands of DNA, each of which has unit length. Solution Let \\(X_i\\) be the number of mutations in the \\(i^{th}\\) strand of DNA. So, under our model, \\[f_{X_i}(x_i;\\theta)=\\frac{e^{-\\theta}\\theta^{x_i}}{(x_i)!}\\] for \\(x_i\\in\\{0,1,2,\\ldots\\}\\), and \\(f_{X_i}(x_i)=0\\) if \\(x_i\\notin\\mathbb{N}\\cup\\{0\\}\\). Since we assume the \\((X_i)\\) are independent, the joint distribution of \\(\\mathbf{X}=(X_1,X_2,\\ldots,X_n)\\) has probability mass function \\[\\begin{align*} f_{\\mathbf{X}}(\\mathbf{x})&amp;= \\prod\\limits_{i=1}^n\\frac{e^{-\\theta}\\theta^{x_i}}{(x_i)!} \\\\ &amp;= \\frac{1}{(x_1)!(x_2)!\\ldots (x_n)!}e^{-n\\theta}\\theta^{\\sum_1^n x_i} \\end{align*}\\] provided all \\(x_i\\in\\mathbb{N}\\cup\\{0\\}\\), and zero otherwise. Therefore, our likelihood function is \\[L(\\theta;\\mathbf{x})=\\frac{1}{(x_1)!(x_2)!\\ldots (x_n)!}e^{-n\\theta}\\theta^{\\sum_1^n x_i}.\\] The range of possible values for \\(\\theta\\) is \\(\\Theta=(0,\\infty)\\). Let \\(\\mathbf{x}\\) be a vector of data, where \\(x_i\\) is the number of mutations observed in a (distinct) unit length segment of DNA. Suppose that at least one of the \\(x_i\\) is non-zero. Find the corresponding log-likelihood function, and hence find the maximum likelihood estimator of \\(\\theta\\). Solution The log-likelihood function is \\(\\ell(\\theta;\\mathbf{x})=\\log L(\\theta;\\mathbf{x})\\), so \\[\\begin{align*} \\log L(\\theta,\\mathbf{x}) &amp;=\\log\\left(\\frac{1}{(x_1)!(x_2)!\\ldots (x_n)!}e^{-n\\theta}\\theta^{\\sum_1^n x_i}\\right)\\\\ &amp;=\\sum\\limits_{i=1}^n(-\\log (x_i)!) -n\\theta + (\\log \\theta)\\sum\\limits_{i=1}^n x_i. \\end{align*}\\] We now look to maximise \\(l(\\theta;\\mathbf{x})\\), over \\(\\theta\\in(0,\\infty)\\). Differentiating, we obtain \\[\\frac{d\\ell}{d\\theta}=-n+\\frac{1}{\\theta}\\sum\\limits_{i=1}^n x_i.\\] Note that this is much simpler than what we’d get if we differentiated \\(L(\\theta;\\mathbf{x})\\). So, the only turning point of \\(\\ell(\\theta,\\mathbf{x})\\) is at \\(\\theta=\\frac{1}{n}\\sum_{i=1}^nx_i\\). Differentiating again, we have \\[\\frac{d^2\\ell}{d\\theta^2}=-\\frac{1}{\\theta^2}\\sum\\limits_{i=1}^nx_i.\\] Since our \\(x_i\\) are counting the occurrences of mutations, \\(x_i\\geq 0\\), and since at least one is non-zero we have \\(\\frac{d^2l}{d\\theta^2}&lt;0\\) (for all \\(\\theta\\)). Hence, our turning point is a maximum and, since it is the only maximum, is also the global maximum. Therefore, the maximum likelihood estimator of \\(\\theta\\) is \\[\\hat\\theta=\\frac{1}{n}\\sum\\limits_{i=1}^nx_i.\\] Mutations rates were measured, for 11 HIV patients, and there were found to be \\[\\begin{align*} &amp;\\mathbf{x}=\\Big\\{19,\\, 16,\\, 37,\\, 28,\\, 24,\\, 34,\\, 37,\\, 126,\\, 32,\\, 48,\\, 45\\Big\\} \\end{align*}\\] mutations per \\(10^4\\) possible locations (i.e.~`per unit length’). This data comes from the article Cuevas et al. (2015)2. Assuming the model suggested above, calculate the maximum likelihood estimator of the mutation rate of HIV. Solution The data has \\[\\bar{x}=\\frac{1}{11}\\sum\\limits_{i=1}^{11}x_i=\\frac{446}{11}\\approx 41\\] so we conclude that the maximum likelihood estimator of the mutation rate \\(\\theta\\), given this data, is \\(\\hat\\theta=\\frac{446}{11}.\\) 4.2 Discrete parameters When we maximise \\(L(\\boldsymbol{\\theta};\\mathbf{x})\\) (or \\(\\ell(\\boldsymbol\\theta;\\mathbf{x})\\)), we need to be careful to keep \\(\\boldsymbol\\theta\\) within the parameter set \\(\\Theta\\). In most of the examples we will meet in this module \\(\\boldsymbol{\\theta}\\) will be continuous and so we can use differentiation to obtain the maximum. However, in some cases, such as the next example, the possible values of \\(\\boldsymbol{\\theta}\\) may be discrete (i.e. \\(\\Theta\\) is a discrete set) and in such cases we cannot use differentiation. Instead, we just check each value of \\(\\boldsymbol\\theta\\) in turn and find out which \\(\\boldsymbol\\theta\\) gives the biggest \\(L(\\boldsymbol\\theta;\\mathbf{x})\\). Example 4.2 (Maximum likelihood estimation for discrete parameters: mass spectroscopy.) Using a mass spectrometer, it is possible to measure the mass3 of individual molecules. For example, it is possible to measure the masses of individual amino acid molecules. A sample of \\(15\\) amino acid molecules, which are all known to be of the same type (and therefore, the same mass), were reported to have masses \\[\\begin{align*} &amp;\\mathbf{x}=\\{ 65.76,\\, 140.40,\\, 94.02,\\, 32.23,\\, 115.00,\\, 4.77,\\, 116.00,\\, 86.41,\\, \\\\ &amp; 91.14,\\, 66.27,\\, 91.00,\\, 144.7.\\, 39.33,\\, 58.90 \\}. \\end{align*}\\] It is known that these molecules are either Alanine, which has mass \\(71.0\\), or Leucine, which has mass \\(113.1\\). Given a molecule of mass \\(\\theta\\), the spectrometer is known to report its mass as \\(X\\sim N(\\theta,35^2)\\), independently for each molecule. Using this model, and the data above, find the likelihoods of Alanine and Leucine. Specify which of these has the greatest the likelihood. Solution Our model, for the reported mass \\(X\\) of a single molecule with (real) weight \\(\\theta\\), is \\(X\\sim N(0,35^2)\\). Therefore, \\(X_i\\sim N(\\theta,35^2)\\) and the p.d.f. of a single data point is \\[f_{X_i}(x_i)=\\frac{1}{\\sqrt{2\\pi}35}\\exp\\left(-\\frac{(x_i-\\theta)^2}{2\\times 35^2}\\right).\\] Therefore, the p.d.f. of the reported masses \\(\\mathbf{X}=(X_1,\\ldots,X_n)\\) of \\(n\\) molecules is \\[f_{\\mathbf{X}}(x)=\\prod\\limits_{i=1}^n f_{X_i}(x_i)=\\frac{1}{(2\\pi)^{n/2}35^n}\\exp\\left(-\\frac{1}{2450}\\sum\\limits_{i=1}^n(x_i-\\theta)^2\\right).\\] We know that, in reality, \\(\\theta\\) must be one of only two different values; 71.0 (for Alanine) and 113.1 (for Leucine). Therefore, our likelihood function is \\[L(\\theta; \\mathbf{x})=\\frac{1}{(2\\pi)^{n/2}35^n}\\exp\\left(-\\frac{1}{2450}\\sum\\limits_{i=1}^n(x_i-\\theta)^2\\right)\\] and the possible range of values for \\(\\theta\\) is the two point set \\(\\Theta=\\{71.0,113.1\\}\\). We need to find out which of these two values maximises the likelihood. Our data \\(\\mathbf{x}\\) contains \\(n=15\\) data points. A short calculation shows that \\[\\frac{1}{2450}\\sum\\limits_{i=1}^{15}(x_i-71.0)^2\\approx 12.70,\\hspace{3pc}\\frac{1}{2450}\\sum\\limits_{i=1}^{15}(x_i-113.1)^2\\approx 20.41.\\] and, therefore, that \\[L(71.0;\\mathbf{x})\\approx 2.19\\times 10^{-34}, \\hspace{3pc}L(113.1;\\mathbf{x})=9.90\\times 10^{-38}.\\] We conclude that \\(\\theta=71.0\\) has (much) greater likelihood than \\(\\theta=113.1\\), so we expect that the molecules sampled are Alanine. Note that, if we were to differentiate (as we did in other examples), we would find the maximiser \\(\\theta\\) for \\(L(\\theta;\\mathbf{x})\\) \\(\\theta\\in(-\\infty,\\infty)\\), which turns out to be \\(\\theta=81.07\\). This is not what we want here! The design of our experiment has meant that the range of possible values for \\(\\theta\\) is restricted to the two point set \\(\\Theta=\\{71.0,113.1\\}.\\) 4.3 Multi-parameter problems For multi-parameter problems, where \\(\\boldsymbol{\\theta}\\) is a vector, a similar procedure can be followed. Here for simplicity we consider only the case where there are 2 parameters (so that \\(\\boldsymbol{\\theta}\\) is a \\(2\\times 1\\) vector) and write \\(\\boldsymbol{\\theta}=(\\theta_1,\\theta_2)\\). Now we find a stationary point \\(\\widehat{\\boldsymbol{\\theta}}=(\\widehat{\\theta}_1,\\widehat{\\theta}_2)\\) of the log-likelihood by solving the simultaneous equations \\[\\begin{equation} \\frac{\\partial \\ell(\\boldsymbol{\\theta},\\mathbf{x})}{\\partial \\theta_1}=0,\\quad \\frac{\\partial \\ell(\\boldsymbol{\\theta},\\mathbf{x})}{\\partial \\theta_2}=0.\\tag{4.1} \\end{equation}\\] These equations are the analogue of the one parameter case in which we solve \\(\\frac{df}{dx}=0\\). In two dimensions and higher, the turning points that we find may be maxima or minima, or saddle points. To check that a turning point is a (local) maximum, we again have to consider second derivatives. First we calculate the so called Hessian matrix: \\[ H=\\left(\\begin{array}{cc} \\dfrac {\\partial^2 \\ell(\\boldsymbol{\\theta};\\mathbf{x})}{\\partial \\theta_1^2 } &amp; \\dfrac {\\partial^2 \\ell(\\boldsymbol{\\theta};\\mathbf{x})}{\\partial \\theta_1 \\partial \\theta_2 }\\\\ \\dfrac {\\partial^2 \\ell(\\boldsymbol{\\theta};\\mathbf{x})}{\\partial \\theta_1\\partial \\theta_2} &amp; \\dfrac {\\partial^2 \\ell(\\boldsymbol{\\theta};\\mathbf{x})}{\\partial \\theta_2^2 } \\end{array}\\right) \\] and then we evaluate \\(H\\) at \\(\\boldsymbol{\\theta}=\\widehat{\\boldsymbol{\\theta}}\\), where \\(\\widehat{\\boldsymbol{\\theta}}\\) is the stationary point we found using (4.1). In the 2 variable case we can use a fact from multi-variable calculus: if \\[\\begin{equation} \\frac{\\partial^2 \\ell(\\boldsymbol{\\theta};\\mathbf{x})}{\\partial \\theta_1^2}\\bigg{|}_{\\boldsymbol\\theta=\\widehat{\\boldsymbol\\theta}}&lt;0 \\hspace{1pc}\\text{ and }\\hspace{1pc} \\det H\\;\\Big{|}_{\\boldsymbol\\theta=\\widehat{\\boldsymbol\\theta}}&gt;0 \\tag{4.2} \\end{equation}\\] then we can conclude that our turning point is a local maximum. Example 4.3 (Multi-parameter maximum likelihood estimation (rainfall).) Find the maximum likelihood estimator of the parameter vector \\(\\boldsymbol{\\theta}=(\\mu,\\sigma^2)\\) when the data \\(\\mathbf{x}=(x_1,x_2,\\ldots,x_n)\\) are modelled as i.i.d. samples from a normal distribution \\(N(\\mu,\\sigma^2)\\). Solution Our parameter vector is \\(\\boldsymbol\\theta=(\\mu,\\sigma^2)\\), so let us write \\(v=\\sigma^2\\) to avoid confusion. As a result, we are interested in the parameters \\(\\boldsymbol\\theta=(\\mu,v)\\), and the range of possible values of \\(\\boldsymbol\\theta\\) is \\(\\Theta=\\mathbb{R}\\times(0,\\infty)\\). The p.d.f. of the univariate normal distribution \\(N(\\mu,v)\\) is \\[f_X(x)=\\frac{1}{\\sqrt{2\\pi v}}e^{-(x-\\mu)^2/2v}.\\] Writing \\(\\mathbf{X}=(X_1,\\ldots,X_n)\\), where the \\(X_i\\) are i.i.d.~univariate \\(N(\\mu,v)\\) random variables, the likelihood function of \\(\\mathbf{X}\\) is \\[L(\\boldsymbol{\\theta};\\mathbf{x})=f_\\mathbf{X}(\\mathbf{x})=\\frac{1}{(2\\pi v)^{n/2}} \\exp\\left(-\\frac{1}{2v}\\sum_{i=1}^n(x_i-\\mu)^2\\right).\\] Therefore, the log likelihood is \\[\\ell(\\boldsymbol{\\theta};\\mathbf{x}) = -\\frac{n}{2}\\left(\\log(2\\pi)+\\log(v)\\right)- \\frac{1}{2v}\\sum_{i=1}^n(x_i-\\mu)^2.\\] We now look to maximise \\(\\ell(\\boldsymbol{\\theta};\\mathbf{x})\\) over \\(\\boldsymbol\\theta\\in\\Theta\\). The partial derivatives are \\[\\begin{align*} \\frac{\\partial \\ell}{\\partial \\mu} &amp;= \\frac{1}{v}\\sum_{i=1}^n (x_i-\\mu)=\\frac{1}{v}\\left( \\sum_{i=1}^n x_i -n\\mu\\right) \\\\ \\frac{\\partial \\ell}{\\partial v} &amp;= -\\frac{n}{2v}+\\frac{1}{2v^2} \\sum_{i=1}^n(x_i-\\mu)^2. \\end{align*}\\] Solving \\(\\frac{\\partial \\ell}{\\partial \\mu}=0\\) gives \\(\\mu=\\frac1n\\sum_{i=1}^n x_i =\\bar{x}\\). Solving \\(\\frac{\\partial \\ell}{\\partial v}=0\\) gives \\(v=\\frac1n\\sum_{i=1}^n (x_i-\\mu)^2\\). So both partial derivatives will be zero if and only if \\[\\begin{equation} \\label{eq:normal_2param_turning_point} \\mu = \\bar{x},\\hspace{4pc} v = \\frac1n\\sum_{i=1}^n (x_i-\\bar{x})^2. \\end{equation}\\] This gives us the value of \\(\\theta=(\\mu,v)\\) at the (single) turning point of \\(\\ell\\). Next, we use the Hessian matrix to check if this point is a local maximum. We have \\[\\begin{align*} \\frac{\\partial^2 \\ell}{\\partial \\mu^2} &amp;= -\\frac{n}{v} \\\\ \\frac{\\partial^2 \\ell}{\\partial \\mu\\partial v} &amp;= \\frac{-1}{v^2}\\left( \\sum_{i=1}^n x_i -n\\mu\\right) \\\\ \\frac{\\partial^2 \\ell}{\\partial v^2} &amp;= \\frac{n}{2v^2}-\\frac{1}{v^3}\\sum_{i=1}^n(x_i-\\mu)^2 \\end{align*}\\] Evaluating these at our turning point, we get \\[\\begin{align*} \\left.\\frac{\\partial^2 \\ell}{\\partial \\mu^2}\\right|_{\\widehat{\\boldsymbol{\\theta}}} &amp;= -\\frac{n}{\\hat{v}} \\\\ \\left.\\frac{ \\partial^2 \\ell}{\\partial \\mu\\partial v} \\right|_{\\widehat{\\boldsymbol{\\theta}}} &amp;= \\frac{-1}{v^2} \\left( \\sum_{i=1}^n x_i -n\\bar{x}\\right)=0 \\\\ \\left.\\frac{\\partial^2 \\ell}{\\partial v^2}\\right|_{\\widehat{\\boldsymbol{\\theta}}} &amp;= \\frac{n}{2v^2}-\\frac{1}{v^3}\\sum_{i=1}^n(x_i-\\bar{x})^2= \\frac{n}{2v^2}-\\frac{1}{v^3}n\\hat{v}=\\frac{-n}{2v^2} \\end{align*}\\] so \\[H=\\begin{pmatrix}-\\frac{n}{v} &amp; 0 \\\\ 0 &amp; \\frac{-n}{2v^2}\\end{pmatrix}.\\] Since \\(-\\frac{n}{v}&lt;0\\) and \\(\\det H=\\frac{n^2}{2v^3}&gt;0\\), our turning point is a local maximum. Since it is the only turning point, it is also the global maximum. Hence, the MLE is \\[\\begin{align*} \\hat{\\mu} &amp;= \\bar{x} \\\\ \\hat{\\sigma}^2=\\hat{v} &amp;= \\frac1n\\sum_{i=1}^n (x_i-\\bar{x})^2. \\end{align*}\\] Note \\(\\hat{\\mu}\\) is the sample mean, and \\(\\hat{\\sigma}^2\\) is the (biased) sample variance. For the years 1985-2015, the amount of rainfall (in millimetres) recorded as falling on Sheffield in December is as follows: \\[\\begin{align*} &amp;\\{78.0,\\, 142.3, \\, 38.2, \\, 36.0, \\, 159.1, \\, 136.0, \\, 78.4, \\, 67.4, \\, 171.4,\\\\ &amp; 103.9, \\, 70.4, \\, 98.2, \\, 79.4,\\, 57.9, \\, 135.6, \\, 118.0, \\, 28.0, \\, 129.8, \\\\ &amp; 106.5, \\, 46.3, \\, 56.7, \\, 114.0, \\, 74.9, \\, 52.8, \\, 66.1, \\, 18.8, \\, 124.6,\\, 136.0, \\\\ &amp; 69.8, \\, 102.0, \\, 121.2\\}. \\end{align*}\\] Denoting the observed rainfall in the \\(i\\)th year by \\(x_i\\), we have \\[\\bar{x}=\\frac{1}{30}\\sum\\limits_{i=1}^{30}\\approx 93.9,\\hspace{3pc}\\frac{1}{30}\\sum\\limits_{i=1}^{30}(x_i-\\bar{x})^2\\approx1631.2\\approx40.4^2\\] This data comes from the historical climate data stored by the Met Office. Meteorologists often model the long run distribution of rainfall by a normal distribution (although in some cases the Gamma distribution is used). Assuming that we choose to model the amount of rainfall in Sheffield each December by a normal distribution, find the maximum likelihood estimators for \\(\\mu\\) and \\(\\sigma^2\\). Solution The data has \\(n=30\\), and \\[\\bar{x}=\\frac{1}{30}\\sum\\limits_{i=1}^{30}\\approx 93.9,\\hspace{3pc}\\frac{1}{30}\\sum\\limits_{i=1}^{30}(x_i-\\bar{x})^2\\approx1631.2\\approx40.4^2\\] So we conclude that, according to our model, the maximum likelihood estimators are \\(\\hat\\mu\\approx 93.9\\) and \\(\\hat{\\sigma^2}\\approx 40.4^2\\), which means that Sheffield receives a \\(N(93.9,40.4^2)\\) quantity of rainfall, in millimetres, each December. In the general multivariate case, to check that a turning point is a local maxima we should check that \\(H\\), when evaluated at the turning point, is a negative definite matrix. This fact is outside of the scope of our course, but we mention it here for completeness. A negative definite \\(k\\times k\\) matrix \\(\\mathbf{M}\\) is a matrix for which \\(\\mathbf{a}^T\\mathbf{M}\\mathbf{a}&lt;0\\) for all non-zero vectors \\(\\mathbf{a}\\in\\mathbb{R}^k\\). When \\(k=2\\) this is equivalent to (4.2). For example, you can easily check that \\(-I\\), where \\(I\\) is the identity matrix, is negative definite. 4.4 Using a computer In some cases, particularly when a complex model is used, or when many parameters are unknown, it is not possible to obtain an expression for the maximum likelihood estimator \\(\\hat\\theta\\). These cases can be approached with the aid of a computer, and , which means using a computer to try and approximate the maximum value of the likelihood function. There are a wide range of algorithms designed to maximise functions numerically, but this is outside the scope of our current course. 4.5 A warning example Sometimes, we have to be very careful about using differentiation to maximise the likelihood function. We illustrate with an example. Example 4.4 (Maximum likelihood estimation for the uniform distribution.) Find the maximum likelihood estimator of the parameter \\(\\theta\\) when the data \\(\\mathbf{x}=(x_1,x_2,\\ldots,x_n)\\) are i.i.d. samples from a uniform distribution \\(U[0,\\theta]\\), with unknown parameter \\(\\theta&gt;0\\). Solution Here the p.d.f. of \\(X_i\\) is \\(f(x)=\\frac{1}{\\theta}\\) for \\(0\\leq x\\leq \\theta\\) and zero otherwise. So the likelihood, for \\(\\theta\\in \\Theta=\\mathbb{R}^+\\), is \\[\\begin{align*} L(\\theta;\\mathbf{x}) &amp;= \\begin{cases} \\frac{1}{\\theta^n} &amp; \\text{if }\\theta \\geq x_i\\text{ for all }i \\\\ 0 &amp; \\text{if }\\theta&lt;x_i\\text{ for some }i \\end{cases}\\\\ &amp;= \\begin{cases} \\frac{1}{\\theta^n} &amp; \\text{if }\\theta \\geq\\max_{i}x_i\\\\ 0 &amp; \\text{if }\\theta&lt;\\max_{i}x_i. \\end{cases} \\end{align*}\\] Differentiating the likelihood, we see that \\(L(\\theta;\\mathbf{x})\\) is decreasing (but positive) for \\(\\theta&gt;\\max_{i}x_i\\). For \\(\\theta&lt;\\max_{i}x_i\\) we know \\(L(\\theta;\\mathbf{x})=0\\), so by looking at the graph, we can see that the maximum occurs at \\[\\theta=\\hat{\\theta}=\\max_{i=1,\\ldots,n}x_i.\\] This is the MLE. The moral of the story is: if something seems strange during maximisation, draw a picture of the function you are trying to maximise. Actually, the biological details here are rather complicated, and we omit discussion of them.↩︎ http://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1002251↩︎ This is a simplification; in reality a mass spectrometer measures the mass to charge ratio of the molecule, but since the charges of molecule are already known, the mass can be inferred later. Atomic masses are measured in so-called ‘atomic mass units.’↩︎ "],["likelihood-for-confidence-intervals-and-hypothesis-tests.html", "Chapter 5 Likelihood for confidence intervals and hypothesis tests 5.1 Confidence intervals 5.2 Hypothesis tests 5.3 A more formal approach", " Chapter 5 Likelihood for confidence intervals and hypothesis tests Maximum likelihood estimation gives us a single value for the unknown parameters \\(\\boldsymbol{\\theta}\\), a so-called point estimate. In many settings in statistical inference we want to go further than point estimation, in particular to give some idea of the uncertainty in our point estimate. For example, where we are trying to estimate a single parameter \\(\\theta\\), we may want to produce an interval estimate, typically a set of values \\([\\theta_1,\\theta_2]\\) which we believe that the true value \\(\\theta\\) lies in. Alternatively, we may want to test a hypothesis about \\(\\theta\\). The likelihood function can often be used to construct appropriate methods in these settings too, and as with maximum likelihood estimation it can often be shown that they are in some sense optimal. 5.1 Confidence intervals We will start off by thinking about interval estimation. Assume, in the one parameter case, that we have a likelihood function \\(L(\\theta;\\mathbf{x})\\) defined for \\(\\theta\\in\\Theta\\), maximised at its maximum likelihood estimate \\(\\hat{\\theta}\\). Then a natural choice of interval estimate is to set some threshold, \\(L_0\\) say, and to use the values of \\(\\theta\\) such that \\(L(\\theta;\\mathbf{x})\\geq L_0\\) as an interval estimate. One common choice for the threshold is to choose \\(L_0\\) to be a fixed multiple of the maximum likelihood, say \\[L_0=e^{-k}L(\\hat{\\theta};\\mathbf{x})\\] for some chosen \\(k&gt;0\\). Equivalently in terms of the log-likelihood, \\[\\log L_0=\\ell(\\hat{\\theta};\\mathbf{x})-k.\\] Our choice of \\(k\\) here will involve a trade off between a precise answer (meaning a narrow interval) and minimising the risk of missing the true value from the interval: a small \\(k\\) will give a narrow interval but relatively low confidence that the interval contains the true value, while a large \\(k\\) will give a larger interval and higher confidence. More generally, we can make the following definition. The \\(k\\)-unit likelihood region for parameters \\(\\boldsymbol{\\theta}\\) based on data \\(\\mathbf{x}\\) is the region \\[R_k=\\left\\{\\boldsymbol{\\theta}:L(\\boldsymbol{\\theta};\\mathbf{x})\\geq e^{-k}L(\\boldsymbol{\\hat{\\theta}};\\mathbf{x})\\right\\},\\] or equivalently \\[R_k=\\left\\{\\boldsymbol{\\theta}:\\ell(\\boldsymbol{\\theta};\\mathbf{x})\\geq \\ell(\\boldsymbol{\\hat{\\theta}};\\mathbf{x})-k\\right\\},\\] where \\(\\boldsymbol{\\hat{\\theta}}\\) is the maximum likelihood estimate of \\(\\boldsymbol{\\theta}\\) based on \\(\\mathbf{x}\\). The values of \\(\\boldsymbol{\\theta}\\) within the \\(k\\)-unit likelihood region are those whose likelihood is at least within a factor \\(e^{-k}\\) of the maximum. For instance, points in the 1-unit region have likelihoods within a factor \\(e^{-1} = 0.368\\) of the maximum. The 2-unit region contains points with likelihoods within a factor \\(e^{-2} = 0.135\\) of the maximum. The 2-unit region is the most commonly used in practice. Example 5.1 (Likelihood regions.) Suppose that we have i.i.d. data \\(\\mathbf{x}=(x_1,x_2,\\ldots,x_n)\\), for which each data point is modelled as a random sample from \\(N(\\mu,\\sigma^2)\\) where \\(\\mu\\) is unknown and \\(\\sigma^2\\) is known. Find the \\(k\\)-likelihood region \\(R_k\\) for the parameter \\(\\mu\\). Solution First, we need to find the MLE \\(\\hat\\mu\\) of \\(\\mu\\). The likelihood function for our model is \\[L(\\mu;\\mathbf{x})=\\prod\\limits_{i=1}^n \\phi(x_i;\\mu)=\\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(x_i-\\mu)^2\\right),\\] where the range of parameter values is all \\(\\mu\\in\\mathbb{R}\\). The log likelihood is \\[\\ell(\\mu;\\mathbf{x}) = -\\frac{n}{2}\\left(\\log(2\\pi)+\\log(\\sigma^2)\\right)-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(x_i-\\mu)^2.\\] The usual process of maximisation shows that the maximum likelihood estimator is the sample mean, \\[\\hat\\mu=\\frac{1}{n}\\sum\\limits_{i=1}^n x_i.\\] Now we are ready to identify the \\(k\\)-likelihood region for \\(\\mu\\). By definition, the \\(k\\)-likelihood region is \\[R_k=\\left\\{\\mu\\in\\mathbb{R}:|l(\\mu;\\mathbf{x})-l(\\hat{\\mu};\\mathbf{x})|\\leq k\\right\\}.\\] So, \\(\\mu\\in R_k\\) if and only if \\[\\left|\\frac{1}{2\\sigma^2}\\sum\\limits_{i=1}^n(x_i-\\mu)^2-\\frac{1}{2\\sigma^2}\\sum\\limits_{i=1}^n(x_i-\\hat\\mu)^2\\right|\\leq k.\\] We can simplify this inequality, by noting that \\[\\begin{align*} \\sum\\limits_{i=1}^n(x_i-\\mu)^2-\\sum\\limits_{i=1}^n(x_i-\\hat\\mu)^2 &amp;=\\sum\\limits_{i=1}^n x_i^2-2x_i\\mu+\\mu^2-x_i^2+2x_i\\hat\\mu-\\hat\\mu^2\\\\ &amp;=n\\mu^2-n\\hat\\mu^2+2(\\hat\\mu-\\mu)\\sum\\limits_{i=1}^n x_i\\\\ &amp;=n\\mu^2-n\\hat\\mu^2+2(\\hat\\mu-\\mu)n\\hat\\mu\\\\ &amp;=n(\\mu^2+\\hat\\mu^2-2\\mu\\hat\\mu)\\\\ &amp;=n(\\hat\\mu-\\mu)^2. \\end{align*}\\] So, \\(\\mu\\in R_k\\) if and only if \\[\\frac{n}{2\\sigma^2}|\\hat\\mu-\\mu|^2\\leq k,\\] or in other words, \\(|\\hat\\mu-\\mu|\\leq \\sigma\\sqrt{\\frac{2k}{n}}\\), so \\[R_k=\\left[\\hat\\mu-\\sigma\\sqrt{\\frac{2k}{n}},\\;\\hat\\mu+\\sigma\\sqrt{\\frac{2k}{n}}\\right].\\] 5.2 Hypothesis tests If we are trying to test a null hypothesis \\(H_0:\\theta=\\theta_0\\) against a general alternative hypothesis \\(H_1:\\theta\\neq \\theta_0\\), then we can use a similar idea: we choose a suitable \\(k\\), construct the \\(k\\)-likelihood region \\(R_k\\), and accept \\(H_0\\) if \\(\\theta_0\\) is inside \\(R_k\\), or reject \\(H_0\\) if \\(\\theta\\) is outside \\(R_k\\). Example 5.2 (Hypothesis tests based on likelihood.) In Example , if we used a \\(2\\)-likelihood test, would we accept the hypothesis that the radioactive decay of carbon-15 is equal to \\(\\lambda=0.27\\)? Solution We had found, given the data, that the likelihood function of \\(\\theta\\) was \\[L(\\lambda;\\mathbf{x})=\\lambda^{15}e^{-47.58\\lambda}\\] and the maximum likelihood estimator of \\(\\lambda\\) was \\(\\hat\\lambda\\approx 0.32.\\) The \\(2\\)-likelihood region for \\(\\lambda\\) is the set \\[R_2=\\left\\{\\lambda&gt;0:L(\\lambda;\\mathbf{x})\\geq e^{-2}L(\\hat\\lambda;\\mathbf{x})\\right\\},\\] so \\(\\lambda\\in R_2\\) if and only if \\[\\lambda^{15}e^{-47.58\\lambda}\\geq e^{-2}L(0.32;\\mathbf{x})=1.24\\times 10^{-15}.\\] Note that, unlike the previous example, we can’t simplify this inequality and find a `nice’ form for the likelihood region. Our hypothesis is that, in fact, \\(\\lambda=0.27\\). Our \\(2\\)-likelihood test will pass if \\(\\lambda=0.27\\) is within the \\(2\\)-likelihood region, and fail if not. We can evaluate (use e.g.~), \\[0.27^{15}e^{-47.58\\times 0.27}\\approx 7.78\\times 10^{-15}\\] and note that \\(7.78\\times 10^{-15}\\geq 1.24\\times 10^{-15}\\). Hence \\(\\lambda=0.27\\) is within the \\(2\\)-likelihood region and we accept the hypothesis. 5.3 A more formal approach The confidence intervals and hypothesis tests described above were justified informally, in a way that is sufficient for this module. We will now briefly discuss a more formal justification. The theory in the following sections may seem overwhelming, but don’t panic! The aim here is just to make you aware of some important results that underpin a lot of statistical theory. We will work through two simple examples, so you can get some sense of how these results can be applied. 5.3.1 Asymptotic normality of the maximum likelihood estimator In the usual notation, we have \\(n\\) i.i.d. random variables \\(X_1,\\ldots,X_n\\) with distribution depending on some unknown vector of parameters. \\(\\boldsymbol{\\theta}\\). Suppose we have derived an expression for the maximum likelihood estimator. Before we observe the data, we can think of the estimator as a function of \\(\\mathbf{X} = (X_1,\\ldots,X_n)\\): we denote it by \\(\\hat{\\boldsymbol{\\theta}}(\\mathbf{X})\\). We think of \\(\\hat{\\boldsymbol{\\theta}}(\\mathbf{X})\\) as a random variable, because it is a function of the random \\(\\mathbf{X}\\). For example, for \\(X_1,\\ldots,X_n\\stackrel{i.i.d}{\\sim}N(\\mu, \\sigma^2)\\), the maximum likelihood estimator for \\(\\mu\\) is \\(\\bar{X}\\), and \\(\\bar{X}\\) is a random variable (normally distributed with mean \\(\\mu\\) and variance \\(\\sigma^2/n\\)). It can be shown that as \\(n\\rightarrow \\infty\\), the distribution of \\(\\hat{\\boldsymbol{\\theta}}(\\mathbf{X})\\) tends to the (multivariate) normal distribution \\[ \\hat{\\boldsymbol{\\theta}}(\\mathbf{X}) \\sim N(\\boldsymbol{\\theta}, I_E(\\boldsymbol{\\theta})^{-1}), \\] where \\(I_E(\\boldsymbol{\\theta})\\) is the Fisher Information Matrix, defined for a vector \\(\\boldsymbol{\\theta}=(\\theta_1,\\ldots,\\theta_d)\\) by \\[\\begin{equation} I_E(\\boldsymbol{\\theta})=\\left(\\begin{array}{ccc}e_{1,1}(\\boldsymbol{\\theta}) &amp; \\cdots &amp; e_{1,d}(\\boldsymbol{\\theta})\\\\ \\vdots &amp; &amp; \\vdots \\\\ e_{d,1}(\\boldsymbol{\\theta}) &amp; \\cdots &amp; e_{d,d}(\\boldsymbol{\\theta}) \\end{array}\\right), \\end{equation}\\] with \\[\\begin{equation} e_{i,j}(\\boldsymbol{\\theta})=E\\left\\{-\\frac{\\partial^2}{\\partial \\theta_i\\,\\partial \\theta_j}\\ell(\\boldsymbol{\\theta}; \\mathbf{X})\\right\\}. \\end{equation}\\] The expectation on the right hand side needs a little unpacking. It is the expectation of a function of the random variable \\(\\mathbf{X}\\). This function is a partial derivative of another function: the log-likelihood function evaluated at \\(\\mathbf{X}\\) and expressed as a function of \\(\\boldsymbol{\\theta}\\). 5.3.2 Confidence intervals based on asymptotic normality Now suppose we want to construct a \\(100(1-\\alpha)\\%\\) confidence interval for any particular element of \\(\\boldsymbol{\\theta}\\), say \\(\\theta_j\\). For suitably large \\(n\\), we have \\[\\begin{equation} \\hat{\\theta_j} \\sim N(\\theta_j,\\gamma_{j,j}),\\tag{5.1} \\end{equation}\\] where we \\(\\gamma_{j,j}\\) is the \\(\\{j,j\\}\\) element of \\(I_E(\\theta)^{-1}\\). This then gives us an approximate interval as \\[\\begin{equation} (\\hat{\\theta_j}-z_{1-\\frac{\\alpha}{2}}\\sqrt{\\gamma_{j,j}},\\hat{\\theta_j}+ z_{1-\\frac{\\alpha}{2}}\\sqrt{\\gamma_{j,j}}), \\end{equation}\\] with \\(z_{1-\\frac{\\alpha}{2}}\\) the appropriate percentile from the standard normal distribution. We will not be able to calculate this interval in practice, as we do not know the true value of \\(\\boldsymbol{\\theta}\\), which we would need to evaluate \\(I_E(\\boldsymbol{\\theta})\\). Instead, we approximate \\(I_E(\\boldsymbol{\\theta})\\) by the observed information matrix \\[\\begin{equation} I_O(\\boldsymbol{\\theta})=\\left(\\begin{array}{ccc}-\\frac{\\partial^2}{\\partial \\theta_1^2}\\ell(\\boldsymbol{\\theta}; \\mathbf{x}) &amp; \\cdots &amp; -\\frac{\\partial^2}{\\partial \\theta_1\\partial \\theta_d}\\ell(\\boldsymbol{\\theta}; \\mathbf{x})\\\\ \\vdots &amp; &amp; \\vdots \\\\ -\\frac{\\partial^2}{\\partial \\theta_d\\partial \\theta_1}\\ell(\\boldsymbol{\\theta}; \\mathbf{x}) &amp; \\cdots &amp;-\\frac{\\partial^2}{\\partial \\theta_d^2}\\ell(\\boldsymbol{\\theta}; \\mathbf{x}) \\end{array}\\right), \\end{equation}\\] evaluated at \\(\\boldsymbol{\\theta}=\\hat{\\boldsymbol{\\theta}}(\\mathbf{x})\\). Then denoting \\(\\tilde{\\gamma}_{i,j}\\) as the \\(i,j\\)th element of the inverse of \\(I_O(\\boldsymbol{\\theta})\\), we use \\[\\begin{equation} (\\hat{\\theta_j}-z_{1-\\frac{\\alpha}{2}}\\sqrt{\\tilde{\\gamma}_{j,j}},\\hat{\\theta_j}+ z_{1-\\frac{\\alpha}{2}}\\sqrt{\\tilde{\\gamma}_{j,j}}), \\end{equation}\\] as an approximate confidence interval. Since we know that \\(\\hat{\\theta} \\rightarrow \\theta\\) as \\(n\\rightarrow \\infty\\), with probability 1, we would expect \\(I_O(\\boldsymbol{\\theta})\\) to be similar to \\(I_E(\\boldsymbol{\\theta})\\) for large sample sizes. Example 5.3 (Asymptotic confidence interval.) Suppose that we have i.i.d. data \\(\\mathbf{x}=(x_1,x_2,\\ldots,x_n)\\), for which each data point is modelled as a random sample from \\(N(\\mu,\\sigma^2)\\) where \\(\\mu\\) is unknown and \\(\\sigma^2\\) is known. Using asymptotic normality of the maximum likelihood estimator, derive an approximate 95% confidence interval for \\(\\mu\\), and show that it approximately the same as the \\(k=2\\) likelihood region found in Example 5.1. Solution Using equation (5.1), our approximate distribution of the maximum likelihood estimator \\(\\hat{\\mu}(\\mathbf{X})\\) is \\[ \\hat{\\mu}(\\mathbf{X}) \\sim N(\\mu, \\gamma), \\] where \\[\\gamma ^{-1}=E\\left(-\\frac{\\partial^2}{\\partial\\mu^2}\\ell(\\mu; \\mathbf{X})\\right).\\] For the log likelihood we have \\[ \\ell(\\mu;\\mathbf{X}) =-\\frac{n}{2} \\log(2\\pi\\sigma^2)- \\frac{1}{2\\sigma^2}\\sum_{i=1}^n(X_i - \\mu)^2, \\] and so \\[ -\\frac{\\partial^2}{\\partial\\mu^2}\\ell(\\mu; \\mathbf{X}) = \\frac{n}{\\sigma^2}. \\] This is a constant, and so \\[E\\left(-\\frac{\\partial^2}{\\partial\\mu^2}\\ell(\\mu; \\mathbf{X})\\right) = \\frac{n}{\\sigma^2},\\] hence our approximate distribution for \\(\\hat{\\mu}(\\mathbf{X})\\) is \\(N(\\mu, \\sigma^2/n)\\) (so the approximation is actually exact in this case.) This gives us the familiar 95% confidence interval \\[ \\left[\\hat{u} - 1.96 \\frac{\\sigma}{\\sqrt{n}},\\quad \\hat{u} + 1.96 \\frac{\\sigma}{\\sqrt{n}}\\right]. \\] We can now see that the \\(k=2\\) likelihood region in Example 5.1 is the same, just with 2 approximating 1.96. 5.3.3 The generalised likelihood ratio test Previously, we suggested performing a (Neyman-Pearson) hypothesis test, based on whether a \\(k\\)-likelihood region contains the hypothesised \\(\\boldsymbol{\\theta}_0\\) under \\(H_0\\). For \\(\\boldsymbol{\\theta}_0\\) to lie outside the \\(k\\)-likelihood region, we require \\[ \\ell(\\hat{\\boldsymbol{\\theta}}; \\mathbf{x}) - \\ell(\\boldsymbol{\\theta}_0;\\mathbf{x}) &gt; k, \\] or equivalently \\[ \\frac{L(\\hat{\\boldsymbol{\\theta}}; \\mathbf{x})}{L(\\boldsymbol{\\theta}_0;\\mathbf{x})} &gt; \\exp(k) \\] This is an example of a generalised likelihood ratio test (GLRT): the test is derived from considering the ratio of two likelihood functions (but note that the ratio is more commonly written the other way round). We’ll now state the GLRT in a more general form. Consider hypotheses of the form \\[\\begin{eqnarray*} H_0&amp;:&amp; \\boldsymbol{\\theta}\\in\\Theta_0,\\\\ H_1&amp;:&amp; \\boldsymbol{\\theta}\\in\\Theta_1. \\end{eqnarray*}\\] The GLRT says reject \\(H_0\\) if \\[\\begin{equation} \\lambda:=\\frac{\\sup_{\\boldsymbol{\\theta}\\in\\Theta_0}L(\\boldsymbol{\\theta};x)}{\\sup_{\\boldsymbol{\\theta}\\in\\Theta}L(\\boldsymbol{\\theta};x)}&lt;k, \\end{equation}\\] where \\(\\Theta\\) is the full parameter space for \\(\\theta\\) and with \\(k\\) chosen such that \\[\\begin{equation} P\\left(\\lambda&lt;k|H_0 \\mbox{ true}\\right)=\\alpha. \\end{equation}\\] Note that because the null hypothesis is in composite form, the numerator involves maximising the likelihood over all \\(\\boldsymbol{\\theta}\\) consistent with \\(H_0\\). Now let \\(\\boldsymbol{\\theta}\\) be a vector of parameters, and write \\(\\boldsymbol{\\theta}=(\\boldsymbol{\\theta}_r,\\boldsymbol{\\theta}_{-r})\\), where \\(\\boldsymbol{\\theta}_r\\) is a subvector of \\(r\\) parameters from \\(\\boldsymbol{\\theta}\\), and \\(\\boldsymbol{\\theta}_{-r}\\) denotes the remaining parameters that make up \\(\\boldsymbol{\\theta}\\). Now consider a hypothesis of the form \\[\\begin{eqnarray*} H_0&amp;:&amp; \\boldsymbol{\\theta}_r=\\boldsymbol{\\theta}_0,\\\\ H_1&amp;:&amp; \\boldsymbol{\\theta}_r\\neq\\boldsymbol{\\theta}_0. \\end{eqnarray*}\\] We write the GLR test statistic as \\[\\begin{equation} \\lambda = \\frac{L((\\boldsymbol{\\theta}_0,\\hat{\\boldsymbol{\\theta}}_{-r});\\mathbf{x})}{L(\\hat{\\boldsymbol{\\theta}};\\mathbf{x})}, \\end{equation}\\] where \\(\\hat{\\boldsymbol{\\theta}}_{-r}\\) is the value of \\(\\boldsymbol{\\theta}_{-r}\\) that maximises the likelihood when \\(\\boldsymbol{\\theta}_r\\) is fixed at \\(\\boldsymbol{\\theta}_0\\), and \\(\\hat{\\boldsymbol{\\theta}}\\) is the usual, unconstrained, m.l.e. It is then possible to prove that as the sample size tends to infinity, the distribution of \\(-2\\log \\lambda\\) tends to the \\(\\chi^2_r\\) distribution, when \\(H_0\\) is true. Hence for a test of size \\(\\alpha\\), we would reject \\(H_0\\) if \\(-2\\log\\lambda\\) is greater than the \\(100(1-\\alpha)\\) percentile of the \\(\\chi^2_r\\) distribution. Example 5.4 (GLRT for normally distributed data, known variance.) Suppose that we have i.i.d. data \\(\\mathbf{x}=(x_1,x_2,\\ldots,x_n)\\), for which each data point is modelled as a random sample from \\(N(\\mu,\\sigma^2)\\) where \\(\\mu\\) is unknown and \\(\\sigma^2\\) is known. To test the hypothesis \\(H_0:\\mu = \\mu_0\\) against a two sided alternative, we use the test statistic \\[ Z = \\frac{\\bar{x} - \\mu_0}{\\sigma/\\sqrt{n}}, \\] where, for a test of size 0.05, we reject if \\(|Z|\\) is greater than the 97.5th percentile of the \\(N(0,1)\\) distribution. Show that this is equivalent to a GLRT, using the approximate distribution for \\(\\lambda\\). Solution Under \\(H_0\\), we have \\(\\mu\\) fixed at \\(\\mu_0\\), and under \\(H_A\\), the likelihood is maximised at \\(\\mu = \\bar{x}\\). So we have \\[ L(\\mu_0; \\mathbf{x}) = \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(x_i - \\mu_0)^2\\right), \\] and \\[ L(\\hat{\\mu}; \\mathbf{x}) = \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(x_i - \\bar{x})^2\\right), \\] and so \\[ \\lambda = \\frac{L(\\mu_0; \\mathbf{x})}{L(\\hat{\\mu}; \\mathbf{x})} = \\exp\\left(-\\frac{1}{2\\sigma^2}\\left(\\sum_{i=1}^n(x_i - \\mu_0)^2 -\\sum_{i=1}^n(x_i - \\bar{x})^2 \\right)\\right). \\] Hence, after a little algebra \\[ -2\\log \\lambda = \\frac{n}{\\sigma^2}(\\bar{x} - \\mu_0)^2. \\] Using the approximate distribution for \\(\\lambda\\), we reject \\(H_0\\) for a test of size 0.05 if \\[-2\\log \\lambda &gt; \\chi^2_{1;\\, 0.05}\\simeq 3.8415\\simeq(1.96)^2 \\]. This is equivalent to rejecting \\(H_0\\) if \\[ \\left|\\frac{\\sqrt{n}}{\\sigma}(\\bar{x} - \\mu_0) \\right| &gt; Z_{0.05}. \\] Note that for \\(Z\\sim N(0,1)\\), we have \\(Z^2\\sim \\chi^2_1\\), which helps us to understand why the tests are equivalent. "],["introducing-linear-models.html", "Chapter 6 Introducing linear models 6.1 Example: relationship between temperature and ozone 6.2 Notation and terminology 6.3 Example: suspected electoral fraud 6.4 Definition of a linear model 6.5 The simple linear regression model", " Chapter 6 Introducing linear models In this part of the module, we will study how to use statistical models to learn about relationships between different variables, using a class of models known as linear models. The aim here is to teach you practical skills in linear modelling, so that you can fit linear models for a variety of different types of datasets, interpret the results, and make predictions. However, there are some important topics such as hypothesis testing and model checking that we will not cover here, but are taught on the MSc. 6.1 Example: relationship between temperature and ozone The data set airquality (one of R’s built-in data sets) includes daily readings of temperature (degrees F) and ozone concentration (parts per billion), taken in New York4. For more details, use the command ?airquality Some rows from the data set and a plot are shown below. Note that some rows have missing values - we will ignore these. head(airquality) ## Ozone Solar.R Wind Temp Month Day ## 1 41 190 7.4 67 5 1 ## 2 36 118 8.0 72 5 2 ## 3 12 149 12.6 74 5 3 ## 4 18 313 11.5 62 5 4 ## 5 NA NA 14.3 56 5 5 ## 6 28 NA 14.9 66 5 6 ggplot(airquality, aes(x = Temp, y = Ozone))+ geom_point() In the data, we see that we some observations have identical (recorded) temperature values, but different ozone values Hence given the temperature, we are not able to say with certainty what the ozone concentration will be. For observation \\(i\\) let \\(x_i\\) denote the recorded temperature and \\(y_i\\) denote the corresponding ozone concentration. If we think of \\(y_i\\) as the observed value of a random variable \\(Y_i\\), we can then consider a statistical model of the form \\[ Y_i=f(x_i) + \\varepsilon_i. \\] We typically choose \\(f\\) to be some fairly simple function of \\(x\\) such as \\(f(x)=\\beta_0 +\\beta_1 x\\) or \\(f(x)=\\beta_0+\\beta_1 x+\\beta_2 x^2\\). The term \\(\\varepsilon_i\\) is a random “error.” Two reasons why an error term would be necessary are as follows. The variable \\(x_i\\) is not sufficient for predicting \\(Y_i\\) with certainty. The variable \\(Y_i\\) may also depend on other variables which are unknown to us, or there may simply be variation in the population that we are not able to explain with the function \\(f\\). We are not able to observe the quantity we are interested in (e.g. ozone concentration) with absolute precision. We observe instead the sum of the true value and a random ‘measurement error’ \\(\\varepsilon_i\\). We further assume that \\(\\varepsilon_1,\\ldots,\\varepsilon_n\\) are i.i.d (independent and identically distributed) with some probability distribution, so that in our model, two observations with the same temperature may have different ozone concentrations. 6.2 Notation and terminology Continuing the example, suppose we choose \\(f(x)=\\beta_0 + \\beta_1 x\\), so that we have \\[\\begin{equation} Y_i=\\beta_0+\\beta_1 x_i +\\varepsilon_i \\end{equation}\\] We call the \\(x\\)-variable (temperature) the independent or regressor variable. We will always treat the independent variable as a known constant. (In the example, one could think of the temperature, or at least recorded with some error. Nevertheless, in this modelling framework, the temperatures are treated as known, fixed values.) The \\(Y\\)-variable (ozone concentration) is called the dependent variable. The dependent variable is treated as a random variable, as it is expressed as a function of the independent variable plus a random error term. The dependent variable will always be a scalar, but we will sometimes consider vector independent variables (e.g. temperature and wind speed). We write \\(\\mathbf{x}\\) to denote a vector independent variable. Throughout this course we will always make the same assumption that \\(\\varepsilon_i \\sim N(0,\\sigma^2)\\), with \\(\\varepsilon_1,\\varepsilon_2,\\ldots\\) independent. We will need to check this assumption, in particular, that each error has the same variance. The parameters in this model are \\(\\beta_0, \\beta_1\\) and \\(\\sigma^2\\). These model parameters are treated as unknown constants. Note that since \\(\\beta_0\\) and \\(\\beta_1\\) are unknown, given \\(x_i\\) and \\(y_i\\) we will still not know the true value of \\(\\varepsilon_i\\). 6.3 Example: suspected electoral fraud In 1993 the election for Pennsylvania’s Second State Senatorial District, Philadelphia, was contested between Republican Marks and Democrat Stinson. Marks received many more machine votes (votes cast in polling booths), but Stinson received many more absentee ballot votes (postal votes), narrowly edging a victory as a result. There were suspicions about the authenticity of many of the absentee ballots, however, and the case ended up in court. The expert witness hired by the judge considered the records of twenty-one previous elections in Philadelphia’s senatorial districts, and looked at the relationship between the difference in machine ballot votes and difference in absentee ballot votes between Democrat and Republican candidates. In the 1993 election, the difference in machine ballot votes (Democrat - Republican) was -564 votes. The difference in absentee ballot votes (Democrat - Republican) was 1025 votes. The data are in the file election.csv, and are plotted below. election &lt;- read_csv(&quot;election.csv&quot;) head(election) ## # A tibble: 6 × 2 ## absentee.diff machine.diff ## &lt;dbl&gt; &lt;dbl&gt; ## 1 346 26427 ## 2 282 15904 ## 3 223 42448 ## 4 593 19444 ## 5 572 71797 ## 6 -229 -1017 ggplot(election, aes(x = machine.diff, y = absentee.diff)) + geom_point() + annotate(&quot;point&quot;, x = -564, y = 1025, col = &quot;red&quot;, pch = 4, size = 4) + annotate(&quot;text&quot;, x = -564, y = 1150, col = &quot;red&quot;, label = &quot;disputed election&quot;) The scatter plot shows a clear relationship between difference in machine votes and difference in absentee votes. Additionally, the result in question is not consistent with the rest (i.e. it is an outlier). However, we see that there is a second (presumably undisputed) election result (44425,1329) that also appears to be outlying. Do these data really suggest fraud? Is it plausible that the observation (-564,1025) is simply due to chance? How can linear modelling be used to investigate this? 6.4 Definition of a linear model One may imagine that a linear model means a ‘straight line’ (linear) relationship between the independent variable \\(x\\) and dependent variable \\(y\\)\", but that is actually not the definition. A linear model is defined to be any model in which the expected value of the dependent variable \\(E(Y_i)\\) is expressed as a linear combination of the parameters The following is an example of a linear model: \\[ E(Y_i)=\\beta_0+\\beta_1 x_i. \\] Is the model \\[ E(Y_i)=\\beta_0+\\beta_1 x_i+ \\beta_2 x_i^2 \\] a linear model? Yes, because \\(E(Y_i)\\) is a linear function of the parameters (\\(\\beta_0,\\beta_1\\) and \\(\\beta_2\\) in this case). The fact that \\(E(Y_i)\\) is not a linear function of the independent variable \\(x_i\\) is not important. The model \\[\\begin{equation} E(Y_i)=\\beta_0+\\beta_1 x + e^{\\beta_2 x_i}, \\end{equation}\\] is not a linear model, because \\(E(Y_i)\\) is not a linear function of the parameter \\(\\beta_2\\). Typically, linear models are sufficient when the dependent variable is a continuous quantity, and so are useful in a wide variety of contexts. Linear models are not suitable when the dependent variable is categorical or ordinal. (This requires the use of generalised linear models, which are taught on the MSc). The independent variable does not have to be continuous, and we can have more than one independent variable. Continuing the air quality example, suppose we want to include Wind speed, and suppose we also know whether it was overcast at the time of measuring the variables. We could extend the model and write \\[\\begin{equation} Y_i=\\beta_0+ \\beta_1 x_i + \\beta_2 w_i + \\beta_3 z_i +\\varepsilon_i, \\end{equation}\\] where \\(w_i\\) is the wind speed for the \\(i\\)th measurement, and \\(z_i\\) is a dummy variable representing cloud conditions: \\[ z_i=\\left\\{\\begin{array}{cl} 0 &amp; \\mbox{overcast conditions for measurement $i$}\\\\ 1 &amp; \\mbox{not overcast conditions} \\end{array}\\right. \\] Example 6.1 (Writing down a linear model.) Two objects \\(A\\) and \\(B\\) are to be weighed. Each measurement is subject to a measurement error, with the measurement errors assumed to be independent \\(N(0,\\sigma^2)\\) random variables. Objects \\(A\\) and \\(B\\) are weighed separately, and then together, so that there are three measurements in total. Write down a linear model that relates the measurements to the true weights. You may use more than one equation to do this. Solution Let \\(\\theta_A\\) and \\(\\theta_B\\) denote the true weights of the two objects, and \\(Y_1,Y_2,Y_3\\) be the measurements. We have \\[\\begin{align*} Y_1 &amp;= \\theta_A + \\varepsilon_1,\\\\ Y_2 &amp;= \\theta_B + \\varepsilon_2,\\\\ Y_3 &amp;= \\theta_A + \\theta_B + \\varepsilon_3, \\end{align*}\\] with \\(\\varepsilon_1,\\varepsilon_2,\\varepsilon_3\\stackrel{i.i.d}{\\sim}N(0,\\sigma^2)\\). This is a linear model because \\(E(Y_i)\\) is a linear combination of \\(\\theta_A\\) and \\(\\theta_B\\) for each \\(i\\). 6.5 The simple linear regression model We call the model \\[ Y_i=\\beta_0+\\beta_1 x_i +\\varepsilon_i, \\] for \\(i=1,\\ldots n\\) with \\(\\varepsilon_1,\\ldots,\\varepsilon_n \\stackrel{i.i.d.}{\\sim} N(0,\\sigma^2)\\) the simple linear regression model. For obvious reasons, we refer to \\(\\beta_0\\) as the intercept parameter and \\(\\beta_1\\) as the gradient or slope parameter. What is the distribution of \\(Y_i\\) for this model? Recall that for a random variable \\(Z\\) and constants \\(a\\) and \\(b\\): \\[\\begin{eqnarray*} E(aZ+b)&amp;=&amp;aE(Z) +b,\\\\ Var(aZ+b)&amp;=&amp;a^2Var(Z). \\end{eqnarray*}\\] Also, if \\(Z\\sim N(\\mu,\\sigma^2)\\) then \\(aZ+b \\sim N(a\\mu+b,a^2\\sigma^2)\\). Now, if \\(\\varepsilon_i\\sim N(0,\\sigma^2)\\) and \\(\\beta_0,\\beta_1\\) and \\(x_i\\) are constants then we have \\[\\begin{eqnarray} E(Y_i)&amp;=&amp; \\beta_0 + \\beta_1 x_i,\\\\ Var(Y_i)&amp;=&amp; \\sigma^2, \\end{eqnarray}\\] and \\[\\begin{equation} Y_i \\sim N(\\beta_0 + \\beta_1 x_i, \\sigma^2). \\end{equation}\\] Additionally, since \\(\\varepsilon_1,\\ldots,\\varepsilon_n\\) are independent, it follows that \\(Y_1,\\ldots,Y_n\\) are independent. Note that we do not need to consider the case \\(E(\\varepsilon_i)\\neq 0\\). Suppose we have a model \\[\\begin{equation} Y_i = \\gamma + \\beta_1 x_i +\\delta_i, \\end{equation}\\] with \\(\\delta_i \\sim N(\\mu,\\sigma^2)\\) and \\(\\mu \\neq 0\\). We can write \\[\\begin{equation} \\delta_i = \\mu + \\varepsilon_i, \\end{equation}\\] with \\(\\varepsilon_i \\sim N(0,\\sigma^2)\\) so that we can re-write this model as \\[\\begin{equation} y_i = \\mu+ \\gamma + \\beta_1 x_i +\\varepsilon_i. \\end{equation}\\] If we now set \\(\\beta_0 = \\mu + \\gamma\\), we see that this model is indistinguishable from the simple linear regression model. To put it another way, as we can write \\(Y_i = E(Y_i) + \\varepsilon_i\\), we must have \\(E(\\varepsilon_i) = 0\\). The data were obtained from the New York State Department of Conservation (ozone data) and the National Weather Service (meteorological data)↩︎ "],["parameter-estimation.html", "Chapter 7 Parameter estimation 7.1 Least squares estimation 7.2 Assumptions for least squares 7.3 Relationship with maximum likelihood estimation 7.4 Estimating \\(\\sigma^2\\)", " Chapter 7 Parameter estimation In this Chapter we study how to fit a linear model to some observed data, i.e., how to use the data to estimate the model parameters. Recall the disputed election example (re-plotted below, with the contested election omitted). Figure 7.1: Difference in absentee votes (Democrat - Republican) against difference in machine votes (Democrat - Republican) against in 21 different elections in Philadelphia’s senatorial districts. For election \\(i\\) let \\(x_i\\) denote the difference in machine votes (Democrat \\(-\\) Republican) and \\(y_i\\) denote the observed difference in absentee votes (Democrat \\(-\\) Republican). We’ll consider the simple linear regression model, and in a slight abuse of notation we will write \\[\\begin{equation} y_i=\\beta_0 + \\beta_1 x_i + \\varepsilon_i, \\end{equation}\\] for \\(i=1,\\ldots,n\\). (Because \\(y_i\\) is an observed value rather than a random variable \\(Y_i\\), the error term \\(\\varepsilon_i\\) is now a constant: the ‘realised’ error rather than a random variable with the \\(N(0,\\sigma^2)\\) distribution. But we will not make this distinction in our notation.) The unknown parameters to be estimated are \\(\\beta_0,\\beta_1\\) and \\(\\sigma^2\\). We will consider how to estimate \\(\\beta_0\\) and \\(\\beta_1\\) first and deal with \\(\\sigma^2\\) later. 7.1 Least squares estimation Firstly, we rearrange the above equation to get \\[ \\varepsilon_i=y_i - \\beta_0 - \\beta_1 x_i. \\] Since we know the values of \\(y_i\\) and \\(x_i\\), for any choice of \\((\\beta_0,\\beta_1)\\) we can then calculate the implied values of \\(\\varepsilon_i\\). Note that in this model the random error term has expectation 0, and so small (absolute) values of the errors are more probable than large (absolute) values. Additionally, some choices of \\((\\beta_0,\\beta_1)\\) are clearly better than others. Consider the following figure: Figure 7.2: The solid line is \\(y=\\beta_0+\\beta_1 x\\), with \\((\\beta_0=0,\\beta_1=0.001)\\) in panel (a), and \\((\\beta_0=-125,\\beta_1=0.013)\\) in panel (b). Lengths of the dotted lines indicate the magnitude of each \\(\\varepsilon_i\\). We see that choosing \\((\\beta_0=-125,\\beta_1=0.013)\\) appears to fit the data better than choosing \\((\\beta_0=0,\\beta_1=0.001)\\) because the observed points lie closer to the line \\(y=\\beta_0 +\\beta_1 x\\). Specifically, we prefer \\((\\beta_0=-125,\\beta_1=0.013)\\) because on average \\(|\\varepsilon_1|,\\ldots,|\\varepsilon_n|\\) are smaller: \\(\\sum_{i=1}^n |\\varepsilon_i|\\) \\(\\sum_{i=1}^n \\varepsilon_i^2\\) \\((\\beta_0=0,\\beta_1=0.001)\\) 8410 4,704,524 \\((\\beta_0=-125,\\beta_1=0.013)\\) 5018 2,007,945 This suggests we should choose \\((\\beta_0,\\beta_1)\\) to make \\(\\sum_{i=1}^n \\varepsilon_i^2\\) as small as possible (note that minimising \\(\\sum_{i=1}^n \\varepsilon_i^2\\) is easier than minimising \\(\\sum_{i=1}^n |\\varepsilon_i|\\)). Define \\[\\begin{equation}R(\\beta_0,\\beta_1)=\\sum_{i=1}^n \\varepsilon_i^2 = \\sum_{i=1}^n (y_i-\\beta_0-\\beta_1 x_i)^2. \\end{equation}\\] The least squares estimates of \\(\\beta_0\\) and \\(\\beta_1\\) are the values of \\(\\beta_0\\) and \\(\\beta_1\\) that minimise \\(R(\\beta_0,\\beta_1)\\). Now \\[\\begin{eqnarray} \\frac{\\partial R(\\beta_0,\\beta_1)}{\\partial \\beta_0}&amp;=&amp; -2\\sum_{i=1}^n(y_i-\\beta_0-\\beta_1 x_i),\\\\ \\frac{\\partial R(\\beta_0,\\beta_1)}{\\partial \\beta_1}&amp;=&amp; -2\\sum_{i=1}^nx_i(y_i-\\beta_0-\\beta_1 x_i). \\end{eqnarray}\\] Let \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) denote the least squares estimates of \\(\\beta_0\\) and \\(\\beta_1\\). Since (by definition) \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) minimise \\(R(\\beta_0,\\beta_1)\\), both partial derivative must equal 0 at \\(\\beta_0=\\hat{\\beta}_0\\) and \\(\\beta_1=\\hat{\\beta}_1\\), so \\[\\begin{eqnarray} 0&amp;=&amp; -\\sum_{i=1}^n(y_i-\\hat{\\beta}_0-\\hat{\\beta}_1 x_i),\\\\ 0&amp;=&amp; -\\sum_{i=1}^nx_i(y_i-\\hat{\\beta}_0-\\hat{\\beta}_1 x_i), \\end{eqnarray}\\] which we can re-write as \\[\\begin{eqnarray} n\\hat{\\beta}_0 +\\hat{\\beta}_1\\sum_{i=1}^n x_i &amp;=&amp; \\sum_{i=1}^n y_i,\\\\ \\hat{\\beta}_0\\sum_{i=1}^n x_i +\\hat{\\beta}_1 \\sum_{i=1}^n x_i^2 &amp;=&amp; \\sum_{i=1}^n x_iy_i, \\end{eqnarray}\\] and these can be solved simultaneously to give \\[\\begin{eqnarray} \\hat{\\beta}_1&amp;=&amp;\\frac{n \\sum_{i=1}^n x_i y_i -\\sum_{i=1}^n x_i \\sum_{i=1}^n y_i}{n \\sum_{i=1}^n x_i^2 -\\left(\\sum_{i=1}^n x_i \\right)^2}, \\\\ \\hat{\\beta}_0&amp;=&amp;\\bar{y} - \\hat{\\beta}_1\\bar{x}. \\end{eqnarray}\\] Although you won’t need to calculate these terms by hand, we will tidy up these expressions to get the formulae that you may see in textbooks. Define \\[\\begin{eqnarray} s_{xy}&amp;=&amp;\\sum_{i=1}^n (x_i-\\bar{x})(y_i-\\bar{y}),\\\\ s_{xx}&amp;=&amp;\\sum_{i=1}^n (x_i-\\bar{x})^2. \\end{eqnarray}\\] Note that \\(\\sum_{i=1}^n (x_i-\\bar{x})=0\\) and so \\(\\sum_{i=1}^n(x_i-\\bar{x})\\bar{y}=\\bar{y}\\sum_{i=1}^n(x_i-\\bar{x})=0\\). Hence \\[\\begin{equation} s_{xy}=\\sum_{i=1}^n(x_i-\\bar{x})y_i = \\sum_{i=1}^n x_i y_i -\\frac{\\sum_{i=1}^n x_i\\sum_{i=1}^n y_i}{n}. \\end{equation}\\] Also, \\[\\begin{equation} s_{xx}=\\sum_{i=1}^n x_i^2 - \\frac{\\left(\\sum_{i=1}^n x_i\\right)^2}{n}, \\end{equation}\\] and so we can write \\[\\begin{equation} \\boxed{\\,\\hat{\\beta}_1=\\frac{s_{xy}}{s_{xx}}\\quad\\hat{\\beta}_0=\\bar{y}-\\hat{\\beta}_1\\bar{x}.}\\label{abhat} \\end{equation}\\] We will see how to fit linear models in R later on, but for now, for the election data we can do x &lt;- election$machine.diff y &lt;- election$absentee.diff beta1Hat &lt;- sum((x - mean(x)) * (y - mean(y)))/ sum((x - mean(x))^2) beta0Hat &lt;- mean(y) - beta1Hat * mean(x) c(beta0Hat, beta1Hat) ## [1] -125.90364382 0.01270346 So we have \\(\\hat{\\beta}_0=-125.9036\\) and \\(\\hat{\\beta}_1=0.0127\\). The line \\(y=-125.9036+0.0127x\\). This gives a line very close to that in panel (b) in Figure 7.2. Example 7.1 (Least squares estimation: weights example.) Continuing from Example 6.1, suppose the observed weights are denoted by \\(y_1, y_2, y_3\\). Derive the least squares estimates of \\(\\theta_A\\) and \\(\\theta_B\\) Solution We define \\[ R(\\theta_A, \\theta_B) := \\sum_{i=1}^3 \\varepsilon_i^2 = (y_1-\\theta_A)^2 + (y_2-\\theta_B)^2+(y_3-\\theta_A -\\theta_B)^2. \\] Then the least squares estimates are the solutions of \\[\\begin{align*} \\frac{\\partial R(\\theta_A, \\theta_B)}{\\partial \\theta_A} = -2(y_1-\\theta_A) -2(y_3 - \\theta_A-\\theta_B) = 0,\\\\ \\frac{\\partial R(\\theta_A, \\theta_B)}{\\partial \\theta_B} = -2(y_2-\\theta_B) -2(y_3 - \\theta_A-\\theta_B) = 0. \\end{align*}\\] After a little algebra to solve these two equations simultaneously, we get \\[ \\hat{\\theta}_A = \\frac{2y_1 -y_2 + y_3}{3}, \\quad \\hat{\\theta}_B = \\frac{2y_2 -y_1 + y_3}{3}. \\] 7.2 Assumptions for least squares Note that we have not made use of the assumption that the errors \\(\\varepsilon_i\\) are normally distributed. If we look again at the function we chose to minimise \\[ R(\\beta_0,\\beta_1)=\\sum_{i=1}^n \\varepsilon_i^2 = \\sum_{i=1}^n (y_i-\\beta_0-\\beta_1 x_i)^2, \\] we note that each error \\(\\varepsilon_i\\) has equal weight in the sum; we are not trying to make some errors smaller than others. This relates back to the assumption that the errors have equal variance; this is an assumption we should try to check. There are analyses we might choose to do that rely on the normality assumption, but these will not be the main focus in this module. There are actually two ways in which a single observation might be unduly influential: the error \\(\\varepsilon_i\\) is an outlier; the independent variable \\(x_i\\) is an outlier. Case (1) may be the result of non-constant variance of the errors. An example is shown below. Figure 7.3: The blue line is the least squares fit using all ten observations. The red dashed line is the least squares fit, omitting the outlier (shown by the blue dot) at \\(x=2\\). Case (2) is sometimes referred to as high leverage. We illustrate this below. Figure 7.4: The blue line is the least squares fit using all ten observations. The red dashed line is the least squares fit, omitting the ‘high leverage’ point at \\(x=20\\); the least squares fit will be particularly sensitive to this one observation. 7.3 Relationship with maximum likelihood estimation Exercise. (This is a good opportunity to test your understanding of likelihood!) Suppose, in the simple linear regression model, \\(\\beta_0\\) and \\(\\beta_1\\) are to be estimated by maximum likelihood. Show that the maximum likelihood estimates are also the least squares estimates. (You will not need to actually obtain the maximum likelihood estimates to prove this: you just need to show that maximum likelihood must result in the same solution.) Solution The likelihood function is given by \\[ L(\\beta_0, \\beta_1, \\sigma^2; \\mathbf{y}) = \\prod_{i=1}^nf_Y(y_i; \\beta_0, \\beta_1,\\sigma^2) \\] We have \\(Y_i \\sim N(\\beta_0 + \\beta_1x_i, \\sigma^2)\\), so \\[ f_Y(y_i; \\beta_0, \\beta_1,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{1}{2\\sigma^2}(y_i - \\beta_0-\\beta_1x_i)^2\\right) \\] and so the log-likelihood is \\[ \\ell(\\beta_0, \\beta_1, \\sigma^2; \\mathbf{y}) = -\\frac{n}{2}\\log\\sigma^2 - \\frac{1}{2\\sigma^2}\\sum_{i=1}^n(y_i - \\beta_0-\\beta_1x_i)^2. \\] The maximum likelihood estimators of \\(\\beta_0\\) and \\(\\beta_1\\) are the solutions of \\[\\begin{align*} \\frac{\\partial \\ell}{\\partial \\beta_0} &amp;= \\frac{1}{\\sigma^2}\\sum_{i=1}^n(y_i - \\beta_0-\\beta_1x_i) = 0,\\\\ \\frac{\\partial \\ell}{\\partial \\beta_1} &amp;= \\frac{1}{\\sigma^2}\\sum_{i=1}^nx_i(y_i - \\beta_0-\\beta_1x_i) = 0, \\end{align*}\\] but these are the same two simultaneous equations we solved to get the least squares estimates of \\(\\beta_0\\) and \\(\\beta_1\\): the least squares estimates are the same as the maximum likelihood estimates. 7.4 Estimating \\(\\sigma^2\\) We’ll just state the estimate of \\(\\sigma^2\\), without going too much into the detail. We define \\[ \\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i \\] to be the \\(i\\)-th fitted value, so that \\((x_i, \\hat{y}_i)\\) is a point on the fitted regression line. We define \\[ e_i = y_i - \\hat{y}_i \\] to be the \\(i\\)-th residual: the difference between the observation \\(y_i\\) and the corresponding fitted value \\(\\hat{y}_i\\). Informally, we could think of \\(e_i = y_i - \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i\\) as an estimate of the error term \\(\\varepsilon_i = y_i - \\beta_0 + \\beta_1 x_i\\). We then estimate \\(\\sigma^2\\) using the residual sum of squares \\[ \\hat{\\sigma}^2 = \\frac{1}{n-2} \\sum_{i=1}^n e_i^2. \\] The denominator \\(n-2\\) is the residual degrees of freedom. Informally, it makes sense that we need at least three observations (for the simple linear regression model) to estimate \\(\\sigma^2\\), because with only two observations, the fitted line would pass through both points; we wouldn’t have any idea how far an observation can be from the regression line. We can get \\(\\hat{\\sigma}^2\\) easily from R, but we will do the calculation manually for now. Repeating the earlier code: x &lt;- election$machine.diff y &lt;- election$absentee.diff beta1Hat &lt;- sum((x - mean(x)) * (y - mean(y)))/ sum((x - mean(x))^2) beta0Hat &lt;- mean(y) - beta1Hat * mean(x) Then e &lt;- y - beta0Hat - beta1Hat*x sum(e^2) / (length(e) - 2) ## [1] 105519.8 This suggests that most points will be within \\(2\\sqrt{105519.8}\\simeq 650\\) of the fitted regression line (on the \\(y\\)-axis). From looking at panel (b) in Figure 7.2, this seems about right. "],["matrix-notation-for-linear-models.html", "Chapter 8 Matrix notation for linear models 8.1 Least squares estimates in matrix form 8.2 Estimate of \\(\\sigma^2\\) in matrix form 8.3 Derivation of \\(\\hat{\\beta}\\)", " Chapter 8 Matrix notation for linear models Any linear model can be represented using the same matrix notation. This is useful because any result or technique derived using the matrix notation can then be applied to all linear models. To illustrate the notation consider the simple linear regression model \\[ Y_i=\\beta_0+\\beta_1 x_i +\\varepsilon_i, \\] for \\(i=1,\\ldots,n\\). We write it in matrix form as \\[ \\mathbf{Y} = X\\boldsymbol{\\beta}+ \\boldsymbol{\\varepsilon}, \\] with \\[ \\mathbf{Y}=\\left(\\begin{array}{c}Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n\\end{array}\\right),\\quad X=\\left(\\begin{array}{cc}1&amp; x_1 \\\\ 1&amp; x_2 \\\\ \\vdots &amp; \\vdots \\\\ 1&amp; x_n\\end{array}\\right),\\quad \\boldsymbol{\\beta}=\\left(\\begin{array}{c}\\beta_0 \\\\ \\beta_1\\end{array}\\right),\\quad \\boldsymbol{\\varepsilon}=\\left(\\begin{array}{c}\\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\vdots \\\\ \\varepsilon_n\\end{array}\\right). \\] We also define \\(\\mathbf{Y}=(y_1,\\ldots,y_n)^T\\) as the vector of observed dependent variables. The matrix \\(X\\) is known as the design matrix and the vector \\(\\boldsymbol{\\beta}\\) is sometimes referred to as the parameter vector. 8.1 Least squares estimates in matrix form For any linear model in matrix form, it can be shown that the least squares estimate is given by \\[\\begin{equation} \\hat{\\boldsymbol{\\beta}}=(X^TX)^{-1}X^T\\mathbf{y}.\\tag{8.1} \\end{equation}\\] This is (probably!) the most important formula in the theory of linear models, because we can use it to obtain parameter estimates for any linear model. 8.1.1 Example: fitting a polynomial regression model Consider again the election data. We could try fitting a polynomial regression model, as the relationship between independent and dependent variables looks nonlinear: \\[ Y_i=\\beta_0+\\beta_1 x_i +\\beta_2 x_i^2 + \\varepsilon_i. \\] How do we estimate \\((\\beta_0,\\beta_1,\\beta_2)\\)? The same argument of choosing \\((\\beta_0,\\beta_1,\\beta_2)\\) to make the errors small still holds. Following the approach in the previous chapter, we would have to solve simultaneously the following three equations: \\[ \\frac{\\partial}{\\partial \\beta_0}\\sum_{i=1}^n \\varepsilon_i^2=0,\\quad \\frac{\\partial}{\\partial \\beta_1}\\sum_{i=1}^n \\varepsilon_i^2=0,\\quad \\frac{\\partial}{\\partial \\beta_2}\\sum_{i=1}^n \\varepsilon_i^2=0, \\] with \\(\\varepsilon_i = y_i -\\beta_0-\\beta_1 x_i -\\beta_2 x_i^2\\). However, with matrix notation we already have the answer. We again write the model as \\[ \\mathbf{Y} = X\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}, \\] now with \\[ \\mathbf{y}=\\left(\\begin{array}{c}y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n\\end{array}\\right),\\quad X=\\left(\\begin{array}{ccc}1&amp; x_1 &amp;x_1^2\\\\ 1&amp; x_2 &amp;x^2_2\\\\ \\vdots &amp; \\vdots &amp; \\vdots \\\\ 1 &amp;x_n &amp; x_n^2\\end{array}\\right),\\quad \\boldsymbol{\\beta}=\\left(\\begin{array}{c}\\beta_0 \\\\ \\beta_1\\\\ \\beta_2\\end{array}\\right),\\quad \\boldsymbol{\\varepsilon}=\\left(\\begin{array}{c}\\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\vdots \\\\ \\varepsilon_n\\end{array}\\right). \\] As before, we have \\(\\sum_{i=1}^n \\varepsilon_i^2 = \\boldsymbol{\\varepsilon}^T\\boldsymbol{\\varepsilon}\\), and so we must solve \\[ \\left(\\begin{array}{c}\\frac{\\partial}{\\partial \\beta_0} \\boldsymbol{\\varepsilon}^T\\boldsymbol{\\varepsilon}\\\\ \\frac{\\partial}{\\partial \\beta_1} \\boldsymbol{\\varepsilon}^T\\boldsymbol{\\varepsilon} \\\\ \\frac{\\partial}{\\partial \\beta_2} \\boldsymbol{\\varepsilon}^T\\boldsymbol{\\varepsilon}\\end{array}\\right)=\\left(\\begin{array}{c}0 \\\\ 0\\\\ 0\\end{array}\\right), \\] i.e. \\(\\frac{\\partial \\boldsymbol{\\varepsilon}^T\\boldsymbol{\\varepsilon}}{\\partial \\boldsymbol{\\beta}} = \\mathbf{0}\\). Hence our least squares estimate is \\(\\hat{\\boldsymbol{\\beta}}=(\\hat{\\beta_0},\\hat{\\beta_1},\\hat{\\beta}_2)^T=(X^TX)^{-1}X^T\\mathbf{y}\\). For the election data this gives \\[ \\hat{\\boldsymbol{\\beta}}=\\left(\\begin{array}{c}-219.0071 \\\\ 0.0297 \\\\ -2.8455\\times 10^{-7} \\\\ \\end{array}\\right) \\] The line \\(y=-219.0071 + 0.0297x -(2.8455\\times 10^{-7})x^2\\) is drawn on the election scatter plot below. Note that the key step in obtaining parameter estimates is simply to identify the form of the design matrix \\(X\\). Once we have \\(X\\) and \\(\\mathbf{y}\\), we just use equation (8.1). Example 8.1 (Linear model in matrix form: weights example.) Consider again Example 6.1. Represent this model in matrix notation, and verify that the least squares estimates are \\[ \\hat{\\theta}_A = \\frac{2y_1 -y_2 + y_3}{3}, \\quad \\hat{\\theta}_B = \\frac{2y_2 -y_1 + y_3}{3}. \\] Solution We have \\[ \\mathbf{Y} = X\\boldsymbol{\\beta}+ \\boldsymbol{\\varepsilon}, \\] with \\[ \\mathbf{Y}=\\left(\\begin{array}{c}Y_1 \\\\ Y_2 \\\\ Y_3\\end{array}\\right),\\quad X=\\left(\\begin{array}{cc} 1 &amp; 0 \\\\ 0 &amp; 1 \\\\ 1 &amp; 1\\end{array}\\right),\\quad \\boldsymbol{\\beta}=\\left(\\begin{array}{c}\\theta_A \\\\ \\theta_B\\end{array}\\right),\\quad \\boldsymbol{\\varepsilon}=\\left(\\begin{array}{c}\\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\\\ \\varepsilon_3\\end{array}\\right). \\] Then the least squares estimates are obtained as \\[ (X^TX)^{-1}X^T\\mathbf{y}, \\] with \\(\\mathbf{y}=(y_1,y_2,y_3)^T\\) the vector of observed measurements. We’ll find \\((X^TX)^{-1}X^T\\) using R: X &lt;- matrix(c(1, 0, 1, 0, 1, 1), nrow = 3, ncol = 2) solve(t(X) %*% X) %*% t(X) ## [,1] [,2] [,3] ## [1,] 0.6666667 -0.3333333 0.3333333 ## [2,] -0.3333333 0.6666667 0.3333333 and from this, we can see that \\((X^TX)^{-1}X^T\\mathbf{y}\\) will give \\[ \\hat{\\theta}_A = \\frac{2y_1 -y_2 + y_3}{3}, \\quad \\hat{\\theta}_B = \\frac{2y_2 -y_1 + y_3}{3}. \\] 8.2 Estimate of \\(\\sigma^2\\) in matrix form Using the matrix notation, the vectors of fitted values and residuals are, respectively, given by \\[ \\hat{\\mathbf{y}}:= X\\hat{\\boldsymbol{\\beta}}, \\quad \\mathbf{e}:=\\mathbf{y}-\\hat{\\mathbf{y}}, \\] We can then write the estimate of \\(\\sigma^2\\) as \\[ \\hat{\\sigma}^2 = \\frac{\\mathbf{e}^T\\mathbf{e}}{n-p}, \\] where \\(n\\) is the length of the vector \\(\\mathbf{e}\\) and \\(p\\) is the length of the vector \\(\\hat{\\boldsymbol{\\beta}}\\). 8.3 Derivation of \\(\\hat{\\beta}\\) This section is included for reference, but you may skip it if you wish. We first give some notation and results for differentiating with respect to a vector. Let \\(\\mathbf{z}\\) be an \\(r\\times 1\\) column vector \\((z_1,\\ldots,z_r)^T\\) and let \\(f(z_1,\\ldots,z_r)\\) be some function of \\(\\mathbf{z}\\). We define \\[ \\frac{\\partial f(z_1,\\ldots,z_r)}{\\partial \\mathbf{z}}=\\left(\\begin{array}{c}\\frac{\\partial f(z_1,\\ldots,z_r)}{\\partial z_1} \\\\ \\vdots \\\\ \\frac{\\partial f(z_1,\\ldots,z_r)}{\\partial z_r}\\end{array}\\right). \\] For any \\(r\\times 1\\) column vector \\(\\mathbf{a}=(a_1,\\ldots,a_r)^T\\) we have \\[ \\frac{\\partial \\mathbf{a}^T\\mathbf{z}}{\\partial \\mathbf{z}}=\\frac{\\partial (a_1z_1+\\ldots + a_rz_r)}{d \\mathbf{z}}=(a_1,\\ldots,a_r)^T=\\mathbf{a}. \\] If \\(M\\) is a square \\(r\\times r\\) matrix then \\[ \\frac{\\partial(\\mathbf{z}^TM\\mathbf{z})}{\\partial \\mathbf{z}}=(M+M^T)\\mathbf{z}. \\] Proof: Let \\(m_{ij}\\) represent the \\(ij\\)th element of \\(M\\). Now \\((M+M^T)\\mathbf{z}\\) is a column vector with the \\(k\\)th element given by \\(\\sum_{i=1}^r m_{ki}z_i + \\sum_{i=1}^r m_{ik}z_i\\). Hence we must show that \\[ \\frac{\\partial(\\mathbf{z}^TM\\mathbf{z})}{\\partial z_k}=\\sum_{i=1}^r m_{ki}z_i + \\sum_{i=1}^r m_{ik}z_i. \\] From the product rule \\[\\begin{eqnarray*} \\frac{\\partial(\\mathbf{z}^TM\\mathbf{z})}{\\partial z_k}&amp;=&amp;\\mathbf{z}^T \\frac{\\partial( M\\mathbf{z})}{\\partial z_k}+\\left(\\frac{\\partial \\mathbf{z}^T}{\\partial z_k}\\right)M\\mathbf{z}\\\\ &amp;=&amp;(z_1,\\ldots,z_r)\\left(\\begin{array}{c}\\frac{\\partial}{\\partial z_k} \\sum_{i=1}^r m_{1i}z_i \\\\ \\vdots \\\\ \\frac{\\partial}{\\partial z_k} \\sum_{i=1}^r m_{ri}z_i \\end{array}\\right)\\\\&amp;&amp; + (0,\\ldots,0,1,0,\\ldots,0) \\left(\\begin{array}{c}\\sum_{i=1}^r m_{1i}z_i \\\\ \\vdots \\\\ \\sum_{i=1}^r m_{ri}z_i \\end{array}\\right), \\end{eqnarray*}\\] (with \\((0,\\ldots,0,1,0,\\ldots,0)\\) a vector of zeros with the \\(k\\)th element replaced by a 1) \\[\\begin{eqnarray*} &amp;=&amp; (z_1,\\ldots,z_r)\\left(\\begin{array}{c} m_{1k} \\\\ \\vdots \\\\ m_{rk} \\end{array}\\right) + (0,\\ldots,0,1,0,\\ldots,0) \\left(\\begin{array}{c}\\sum_{i=1}^r m_{1i}z_i \\\\ \\vdots \\\\ \\sum_{i=1}^r m_{ri}z_i \\end{array}\\right)\\\\ &amp;=&amp; \\sum_{i=1}^r m_{ik}z_i+\\sum_{i=1}^r m_{ki}z_i, \\end{eqnarray*}\\] as required. Now, note that \\[ \\sum_{i=1}^n \\varepsilon_i^2 = \\boldsymbol{\\varepsilon}^T\\boldsymbol{\\varepsilon} = (\\mathbf{y}-X\\boldsymbol{\\beta})^T(\\mathbf{y}-X\\boldsymbol{\\beta}) \\] Hence in vector notation, to minimise the sum of squared errors we must solve the equation \\[\\frac{\\partial \\boldsymbol{\\varepsilon}^T\\boldsymbol{\\varepsilon}}{\\partial \\boldsymbol{\\beta}} = \\mathbf{0}.\\] Now \\[\\begin{eqnarray} \\frac{\\partial \\boldsymbol{\\varepsilon}^T\\boldsymbol{\\varepsilon}}{\\partial \\boldsymbol{\\beta}} &amp;=&amp; \\frac{\\partial }{\\partial \\boldsymbol{\\beta}} (\\mathbf{y}-X\\boldsymbol{\\beta})^T(\\mathbf{y}-X\\boldsymbol{\\beta})\\\\ &amp;=&amp; \\frac{\\partial }{\\partial \\boldsymbol{\\beta}} \\left(\\mathbf{y}^T \\mathbf{y} -\\boldsymbol{\\beta}^TX^T\\mathbf{y} -\\mathbf{y}^TX\\boldsymbol{\\beta} + \\boldsymbol{\\beta}^T(X^T X)\\boldsymbol{\\beta}\\right)\\\\ &amp;=&amp; - X^T\\mathbf{y} - (\\mathbf{y}^T X)^T +\\left\\{(X^TX)^T+ (X^TX)\\right\\}\\boldsymbol{\\beta}\\\\ &amp;=&amp;-2X^T\\mathbf{y} + 2(X^TX)\\boldsymbol{\\beta}. \\end{eqnarray}\\] Finally, when \\(\\boldsymbol{\\beta}=\\hat{\\boldsymbol{\\beta}}\\), the least squares estimator, we have \\[ \\mathbf{0}=-2X^T\\mathbf{y} + 2(X^TX)\\hat{\\boldsymbol{\\beta}},\\label{normalequation} \\] (sometimes referred to as the normal equation) which gives us the result \\[ \\hat{\\boldsymbol{\\beta}}=(X^TX)^{-1}X^T\\mathbf{y}. \\] "],["fitting-a-linear-model-in-r.html", "Chapter 9 Fitting a linear model in R 9.1 Plotting a fitted regression line 9.2 The summary() command", " Chapter 9 Fitting a linear model in R To illustrate fitting a linear model in R, for convenience we’ll use one of R’s built in data sets: cars head(cars) ## speed dist ## 1 4 2 ## 2 4 10 ## 3 7 4 ## 4 7 22 ## 5 8 16 ## 6 9 10 Our dependent variable (dist) is the stopping distance in feet, and the independent variable is (speed) is the speed in miles per hour. The cars are very old (1920s!), but it’s a convenient data set for illustrating the linear modelling syntax. The basic syntax is lm(formula, data ) where the formula argument corresponds to the equation for \\(E(Y_i)\\) and uses column names from the data frame specified by the data argument. Some formula examples are as follows. Model formula \\(Y_i = \\beta_0 + \\varepsilon_i\\) dist ~ 1 \\(Y_i = \\beta_0 + \\beta_1 x_i+ \\varepsilon_i\\) dist ~ speed \\(Y_i = \\beta_0 + \\beta_1 x_i+ \\beta_2 x_i^2+\\varepsilon_i\\) dist ~ speed + I(speed^2) \\(Y_i = \\beta_0 + \\beta_1 \\log x_i+\\varepsilon_i\\) dist ~ log(speed) For example, to fit the simple linear regression model we do lm(dist ~ speed, cars) ## ## Call: ## lm(formula = dist ~ speed, data = cars) ## ## Coefficients: ## (Intercept) speed ## -17.579 3.932 and from the Coefficients output we read off \\(\\hat{\\beta}_0= -17.579\\) and \\(\\hat{\\beta}_1 = 3.932\\). 9.1 Plotting a fitted regression line Regression lines can be plotted using the geom_smooth() command in ggplot2: ggplot(cars, aes(x = dist, y = speed)) + geom_point() + geom_smooth(method = &quot;lm&quot;, formula = &quot;y ~ x&quot;) (We did this in semester 1 - now you know how the blue line is obtained! We’re not going to study the details here, but the grey shaded region indicates, point-wise, 95% confidence intervals for \\(E(Y)\\) for each possible \\(x\\); the region indicates uncertainty in the fitted regression line.) 9.2 The summary() command The summary command will give us more information about our model fit, if we first assign the fitted model to a variable. Although we will not cover hypothesis testing for linear models in this module, the summary() command does report some hypothesis tests, and we will describe these briefly. For example: lmCars &lt;- lm(dist ~ speed, cars) summary(lmCars) ## ## Call: ## lm(formula = dist ~ speed, data = cars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -29.069 -9.525 -2.272 9.215 43.201 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -17.5791 6.7584 -2.601 0.0123 * ## speed 3.9324 0.4155 9.464 1.49e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 15.38 on 48 degrees of freedom ## Multiple R-squared: 0.6511, Adjusted R-squared: 0.6438 ## F-statistic: 89.57 on 1 and 48 DF, p-value: 1.49e-12 We interpret the output as follows. Residuals refer to those residuals we defined previously: \\(e_i = y_i - \\hat{y}_i\\). The Estimate column gives the least squares estimate. The Std. Error column gives the estimated standard error for each least squares estimate. The t value and Pr(&gt;|t|) refer to a hypothesis test that the corresponding model parameter is 0. For example, in the speed row the hypotheses are \\[ H_0: \\beta_1 = 0, \\quad H_A: \\beta_1 \\neq 0. \\] For reference, the test statistic is the least squares estimate, divided by its estimated standard error, and this is compared with a \\(t_{n-p}\\) distribution, where \\(n\\) is the number of observations, and \\(p\\) is the length of the parameter vector \\(\\boldsymbol{\\beta}\\) The residual standard error is the estimated value \\(\\hat{\\sigma}\\) of \\(\\sigma\\) (standard deviation of the errors, not the variance). Recall that this is computed using the residual sum of squares: \\(\\hat{\\sigma}^2 = \\mathbf{e}^T\\mathbf{e}/(n-p)\\) The Multiple R-squared and Adjusted R-squared both report how ‘useful’ the model is for predicting the dependent variable: they report the proportion of the variation in \\(y_1,\\ldots,y_n\\) that can be explained by variation in the dependent variables. The Adjusted R-squared corrects for the number of independent variables in the model. The F-statistic and p-value report a test of the hypothesis that all elements of the parameter vector \\(\\boldsymbol{\\beta}\\) are zero, except for the parameter corresponding to the intercept. This tests the hypothesis that none of the independent variables are related to the dependent variable. Example 9.1 (Fitting a linear model in R: election example.) Using the airquality data head(airquality) ## Ozone Solar.R Wind Temp Month Day ## 1 41 190 7.4 67 5 1 ## 2 36 118 8.0 72 5 2 ## 3 12 149 12.6 74 5 3 ## 4 18 313 11.5 62 5 4 ## 5 NA NA 14.3 56 5 5 ## 6 28 NA 14.9 66 5 6 fit the simple linear regression model in R, with Ozone as the dependent variable and Temp as the independent variable. Obtain the parameter estimates, including the estimated error variance. Produce a scatter plot of the data in R, and add the fitted regression line. Try the plot and fitted regression line in which log of Ozone is the dependent variable. Solution The model is \\[ Y_i = \\beta_0 + \\beta_1x_i + \\varepsilon_i, \\] where \\(Y_i\\) is the ozone concentration for the \\(i\\)-th observation, \\(x_i\\) is the corresponding temperature, and \\(\\varepsilon_i \\sim N(0,\\sigma^2)\\). We fit the model with the command lmAirQuality &lt;- lm(Ozone ~ Temp, airquality) To get all the parameter estimates, we do summary(lmAirQuality) ## ## Call: ## lm(formula = Ozone ~ Temp, data = airquality) ## ## Residuals: ## Min 1Q Median 3Q Max ## -40.729 -17.409 -0.587 11.306 118.271 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -146.9955 18.2872 -8.038 9.37e-13 *** ## Temp 2.4287 0.2331 10.418 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 23.71 on 114 degrees of freedom ## (37 observations deleted due to missingness) ## Multiple R-squared: 0.4877, Adjusted R-squared: 0.4832 ## F-statistic: 108.5 on 1 and 114 DF, p-value: &lt; 2.2e-16 so we have \\(\\hat{\\beta}_0 = -146.9955\\), \\(\\hat{\\beta}_1 = 2.4287\\) and \\(\\hat{\\sigma} = 23.71\\). To produce the plot, we do ggplot(lmAirQuality, aes(x = Temp, y = Ozone)) + geom_point() + geom_smooth(method = &quot;lm&quot;, formula = y ~ x) We can see the fitted regression line dipping below 0, so perhaps log ozone concentration is a better choice for the dependent variable: ggplot(lmAirQuality, aes(x = Temp, y = log(Ozone))) + geom_point() + geom_smooth(method = &quot;lm&quot;, formula = y ~ x) We note one outlier, but otherwise this is perhaps a better fit. To fit this with the lm command, we just do lm(log(Ozone) ~ Temp, airquality) ## ## Call: ## lm(formula = log(Ozone) ~ Temp, data = airquality) ## ## Coefficients: ## (Intercept) Temp ## -1.8380 0.0675 "],["qualitative-independent-variables.html", "Chapter 10 Qualitative independent variables 10.1 Example: cancer survival data 10.2 A linear model for the cancer data 10.3 Notation for qualitative independent variables 10.4 The one-way ANOVA model in matrix form 10.5 Least squares estimates for the one-way ANOVA model 10.6 Fitting a one-way ANOVA model in R 10.7 An alternative parameterisation", " Chapter 10 Qualitative independent variables 10.1 Example: cancer survival data The file cancer.csv has data from patients with advanced cancers of the stomach, bronchus, colon, ovary or breast were treated with ascorbate. We suppose that this aim is to determine if patient survival differs with respect to the organ affected by the cancer. The survival time in days was recorded for each patient. cancer &lt;- read_csv(&quot;cancer.csv&quot;) head(cancer) ## # A tibble: 6 × 2 ## survival organ ## &lt;dbl&gt; &lt;chr&gt; ## 1 124 Stomach ## 2 42 Stomach ## 3 25 Stomach ## 4 45 Stomach ## 5 412 Stomach ## 6 51 Stomach We can use a box plot to compare the survival times for each type (organ) of cancer patient: ggplot(cancer, aes(x = organ, y = survival)) + geom_boxplot() We suppose that the survival time is the dependent variable of interest, which we can treat as continuous as before. But our independent variable, organ, is qualitative. Can we still use a linear model to analyse these data? 10.2 A linear model for the cancer data Clearly, it would not make sense to write a model such as \\[ Y_i=\\beta_0 + \\beta_1 x_i +\\varepsilon_i, \\] where \\(Y_i\\) is the survival time of the \\(i\\)-th patient and \\(x_i\\) is the cancer type of the \\(i\\)-th patient, because the independent variable (cancer type) is a categorical variable (i.e. it doesn’t make sense to say “survival time \\(= 50 +3\\times\\) stomach cancer”). We could write our model as \\[ Y_i= \\beta_1 x_{i,1} + \\beta_2 x_{i,2}+\\beta_3 x_{i,3}+\\beta_4 x_{i,4}+\\beta_5 x_{i,5} +\\varepsilon_i, \\] where \\(x_{i,j} = 1\\) if patient had cancer type \\(j\\) (\\(j=1\\) for Breast, \\(j=2\\) for Bronchus and so on) and \\(x_{i,j} = 0\\) otherwise. We say that \\(x_{i,j}\\) is a dummy variable. The five dummy variables here ensure that the correct \\(\\beta\\) term is selected for each observation. This notation can be a little cumbersome, so we typically write these sorts of models in a different way. 10.3 Notation for qualitative independent variables Each observation is associated with a particular group, where the group is specified by the value (level) of the qualitative independent variable. The organ variable can be one of five possibilities, so we think the data as being organised in five groups. We write \\(Y_{ij}\\) as the \\(j\\)-th observation within group \\(i\\). Let \\(g\\) be the total number of groups (with \\(g=5\\) in the cancer data). We then have \\(i=1,\\ldots,g\\). Whereas a quantitative independent variable is typically represented by its own letter (e.g. \\(x_i\\)), we typically represent a qualitative independent variable using an additional subscript on the dependent variable. Within group \\(i\\) we let \\(n_i\\) be the total number of observations, so that we have \\(j=1,\\ldots,n_i\\). In the cancer data, if we call the group of patients with breast cancer group 1, there are 11 patients in this group, so \\(n_1=11\\), and the 11 survival times for patients with stomach cancer are denoted \\(Y_{1,1},Y_{1,2},\\ldots,Y_{1,11}\\). As usual, we think of \\(Y_{ij}\\) as a random variable, and \\(y_{ij}\\) as the observed value of that random variable. We let \\(n\\) denote the total number of observations, so that \\(n=\\sum_{i=1}^g n_i\\). Now let \\(\\mu_i\\) denote the population mean of the dependent variable in group \\(i\\). We can now write a model for the data as follows: \\[ Y_{ij}=\\mu_i + \\varepsilon_{ij}, \\] for \\(i=1,\\ldots,g\\), \\(j=1,\\ldots,n_i\\) and \\(\\varepsilon_{ij}\\sim N(0,\\sigma^2)\\). Analysis of data using this model is sometimes referred to as one-way analysis of variance (ANOVA), and we will refer to the above model as the one-way ANOVA model. 10.4 The one-way ANOVA model in matrix form The model \\[ Y_{ij}=\\mu_i + \\varepsilon_{ij},\\] for \\(i=1,\\ldots,g\\), \\(j=1,\\ldots,n_i\\) is written in matrix form as \\[ \\mathbf{Y} = X\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}, \\] with \\[ \\mathbf{Y}=\\left(\\begin{array}{c}Y_{1,1} \\\\ \\vdots \\\\ Y_{1,n_1} \\\\ Y_{2,1}\\\\ \\vdots \\\\ Y_{2,n_2} \\\\ \\vdots \\\\ Y_{g,1} \\\\ \\vdots \\\\ Y_{g,n_g}\\end{array}\\right),\\quad X=\\left(\\begin{array}{ccccc}1&amp; 0 &amp; 0 &amp; \\ldots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ 1&amp; 0 &amp; 0 &amp; \\ldots &amp; 0 \\\\ 0&amp; 1 &amp; 0 &amp; \\ldots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ 0&amp; 1 &amp; 0 &amp; \\ldots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ 0&amp; 0 &amp; 0 &amp; \\ldots &amp; 1 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ 0&amp; 0 &amp; 0 &amp; \\ldots &amp; 1 \\end{array}\\right),\\quad \\boldsymbol{\\beta}=\\left(\\begin{array}{c}\\mu_1 \\\\ \\vdots \\\\ \\mu_g\\end{array}\\right),\\quad \\boldsymbol{\\varepsilon}=\\left(\\begin{array}{c}\\varepsilon_{1,1} \\\\ \\vdots \\\\ \\varepsilon_{1,n_1} \\\\ \\varepsilon_{2,1}\\\\ \\vdots \\\\ \\varepsilon_{2,n_2} \\\\ \\vdots \\\\ \\varepsilon_{g,1} \\\\ \\vdots \\\\ \\varepsilon_{g,n_g}\\end{array}\\right) \\] 10.5 Least squares estimates for the one-way ANOVA model Now that we have written the model in matrix notation, we can immediately obtain least squares estimates for the unknown group means \\(\\mu_1,\\ldots,\\mu_g\\), using the formula \\(\\hat{\\boldsymbol{\\beta}}=(X^TX)^{-1}X^T\\mathbf{y}\\). Since \\[ (X^TX)^{-1}=\\left(\\begin{array}{cccc}n_1 &amp; 0 &amp; \\ldots &amp; 0 \\\\ 0 &amp; n_2 &amp; \\ldots &amp; 0 \\\\ \\vdots &amp; &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\ldots &amp; n_g \\end{array} \\right)^{-1},\\quad X^T\\mathbf{y}=\\left(\\begin{array}{c} \\sum_{j=1}^{n_1} y_{1,j} \\\\ \\vdots \\\\ \\sum_{j=1}^{n_g} y_{g,j} \\end{array}\\right) \\] we have \\[ \\hat{\\boldsymbol{\\beta}}=\\left(\\begin{array}{c}\\hat{\\mu}_1 \\\\ \\vdots \\\\ \\hat{\\mu}_g \\end{array}\\right) = \\left(\\begin{array}{c} \\frac{1}{n_1} \\sum_{j=1}^{n_1} y_{1,j} \\\\ \\vdots \\\\ \\frac{1}{n_g} \\sum_{j=1}^{n_g} y_{g,j} \\end{array}\\right) \\] This result is intuitive. For example, in group 1 we have \\(n_1\\) observations \\(y_{1,1},\\ldots,y_{1,n_1}\\), all with expected value \\(\\mu_1\\). The obvious estimate for \\(\\mu_1\\) is the sample mean \\(\\frac{1}{n_1} \\sum_{j=1}^{n_1} y_{1,j}\\). 10.6 Fitting a one-way ANOVA model in R We fit the model \\[ Y_{ij}=\\mu_i + \\varepsilon_{ij},\\] for \\(i=1,\\ldots,g\\), \\(j=1,\\ldots,n_i\\) to the cancer data as follows. lmCancer &lt;- lm(survival ~ organ - 1, cancer) (the reason for the - 1 in the formula will become clearer shortly, when we look at an alternative parametrisation of the model.) We can now use the summary() command to get the parameter estimates summary(lmCancer) ## ## Call: ## lm(formula = survival ~ organ - 1, data = cancer) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1371.91 -241.75 -111.50 87.19 2412.09 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## organBreast 1395.9 201.9 6.915 3.77e-09 *** ## organBronchus 211.6 162.4 1.303 0.19764 ## organColon 457.4 162.4 2.817 0.00659 ** ## organOvary 884.3 273.3 3.235 0.00199 ** ## organStomach 286.0 185.7 1.540 0.12887 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 669.5 on 59 degrees of freedom ## Multiple R-squared: 0.5437, Adjusted R-squared: 0.505 ## F-statistic: 14.06 on 5 and 59 DF, p-value: 4.766e-09 We have \\(\\hat{\\beta}_1 = 1395.9\\), \\(\\hat{\\beta}_2 = 211.6\\) and so on. Note that this model does not have an intercept, so the F-statstic and p-value in the last line report a hypothesis test of \\(H_0: \\beta_i = 0\\) for \\(i=1,\\ldots,5\\), with \\(H_A:\\beta_i \\neq 0\\) for at least one \\(i\\). 10.7 An alternative parameterisation None of the hypothesis tests in the summary() output above are likely to be of any interest; we are more likely to be interested in differences between groups, rather than whether some or all of the groups have a population mean response of 0. An alternative way of writing the one-way ANOVA model is as follows: \\[ Y_{i,j}=\\mu + \\tau_i + \\varepsilon_{i,j}, \\] for \\(i=1,\\ldots,g\\), \\(j=1,\\ldots,n_i\\) and with \\(\\varepsilon_{i,j}\\sim N(0,\\sigma^2)\\). The intention of this parametrisation could be to think of \\(\\mu\\) as the grand mean, and \\(\\tau_i\\) as the difference between the mean of group \\(i\\) and the grand mean \\(\\mu\\). However, this model is over-parametrised. In matrix notation, we would write this as \\[ \\mathbf{Y} = X\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}, \\] with \\[ \\mathbf{Y}=\\left(\\begin{array}{c}Y_{1,1} \\\\ \\vdots \\\\ Y_{1,n_1} \\\\ Y_{2,1}\\\\ \\vdots \\\\ Y_{2,n_2} \\\\ \\vdots \\\\ Y_{g,1} \\\\ \\vdots \\\\ Y_{g,n_g}\\end{array}\\right),\\quad X=\\left(\\begin{array}{cccccc}1&amp;1&amp; 0 &amp; 0 &amp; \\ldots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ 1&amp; 1&amp; 0 &amp; 0 &amp; \\ldots &amp; 0 \\\\ 1 &amp; 0&amp; 1 &amp; 0 &amp; \\ldots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ 1 &amp; 0&amp; 1 &amp; 0 &amp; \\ldots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ 1 &amp; 0&amp; 0 &amp; 0 &amp; \\ldots &amp; 1 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ 1 &amp; 0&amp; 0 &amp; 0 &amp; \\ldots &amp; 1 \\end{array}\\right),\\quad \\boldsymbol{\\beta}=\\left(\\begin{array}{c}\\mu \\\\\\tau_1 \\\\ \\vdots \\\\ \\tau_g\\end{array}\\right),\\quad \\boldsymbol{\\varepsilon}=\\left(\\begin{array}{c}\\varepsilon{1,1} \\\\ \\vdots \\\\ \\varepsilon{1,n_1} \\\\ \\varepsilon{2,1}\\\\ \\vdots \\\\ \\varepsilon{2,n_2} \\\\ \\vdots \\\\ \\varepsilon{g,1} \\\\ \\vdots \\\\ \\varepsilon{g,n_g}\\end{array}\\right). \\] For this particular design matrix \\(X\\), we find that \\[ X^TX=\\left(\\begin{array}{ccccc}n &amp; n_1 &amp; n_2 &amp; \\cdots &amp; n_g \\\\ n_1 &amp; n_1 &amp; 0 &amp; \\cdots &amp; 0\\\\ n_2 &amp; 0 &amp; n_2 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp;\\vdots &amp; \\ddots &amp; \\vdots \\\\ n_g &amp; 0 &amp; 0 &amp; \\cdots &amp; n_g \\end{array}\\right). \\] The matrix \\(X^TX\\) cannot be inverted, as \\(\\det(X^TX)=0\\) (the first column is the sum of columns 2 to \\(g+1\\), noting that \\(\\sum_{i=1}^g n_i=n\\)). Hence it is not possible to obtain least squares parameter estimates for this model, as we cannot evaluate the expression \\(\\boldsymbol{\\beta}=(X^TX)^{-1}X^T\\mathbf{y}\\). Intuitively, this makes sense as we are trying to estimate \\(g+1\\) parameters representing group means (\\(\\mu\\) and \\(\\tau_1,\\ldots,\\tau_g\\)) with data from only \\(g\\) groups. The solution is to apply constraints to the parameters. One possibility is to state that \\(\\tau_1=0\\), so that the model can be written as \\[ Y_{i,j}=\\left\\{\\begin{array}{ll}\\mu + \\varepsilon_{1,j} &amp; \\mbox{$i=1$, $j=1,\\ldots n_1$}\\\\ \\mu + \\tau_i + \\varepsilon_{i,j} &amp; \\mbox{$i=2,\\ldots,g$, $j=1,\\ldots n_i$} \\end{array}\\right. \\] For this parametrisation, \\(\\mu\\) is interpreted as the (population) mean for group 1, and \\(\\tau_i\\) gives the difference in means between group \\(i\\) and group 1, for \\(i\\neq 1\\). The null hypothesis of no difference between group means is written as \\[ H_0:\\tau_2=\\ldots=\\tau_g=0, \\] and any individual \\(\\tau_i=0\\) implies no difference in means between groups \\(i\\) and 1. This model is written in matrix form as \\[ \\mathbf{Y} = X\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}, \\] with \\[ \\mathbf{Y}=\\left(\\begin{array}{c}Y_{1,1} \\\\ \\vdots \\\\ Y_{1,n_1} \\\\ Y_{2,1}\\\\ \\vdots \\\\ Y_{2,n_2} \\\\ \\vdots \\\\ Y_{g,1} \\\\ \\vdots \\\\ Y_{g,n_g}\\end{array}\\right),\\quad X=\\left(\\begin{array}{ccccc}1&amp; 0 &amp; 0 &amp; \\ldots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ 1&amp; 0 &amp; 0 &amp; \\ldots &amp; 0 \\\\ 1&amp; 1 &amp; 0 &amp; \\ldots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ 1&amp; 1 &amp; 0 &amp; \\ldots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ 1&amp; 0 &amp; 0 &amp; \\ldots &amp; 1 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ 1&amp; 0 &amp; 0 &amp; \\ldots &amp; 1 \\end{array}\\right),\\quad \\boldsymbol{\\beta}=\\left(\\begin{array}{c}\\mu \\\\ \\tau_2 \\\\ \\vdots \\\\ \\tau_g\\end{array}\\right),\\quad \\boldsymbol{\\varepsilon}=\\left(\\begin{array}{c}\\varepsilon{1,1} \\\\ \\vdots \\\\ \\varepsilon{1,n_1} \\\\ \\varepsilon{2,1}\\\\ \\vdots \\\\ \\varepsilon{2,n_2} \\\\ \\vdots \\\\ \\varepsilon{g,1} \\\\ \\vdots \\\\ \\varepsilon{g,n_g}\\end{array}\\right). \\] For this design matrix we find that \\[ \\hat{\\boldsymbol{\\beta}}=\\left(\\begin{array}{c}\\frac{1}{n_1}\\sum_{j=1}^{n_1} y_{1,j} \\\\ \\frac{1}{n_2}\\sum_{j=1}^{n_2} y_{2,j} - \\frac{1}{n_1}\\sum_{j=1}^{n_1} y_{1,j} \\\\ \\vdots \\\\ \\frac{1}{n_g}\\sum_{j=1}^{n_g} y_{g,j} - \\frac{1}{n_1}\\sum_{j=1}^{n_1} y_{1,j} \\end{array}\\right), \\] (details omitted), hence \\(\\hat{\\mu}\\) is the sample mean of the observations in group 1, and \\(\\hat{\\tau}_i\\) is the difference between the sample means of groups \\(i\\) and 1. This is actually the default parametrisation in R. If we leave out the -1 from the previous command, we just do lmCancer &lt;- lm(survival ~ organ, cancer) and then use the summary() command as before. summary(lmCancer) ## ## Call: ## lm(formula = survival ~ organ, data = cancer) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1371.91 -241.75 -111.50 87.19 2412.09 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1395.9 201.9 6.915 3.77e-09 *** ## organBronchus -1184.3 259.1 -4.571 2.53e-05 *** ## organColon -938.5 259.1 -3.622 0.000608 *** ## organOvary -511.6 339.8 -1.506 0.137526 ## organStomach -1109.9 274.3 -4.046 0.000153 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 669.5 on 59 degrees of freedom ## Multiple R-squared: 0.3037, Adjusted R-squared: 0.2565 ## F-statistic: 6.433 on 4 and 59 DF, p-value: 0.0002295 (Intercept) refers to \\(\\mu\\). (So we can interpret the -1 term in the formula argument as saying that we do not want an intercept.) We have \\(\\hat{\\mu} = 1395.9\\), \\(\\hat{\\tau}_2 = -1184.3,\\ldots,\\hat{\\tau}_5 = -1109.9\\). Note that the F-statistic and p-value output now refer to a hypothesis test of \\[ H_0: \\tau_2 = \\tau_3 = \\tau_4 = \\tau_5, \\] with the alternative that at least two \\(\\tau\\) parameters are not equal. Example 10.1 (Fitting a linear model in R: ANOVA.) Using R, fit a one-way ANOVA model to the built in data set PlantGrowth, using weight as the dependent variable. Type ?PlantGrowth for details. head(PlantGrowth) ## weight group ## 1 4.17 ctrl ## 2 5.58 ctrl ## 3 5.18 ctrl ## 4 6.11 ctrl ## 5 4.50 ctrl ## 6 4.61 ctrl Obtain the parameter estimates, and comment on the F-statistic output from the summary() command. Use a suitable plot of the data to informally check your results. Solution The model is \\[ Y_{ij} = \\mu + \\tau_i + \\varepsilon_{ij}, \\] where \\(Y_{ij}\\) is the yield for the \\(j\\)-th plant in treatment group \\(i\\), for \\(i=1,2,3\\) and \\(j=1,\\ldots,10\\). We set \\(\\tau_1=0\\) and assume \\(\\varepsilon_{ij}\\sim N(0,\\sigma^2)\\). We fit the model with the command lmPlant &lt;- lm(weight ~ group, PlantGrowth) and then do summary(lmPlant) ## ## Call: ## lm(formula = weight ~ group, data = PlantGrowth) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.0710 -0.4180 -0.0060 0.2627 1.3690 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.0320 0.1971 25.527 &lt;2e-16 *** ## grouptrt1 -0.3710 0.2788 -1.331 0.1944 ## grouptrt2 0.4940 0.2788 1.772 0.0877 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6234 on 27 degrees of freedom ## Multiple R-squared: 0.2641, Adjusted R-squared: 0.2096 ## F-statistic: 4.846 on 2 and 27 DF, p-value: 0.01591 From this we read off \\(\\hat{\\mu}=5.0320\\), \\(\\hat{\\tau}_2=-0.3710\\), \\(\\hat{\\tau}_3=0.4940\\) and \\(\\hat{\\sigma}=0.6234\\). From the F-statistic line, we see that the p-value is 0.01591. Hence there is moderately strong evidence against the hypothesis that \\(\\tau_2 = \\tau_3 = 0\\); moderately strong evidence against the hypothesis that all three groups have the same mean yield. We can check these results informally using a box plot. ggplot(PlantGrowth, aes(x = group, y = weight)) + geom_boxplot() If we suppose that each group mean is approximately the same as the group median, then this confirms that trt1 has the lowest mean (\\(\\hat{\\tau}_2\\)) was negative; although \\(\\tau_2\\) and \\(\\tau_3\\) may not be significantly different from 0 (ctrl and trt1 have similar means; ctrl and trt2 have similar means), it doesn’t not look plausible that all three groups have the same population mean (trt1 and trt2 look different). "],["multiple-independent-variables.html", "Chapter 11 Multiple independent variables 11.1 Causation versus association 11.2 Analysis of Covariance", " Chapter 11 Multiple independent variables Recall the PISA maths test data from Semester 1. Here, our dependent variable was a country’s maths test score, and we had multiple independent variables: gdp, income inequality (gini coefficient), homework hours, and school starting age. We’ll now look at how to incorporate multiple independent variables in a linear model. If the main interest is in, say, the effect of gdp, incorporating the variables enables us to assess the effect of gdp, whilst accounting for (sometimes described as “adjusting for” or “controlling for”) possible effects of other variables. We’ll first import the data, and create one extra column (wealthiest): a binary variable that indicates whether a country as a gdp higher and 17000. maths &lt;- read_csv(&quot;http://www.jeremy-oakley.staff.shef.ac.uk/mas113/maths.csv&quot;) %&gt;% mutate(wealthiest = gdp &gt; 17000) head(maths) ## # A tibble: 6 × 8 ## country continent score gdp gini homework start.age wealthiest ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 Albania Europe 413 4147 29 5.1 6 FALSE ## 2 Algeria Africa 360 3844 27.6 NA 6 FALSE ## 3 Argentina South America 409 12449 42.7 3.7 6 FALSE ## 4 Australia Oceanea 494 49928 34.7 6 5 TRUE ## 5 Austria Europe 497 44177 30.5 4.5 6 TRUE ## 6 B-S-J-G (China) Asia 531 8123 42.2 13.8 6 FALSE For illustration, we’ll treat start.age as a factor variable with three levels, and consider modelling the effects of gdp, gini, homework and factor(start.age) on score. How might we write down such a model? When thinking about suitable notation: use one letter for each quantitative independent variable; use one additional subscript for each qualitative independent variable. We imagine splitting the data up into three groups: group \\(i=1\\) for countries with a start.age of 5; group \\(i=2\\) for countries with a start.age of 6; group \\(i=3\\) for countries with a start.age of 7. Then, for the \\(j\\)-th observation (country) within group \\(i\\), define \\(Y_{ij}\\) to be the country’s score; \\(w_{ij}\\) to be the country’s gdp; \\(x_{ij}\\) to be the country’s gini coefficient; \\(z_{ij}\\) to be the mean number of homework hours per week in that country. We can now write the model as \\[ Y_{ij} = \\mu + \\tau_i + \\beta_1 w_{ij} + \\beta_2 x_{ij} + \\beta_3z_{ij}+ \\varepsilon_{ij}, \\] with \\(\\varepsilon_{ij}\\sim N(0, \\sigma^2)\\). This model is overparameterised, so we apply the constraint \\(\\tau_1 = 0\\). We interpret \\(\\tau_i\\), for \\(i=2\\) (or 3) as the difference in mean score between two countries with identical values of gdp, gini and homework, but in which one has a school start.age of 6 (or 7) and one with a school start.age of 5. We fit the model in R as follows: lmMaths &lt;- lm(score ~ gdp + gini + homework + factor(start.age), maths) summary(lmMaths) ## ## Call: ## lm(formula = score ~ gdp + gini + homework + factor(start.age), ## data = maths) ## ## Residuals: ## Min 1Q Median 3Q Max ## -61.398 -17.884 -0.204 17.867 69.026 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.827e+02 3.743e+01 12.896 &lt; 2e-16 *** ## gdp 1.025e-03 2.181e-04 4.701 2.07e-05 *** ## gini -2.634e+00 6.939e-01 -3.796 0.000399 *** ## homework 8.343e+00 2.337e+00 3.569 0.000802 *** ## factor(start.age)6 3.062e+00 1.971e+01 0.155 0.877183 ## factor(start.age)7 1.023e+01 2.118e+01 0.483 0.630987 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 31.31 on 50 degrees of freedom ## (14 observations deleted due to missingness) ## Multiple R-squared: 0.5947, Adjusted R-squared: 0.5542 ## F-statistic: 14.67 on 5 and 50 DF, p-value: 7.547e-09 We read off from this \\(\\hat{\\mu}=482.7\\), \\(\\hat{\\tau}_2= 3.06\\), \\(\\hat{\\tau}_3=10.23\\), \\(\\hat{\\beta}_1=0.001\\), \\(\\hat{\\beta}_2=-2.634\\), and \\(\\hat{\\beta}_3=8.343\\). 11.1 Causation versus association Suppose we were interested in the effect of gdp only. Let’s try fitting the model with this independent variable only: lmMathsGDP &lt;- lm(score ~ gdp , maths) summary(lmMathsGDP) ## ## Call: ## lm(formula = score ~ gdp, data = maths) ## ## Residuals: ## Min 1Q Median 3Q Max ## -105.932 -27.488 5.583 25.925 95.609 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.239e+02 7.975e+00 53.153 &lt; 2e-16 *** ## gdp 1.417e-03 2.322e-04 6.101 5.67e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 43.18 on 68 degrees of freedom ## Multiple R-squared: 0.3537, Adjusted R-squared: 0.3442 ## F-statistic: 37.22 on 1 and 68 DF, p-value: 5.667e-08 Notice how the slope estimate for gdp has changed from that in lmMaths: it has increased by about 50% to 0.00147. Two points to note are gdp and gini are correlated. This means that removing gini from the model is likely to change the estimated slope for gdp. This is something we try to avoid, if we can design the experiment (i.e. we can choose the values of the independent variables.) If we are hoping to learn a causal relationship between gdp and score, we need to include other variables in the model which may also effect score. If these other variables are omitted, we are (potentially) instead learning about the association between gdp and score. An interpretation of this is as follows. lmMaths tells us that when we increase gdp by one unit, and all other variables stay the same, we expect score to increase by 0.001 units. If we were confident that all other relevant variables were unchanged, the change in score must be caused by the change in gdp. lmMathsGDP (together with our knowledge of the correlation of gdp and gini) tells us that when we increase gdp by one unit, we expect other variables such as gini to change. The combined change in all these variables means that we expect score to increase by 0.0014 units. We cannot necessarily attribute the change in score to the change in gpd: it might have been caused by another variables changing. 11.2 Analysis of Covariance Recall this plot we produced in Semester 1, Chapter 1: ggplot(maths, aes(x = gini, y = score, colour = wealthiest))+ geom_point() + geom_smooth(method = &quot;lm&quot;) We have two fitted regression lines: one for the group wealthiest == TRUE (countries with gdp\\(&gt;17000\\)), and one for the remaining countries. This sort of modelling with multiple regression lines is sometimes referred to as Analysis of Covariance (ANCOVA). The emphasis is usually on comparing means between groups, adjusting for different values of covariates between groups. We will now consider how we would write down this model, and how to fit it in R. Again, the idea is to use an extra subscript for the qualitative variable (wealthiest): define \\(Y_{ij}\\) to be the score for the \\(j\\)th country in group \\(i\\), where \\(i=1\\) corresponds to wealthiest==FALSE and \\(i=2\\) corresponds to wealthiest==TRUE, with \\(x_{ij}\\) the corresponding gdp value. Our model is \\[ Y_{ij} = \\mu + \\tau_i + \\beta_i x_{ij} + \\varepsilon_{ij} \\] with \\(\\varepsilon_{ij}\\sim N(0,\\sigma^2)\\). Again this is overparametrised, so we set \\(\\tau_1=0\\). Note that the slope parameter for gdp depends on whether wealthiest is FALSE or TRUE. We think of this as an interaction between gdp and wealthiest. We fit the model in R as follows: lmMathsWealthiest &lt;- lm(score ~ gini * wealthiest, maths) summary(lmMathsWealthiest) ## ## Call: ## lm(formula = score ~ gini * wealthiest, data = maths) ## ## Residuals: ## Min 1Q Median 3Q Max ## -84.730 -17.966 -0.595 15.104 113.483 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 492.3368 32.5735 15.115 &lt;2e-16 *** ## gini -1.7730 0.8507 -2.084 0.0415 * ## wealthiestTRUE 75.0248 61.7823 1.214 0.2295 ## gini:wealthiestTRUE -0.4358 1.8385 -0.237 0.8134 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 36.19 on 59 degrees of freedom ## (7 observations deleted due to missingness) ## Multiple R-squared: 0.5292, Adjusted R-squared: 0.5053 ## F-statistic: 22.11 on 3 and 59 DF, p-value: 1.022e-09 We read off \\(\\hat{\\mu}=492.3368\\), \\(\\hat{\\tau}_2 = 75.0248\\) and \\(\\hat{\\beta_1} = -1.7730\\). The row gini:wealthiestTRUE corresponds to the difference \\(\\hat{\\beta}_2- \\hat{\\beta}_1\\), so we read off \\(\\hat{\\beta}_2= -1.7730 -0.4358\\). We’ll try to redraw the plot, to check we’ve interpreted everything correctly! (To see what the default colours are in hex format, try scales::show_col(hue_pal()(2))) ggplot(maths, aes(x = gini, y = score, colour = wealthiest))+ geom_point() + geom_abline(slope = -1.773, intercept = 492.3368, col = &quot;#F8766D&quot; )+ geom_abline(slope = -1.773 - 0.4358, intercept = 492.3368 + 75.0248, col = &quot;#00BFC4&quot; ) The formatting of the lines is slightly different, but otherwise this looks correct. Example 11.1 (Fitting a linear model in R: ANCOVA.) Consider the built in dataset mtcars (see ?mtcars for details) head(mtcars) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 We’ll be using the column am which describes transmission type. To make it more readable, we’ll do mtcars2 &lt;- mtcars %&gt;% mutate(transmission = factor(am, labels = c(&quot;automatic&quot;, &quot;manual&quot;))) Fit the following model in R: \\[ Y_{ij} = \\mu + \\tau_i + \\beta_i x_{ij} + \\varepsilon_{ij} \\] where \\(Y_{ij}\\) is the fuel economy mpg of the \\(j\\)th car in group \\(i\\), and \\(x_{ij}\\) is the corresponding weight wt, for \\(i=1,2\\). Group \\(i=1\\) corresponds to automatic cars, and \\(i=2\\) corresponds to manual cars. For a car with weight of 3 units (1000 lbs), what is the expected change in fuel economy from changing from automatic to manual? We can plot the fitted model with the commands ggplot(mtcars2, aes(x = wt, y = mpg, colour = transmission)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) Try to (approximately) redraw this plot, without using geom_smooth(): use geom_abline() instead. In R we do lmCars &lt;- lm(mpg ~ wt * transmission, mtcars2) summary(lmCars) ## ## Call: ## lm(formula = mpg ~ wt * transmission, data = mtcars2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.6004 -1.5446 -0.5325 0.9012 6.0909 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 31.4161 3.0201 10.402 4.00e-11 *** ## wt -3.7859 0.7856 -4.819 4.55e-05 *** ## transmissionmanual 14.8784 4.2640 3.489 0.00162 ** ## wt:transmissionmanual -5.2984 1.4447 -3.667 0.00102 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.591 on 28 degrees of freedom ## Multiple R-squared: 0.833, Adjusted R-squared: 0.8151 ## F-statistic: 46.57 on 3 and 28 DF, p-value: 5.209e-11 From this, we read off \\(\\hat{\\mu}=31.4161\\), \\(\\hat{\\beta}_1=-3.7859\\), \\(\\hat{\\tau}_2=14.8784\\), \\(\\hat{\\beta}_2=-3.7859 -5.2984\\). An automatic car with weight 3 units has estimated expected mgp: \\[ E(Y) = \\hat{\\mu} +\\hat{\\beta}_1\\times 3= 31.4161 -3.7859 \\times 3 = 20.0584 \\] If the car changes from automatic to manual, the estimated expected mgp is \\[ \\hat{\\mu} +\\hat{\\tau}_2 +\\hat{\\beta}_2\\times 3 \\] The change is estimated to be \\[\\hat{\\tau}_2 + 3(\\hat{\\beta}_2 - \\hat{\\beta}_1) = 14.8784 -5.2984\\times 3. \\] We (approximately) reproduce the plot with the commands ggplot(mtcars2, aes(x = wt, y = mpg, colour = transmission))+ geom_point() + geom_abline(slope = -3.7859, intercept = 31.4161, col = &quot;#F8766D&quot; )+ geom_abline(slope = -3.7859 -5.2984, intercept = 31.4161 + 14.8784, col = &quot;#00BFC4&quot; ) "],["using-linear-models-for-prediction.html", "Chapter 12 Using linear models for prediction 12.1 Obtaining predictions in R", " Chapter 12 Using linear models for prediction Once we have fitted a linear model, we can predict what the dependent variable would be for particular values of the independent variables, and provide an interval around our prediction. Returning to the election example, we would like to predict the difference in absentee votes, given the difference in machine votes of -564. We define a vector \\(\\mathbf{x}\\) that includes the independent variables of interest, and a 1 for the intercept, such that \\(\\mathbf{x}^T \\hat{\\boldsymbol{\\beta}}\\) gives the desired point on the fitted regression line. So we would have \\(\\mathbf{x}^T = (1,\\, -564)\\) and \\[ \\mathbf{x}^T \\hat{\\boldsymbol{\\beta}} = \\hat{\\beta}_0 - 564\\hat{\\beta_1}. \\] This is our point estimate of the dependent variable. We won’t give the derivation here, but a \\(100(1-\\alpha)\\)% prediction interval is given by \\[ \\mathbf{x}^T \\hat{\\boldsymbol{\\beta}} \\pm t_{n-p,1-\\alpha/2}\\sqrt{\\hat{\\sigma}^2(1+ \\mathbf{x}^T (X^TX)^{-1} \\mathbf{x})}. \\] 12.1 Obtaining predictions in R We first fit our model: election &lt;- read_csv(&quot;election.csv&quot;) lmElection &lt;- lm(absentee.diff ~ machine.diff + I(machine.diff^2), election) If we want to predict at new values of the independent variable, we need to set up a new data frame with the desired independent variable(s) values, in which the column name matches that in our original data frame: newElection &lt;- data.frame(machine.diff = -564) We can then use the predict() command predict(lmElection, newdata = newElection, interval = &quot;prediction&quot;, level = 0.95) ## fit lwr upr ## 1 -235.8537 -871.2089 399.5016 If we don’t specify newdata, the default is to compute prediction intervals for all independent variables in the original data frame, used to fit the linear model. We can make use of this to plot prediction intervals. (Given the context of suspected fraud, we’ll use slightly wider intervals: 99% intervals.) First, we make the prediction intervals, and combine them with the original data in a new data frame: electionPredictions &lt;- cbind(election, predict(lmElection, interval = &quot;prediction&quot;, level = 0.99)) We then can now make a plot. We use geom_ribbon() to draw the intervals, with the argument ymin set to the interval lower limit (lwr) and ymax set to the interval upper limit (upr). The alpha argument controls the transparency of the interval in the plot. ggplot(electionPredictions, aes(x = machine.diff, y = absentee.diff)) + geom_point() + geom_smooth(method = &quot;lm&quot;, formula = y ~ x + I(x^2), se = FALSE) + geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.1, fill = &quot;red&quot;) + annotate(&quot;point&quot;, x = -564, y = 1025, col = &quot;red&quot;, pch = 4, size = 4) + annotate(&quot;text&quot;, x = -564, y = 1150, col = &quot;red&quot;, label = &quot;disputed election&quot;) We can see that the disputed election result lies some way outside the prediction interval: it does appear to be an outlier. In fact the judge did rule that there had been fraud, though on the basis of other evidence, rather than this particular analysis. Example 12.1 (Prediction with a linear model.) Using the airquality data, first make a data frame with the missing values removed. airQualityReduced &lt;- na.omit(airquality) Then fit a simple linear regression model of log ozone concentration on temperature using the data frame airQualityReduced (taking out the missing values will make plotting prediction intervals easier). Make a plot showing point-wise 95% prediction intervals for the log ozone concentration given the temperature, and obtain a 99% prediction interval for the log ozone concentration, given a temperature of 80.5 degrees F. Solution We first fit the model. lmAirQuality &lt;- lm(log(Ozone) ~ Temp, airQualityReduced) Now we make a new data frame with the prediction intervals: airQualityPredictions &lt;- cbind(airQualityReduced, predict(lmAirQuality, interval = &quot;prediction&quot;, level = 0.95)) Then, to make the plot: ggplot(airQualityPredictions, aes(x = Temp, y = log(Ozone))) + geom_point() + geom_smooth(method = &quot;lm&quot;, formula = y ~ x , se = FALSE) + geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.1, fill = &quot;red&quot;) To get the prediction interval for a particular value of Temp, we need to set up a new data frame first: newTemp &lt;- data.frame(Temp = 80.5) We then use this in the predict() command: predict(lmAirQuality, newdata = newTemp, interval = &quot;prediction&quot;, level = 0.99) ## fit lwr upr ## 1 3.599131 2.070096 5.128166 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
