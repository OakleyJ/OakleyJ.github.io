<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 Likelihood for confidence intervals and hypothesis tests | MAS5052 Part 2: Likelihood and Linear Models</title>
  <meta name="description" content="Lecture notes for MAS5052" />
  <meta name="generator" content="bookdown 0.27 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 Likelihood for confidence intervals and hypothesis tests | MAS5052 Part 2: Likelihood and Linear Models" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Lecture notes for MAS5052" />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Likelihood for confidence intervals and hypothesis tests | MAS5052 Part 2: Likelihood and Linear Models" />
  
  <meta name="twitter:description" content="Lecture notes for MAS5052" />
  

<meta name="author" content="Jeremy Oakley" />


<meta name="date" content="2022-07-06" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="maximisation-techniques.html"/>
<link rel="next" href="introducing-linear-models.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">MAS5052 Part 2</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>About these notes</a></li>
<li class="part"><span><b>I Likelihood methods</b></span></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introducing likelihood methods</a>
<ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#recap-maximising-functions"><i class="fa fa-check"></i><b>1.1</b> Recap: maximising functions</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#maximum-likelihood-estimation-a-first-example"><i class="fa fa-check"></i><b>1.2</b> Maximum likelihood estimation: a first example</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introducing-the-likelihood-function.html"><a href="introducing-the-likelihood-function.html"><i class="fa fa-check"></i><b>2</b> Introducing the likelihood function</a></li>
<li class="chapter" data-level="3" data-path="models-and-data.html"><a href="models-and-data.html"><i class="fa fa-check"></i><b>3</b> Models and data</a>
<ul>
<li class="chapter" data-level="3.1" data-path="models-and-data.html"><a href="models-and-data.html#data-notation"><i class="fa fa-check"></i><b>3.1</b> Data: notation</a></li>
<li class="chapter" data-level="3.2" data-path="models-and-data.html"><a href="models-and-data.html#models-and-parameters"><i class="fa fa-check"></i><b>3.2</b> Models and parameters</a></li>
<li class="chapter" data-level="3.3" data-path="models-and-data.html"><a href="models-and-data.html#likelihood-functions-for-i.i.d-data"><i class="fa fa-check"></i><b>3.3</b> Likelihood functions for i.i.d data</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="maximisation-techniques.html"><a href="maximisation-techniques.html"><i class="fa fa-check"></i><b>4</b> Maximisation techniques</a>
<ul>
<li class="chapter" data-level="4.1" data-path="maximisation-techniques.html"><a href="maximisation-techniques.html#log-likelihood"><i class="fa fa-check"></i><b>4.1</b> Log-likelihood</a></li>
<li class="chapter" data-level="4.2" data-path="maximisation-techniques.html"><a href="maximisation-techniques.html#discrete-parameters"><i class="fa fa-check"></i><b>4.2</b> Discrete parameters</a></li>
<li class="chapter" data-level="4.3" data-path="maximisation-techniques.html"><a href="maximisation-techniques.html#multi-parameter-problems"><i class="fa fa-check"></i><b>4.3</b> Multi-parameter problems</a></li>
<li class="chapter" data-level="4.4" data-path="maximisation-techniques.html"><a href="maximisation-techniques.html#using-a-computer"><i class="fa fa-check"></i><b>4.4</b> Using a computer</a></li>
<li class="chapter" data-level="4.5" data-path="maximisation-techniques.html"><a href="maximisation-techniques.html#a-warning-example"><i class="fa fa-check"></i><b>4.5</b> A warning example</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="likelihood-for-confidence-intervals-and-hypothesis-tests.html"><a href="likelihood-for-confidence-intervals-and-hypothesis-tests.html"><i class="fa fa-check"></i><b>5</b> Likelihood for confidence intervals and hypothesis tests</a>
<ul>
<li class="chapter" data-level="5.1" data-path="likelihood-for-confidence-intervals-and-hypothesis-tests.html"><a href="likelihood-for-confidence-intervals-and-hypothesis-tests.html#confidence-intervals"><i class="fa fa-check"></i><b>5.1</b> Confidence intervals</a></li>
<li class="chapter" data-level="5.2" data-path="likelihood-for-confidence-intervals-and-hypothesis-tests.html"><a href="likelihood-for-confidence-intervals-and-hypothesis-tests.html#hypothesis-tests"><i class="fa fa-check"></i><b>5.2</b> Hypothesis tests</a></li>
<li class="chapter" data-level="5.3" data-path="likelihood-for-confidence-intervals-and-hypothesis-tests.html"><a href="likelihood-for-confidence-intervals-and-hypothesis-tests.html#a-more-formal-approach"><i class="fa fa-check"></i><b>5.3</b> A more formal approach</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="likelihood-for-confidence-intervals-and-hypothesis-tests.html"><a href="likelihood-for-confidence-intervals-and-hypothesis-tests.html#asymptotic-normality-of-the-maximum-likelihood-estimator"><i class="fa fa-check"></i><b>5.3.1</b> Asymptotic normality of the maximum likelihood estimator</a></li>
<li class="chapter" data-level="5.3.2" data-path="likelihood-for-confidence-intervals-and-hypothesis-tests.html"><a href="likelihood-for-confidence-intervals-and-hypothesis-tests.html#confidence-intervals-based-on-asymptotic-normality"><i class="fa fa-check"></i><b>5.3.2</b> Confidence intervals based on asymptotic normality</a></li>
<li class="chapter" data-level="5.3.3" data-path="likelihood-for-confidence-intervals-and-hypothesis-tests.html"><a href="likelihood-for-confidence-intervals-and-hypothesis-tests.html#the-generalised-likelihood-ratio-test"><i class="fa fa-check"></i><b>5.3.3</b> The generalised likelihood ratio test</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Linear Models</b></span></li>
<li class="chapter" data-level="6" data-path="introducing-linear-models.html"><a href="introducing-linear-models.html"><i class="fa fa-check"></i><b>6</b> Introducing linear models</a>
<ul>
<li class="chapter" data-level="6.1" data-path="introducing-linear-models.html"><a href="introducing-linear-models.html#example-relationship-between-temperature-and-ozone"><i class="fa fa-check"></i><b>6.1</b> Example: relationship between temperature and ozone</a></li>
<li class="chapter" data-level="6.2" data-path="introducing-linear-models.html"><a href="introducing-linear-models.html#notation-and-terminology"><i class="fa fa-check"></i><b>6.2</b> Notation and terminology</a></li>
<li class="chapter" data-level="6.3" data-path="introducing-linear-models.html"><a href="introducing-linear-models.html#example-suspected-electoral-fraud"><i class="fa fa-check"></i><b>6.3</b> Example: suspected electoral fraud</a></li>
<li class="chapter" data-level="6.4" data-path="introducing-linear-models.html"><a href="introducing-linear-models.html#definition-of-a-linear-model"><i class="fa fa-check"></i><b>6.4</b> Definition of a linear model</a></li>
<li class="chapter" data-level="6.5" data-path="introducing-linear-models.html"><a href="introducing-linear-models.html#the-simple-linear-regression-model"><i class="fa fa-check"></i><b>6.5</b> The simple linear regression model</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="parameter-estimation.html"><a href="parameter-estimation.html"><i class="fa fa-check"></i><b>7</b> Parameter estimation</a>
<ul>
<li class="chapter" data-level="7.1" data-path="parameter-estimation.html"><a href="parameter-estimation.html#least-squares-estimation"><i class="fa fa-check"></i><b>7.1</b> Least squares estimation</a></li>
<li class="chapter" data-level="7.2" data-path="parameter-estimation.html"><a href="parameter-estimation.html#assumptions-for-least-squares"><i class="fa fa-check"></i><b>7.2</b> Assumptions for least squares</a></li>
<li class="chapter" data-level="7.3" data-path="parameter-estimation.html"><a href="parameter-estimation.html#relationship-with-maximum-likelihood-estimation"><i class="fa fa-check"></i><b>7.3</b> Relationship with maximum likelihood estimation</a></li>
<li class="chapter" data-level="7.4" data-path="parameter-estimation.html"><a href="parameter-estimation.html#estimating-sigma2"><i class="fa fa-check"></i><b>7.4</b> Estimating <span class="math inline">\(\sigma^2\)</span></a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="matrix-notation-for-linear-models.html"><a href="matrix-notation-for-linear-models.html"><i class="fa fa-check"></i><b>8</b> Matrix notation for linear models</a>
<ul>
<li class="chapter" data-level="8.1" data-path="matrix-notation-for-linear-models.html"><a href="matrix-notation-for-linear-models.html#least-squares-estimates-in-matrix-form"><i class="fa fa-check"></i><b>8.1</b> Least squares estimates in matrix form</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="matrix-notation-for-linear-models.html"><a href="matrix-notation-for-linear-models.html#example-fitting-a-polynomial-regression-model"><i class="fa fa-check"></i><b>8.1.1</b> Example: fitting a polynomial regression model</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="matrix-notation-for-linear-models.html"><a href="matrix-notation-for-linear-models.html#estimate-of-sigma2-in-matrix-form"><i class="fa fa-check"></i><b>8.2</b> Estimate of <span class="math inline">\(\sigma^2\)</span> in matrix form</a></li>
<li class="chapter" data-level="8.3" data-path="matrix-notation-for-linear-models.html"><a href="matrix-notation-for-linear-models.html#derivation-of-hatbeta"><i class="fa fa-check"></i><b>8.3</b> Derivation of <span class="math inline">\(\hat{\beta}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="fitting-a-linear-model-in-r.html"><a href="fitting-a-linear-model-in-r.html"><i class="fa fa-check"></i><b>9</b> Fitting a linear model in R</a>
<ul>
<li class="chapter" data-level="9.1" data-path="fitting-a-linear-model-in-r.html"><a href="fitting-a-linear-model-in-r.html#plotting-a-fitted-regression-line"><i class="fa fa-check"></i><b>9.1</b> Plotting a fitted regression line</a></li>
<li class="chapter" data-level="9.2" data-path="fitting-a-linear-model-in-r.html"><a href="fitting-a-linear-model-in-r.html#the-summary-command"><i class="fa fa-check"></i><b>9.2</b> The <code>summary()</code> command</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="qualitative-independent-variables.html"><a href="qualitative-independent-variables.html"><i class="fa fa-check"></i><b>10</b> Qualitative independent variables</a>
<ul>
<li class="chapter" data-level="10.1" data-path="qualitative-independent-variables.html"><a href="qualitative-independent-variables.html#example-cancer-survival-data"><i class="fa fa-check"></i><b>10.1</b> Example: cancer survival data</a></li>
<li class="chapter" data-level="10.2" data-path="qualitative-independent-variables.html"><a href="qualitative-independent-variables.html#a-linear-model-for-the-cancer-data"><i class="fa fa-check"></i><b>10.2</b> A linear model for the cancer data</a></li>
<li class="chapter" data-level="10.3" data-path="qualitative-independent-variables.html"><a href="qualitative-independent-variables.html#notation-for-qualitative-independent-variables"><i class="fa fa-check"></i><b>10.3</b> Notation for qualitative independent variables</a></li>
<li class="chapter" data-level="10.4" data-path="qualitative-independent-variables.html"><a href="qualitative-independent-variables.html#the-one-way-anova-model-in-matrix-form"><i class="fa fa-check"></i><b>10.4</b> The one-way ANOVA model in matrix form</a></li>
<li class="chapter" data-level="10.5" data-path="qualitative-independent-variables.html"><a href="qualitative-independent-variables.html#least-squares-estimates-for-the-one-way-anova-model"><i class="fa fa-check"></i><b>10.5</b> Least squares estimates for the one-way ANOVA model</a></li>
<li class="chapter" data-level="10.6" data-path="qualitative-independent-variables.html"><a href="qualitative-independent-variables.html#fitting-a-one-way-anova-model-in-r"><i class="fa fa-check"></i><b>10.6</b> Fitting a one-way ANOVA model in R</a></li>
<li class="chapter" data-level="10.7" data-path="qualitative-independent-variables.html"><a href="qualitative-independent-variables.html#an-alternative-parameterisation"><i class="fa fa-check"></i><b>10.7</b> An alternative parameterisation</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="multiple-independent-variables.html"><a href="multiple-independent-variables.html"><i class="fa fa-check"></i><b>11</b> Multiple independent variables</a>
<ul>
<li class="chapter" data-level="11.1" data-path="multiple-independent-variables.html"><a href="multiple-independent-variables.html#causation-versus-association"><i class="fa fa-check"></i><b>11.1</b> Causation versus association</a></li>
<li class="chapter" data-level="11.2" data-path="multiple-independent-variables.html"><a href="multiple-independent-variables.html#analysis-of-covariance"><i class="fa fa-check"></i><b>11.2</b> Analysis of Covariance</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="using-linear-models-for-prediction.html"><a href="using-linear-models-for-prediction.html"><i class="fa fa-check"></i><b>12</b> Using linear models for prediction</a>
<ul>
<li class="chapter" data-level="12.1" data-path="using-linear-models-for-prediction.html"><a href="using-linear-models-for-prediction.html#obtaining-predictions-in-r"><i class="fa fa-check"></i><b>12.1</b> Obtaining predictions in R</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">MAS5052 Part 2: Likelihood and Linear Models</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="likelihood-for-confidence-intervals-and-hypothesis-tests" class="section level1 hasAnchor" number="5">
<h1><span class="header-section-number">Chapter 5</span> Likelihood for confidence intervals and hypothesis tests<a href="likelihood-for-confidence-intervals-and-hypothesis-tests.html#likelihood-for-confidence-intervals-and-hypothesis-tests" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<script type="text/javascript">
$(document).ready(function() {
  $folds = $(".fold");
  $folds.wrapInner("<div class=\"fold-blck\">"); // wrap a div container around content
  $folds.prepend("<button class=\"fold-btn\">Show solution</button>");  // add a button
  $(".fold-blck").toggle();  // fold all blocks
  $(".fold-btn").on("click", function() {  // add onClick event
    $(this).text($(this).text() === "Show solution" ? "Hide solution" : "Show solution");  // if the text equals "Fold", change it to "Unfold"or else to "Fold" 
    $(this).next(".fold-blck").toggle("linear");  // "swing" is the default easing function. This can be further customized in its speed or the overall animation itself.
  })
});
</script>
<p>Maximum likelihood estimation gives us a single value for the unknown parameters <span class="math inline">\(\boldsymbol{\theta}\)</span>, a so-called point estimate. In many settings in statistical inference we want to go further than point estimation, in particular to give some idea of the uncertainty in our point estimate. For example, where we are trying to estimate a single parameter <span class="math inline">\(\theta\)</span>, we may want to produce an interval estimate, typically a set of values <span class="math inline">\([\theta_1,\theta_2]\)</span> which we believe that the true value <span class="math inline">\(\theta\)</span> lies in. Alternatively, we may want to test a hypothesis about <span class="math inline">\(\theta\)</span>. The likelihood function can often be used to construct appropriate methods in these settings too, and as with maximum likelihood estimation it can often be shown that they are in some sense optimal.</p>
<div id="confidence-intervals" class="section level2 hasAnchor" number="5.1">
<h2><span class="header-section-number">5.1</span> Confidence intervals<a href="likelihood-for-confidence-intervals-and-hypothesis-tests.html#confidence-intervals" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We will start off by thinking about interval estimation. Assume, in the one parameter case, that we have a likelihood function <span class="math inline">\(L(\theta;\mathbf{x})\)</span> defined for <span class="math inline">\(\theta\in\Theta\)</span>, maximised at its maximum likelihood estimate <span class="math inline">\(\hat{\theta}\)</span>. Then a natural choice of interval estimate is to set some threshold, <span class="math inline">\(L_0\)</span> say, and to use the values of <span class="math inline">\(\theta\)</span> such that <span class="math inline">\(L(\theta;\mathbf{x})\geq L_0\)</span> as an interval estimate. One common choice for the threshold is to choose <span class="math inline">\(L_0\)</span> to be a fixed multiple of the maximum likelihood, say
<span class="math display">\[L_0=e^{-k}L(\hat{\theta};\mathbf{x})\]</span>
for some chosen <span class="math inline">\(k&gt;0\)</span>. Equivalently in terms of the log-likelihood, <span class="math display">\[\log L_0=\ell(\hat{\theta};\mathbf{x})-k.\]</span> Our choice of <span class="math inline">\(k\)</span> here will involve a trade off between a precise answer (meaning a narrow interval) and minimising the risk of missing the true value from the interval: a small <span class="math inline">\(k\)</span> will give a narrow interval but relatively low confidence that the interval contains the true value, while a large <span class="math inline">\(k\)</span> will give a larger interval and higher confidence.</p>
<p>More generally, we can make the following definition. The <strong><span class="math inline">\(k\)</span>-unit likelihood region</strong> for parameters <span class="math inline">\(\boldsymbol{\theta}\)</span> based on data <span class="math inline">\(\mathbf{x}\)</span> is the region
<span class="math display">\[R_k=\left\{\boldsymbol{\theta}:L(\boldsymbol{\theta};\mathbf{x})\geq e^{-k}L(\boldsymbol{\hat{\theta}};\mathbf{x})\right\},\]</span>
or equivalently
<span class="math display">\[R_k=\left\{\boldsymbol{\theta}:\ell(\boldsymbol{\theta};\mathbf{x})\geq \ell(\boldsymbol{\hat{\theta}};\mathbf{x})-k\right\},\]</span>
where <span class="math inline">\(\boldsymbol{\hat{\theta}}\)</span> is the maximum likelihood estimate of <span class="math inline">\(\boldsymbol{\theta}\)</span> based on <span class="math inline">\(\mathbf{x}\)</span>.</p>
<p>The values of <span class="math inline">\(\boldsymbol{\theta}\)</span> within the <span class="math inline">\(k\)</span>-unit likelihood region are those whose likelihood is at least within a
factor <span class="math inline">\(e^{-k}\)</span> of the maximum. For instance, points in the 1-unit region have likelihoods within a factor
<span class="math inline">\(e^{-1} = 0.368\)</span> of the maximum. The 2-unit region contains points with
likelihoods within a factor <span class="math inline">\(e^{-2} = 0.135\)</span> of the maximum. The 2-unit region is
the most commonly used in practice.</p>
<div class="example">
<p><span id="exm:exampleLikelihoodRegions" class="example"><strong>Example 5.1  (Likelihood regions.) </strong></span><br></p>
</div>
<p>Suppose that we have i.i.d. data <span class="math inline">\(\mathbf{x}=(x_1,x_2,\ldots,x_n)\)</span>, for which each data point is modelled as a random sample from <span class="math inline">\(N(\mu,\sigma^2)\)</span> where <span class="math inline">\(\mu\)</span> is unknown and <span class="math inline">\(\sigma^2\)</span> is known. Find the <span class="math inline">\(k\)</span>-likelihood region <span class="math inline">\(R_k\)</span> for the parameter <span class="math inline">\(\mu\)</span>.</p>
<div class="fold">
<p><strong>Solution</strong></p>
<p>First, we need to find the MLE <span class="math inline">\(\hat\mu\)</span> of <span class="math inline">\(\mu\)</span>. The likelihood function for our model is
<span class="math display">\[L(\mu;\mathbf{x})=\prod\limits_{i=1}^n \phi(x_i;\mu)=\frac{1}{(2\pi\sigma^2)^{n/2}}\exp\left(-\frac{1}{2\sigma^2}\sum_{i=1}^n(x_i-\mu)^2\right),\]</span>
where the range of parameter values is all <span class="math inline">\(\mu\in\mathbb{R}\)</span>. The log
likelihood is
<span class="math display">\[\ell(\mu;\mathbf{x}) = -\frac{n}{2}\left(\log(2\pi)+\log(\sigma^2)\right)-\frac{1}{2\sigma^2}\sum_{i=1}^n(x_i-\mu)^2.\]</span>
The usual process of maximisation
shows that the maximum likelihood estimator is the sample mean,
<span class="math display">\[\hat\mu=\frac{1}{n}\sum\limits_{i=1}^n x_i.\]</span></p>
<p>Now we are ready to identify the <span class="math inline">\(k\)</span>-likelihood region for <span class="math inline">\(\mu\)</span>. By definition, the <span class="math inline">\(k\)</span>-likelihood region is
<span class="math display">\[R_k=\left\{\mu\in\mathbb{R}:|l(\mu;\mathbf{x})-l(\hat{\mu};\mathbf{x})|\leq k\right\}.\]</span>
So, <span class="math inline">\(\mu\in R_k\)</span> if and only if
<span class="math display">\[\left|\frac{1}{2\sigma^2}\sum\limits_{i=1}^n(x_i-\mu)^2-\frac{1}{2\sigma^2}\sum\limits_{i=1}^n(x_i-\hat\mu)^2\right|\leq k.\]</span>
We can simplify this inequality, by noting that
<span class="math display">\[\begin{align*}
\sum\limits_{i=1}^n(x_i-\mu)^2-\sum\limits_{i=1}^n(x_i-\hat\mu)^2
&amp;=\sum\limits_{i=1}^n x_i^2-2x_i\mu+\mu^2-x_i^2+2x_i\hat\mu-\hat\mu^2\\
&amp;=n\mu^2-n\hat\mu^2+2(\hat\mu-\mu)\sum\limits_{i=1}^n x_i\\
&amp;=n\mu^2-n\hat\mu^2+2(\hat\mu-\mu)n\hat\mu\\
&amp;=n(\mu^2+\hat\mu^2-2\mu\hat\mu)\\
&amp;=n(\hat\mu-\mu)^2.
\end{align*}\]</span>
So, <span class="math inline">\(\mu\in R_k\)</span> if and only if
<span class="math display">\[\frac{n}{2\sigma^2}|\hat\mu-\mu|^2\leq k,\]</span>
or in other words, <span class="math inline">\(|\hat\mu-\mu|\leq \sigma\sqrt{\frac{2k}{n}}\)</span>, so
<span class="math display">\[R_k=\left[\hat\mu-\sigma\sqrt{\frac{2k}{n}},\;\hat\mu+\sigma\sqrt{\frac{2k}{n}}\right].\]</span></p>
</div>
<p><br></p>
</div>
<div id="hypothesis-tests" class="section level2 hasAnchor" number="5.2">
<h2><span class="header-section-number">5.2</span> Hypothesis tests<a href="likelihood-for-confidence-intervals-and-hypothesis-tests.html#hypothesis-tests" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>If we are trying to test a null hypothesis <span class="math inline">\(H_0:\theta=\theta_0\)</span> against a general alternative hypothesis <span class="math inline">\(H_1:\theta\neq \theta_0\)</span>, then we can use a similar idea: we choose a suitable <span class="math inline">\(k\)</span>, construct the <span class="math inline">\(k\)</span>-likelihood region <span class="math inline">\(R_k\)</span>, and accept <span class="math inline">\(H_0\)</span> if <span class="math inline">\(\theta_0\)</span> is inside <span class="math inline">\(R_k\)</span>, or reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(\theta\)</span> is outside <span class="math inline">\(R_k\)</span>.</p>
<div class="example">
<p><span id="exm:exampleHypothesisTests" class="example"><strong>Example 5.2  (Hypothesis tests based on likelihood.) </strong></span><br></p>
</div>
<p>In Example , if we used a <span class="math inline">\(2\)</span>-likelihood test, would we accept the hypothesis that the radioactive decay of carbon-15 is equal to <span class="math inline">\(\lambda=0.27\)</span>?</p>
<div class="fold">
<p><strong>Solution</strong></p>
<p>We had found, given the data, that the likelihood function of <span class="math inline">\(\theta\)</span> was
<span class="math display">\[L(\lambda;\mathbf{x})=\lambda^{15}e^{-47.58\lambda}\]</span>
and the maximum likelihood estimator of <span class="math inline">\(\lambda\)</span> was
<span class="math inline">\(\hat\lambda\approx 0.32.\)</span>
The <span class="math inline">\(2\)</span>-likelihood region for <span class="math inline">\(\lambda\)</span> is the set
<span class="math display">\[R_2=\left\{\lambda&gt;0:L(\lambda;\mathbf{x})\geq e^{-2}L(\hat\lambda;\mathbf{x})\right\},\]</span>
so <span class="math inline">\(\lambda\in R_2\)</span> if and only if
<span class="math display">\[\lambda^{15}e^{-47.58\lambda}\geq e^{-2}L(0.32;\mathbf{x})=1.24\times 10^{-15}.\]</span>
Note that, unlike the previous example, we can’t simplify this inequality and find a `nice’ form for the likelihood region.</p>
<p>Our hypothesis is that, in fact, <span class="math inline">\(\lambda=0.27\)</span>. Our <span class="math inline">\(2\)</span>-likelihood test will pass if <span class="math inline">\(\lambda=0.27\)</span> is within the <span class="math inline">\(2\)</span>-likelihood region, and fail if not. We can evaluate (use e.g.~),
<span class="math display">\[0.27^{15}e^{-47.58\times 0.27}\approx 7.78\times 10^{-15}\]</span>
and note that <span class="math inline">\(7.78\times 10^{-15}\geq 1.24\times 10^{-15}\)</span>. Hence <span class="math inline">\(\lambda=0.27\)</span> is within the <span class="math inline">\(2\)</span>-likelihood region and we accept the hypothesis.</p>
</div>
</div>
<div id="a-more-formal-approach" class="section level2 hasAnchor" number="5.3">
<h2><span class="header-section-number">5.3</span> A more formal approach<a href="likelihood-for-confidence-intervals-and-hypothesis-tests.html#a-more-formal-approach" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The confidence intervals and hypothesis tests described above were justified informally, in a way that is sufficient for this module. We will now briefly discuss a more formal justification.</p>
<div class="warning" latex-info="">
<p>The theory in the following sections may seem overwhelming, but don’t panic! The aim here is just to make you aware of some important results that underpin a lot of statistical theory. We will work through two simple examples, so you can get some sense of how these results can be applied.</p>
</div>
<div id="asymptotic-normality-of-the-maximum-likelihood-estimator" class="section level3 hasAnchor" number="5.3.1">
<h3><span class="header-section-number">5.3.1</span> Asymptotic normality of the maximum likelihood estimator<a href="likelihood-for-confidence-intervals-and-hypothesis-tests.html#asymptotic-normality-of-the-maximum-likelihood-estimator" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In the usual notation, we have <span class="math inline">\(n\)</span> i.i.d. random variables <span class="math inline">\(X_1,\ldots,X_n\)</span> with distribution depending on some unknown vector of parameters. <span class="math inline">\(\boldsymbol{\theta}\)</span>. Suppose we have derived an expression for the maximum likelihood estimator. Before we observe the data, we can think of the estimator as a function of <span class="math inline">\(\mathbf{X} = (X_1,\ldots,X_n)\)</span>: we denote it by <span class="math inline">\(\hat{\boldsymbol{\theta}}(\mathbf{X})\)</span>.</p>
<p>We think of <span class="math inline">\(\hat{\boldsymbol{\theta}}(\mathbf{X})\)</span> as a <em>random variable</em>, because it is a function of the random <span class="math inline">\(\mathbf{X}\)</span>. For example, for <span class="math inline">\(X_1,\ldots,X_n\stackrel{i.i.d}{\sim}N(\mu, \sigma^2)\)</span>, the maximum likelihood estimator for <span class="math inline">\(\mu\)</span> is <span class="math inline">\(\bar{X}\)</span>, and <span class="math inline">\(\bar{X}\)</span> is a random variable (normally distributed with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2/n\)</span>).</p>
<p>It can be shown that as <span class="math inline">\(n\rightarrow \infty\)</span>, the distribution of <span class="math inline">\(\hat{\boldsymbol{\theta}}(\mathbf{X})\)</span> tends to the (multivariate) normal distribution
<span class="math display">\[
\hat{\boldsymbol{\theta}}(\mathbf{X}) \sim N(\boldsymbol{\theta}, I_E(\boldsymbol{\theta})^{-1}),
\]</span>
where <span class="math inline">\(I_E(\boldsymbol{\theta})\)</span> is the <strong>Fisher Information Matrix</strong>, defined for a vector
<span class="math inline">\(\boldsymbol{\theta}=(\theta_1,\ldots,\theta_d)\)</span> by
<span class="math display">\[\begin{equation}
I_E(\boldsymbol{\theta})=\left(\begin{array}{ccc}e_{1,1}(\boldsymbol{\theta}) &amp; \cdots &amp;
e_{1,d}(\boldsymbol{\theta})\\ \vdots &amp; &amp; \vdots \\ e_{d,1}(\boldsymbol{\theta}) &amp; \cdots &amp;
e_{d,d}(\boldsymbol{\theta})
\end{array}\right),
\end{equation}\]</span>
with
<span class="math display">\[\begin{equation}
e_{i,j}(\boldsymbol{\theta})=E\left\{-\frac{\partial^2}{\partial
\theta_i\,\partial \theta_j}\ell(\boldsymbol{\theta}; \mathbf{X})\right\}.
\end{equation}\]</span></p>
<p>The expectation on the right hand side needs a little unpacking. It is the expectation of a function of the random variable <span class="math inline">\(\mathbf{X}\)</span>. This function is a partial derivative of another function: the log-likelihood function evaluated at <span class="math inline">\(\mathbf{X}\)</span> and expressed as a function of <span class="math inline">\(\boldsymbol{\theta}\)</span>.</p>
</div>
<div id="confidence-intervals-based-on-asymptotic-normality" class="section level3 hasAnchor" number="5.3.2">
<h3><span class="header-section-number">5.3.2</span> Confidence intervals based on asymptotic normality<a href="likelihood-for-confidence-intervals-and-hypothesis-tests.html#confidence-intervals-based-on-asymptotic-normality" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Now suppose we want to construct a <span class="math inline">\(100(1-\alpha)\%\)</span> confidence
interval for any particular element of <span class="math inline">\(\boldsymbol{\theta}\)</span>, say <span class="math inline">\(\theta_j\)</span>. For
suitably large <span class="math inline">\(n\)</span>, we have
<span class="math display" id="eq:MLEdist">\[\begin{equation}
\hat{\theta_j} \sim N(\theta_j,\gamma_{j,j}),\tag{5.1}
\end{equation}\]</span>
where we <span class="math inline">\(\gamma_{j,j}\)</span> is the <span class="math inline">\(\{j,j\}\)</span> element of
<span class="math inline">\(I_E(\theta)^{-1}\)</span>. This then gives us an approximate interval as
<span class="math display">\[\begin{equation}
(\hat{\theta_j}-z_{1-\frac{\alpha}{2}}\sqrt{\gamma_{j,j}},\hat{\theta_j}+
z_{1-\frac{\alpha}{2}}\sqrt{\gamma_{j,j}}),
\end{equation}\]</span>
with <span class="math inline">\(z_{1-\frac{\alpha}{2}}\)</span> the appropriate percentile from
the standard normal distribution.</p>
<p>We will not be able to calculate this interval in practice, as we do not know the true value of <span class="math inline">\(\boldsymbol{\theta}\)</span>, which we would need to evaluate <span class="math inline">\(I_E(\boldsymbol{\theta})\)</span>. Instead, we approximate <span class="math inline">\(I_E(\boldsymbol{\theta})\)</span> by the observed information matrix</p>
<p><span class="math display">\[\begin{equation}
I_O(\boldsymbol{\theta})=\left(\begin{array}{ccc}-\frac{\partial^2}{\partial
\theta_1^2}\ell(\boldsymbol{\theta}; \mathbf{x}) &amp; \cdots &amp; -\frac{\partial^2}{\partial
\theta_1\partial \theta_d}\ell(\boldsymbol{\theta}; \mathbf{x})\\ \vdots &amp; &amp; \vdots \\
-\frac{\partial^2}{\partial \theta_d\partial \theta_1}\ell(\boldsymbol{\theta}; \mathbf{x}) &amp;
\cdots &amp;-\frac{\partial^2}{\partial \theta_d^2}\ell(\boldsymbol{\theta}; \mathbf{x})
\end{array}\right),
\end{equation}\]</span>
evaluated at <span class="math inline">\(\boldsymbol{\theta}=\hat{\boldsymbol{\theta}}(\mathbf{x})\)</span>. Then denoting
<span class="math inline">\(\tilde{\gamma}_{i,j}\)</span> as the <span class="math inline">\(i,j\)</span>th element of the inverse of
<span class="math inline">\(I_O(\boldsymbol{\theta})\)</span>, we use
<span class="math display">\[\begin{equation}
(\hat{\theta_j}-z_{1-\frac{\alpha}{2}}\sqrt{\tilde{\gamma}_{j,j}},\hat{\theta_j}+
z_{1-\frac{\alpha}{2}}\sqrt{\tilde{\gamma}_{j,j}}),
\end{equation}\]</span>
as an approximate confidence interval. Since we know that
<span class="math inline">\(\hat{\theta} \rightarrow \theta\)</span> as <span class="math inline">\(n\rightarrow \infty\)</span>, with
probability 1, we would expect <span class="math inline">\(I_O(\boldsymbol{\theta})\)</span> to be similar to
<span class="math inline">\(I_E(\boldsymbol{\theta})\)</span> for large sample sizes.</p>
<div class="example">
<p><span id="exm:exampleAsymptoticCI" class="example"><strong>Example 5.3  (Asymptotic confidence interval.) </strong></span><br></p>
</div>
<p>Suppose that we have i.i.d. data <span class="math inline">\(\mathbf{x}=(x_1,x_2,\ldots,x_n)\)</span>, for which each data point is modelled as a random sample from <span class="math inline">\(N(\mu,\sigma^2)\)</span> where <span class="math inline">\(\mu\)</span> is unknown and <span class="math inline">\(\sigma^2\)</span> is known. Using asymptotic normality of the maximum likelihood estimator, derive an approximate 95% confidence interval for <span class="math inline">\(\mu\)</span>, and show that it approximately the same as the <span class="math inline">\(k=2\)</span> likelihood region found in Example <a href="likelihood-for-confidence-intervals-and-hypothesis-tests.html#exm:exampleLikelihoodRegions">5.1</a>.</p>
<div class="fold">
<p><strong>Solution</strong></p>
<p>Using equation <a href="likelihood-for-confidence-intervals-and-hypothesis-tests.html#eq:MLEdist">(5.1)</a>, our approximate distribution of the maximum likelihood estimator <span class="math inline">\(\hat{\mu}(\mathbf{X})\)</span> is
<span class="math display">\[
\hat{\mu}(\mathbf{X}) \sim N(\mu, \gamma),
\]</span>
where</p>
<p><span class="math display">\[\gamma ^{-1}=E\left(-\frac{\partial^2}{\partial\mu^2}\ell(\mu; \mathbf{X})\right).\]</span>
For the log likelihood we have
<span class="math display">\[
\ell(\mu;\mathbf{X}) =-\frac{n}{2} \log(2\pi\sigma^2)- \frac{1}{2\sigma^2}\sum_{i=1}^n(X_i - \mu)^2,
\]</span>
and so
<span class="math display">\[
-\frac{\partial^2}{\partial\mu^2}\ell(\mu; \mathbf{X}) = \frac{n}{\sigma^2}.
\]</span>
This is a constant, and so
<span class="math display">\[E\left(-\frac{\partial^2}{\partial\mu^2}\ell(\mu; \mathbf{X})\right) = \frac{n}{\sigma^2},\]</span>
hence our approximate distribution for <span class="math inline">\(\hat{\mu}(\mathbf{X})\)</span> is <span class="math inline">\(N(\mu, \sigma^2/n)\)</span> (so the approximation is actually exact in this case.) This gives us the familiar 95% confidence interval</p>
<p><span class="math display">\[
\left[\hat{u} - 1.96 \frac{\sigma}{\sqrt{n}},\quad \hat{u} + 1.96 \frac{\sigma}{\sqrt{n}}\right].  
\]</span>
We can now see that the <span class="math inline">\(k=2\)</span> likelihood region in Example <a href="likelihood-for-confidence-intervals-and-hypothesis-tests.html#exm:exampleLikelihoodRegions">5.1</a> is the same, just with 2 approximating 1.96.
</p>
</div>
</div>
<div id="the-generalised-likelihood-ratio-test" class="section level3 hasAnchor" number="5.3.3">
<h3><span class="header-section-number">5.3.3</span> The generalised likelihood ratio test<a href="likelihood-for-confidence-intervals-and-hypothesis-tests.html#the-generalised-likelihood-ratio-test" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Previously, we suggested performing a (Neyman-Pearson) hypothesis test, based on whether a <span class="math inline">\(k\)</span>-likelihood region contains the hypothesised <span class="math inline">\(\boldsymbol{\theta}_0\)</span> under <span class="math inline">\(H_0\)</span>. For <span class="math inline">\(\boldsymbol{\theta}_0\)</span> to lie outside the <span class="math inline">\(k\)</span>-likelihood region, we require
<span class="math display">\[
\ell(\hat{\boldsymbol{\theta}}; \mathbf{x}) - \ell(\boldsymbol{\theta}_0;\mathbf{x}) &gt; k,
\]</span></p>
<p>or equivalently
<span class="math display">\[
\frac{L(\hat{\boldsymbol{\theta}}; \mathbf{x})}{L(\boldsymbol{\theta}_0;\mathbf{x})} &gt; \exp(k)
\]</span></p>
<p>This is an example of a <strong>generalised likelihood ratio test</strong> (GLRT): the test is derived from considering the ratio of two likelihood functions (but note that the ratio is more commonly written the other way round).</p>
<p>We’ll now state the GLRT in a more general form. Consider hypotheses of the form
<span class="math display">\[\begin{eqnarray*}
H_0&amp;:&amp; \boldsymbol{\theta}\in\Theta_0,\\
H_1&amp;:&amp; \boldsymbol{\theta}\in\Theta_1.
\end{eqnarray*}\]</span>
The GLRT says reject <span class="math inline">\(H_0\)</span> if
<span class="math display">\[\begin{equation}
\lambda:=\frac{\sup_{\boldsymbol{\theta}\in\Theta_0}L(\boldsymbol{\theta};x)}{\sup_{\boldsymbol{\theta}\in\Theta}L(\boldsymbol{\theta};x)}&lt;k,
\end{equation}\]</span>
where <span class="math inline">\(\Theta\)</span> is the full parameter space for <span class="math inline">\(\theta\)</span> and with <span class="math inline">\(k\)</span> chosen such that
<span class="math display">\[\begin{equation}
P\left(\lambda&lt;k|H_0 \mbox{ true}\right)=\alpha.
\end{equation}\]</span></p>
<p>Note that because the null hypothesis is in composite form, the numerator involves maximising the likelihood over all <span class="math inline">\(\boldsymbol{\theta}\)</span> consistent with <span class="math inline">\(H_0\)</span>.</p>
<p>Now let <span class="math inline">\(\boldsymbol{\theta}\)</span> be a vector of parameters, and
write <span class="math inline">\(\boldsymbol{\theta}=(\boldsymbol{\theta}_r,\boldsymbol{\theta}_{-r})\)</span>, where <span class="math inline">\(\boldsymbol{\theta}_r\)</span> is a subvector
of <span class="math inline">\(r\)</span> parameters from <span class="math inline">\(\boldsymbol{\theta}\)</span>, and <span class="math inline">\(\boldsymbol{\theta}_{-r}\)</span> denotes the remaining parameters that make up <span class="math inline">\(\boldsymbol{\theta}\)</span>. Now consider a hypothesis of the form
<span class="math display">\[\begin{eqnarray*}
H_0&amp;:&amp; \boldsymbol{\theta}_r=\boldsymbol{\theta}_0,\\
H_1&amp;:&amp; \boldsymbol{\theta}_r\neq\boldsymbol{\theta}_0.
\end{eqnarray*}\]</span>
We write the GLR test statistic as
<span class="math display">\[\begin{equation}
\lambda =
\frac{L((\boldsymbol{\theta}_0,\hat{\boldsymbol{\theta}}_{-r});\mathbf{x})}{L(\hat{\boldsymbol{\theta}};\mathbf{x})},
\end{equation}\]</span>
where <span class="math inline">\(\hat{\boldsymbol{\theta}}_{-r}\)</span> is the value of <span class="math inline">\(\boldsymbol{\theta}_{-r}\)</span> that maximises the likelihood when <span class="math inline">\(\boldsymbol{\theta}_r\)</span> is fixed at <span class="math inline">\(\boldsymbol{\theta}_0\)</span>, and <span class="math inline">\(\hat{\boldsymbol{\theta}}\)</span> is the usual,
unconstrained, m.l.e. It is then possible to prove that as the
sample size tends to infinity, the distribution of <span class="math inline">\(-2\log \lambda\)</span>
tends to the <span class="math inline">\(\chi^2_r\)</span> distribution, when <span class="math inline">\(H_0\)</span> is true. Hence for
a test of size <span class="math inline">\(\alpha\)</span>, we would reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(-2\log\lambda\)</span> is
greater than the <span class="math inline">\(100(1-\alpha)\)</span> percentile of the <span class="math inline">\(\chi^2_r\)</span>
distribution.</p>
<div class="example">
<p><span id="exm:exampleGLRT" class="example"><strong>Example 5.4  (GLRT for normally distributed data, known variance.) </strong></span><br></p>
</div>
<p>Suppose that we have i.i.d. data <span class="math inline">\(\mathbf{x}=(x_1,x_2,\ldots,x_n)\)</span>, for which each data point is modelled as a random sample from <span class="math inline">\(N(\mu,\sigma^2)\)</span> where <span class="math inline">\(\mu\)</span> is unknown and <span class="math inline">\(\sigma^2\)</span> is known. To test the hypothesis <span class="math inline">\(H_0:\mu = \mu_0\)</span> against a two sided alternative, we use the test statistic
<span class="math display">\[
Z = \frac{\bar{x} - \mu_0}{\sigma/\sqrt{n}},
\]</span>
where, for a test of size 0.05, we reject if <span class="math inline">\(|Z|\)</span> is greater than the 97.5th percentile of the <span class="math inline">\(N(0,1)\)</span> distribution. Show that this is equivalent to a GLRT, using the approximate distribution for <span class="math inline">\(\lambda\)</span>.</p>
<div class="fold">
<p><strong>Solution</strong></p>
<p>Under <span class="math inline">\(H_0\)</span>, we have <span class="math inline">\(\mu\)</span> fixed at <span class="math inline">\(\mu_0\)</span>, and under <span class="math inline">\(H_A\)</span>, the likelihood is maximised at <span class="math inline">\(\mu = \bar{x}\)</span>. So we have
<span class="math display">\[
L(\mu_0; \mathbf{x}) = \frac{1}{(2\pi\sigma^2)^{n/2}}\exp\left(-\frac{1}{2\sigma^2}\sum_{i=1}^n(x_i - \mu_0)^2\right),
\]</span>
and
<span class="math display">\[
L(\hat{\mu}; \mathbf{x}) = \frac{1}{(2\pi\sigma^2)^{n/2}}\exp\left(-\frac{1}{2\sigma^2}\sum_{i=1}^n(x_i - \bar{x})^2\right),
\]</span>
and so
<span class="math display">\[
\lambda = \frac{L(\mu_0; \mathbf{x})}{L(\hat{\mu}; \mathbf{x})} = \exp\left(-\frac{1}{2\sigma^2}\left(\sum_{i=1}^n(x_i - \mu_0)^2 -\sum_{i=1}^n(x_i - \bar{x})^2 \right)\right).
\]</span></p>
<p>Hence, after a little algebra</p>
<p><span class="math display">\[
-2\log \lambda = \frac{n}{\sigma^2}(\bar{x} - \mu_0)^2.
\]</span>
Using the approximate distribution for <span class="math inline">\(\lambda\)</span>, we reject <span class="math inline">\(H_0\)</span> for a test of size 0.05 if
<span class="math display">\[-2\log \lambda &gt; \chi^2_{1;\, 0.05}\simeq 3.8415\simeq(1.96)^2 \]</span>.
This is equivalent to rejecting <span class="math inline">\(H_0\)</span> if
<span class="math display">\[
\left|\frac{\sqrt{n}}{\sigma}(\bar{x} - \mu_0) \right| &gt; Z_{0.05}.
\]</span>
Note that for <span class="math inline">\(Z\sim N(0,1)\)</span>, we have <span class="math inline">\(Z^2\sim \chi^2_1\)</span>, which helps us to understand why the tests are equivalent.</p>
</div>

</div>
</div>
</div>



            </section>

          </div>
        </div>
      </div>
<a href="maximisation-techniques.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="introducing-linear-models.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": null,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
