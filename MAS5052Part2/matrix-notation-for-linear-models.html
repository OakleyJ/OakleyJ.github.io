<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 8 Matrix notation for linear models | MAS5052 Part 2: Likelihood and Linear Models</title>
  <meta name="description" content="Lecture notes for MAS5052" />
  <meta name="generator" content="bookdown 0.27 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 8 Matrix notation for linear models | MAS5052 Part 2: Likelihood and Linear Models" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Lecture notes for MAS5052" />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 8 Matrix notation for linear models | MAS5052 Part 2: Likelihood and Linear Models" />
  
  <meta name="twitter:description" content="Lecture notes for MAS5052" />
  

<meta name="author" content="Jeremy Oakley" />


<meta name="date" content="2022-07-06" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="parameter-estimation.html"/>
<link rel="next" href="fitting-a-linear-model-in-r.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">MAS5052 Part 2</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>About these notes</a></li>
<li class="part"><span><b>I Likelihood methods</b></span></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introducing likelihood methods</a>
<ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#recap-maximising-functions"><i class="fa fa-check"></i><b>1.1</b> Recap: maximising functions</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#maximum-likelihood-estimation-a-first-example"><i class="fa fa-check"></i><b>1.2</b> Maximum likelihood estimation: a first example</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introducing-the-likelihood-function.html"><a href="introducing-the-likelihood-function.html"><i class="fa fa-check"></i><b>2</b> Introducing the likelihood function</a></li>
<li class="chapter" data-level="3" data-path="models-and-data.html"><a href="models-and-data.html"><i class="fa fa-check"></i><b>3</b> Models and data</a>
<ul>
<li class="chapter" data-level="3.1" data-path="models-and-data.html"><a href="models-and-data.html#data-notation"><i class="fa fa-check"></i><b>3.1</b> Data: notation</a></li>
<li class="chapter" data-level="3.2" data-path="models-and-data.html"><a href="models-and-data.html#models-and-parameters"><i class="fa fa-check"></i><b>3.2</b> Models and parameters</a></li>
<li class="chapter" data-level="3.3" data-path="models-and-data.html"><a href="models-and-data.html#likelihood-functions-for-i.i.d-data"><i class="fa fa-check"></i><b>3.3</b> Likelihood functions for i.i.d data</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="maximisation-techniques.html"><a href="maximisation-techniques.html"><i class="fa fa-check"></i><b>4</b> Maximisation techniques</a>
<ul>
<li class="chapter" data-level="4.1" data-path="maximisation-techniques.html"><a href="maximisation-techniques.html#log-likelihood"><i class="fa fa-check"></i><b>4.1</b> Log-likelihood</a></li>
<li class="chapter" data-level="4.2" data-path="maximisation-techniques.html"><a href="maximisation-techniques.html#discrete-parameters"><i class="fa fa-check"></i><b>4.2</b> Discrete parameters</a></li>
<li class="chapter" data-level="4.3" data-path="maximisation-techniques.html"><a href="maximisation-techniques.html#multi-parameter-problems"><i class="fa fa-check"></i><b>4.3</b> Multi-parameter problems</a></li>
<li class="chapter" data-level="4.4" data-path="maximisation-techniques.html"><a href="maximisation-techniques.html#using-a-computer"><i class="fa fa-check"></i><b>4.4</b> Using a computer</a></li>
<li class="chapter" data-level="4.5" data-path="maximisation-techniques.html"><a href="maximisation-techniques.html#a-warning-example"><i class="fa fa-check"></i><b>4.5</b> A warning example</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="likelihood-for-confidence-intervals-and-hypothesis-tests.html"><a href="likelihood-for-confidence-intervals-and-hypothesis-tests.html"><i class="fa fa-check"></i><b>5</b> Likelihood for confidence intervals and hypothesis tests</a>
<ul>
<li class="chapter" data-level="5.1" data-path="likelihood-for-confidence-intervals-and-hypothesis-tests.html"><a href="likelihood-for-confidence-intervals-and-hypothesis-tests.html#confidence-intervals"><i class="fa fa-check"></i><b>5.1</b> Confidence intervals</a></li>
<li class="chapter" data-level="5.2" data-path="likelihood-for-confidence-intervals-and-hypothesis-tests.html"><a href="likelihood-for-confidence-intervals-and-hypothesis-tests.html#hypothesis-tests"><i class="fa fa-check"></i><b>5.2</b> Hypothesis tests</a></li>
<li class="chapter" data-level="5.3" data-path="likelihood-for-confidence-intervals-and-hypothesis-tests.html"><a href="likelihood-for-confidence-intervals-and-hypothesis-tests.html#a-more-formal-approach"><i class="fa fa-check"></i><b>5.3</b> A more formal approach</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="likelihood-for-confidence-intervals-and-hypothesis-tests.html"><a href="likelihood-for-confidence-intervals-and-hypothesis-tests.html#asymptotic-normality-of-the-maximum-likelihood-estimator"><i class="fa fa-check"></i><b>5.3.1</b> Asymptotic normality of the maximum likelihood estimator</a></li>
<li class="chapter" data-level="5.3.2" data-path="likelihood-for-confidence-intervals-and-hypothesis-tests.html"><a href="likelihood-for-confidence-intervals-and-hypothesis-tests.html#confidence-intervals-based-on-asymptotic-normality"><i class="fa fa-check"></i><b>5.3.2</b> Confidence intervals based on asymptotic normality</a></li>
<li class="chapter" data-level="5.3.3" data-path="likelihood-for-confidence-intervals-and-hypothesis-tests.html"><a href="likelihood-for-confidence-intervals-and-hypothesis-tests.html#the-generalised-likelihood-ratio-test"><i class="fa fa-check"></i><b>5.3.3</b> The generalised likelihood ratio test</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Linear Models</b></span></li>
<li class="chapter" data-level="6" data-path="introducing-linear-models.html"><a href="introducing-linear-models.html"><i class="fa fa-check"></i><b>6</b> Introducing linear models</a>
<ul>
<li class="chapter" data-level="6.1" data-path="introducing-linear-models.html"><a href="introducing-linear-models.html#example-relationship-between-temperature-and-ozone"><i class="fa fa-check"></i><b>6.1</b> Example: relationship between temperature and ozone</a></li>
<li class="chapter" data-level="6.2" data-path="introducing-linear-models.html"><a href="introducing-linear-models.html#notation-and-terminology"><i class="fa fa-check"></i><b>6.2</b> Notation and terminology</a></li>
<li class="chapter" data-level="6.3" data-path="introducing-linear-models.html"><a href="introducing-linear-models.html#example-suspected-electoral-fraud"><i class="fa fa-check"></i><b>6.3</b> Example: suspected electoral fraud</a></li>
<li class="chapter" data-level="6.4" data-path="introducing-linear-models.html"><a href="introducing-linear-models.html#definition-of-a-linear-model"><i class="fa fa-check"></i><b>6.4</b> Definition of a linear model</a></li>
<li class="chapter" data-level="6.5" data-path="introducing-linear-models.html"><a href="introducing-linear-models.html#the-simple-linear-regression-model"><i class="fa fa-check"></i><b>6.5</b> The simple linear regression model</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="parameter-estimation.html"><a href="parameter-estimation.html"><i class="fa fa-check"></i><b>7</b> Parameter estimation</a>
<ul>
<li class="chapter" data-level="7.1" data-path="parameter-estimation.html"><a href="parameter-estimation.html#least-squares-estimation"><i class="fa fa-check"></i><b>7.1</b> Least squares estimation</a></li>
<li class="chapter" data-level="7.2" data-path="parameter-estimation.html"><a href="parameter-estimation.html#assumptions-for-least-squares"><i class="fa fa-check"></i><b>7.2</b> Assumptions for least squares</a></li>
<li class="chapter" data-level="7.3" data-path="parameter-estimation.html"><a href="parameter-estimation.html#relationship-with-maximum-likelihood-estimation"><i class="fa fa-check"></i><b>7.3</b> Relationship with maximum likelihood estimation</a></li>
<li class="chapter" data-level="7.4" data-path="parameter-estimation.html"><a href="parameter-estimation.html#estimating-sigma2"><i class="fa fa-check"></i><b>7.4</b> Estimating <span class="math inline">\(\sigma^2\)</span></a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="matrix-notation-for-linear-models.html"><a href="matrix-notation-for-linear-models.html"><i class="fa fa-check"></i><b>8</b> Matrix notation for linear models</a>
<ul>
<li class="chapter" data-level="8.1" data-path="matrix-notation-for-linear-models.html"><a href="matrix-notation-for-linear-models.html#least-squares-estimates-in-matrix-form"><i class="fa fa-check"></i><b>8.1</b> Least squares estimates in matrix form</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="matrix-notation-for-linear-models.html"><a href="matrix-notation-for-linear-models.html#example-fitting-a-polynomial-regression-model"><i class="fa fa-check"></i><b>8.1.1</b> Example: fitting a polynomial regression model</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="matrix-notation-for-linear-models.html"><a href="matrix-notation-for-linear-models.html#estimate-of-sigma2-in-matrix-form"><i class="fa fa-check"></i><b>8.2</b> Estimate of <span class="math inline">\(\sigma^2\)</span> in matrix form</a></li>
<li class="chapter" data-level="8.3" data-path="matrix-notation-for-linear-models.html"><a href="matrix-notation-for-linear-models.html#derivation-of-hatbeta"><i class="fa fa-check"></i><b>8.3</b> Derivation of <span class="math inline">\(\hat{\beta}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="fitting-a-linear-model-in-r.html"><a href="fitting-a-linear-model-in-r.html"><i class="fa fa-check"></i><b>9</b> Fitting a linear model in R</a>
<ul>
<li class="chapter" data-level="9.1" data-path="fitting-a-linear-model-in-r.html"><a href="fitting-a-linear-model-in-r.html#plotting-a-fitted-regression-line"><i class="fa fa-check"></i><b>9.1</b> Plotting a fitted regression line</a></li>
<li class="chapter" data-level="9.2" data-path="fitting-a-linear-model-in-r.html"><a href="fitting-a-linear-model-in-r.html#the-summary-command"><i class="fa fa-check"></i><b>9.2</b> The <code>summary()</code> command</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="qualitative-independent-variables.html"><a href="qualitative-independent-variables.html"><i class="fa fa-check"></i><b>10</b> Qualitative independent variables</a>
<ul>
<li class="chapter" data-level="10.1" data-path="qualitative-independent-variables.html"><a href="qualitative-independent-variables.html#example-cancer-survival-data"><i class="fa fa-check"></i><b>10.1</b> Example: cancer survival data</a></li>
<li class="chapter" data-level="10.2" data-path="qualitative-independent-variables.html"><a href="qualitative-independent-variables.html#a-linear-model-for-the-cancer-data"><i class="fa fa-check"></i><b>10.2</b> A linear model for the cancer data</a></li>
<li class="chapter" data-level="10.3" data-path="qualitative-independent-variables.html"><a href="qualitative-independent-variables.html#notation-for-qualitative-independent-variables"><i class="fa fa-check"></i><b>10.3</b> Notation for qualitative independent variables</a></li>
<li class="chapter" data-level="10.4" data-path="qualitative-independent-variables.html"><a href="qualitative-independent-variables.html#the-one-way-anova-model-in-matrix-form"><i class="fa fa-check"></i><b>10.4</b> The one-way ANOVA model in matrix form</a></li>
<li class="chapter" data-level="10.5" data-path="qualitative-independent-variables.html"><a href="qualitative-independent-variables.html#least-squares-estimates-for-the-one-way-anova-model"><i class="fa fa-check"></i><b>10.5</b> Least squares estimates for the one-way ANOVA model</a></li>
<li class="chapter" data-level="10.6" data-path="qualitative-independent-variables.html"><a href="qualitative-independent-variables.html#fitting-a-one-way-anova-model-in-r"><i class="fa fa-check"></i><b>10.6</b> Fitting a one-way ANOVA model in R</a></li>
<li class="chapter" data-level="10.7" data-path="qualitative-independent-variables.html"><a href="qualitative-independent-variables.html#an-alternative-parameterisation"><i class="fa fa-check"></i><b>10.7</b> An alternative parameterisation</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="multiple-independent-variables.html"><a href="multiple-independent-variables.html"><i class="fa fa-check"></i><b>11</b> Multiple independent variables</a>
<ul>
<li class="chapter" data-level="11.1" data-path="multiple-independent-variables.html"><a href="multiple-independent-variables.html#causation-versus-association"><i class="fa fa-check"></i><b>11.1</b> Causation versus association</a></li>
<li class="chapter" data-level="11.2" data-path="multiple-independent-variables.html"><a href="multiple-independent-variables.html#analysis-of-covariance"><i class="fa fa-check"></i><b>11.2</b> Analysis of Covariance</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="using-linear-models-for-prediction.html"><a href="using-linear-models-for-prediction.html"><i class="fa fa-check"></i><b>12</b> Using linear models for prediction</a>
<ul>
<li class="chapter" data-level="12.1" data-path="using-linear-models-for-prediction.html"><a href="using-linear-models-for-prediction.html#obtaining-predictions-in-r"><i class="fa fa-check"></i><b>12.1</b> Obtaining predictions in R</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">MAS5052 Part 2: Likelihood and Linear Models</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="matrix-notation-for-linear-models" class="section level1 hasAnchor" number="8">
<h1><span class="header-section-number">Chapter 8</span> Matrix notation for linear models<a href="matrix-notation-for-linear-models.html#matrix-notation-for-linear-models" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<script type="text/javascript">
$(document).ready(function() {
  $folds = $(".fold");
  $folds.wrapInner("<div class=\"fold-blck\">"); // wrap a div container around content
  $folds.prepend("<button class=\"fold-btn\">Show solution</button>");  // add a button
  $(".fold-blck").toggle();  // fold all blocks
  $(".fold-btn").on("click", function() {  // add onClick event
    $(this).text($(this).text() === "Show solution" ? "Hide solution" : "Show solution");  // if the text equals "Fold", change it to "Unfold"or else to "Fold" 
    $(this).next(".fold-blck").toggle("linear");  // "swing" is the default easing function. This can be further customized in its speed or the overall animation itself.
  })
});
</script>
<p>Any linear model can be represented using the same matrix notation. This is useful because any result or technique derived using the matrix notation can then be applied to <em>all</em> linear models.</p>
<p>To illustrate the notation consider the simple linear regression model
<span class="math display">\[
Y_i=\beta_0+\beta_1 x_i +\varepsilon_i,
\]</span>
for <span class="math inline">\(i=1,\ldots,n\)</span>. We write it in matrix form as
<span class="math display">\[
\mathbf{Y} = X\boldsymbol{\beta}+ \boldsymbol{\varepsilon},
\]</span>
with
<span class="math display">\[
\mathbf{Y}=\left(\begin{array}{c}Y_1 \\ Y_2 \\ \vdots
\\ Y_n\end{array}\right),\quad
X=\left(\begin{array}{cc}1&amp; x_1 \\ 1&amp; x_2 \\ \vdots &amp; \vdots
\\ 1&amp; x_n\end{array}\right),\quad
\boldsymbol{\beta}=\left(\begin{array}{c}\beta_0 \\
\beta_1\end{array}\right),\quad
\boldsymbol{\varepsilon}=\left(\begin{array}{c}\varepsilon_1 \\ \varepsilon_2 \\ \vdots
\\ \varepsilon_n\end{array}\right).
\]</span>
We also define <span class="math inline">\(\mathbf{Y}=(y_1,\ldots,y_n)^T\)</span> as the vector of observed dependent variables.</p>
<p>The matrix <span class="math inline">\(X\)</span> is known as the <strong>design matrix</strong> and the vector <span class="math inline">\(\boldsymbol{\beta}\)</span> is sometimes referred to as the parameter vector.</p>
<div id="least-squares-estimates-in-matrix-form" class="section level2 hasAnchor" number="8.1">
<h2><span class="header-section-number">8.1</span> Least squares estimates in matrix form<a href="matrix-notation-for-linear-models.html#least-squares-estimates-in-matrix-form" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>For any linear model in matrix form, it can be shown that the least squares estimate is given by</p>
<p><span class="math display" id="eq:betahat">\[\begin{equation}
\hat{\boldsymbol{\beta}}=(X^TX)^{-1}X^T\mathbf{y}.\tag{8.1}
\end{equation}\]</span></p>
<div class="rmdnote" latex-info="">
<p>This is (probably!) the most important formula in the theory of linear models, because we can use it to obtain parameter estimates for <em>any</em> linear model.</p>
</div>
<div id="example-fitting-a-polynomial-regression-model" class="section level3 hasAnchor" number="8.1.1">
<h3><span class="header-section-number">8.1.1</span> Example: fitting a polynomial regression model<a href="matrix-notation-for-linear-models.html#example-fitting-a-polynomial-regression-model" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Consider again the election data. We could try fitting a polynomial regression model, as the relationship between independent and dependent variables looks nonlinear:
<span class="math display">\[
Y_i=\beta_0+\beta_1 x_i +\beta_2 x_i^2 + \varepsilon_i.
\]</span>
How do we estimate <span class="math inline">\((\beta_0,\beta_1,\beta_2)\)</span>? The same argument of
choosing <span class="math inline">\((\beta_0,\beta_1,\beta_2)\)</span> to make the errors small still
holds. Following the approach in the previous chapter, we would
have to solve simultaneously the following three equations:
<span class="math display">\[ \frac{\partial}{\partial \beta_0}\sum_{i=1}^n
\varepsilon_i^2=0,\quad \frac{\partial}{\partial \beta_1}\sum_{i=1}^n
\varepsilon_i^2=0,\quad \frac{\partial}{\partial \beta_2}\sum_{i=1}^n
\varepsilon_i^2=0,
\]</span>
with <span class="math inline">\(\varepsilon_i = y_i -\beta_0-\beta_1 x_i -\beta_2 x_i^2\)</span>. However, with
matrix notation we already have the answer. We again write
the model as
<span class="math display">\[
\mathbf{Y} = X\boldsymbol{\beta} + \boldsymbol{\varepsilon},
\]</span>
now with
<span class="math display">\[
\mathbf{y}=\left(\begin{array}{c}y_1 \\ y_2 \\ \vdots
\\ y_n\end{array}\right),\quad
X=\left(\begin{array}{ccc}1&amp; x_1 &amp;x_1^2\\ 1&amp; x_2 &amp;x^2_2\\ \vdots &amp;
\vdots &amp; \vdots
\\ 1 &amp;x_n &amp; x_n^2\end{array}\right),\quad
\boldsymbol{\beta}=\left(\begin{array}{c}\beta_0 \\
\beta_1\\ \beta_2\end{array}\right),\quad
\boldsymbol{\varepsilon}=\left(\begin{array}{c}\varepsilon_1 \\ \varepsilon_2 \\ \vdots
\\ \varepsilon_n\end{array}\right).
\]</span>
As before, we have <span class="math inline">\(\sum_{i=1}^n \varepsilon_i^2 = \boldsymbol{\varepsilon}^T\boldsymbol{\varepsilon}\)</span>, and so we must solve
<span class="math display">\[
\left(\begin{array}{c}\frac{\partial}{\partial \beta_0} \boldsymbol{\varepsilon}^T\boldsymbol{\varepsilon}\\
\frac{\partial}{\partial \beta_1} \boldsymbol{\varepsilon}^T\boldsymbol{\varepsilon}
\\ \frac{\partial}{\partial \beta_2} \boldsymbol{\varepsilon}^T\boldsymbol{\varepsilon}\end{array}\right)=\left(\begin{array}{c}0
\\ 0\\ 0\end{array}\right),
\]</span>
i.e. <span class="math inline">\(\frac{\partial \boldsymbol{\varepsilon}^T\boldsymbol{\varepsilon}}{\partial \boldsymbol{\beta}} = \mathbf{0}\)</span>. Hence
our least squares estimate is
<span class="math inline">\(\hat{\boldsymbol{\beta}}=(\hat{\beta_0},\hat{\beta_1},\hat{\beta}_2)^T=(X^TX)^{-1}X^T\mathbf{y}\)</span>.
For the election data this gives
<span class="math display">\[
\hat{\boldsymbol{\beta}}=\left(\begin{array}{c}-219.0071 \\ 0.0297 \\ -2.8455\times 10^{-7} \\
\end{array}\right)
\]</span></p>
<p>The line <span class="math inline">\(y=-219.0071 + 0.0297x -(2.8455\times 10^{-7})x^2\)</span> is
drawn on the election scatter plot below.</p>
<p><img src="MAS5052-Part2_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<p>Note that the key step in obtaining parameter estimates is simply to
identify the form of the design matrix <span class="math inline">\(X\)</span>. Once we have <span class="math inline">\(X\)</span> and
<span class="math inline">\(\mathbf{y}\)</span>, we just use equation <a href="matrix-notation-for-linear-models.html#eq:betahat">(8.1)</a>.</p>
<div class="example">
<p><span id="exm:exampleLMweightsMatrix" class="example"><strong>Example 8.1  (Linear model in matrix form: weights example.) </strong></span><br></p>
</div>
Consider again Example <a href="introducing-linear-models.html#exm:exampleLMweights">6.1</a>. Represent this model in matrix notation, and verify that the least squares estimates are
<span class="math display">\[
\hat{\theta}_A = \frac{2y_1 -y_2 + y_3}{3}, \quad \hat{\theta}_B = \frac{2y_2 -y_1 + y_3}{3}.
\]</span>
<div class="fold">
<p><strong>Solution</strong></p>
<p>We have
<span class="math display">\[
\mathbf{Y} = X\boldsymbol{\beta}+ \boldsymbol{\varepsilon},
\]</span>
with
<span class="math display">\[
\mathbf{Y}=\left(\begin{array}{c}Y_1 \\ Y_2 \\ Y_3\end{array}\right),\quad
X=\left(\begin{array}{cc} 1 &amp; 0 \\ 0 &amp; 1 \\ 1 &amp; 1\end{array}\right),\quad
\boldsymbol{\beta}=\left(\begin{array}{c}\theta_A \\
\theta_B\end{array}\right),\quad
\boldsymbol{\varepsilon}=\left(\begin{array}{c}\varepsilon_1 \\ \varepsilon_2 \\ 
\\ \varepsilon_3\end{array}\right).
\]</span></p>
<p>Then the least squares estimates are obtained as
<span class="math display">\[
(X^TX)^{-1}X^T\mathbf{y},
\]</span>
with <span class="math inline">\(\mathbf{y}=(y_1,y_2,y_3)^T\)</span> the vector of observed measurements. We’ll find <span class="math inline">\((X^TX)^{-1}X^T\)</span> using R:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="matrix-notation-for-linear-models.html#cb14-1" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>,</span>
<span id="cb14-2"><a href="matrix-notation-for-linear-models.html#cb14-2" aria-hidden="true" tabindex="-1"></a>              <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>),</span>
<span id="cb14-3"><a href="matrix-notation-for-linear-models.html#cb14-3" aria-hidden="true" tabindex="-1"></a>            <span class="at">nrow =</span> <span class="dv">3</span>, <span class="at">ncol =</span> <span class="dv">2</span>)</span>
<span id="cb14-4"><a href="matrix-notation-for-linear-models.html#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X) <span class="sc">%*%</span> <span class="fu">t</span>(X)</span></code></pre></div>
<pre><code>##            [,1]       [,2]      [,3]
## [1,]  0.6666667 -0.3333333 0.3333333
## [2,] -0.3333333  0.6666667 0.3333333</code></pre>
<p>and from this, we can see that <span class="math inline">\((X^TX)^{-1}X^T\mathbf{y}\)</span> will give
<span class="math display">\[
\hat{\theta}_A = \frac{2y_1 -y_2 + y_3}{3}, \quad \hat{\theta}_B = \frac{2y_2 -y_1 + y_3}{3}.
\]</span></p>
</div>
</div>
</div>
<div id="estimate-of-sigma2-in-matrix-form" class="section level2 hasAnchor" number="8.2">
<h2><span class="header-section-number">8.2</span> Estimate of <span class="math inline">\(\sigma^2\)</span> in matrix form<a href="matrix-notation-for-linear-models.html#estimate-of-sigma2-in-matrix-form" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Using the matrix notation, the vectors of fitted values and residuals are, respectively, given by
<span class="math display">\[
\hat{\mathbf{y}}:= X\hat{\boldsymbol{\beta}}, \quad \mathbf{e}:=\mathbf{y}-\hat{\mathbf{y}},
\]</span>
We can then write the estimate of <span class="math inline">\(\sigma^2\)</span> as
<span class="math display">\[
\hat{\sigma}^2 = \frac{\mathbf{e}^T\mathbf{e}}{n-p},
\]</span>
where <span class="math inline">\(n\)</span> is the length of the vector <span class="math inline">\(\mathbf{e}\)</span> and <span class="math inline">\(p\)</span> is the length of the vector <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>.</p>
</div>
<div id="derivation-of-hatbeta" class="section level2 hasAnchor" number="8.3">
<h2><span class="header-section-number">8.3</span> Derivation of <span class="math inline">\(\hat{\beta}\)</span><a href="matrix-notation-for-linear-models.html#derivation-of-hatbeta" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This section is included for reference, but you may skip it if you wish.</p>
<p>We first give some notation and results for differentiating with respect to a vector.
Let <span class="math inline">\(\mathbf{z}\)</span> be an <span class="math inline">\(r\times 1\)</span> column vector <span class="math inline">\((z_1,\ldots,z_r)^T\)</span> and
let <span class="math inline">\(f(z_1,\ldots,z_r)\)</span> be some function of <span class="math inline">\(\mathbf{z}\)</span>. We define
<span class="math display">\[
\frac{\partial f(z_1,\ldots,z_r)}{\partial
\mathbf{z}}=\left(\begin{array}{c}\frac{\partial
f(z_1,\ldots,z_r)}{\partial z_1} \\ \vdots
\\ \frac{\partial f(z_1,\ldots,z_r)}{\partial z_r}\end{array}\right).
\]</span>
For any <span class="math inline">\(r\times 1\)</span> column vector <span class="math inline">\(\mathbf{a}=(a_1,\ldots,a_r)^T\)</span> we
have
<span class="math display">\[
\frac{\partial \mathbf{a}^T\mathbf{z}}{\partial \mathbf{z}}=\frac{\partial
(a_1z_1+\ldots + a_rz_r)}{d \mathbf{z}}=(a_1,\ldots,a_r)^T=\mathbf{a}.
\]</span>
If <span class="math inline">\(M\)</span> is a square <span class="math inline">\(r\times r\)</span> matrix then
<span class="math display">\[
\frac{\partial(\mathbf{z}^TM\mathbf{z})}{\partial \mathbf{z}}=(M+M^T)\mathbf{z}.
\]</span>
Proof: Let <span class="math inline">\(m_{ij}\)</span> represent the <span class="math inline">\(ij\)</span>th element of <span class="math inline">\(M\)</span>. Now
<span class="math inline">\((M+M^T)\mathbf{z}\)</span> is a column vector with the <span class="math inline">\(k\)</span>th element given by
<span class="math inline">\(\sum_{i=1}^r m_{ki}z_i + \sum_{i=1}^r m_{ik}z_i\)</span>. Hence we must
show that
<span class="math display">\[
\frac{\partial(\mathbf{z}^TM\mathbf{z})}{\partial z_k}=\sum_{i=1}^r m_{ki}z_i +
\sum_{i=1}^r m_{ik}z_i.
\]</span>
From the product rule <span class="math display">\[\begin{eqnarray*}
\frac{\partial(\mathbf{z}^TM\mathbf{z})}{\partial z_k}&amp;=&amp;\mathbf{z}^T \frac{\partial(
M\mathbf{z})}{\partial z_k}+\left(\frac{\partial \mathbf{z}^T}{\partial z_k}\right)M\mathbf{z}\\
&amp;=&amp;(z_1,\ldots,z_r)\left(\begin{array}{c}\frac{\partial}{\partial z_k} \sum_{i=1}^r m_{1i}z_i \\
\vdots \\ \frac{\partial}{\partial z_k} \sum_{i=1}^r m_{ri}z_i
\end{array}\right)\\&amp;&amp; + (0,\ldots,0,1,0,\ldots,0) \left(\begin{array}{c}\sum_{i=1}^r m_{1i}z_i \\
\vdots \\  \sum_{i=1}^r m_{ri}z_i
\end{array}\right),
\end{eqnarray*}\]</span>
(with <span class="math inline">\((0,\ldots,0,1,0,\ldots,0)\)</span> a vector of zeros with the <span class="math inline">\(k\)</span>th
element replaced by a 1)
<span class="math display">\[\begin{eqnarray*}
&amp;=&amp; (z_1,\ldots,z_r)\left(\begin{array}{c} m_{1k} \\
\vdots \\  m_{rk}
\end{array}\right) + (0,\ldots,0,1,0,\ldots,0) \left(\begin{array}{c}\sum_{i=1}^r m_{1i}z_i \\
\vdots \\  \sum_{i=1}^r m_{ri}z_i
\end{array}\right)\\
 &amp;=&amp; \sum_{i=1}^r m_{ik}z_i+\sum_{i=1}^r m_{ki}z_i,
\end{eqnarray*}\]</span>
as required.</p>
<p>Now, note that
<span class="math display">\[
\sum_{i=1}^n \varepsilon_i^2 = \boldsymbol{\varepsilon}^T\boldsymbol{\varepsilon} = (\mathbf{y}-X\boldsymbol{\beta})^T(\mathbf{y}-X\boldsymbol{\beta})
\]</span></p>
<p>Hence in vector notation, to minimise the sum of squared errors we must solve the equation
<span class="math display">\[\frac{\partial \boldsymbol{\varepsilon}^T\boldsymbol{\varepsilon}}{\partial \boldsymbol{\beta}} =
\mathbf{0}.\]</span>
Now
<span class="math display">\[\begin{eqnarray}
\frac{\partial \boldsymbol{\varepsilon}^T\boldsymbol{\varepsilon}}{\partial \boldsymbol{\beta}} &amp;=&amp; \frac{\partial
}{\partial \boldsymbol{\beta}} (\mathbf{y}-X\boldsymbol{\beta})^T(\mathbf{y}-X\boldsymbol{\beta})\\
&amp;=&amp; \frac{\partial }{\partial \boldsymbol{\beta}} \left(\mathbf{y}^T \mathbf{y}
-\boldsymbol{\beta}^TX^T\mathbf{y} -\mathbf{y}^TX\boldsymbol{\beta} + \boldsymbol{\beta}^T(X^T X)\boldsymbol{\beta}\right)\\
&amp;=&amp; -  X^T\mathbf{y} - (\mathbf{y}^T X)^T +\left\{(X^TX)^T+ (X^TX)\right\}\boldsymbol{\beta}\\
&amp;=&amp;-2X^T\mathbf{y} + 2(X^TX)\boldsymbol{\beta}.
\end{eqnarray}\]</span>
Finally, when <span class="math inline">\(\boldsymbol{\beta}=\hat{\boldsymbol{\beta}}\)</span>, the least squares estimator, we
have
<span class="math display">\[
\mathbf{0}=-2X^T\mathbf{y} +
2(X^TX)\hat{\boldsymbol{\beta}},\label{normalequation}
\]</span>
(sometimes referred to as the <em>normal equation</em>) which gives
us the result
<span class="math display">\[
\hat{\boldsymbol{\beta}}=(X^TX)^{-1}X^T\mathbf{y}.
\]</span></p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="parameter-estimation.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="fitting-a-linear-model-in-r.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": null,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
